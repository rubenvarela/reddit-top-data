{"kind": "Listing", "data": {"after": "t3_12bwd3o", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \n\nI know projects are not that important but I have a to fun building them and I thought maybe someone else is interested in some of mine. \n\nSo basically this is a very simple Data Lakehouse deployed in Docker containers, which uses Iceberg, Trino, Minio and a Hive Metastore. Since someone maybe directly wants to play with some data I have built an init container which creates an Iceberg table based on a parquet file in the object storage. Furthermore there is a BI Service pre configured to visualize it. \n\nI thought this project might be interesting to some of you who have only worked with traditional Data Warehouses (not that I am an expert with \"new types\" of storages) or want a more real life like storage, without paying a cloud provider, for your own Data projects. \n\nHere is the Github repo: [https://github.com/dominikhei/Local-Data-LakeHouse](https://github.com/dominikhei/Local-Data-LakeHouse)\n\nFeedback is well appreciated :)", "author_fullname": "t2_v219tksh", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Project showcase: sample Data Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12buxtq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680639043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;I know projects are not that important but I have a to fun building them and I thought maybe someone else is interested in some of mine. &lt;/p&gt;\n\n&lt;p&gt;So basically this is a very simple Data Lakehouse deployed in Docker containers, which uses Iceberg, Trino, Minio and a Hive Metastore. Since someone maybe directly wants to play with some data I have built an init container which creates an Iceberg table based on a parquet file in the object storage. Furthermore there is a BI Service pre configured to visualize it. &lt;/p&gt;\n\n&lt;p&gt;I thought this project might be interesting to some of you who have only worked with traditional Data Warehouses (not that I am an expert with &amp;quot;new types&amp;quot; of storages) or want a more real life like storage, without paying a cloud provider, for your own Data projects. &lt;/p&gt;\n\n&lt;p&gt;Here is the Github repo: &lt;a href=\"https://github.com/dominikhei/Local-Data-LakeHouse\"&gt;https://github.com/dominikhei/Local-Data-LakeHouse&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feedback is well appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?auto=webp&amp;v=enabled&amp;s=a58192c664108607ded3daf9478385a01fbd0ecc", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d0d0e069191b2d1de8fceddc2d8d67b6afdcfa6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0a1d064f39318c98ded16c5bb222f58d2359e5a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=583b60681f50e05fd927771f145bb86a48ed662e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f897b46d33f821bc4292cb42076abfee2a38226a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c36c9251e310b24d30ca2ed0c1dabe70c3421f3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=25561e443cddffff0a625175d488742cc40d37ca", "width": 1080, "height": 540}], "variants": {}, "id": "FLYBfmpfBaqdgNeA_3kK487GKlC9BiAetvGC4EzqLpo"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12buxtq", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Hand-577", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12buxtq/project_showcase_sample_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12buxtq/project_showcase_sample_data_lakehouse/", "subreddit_subscribers": 96101, "created_utc": 1680639043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "are you a python+sql monkey? are you basically a PM  and a DE as the same time? Share your experience in the comments!", "author_fullname": "t2_82dwrpz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how much business knowledge do you *really* have", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cfomc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680690419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;are you a python+sql monkey? are you basically a PM  and a DE as the same time? Share your experience in the comments!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cfomc", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Ad-8440", "discussion_type": null, "num_comments": 61, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cfomc/how_much_business_knowledge_do_you_really_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cfomc/how_much_business_knowledge_do_you_really_have/", "subreddit_subscribers": 96101, "created_utc": 1680690419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 Helpful extract and load practices for high-quality raw data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": false, "name": "t3_12cbfkr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3IbeqnonkWY6c95i99CP2I6Pi9cMfPoHL07ztkGQz-M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680677757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/towards-data-science/5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?auto=webp&amp;v=enabled&amp;s=d8f939dca3db41615898fce3d5e02edc9f722adb", "width": 1200, "height": 822}, "resolutions": [{"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa41fd85a49ef460db9ee8077a4abfd87558c2c7", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=829c74629b975010398356bf520e93eca2578c4e", "width": 216, "height": 147}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18f23ee8055d21bd26220415934f4ec6a335c900", "width": 320, "height": 219}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7badc32e3c69d46b25a0f6de158584224a17d978", "width": 640, "height": 438}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7aeea8014828d04f94f6bfafb812ba166f37355b", "width": 960, "height": 657}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be89059f76c9ca503a50ed7ad920adf5ea62a864", "width": 1080, "height": 739}], "variants": {}, "id": "IZ72cRWUnIq5zNuKmC1_z1idxm6cWGgUbAFXhn03tdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12cbfkr", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cbfkr/5_helpful_extract_and_load_practices_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/towards-data-science/5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721", "subreddit_subscribers": 96101, "created_utc": 1680677757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a question I'm hoping someone can answer.\n\nIf you have a Type 2 SCD, (customer), and you have an accumulating status fact table (application status) that references customer, and said customer change over the course of the accumulation of statuses on their application, which single customer dim id to you assign to the fact record for the application? Is it the first (so it never changes), or is it the most recent (so you're rebuilding the fact each day)?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kimball approach question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c816i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680668140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a question I&amp;#39;m hoping someone can answer.&lt;/p&gt;\n\n&lt;p&gt;If you have a Type 2 SCD, (customer), and you have an accumulating status fact table (application status) that references customer, and said customer change over the course of the accumulation of statuses on their application, which single customer dim id to you assign to the fact record for the application? Is it the first (so it never changes), or is it the most recent (so you&amp;#39;re rebuilding the fact each day)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c816i", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c816i/kimball_approach_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c816i/kimball_approach_question/", "subreddit_subscribers": 96101, "created_utc": 1680668140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nI was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.\n\nAs I've only had experience with RDBM's such as PostgreSQL or MySQL, I'd like to know some _Gotcha's_ before I'm running against a never moving wall.\n\nTherefore, what things that we commonly do in MySQL should we be considerate when switching to BigQuery?", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I know before using BigQuery, having traditional MySQL knowledge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cbbw0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680677439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.&lt;/p&gt;\n\n&lt;p&gt;As I&amp;#39;ve only had experience with RDBM&amp;#39;s such as PostgreSQL or MySQL, I&amp;#39;d like to know some &lt;em&gt;Gotcha&amp;#39;s&lt;/em&gt; before I&amp;#39;m running against a never moving wall.&lt;/p&gt;\n\n&lt;p&gt;Therefore, what things that we commonly do in MySQL should we be considerate when switching to BigQuery?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cbbw0", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/", "subreddit_subscribers": 96101, "created_utc": 1680677439.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Essentially I run a potentiostat where I get a piece of current/density data every 30 seconds. These run for a 10-day cycle so I save the file as an Excel spreadsheet from time 0s to 999990s in 30-second increments with 66668 different data points. This has been running for 3 months so I have hundreds of thousands of time-stamped data points.\n\nI need to put this info into a nice graph to collate my work but Excel constantly crashes, is very laggy and makes the fan on my laptop sound like an aeroplane is about to take off. \n\nDoes anyone have a suggestion for another programme I can use, please?", "author_fullname": "t2_o1nf1i8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Excel for large data processing, please?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cehg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680686773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Essentially I run a potentiostat where I get a piece of current/density data every 30 seconds. These run for a 10-day cycle so I save the file as an Excel spreadsheet from time 0s to 999990s in 30-second increments with 66668 different data points. This has been running for 3 months so I have hundreds of thousands of time-stamped data points.&lt;/p&gt;\n\n&lt;p&gt;I need to put this info into a nice graph to collate my work but Excel constantly crashes, is very laggy and makes the fan on my laptop sound like an aeroplane is about to take off. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a suggestion for another programme I can use, please?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cehg3", "is_robot_indexable": true, "report_reasons": null, "author": "HanSo-High", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cehg3/alternative_to_excel_for_large_data_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cehg3/alternative_to_excel_for_large_data_processing/", "subreddit_subscribers": 96101, "created_utc": 1680686773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've only mostly used SQL and Python in my previous and current job. I'm looking to learn another programming language, which of the 2 is most useful?", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to learn next, Scala or Java?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cn55x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680706849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve only mostly used SQL and Python in my previous and current job. I&amp;#39;m looking to learn another programming language, which of the 2 is most useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cn55x", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cn55x/what_to_learn_next_scala_or_java/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cn55x/what_to_learn_next_scala_or_java/", "subreddit_subscribers": 96101, "created_utc": 1680706849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello chaps,\n\nI'm in a bit of a pickle and could use your expertise. My team and I have been tasked with designing an infrastructure to support our ML Team. Here are the key requirements:\n\n**Database:** Microsoft SQL Server\n\n**Daily data volume:** \\~12 million rows (each file contains data for one day)\n\nOur objective is to accumulate data for an entire year, which will be utilised for constructing inventory predictions across our retail shops.\n\nGiven the substantial volume of data and our reliance on MS SQL Server, I would appreciate any suggestions on how to approach the creation of such a data infrastructure.\n\nOne idea that crossed my mind was to generate a snapshot of the complete data set for day one, and then proceed by gathering only the data that has changed on a daily basis. I'm curious to know if this is a viable approach.\n\nAdditionally, how might we go about concatenating all the data efficiently? And, crucially, what are the potential implications for data consistency in such a scenario?", "author_fullname": "t2_8u34pgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS SQL Server and Big Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cew5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680688195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello chaps,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in a bit of a pickle and could use your expertise. My team and I have been tasked with designing an infrastructure to support our ML Team. Here are the key requirements:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Database:&lt;/strong&gt; Microsoft SQL Server&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Daily data volume:&lt;/strong&gt; ~12 million rows (each file contains data for one day)&lt;/p&gt;\n\n&lt;p&gt;Our objective is to accumulate data for an entire year, which will be utilised for constructing inventory predictions across our retail shops.&lt;/p&gt;\n\n&lt;p&gt;Given the substantial volume of data and our reliance on MS SQL Server, I would appreciate any suggestions on how to approach the creation of such a data infrastructure.&lt;/p&gt;\n\n&lt;p&gt;One idea that crossed my mind was to generate a snapshot of the complete data set for day one, and then proceed by gathering only the data that has changed on a daily basis. I&amp;#39;m curious to know if this is a viable approach.&lt;/p&gt;\n\n&lt;p&gt;Additionally, how might we go about concatenating all the data efficiently? And, crucially, what are the potential implications for data consistency in such a scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cew5o", "is_robot_indexable": true, "report_reasons": null, "author": "Sa1kon", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cew5o/ms_sql_server_and_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cew5o/ms_sql_server_and_big_data/", "subreddit_subscribers": 96101, "created_utc": 1680688195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just took on a job that is asking me to build a data mart using data that is all currently being stored in multiple excel files that are manually updated on a weekly basis and being used as the data source for Tableau visualizations.  They are using Keboola for ETL (which I have never heard of before) and snowflake.  Can you help me with the best way to get this data into snowflake and build some sort of data mart / data warehouse that then can be used as the data source for Tableau?  I just need to guidance on best practices and what tools / process you would use.  Data is basically all operational / logistics data.  Thank you for your time.", "author_fullname": "t2_a7qtuvlq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Excel to Snowflake - Best practices ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c717a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680665616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just took on a job that is asking me to build a data mart using data that is all currently being stored in multiple excel files that are manually updated on a weekly basis and being used as the data source for Tableau visualizations.  They are using Keboola for ETL (which I have never heard of before) and snowflake.  Can you help me with the best way to get this data into snowflake and build some sort of data mart / data warehouse that then can be used as the data source for Tableau?  I just need to guidance on best practices and what tools / process you would use.  Data is basically all operational / logistics data.  Thank you for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c717a", "is_robot_indexable": true, "report_reasons": null, "author": "FaithlessnessSea7467", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c717a/excel_to_snowflake_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c717a/excel_to_snowflake_best_practices/", "subreddit_subscribers": 96101, "created_utc": 1680665616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your experiences with Debezium? What are common issues you face with it? Do you find it easy to maintain?", "author_fullname": "t2_dhmtsxhx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debezium", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bxvsz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680644986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your experiences with Debezium? What are common issues you face with it? Do you find it easy to maintain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bxvsz", "is_robot_indexable": true, "report_reasons": null, "author": "marioco__", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bxvsz/debezium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bxvsz/debezium/", "subreddit_subscribers": 96101, "created_utc": 1680644986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking at some foundational components for a new data platform. Looks like data lake + Spark is just all you need. Spark as a compute engine with it's different APIs seems extremely powerful. \n\nWhat are the drawbacks? If self hosting I guess it's what comes with self hosting anything. If using a managed service like databricks I guess it's cost? \n\nAny insights?", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark/databricks seems amazing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12ctygq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680720453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at some foundational components for a new data platform. Looks like data lake + Spark is just all you need. Spark as a compute engine with it&amp;#39;s different APIs seems extremely powerful. &lt;/p&gt;\n\n&lt;p&gt;What are the drawbacks? If self hosting I guess it&amp;#39;s what comes with self hosting anything. If using a managed service like databricks I guess it&amp;#39;s cost? &lt;/p&gt;\n\n&lt;p&gt;Any insights?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ctygq", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ctygq/sparkdatabricks_seems_amazing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ctygq/sparkdatabricks_seems_amazing/", "subreddit_subscribers": 96101, "created_utc": 1680720453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, for those of you already working with Snowflake, can you please share how you set up the environment for DEV/TEST/PROD?  Do you prefer having individual databases per use-case? How to handle the RBAC properly then? Do you separate the virtual warehouses based on the roles?\nAlso what naming convention did you established for all of it?\nAny suggestions will be welcomed because we are trying to set up our new Snowflake account properly.\nThank you very much!", "author_fullname": "t2_8ouamdf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake environment architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ck24z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680700604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, for those of you already working with Snowflake, can you please share how you set up the environment for DEV/TEST/PROD?  Do you prefer having individual databases per use-case? How to handle the RBAC properly then? Do you separate the virtual warehouses based on the roles?\nAlso what naming convention did you established for all of it?\nAny suggestions will be welcomed because we are trying to set up our new Snowflake account properly.\nThank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ck24z", "is_robot_indexable": true, "report_reasons": null, "author": "adaptrix", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ck24z/snowflake_environment_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ck24z/snowflake_environment_architecture/", "subreddit_subscribers": 96101, "created_utc": 1680700604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Originating from this [SO Answer](https://stackoverflow.com/a/48177344/3673659) lead me down to read BigQuery Quota and Limitations page.\n\nMore specifically, running queries against a table has [a limit of 1500](https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements) operations per day.\n\nBut then, the [DML statements per day is unlimited](https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements), quoting:\n&gt; DML statements count toward the number of table modifications per day (or the number of partitioned table modifications per day for partitioned tables). However, the number of DML statements your project can run per day is unlimited and is not constrained by the table modifications per day quota\n\nHow should I understand these limitations?", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I understand query limitations in BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cicp2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680696840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Originating from this &lt;a href=\"https://stackoverflow.com/a/48177344/3673659\"&gt;SO Answer&lt;/a&gt; lead me down to read BigQuery Quota and Limitations page.&lt;/p&gt;\n\n&lt;p&gt;More specifically, running queries against a table has &lt;a href=\"https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements\"&gt;a limit of 1500&lt;/a&gt; operations per day.&lt;/p&gt;\n\n&lt;p&gt;But then, the &lt;a href=\"https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements\"&gt;DML statements per day is unlimited&lt;/a&gt;, quoting:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;DML statements count toward the number of table modifications per day (or the number of partitioned table modifications per day for partitioned tables). However, the number of DML statements your project can run per day is unlimited and is not constrained by the table modifications per day quota&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;How should I understand these limitations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cicp2", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/", "subreddit_subscribers": 96101, "created_utc": 1680696840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Airflow noob and looking for critiques.  \n\nHave a requirement to send data in csv format daily to a handful of clients. In airflow, I have a 'modules' folder containing my custom functions/methods. A 'dag' folder for dags and 'extracts' folder containing the extract scripts. There's one extract script per client and it contains a function to query  our db and send to destination.  The dag i set it up with one @task per client, where it imports the extract script function and executes.  Any issues with that so far?  \n   \n\nI use one script per client because it's only a handful of clients and it'll be easier to quickly adjust to client requirements for now. What would I change or do, to the airflow/python structure, if I needed to scale to like 50 clients?  The extract scripts are generally the same where you import custom modules -&gt; set up connection to db and client destination-&gt; process queries and land in destination.  The queries are the same inasmuch I just have to alter the schema/filters between clients/client_ids.  \n\nIt's fine for now, but struggling to see how I would scale this up quickly while being quick to adjust. Would you loop through a client list in 1 extract script and adjust destination/query settings dynamically? Idk!  \n\n\n\nMuch appreciated all!", "author_fullname": "t2_szv0ygic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctdtc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680719277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Airflow noob and looking for critiques.  &lt;/p&gt;\n\n&lt;p&gt;Have a requirement to send data in csv format daily to a handful of clients. In airflow, I have a &amp;#39;modules&amp;#39; folder containing my custom functions/methods. A &amp;#39;dag&amp;#39; folder for dags and &amp;#39;extracts&amp;#39; folder containing the extract scripts. There&amp;#39;s one extract script per client and it contains a function to query  our db and send to destination.  The dag i set it up with one @task per client, where it imports the extract script function and executes.  Any issues with that so far?  &lt;/p&gt;\n\n&lt;p&gt;I use one script per client because it&amp;#39;s only a handful of clients and it&amp;#39;ll be easier to quickly adjust to client requirements for now. What would I change or do, to the airflow/python structure, if I needed to scale to like 50 clients?  The extract scripts are generally the same where you import custom modules -&amp;gt; set up connection to db and client destination-&amp;gt; process queries and land in destination.  The queries are the same inasmuch I just have to alter the schema/filters between clients/client_ids.  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s fine for now, but struggling to see how I would scale this up quickly while being quick to adjust. Would you loop through a client list in 1 extract script and adjust destination/query settings dynamically? Idk!  &lt;/p&gt;\n\n&lt;p&gt;Much appreciated all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ctdtc", "is_robot_indexable": true, "report_reasons": null, "author": "Hippodick666420", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ctdtc/airflow_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ctdtc/airflow_architecture/", "subreddit_subscribers": 96101, "created_utc": 1680719277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious on the thoughts here.\n\nIf you were going to create an abstraction layer over your data warehouse to allow transactional systems access, would you be going with FastAPI or [Cube.dev](https://Cube.dev), or something else?\n\nMy first thought here was FastAPI, which is an API after all. It's exactly what I'm looking for. Create the API, then have my transactional systems able to connect via the API and access the data that exists in the warehouse.\n\nHowever [Cube.dev](https://Cube.dev) has both Rest and Graph API access, as well as a SQL style API. This seems like a better option then. I get the Rest API for my transactional systems, and the SQL style API for my reporting and analytics tools.\n\nHave I missed something?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cube.dev vs. FastAPI for an abstraction layer over a data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cfz6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680691195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious on the thoughts here.&lt;/p&gt;\n\n&lt;p&gt;If you were going to create an abstraction layer over your data warehouse to allow transactional systems access, would you be going with FastAPI or &lt;a href=\"https://Cube.dev\"&gt;Cube.dev&lt;/a&gt;, or something else?&lt;/p&gt;\n\n&lt;p&gt;My first thought here was FastAPI, which is an API after all. It&amp;#39;s exactly what I&amp;#39;m looking for. Create the API, then have my transactional systems able to connect via the API and access the data that exists in the warehouse.&lt;/p&gt;\n\n&lt;p&gt;However &lt;a href=\"https://Cube.dev\"&gt;Cube.dev&lt;/a&gt; has both Rest and Graph API access, as well as a SQL style API. This seems like a better option then. I get the Rest API for my transactional systems, and the SQL style API for my reporting and analytics tools.&lt;/p&gt;\n\n&lt;p&gt;Have I missed something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?auto=webp&amp;v=enabled&amp;s=4c2ee9ced32cf7f44c9acfadaf0fc6138d934235", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fc2899acfe1fd7fec2ad6a9c6a16ed630cc31d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb8a7f81c9f0c3327863c615505b600bdd32ead4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0747c7e295fa50f4a169ff2c77b4e38dfe3c70c5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ddb5bc88a05e68b02981b71d6f63b8130ecb7a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd7a3df58c7294394cdfee68c359fedcde4abc6f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=450ad0a42363c74fd12f5edf265b485734b70be3", "width": 1080, "height": 567}], "variants": {}, "id": "CYFlWqFefFx0WAlgFZvtSzIVYhX58H2hKywSvmvXXxw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cfz6t", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cfz6t/cubedev_vs_fastapi_for_an_abstraction_layer_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cfz6t/cubedev_vs_fastapi_for_an_abstraction_layer_over/", "subreddit_subscribers": 96101, "created_utc": 1680691195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to transfer all new ANR events into Redis DB. Is it possibile to trigger Google CF for each new Firebase ANR event?", "author_fullname": "t2_j32k9s23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google CF triggered by Firebase ANR events", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cdal8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680699432.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680683216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to transfer all new ANR events into Redis DB. Is it possibile to trigger Google CF for each new Firebase ANR event?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cdal8", "is_robot_indexable": true, "report_reasons": null, "author": "Away_Efficiency_5837", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cdal8/google_cf_triggered_by_firebase_anr_events/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cdal8/google_cf_triggered_by_firebase_anr_events/", "subreddit_subscribers": 96101, "created_utc": 1680683216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've always had this question: when a company is listed, we usually receive base salary, bonus, and stocks as part of the offer. However, what happens if the company is not listed? What will the company offer instead of stocks?\n\nAlso, as a data engineer with 6 years of experience based in Illinois, l'm currently making around $120K per year with a 10% bonus. My position is remote though, can practically live anywhere in the states. Do you think this is reasonable compensation, or am I being underpaid? I'm just looking for a ballpark figure to discuss with my manager during my next performance review. As a newcomer to the country, l'm not sure how these discussions go here.\n\nI know one pointer though, a senior DE was hired in my company 2 years ago on $150K base (remote)", "author_fullname": "t2_tacjangv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Negotiating Compensation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c2b2a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680654529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve always had this question: when a company is listed, we usually receive base salary, bonus, and stocks as part of the offer. However, what happens if the company is not listed? What will the company offer instead of stocks?&lt;/p&gt;\n\n&lt;p&gt;Also, as a data engineer with 6 years of experience based in Illinois, l&amp;#39;m currently making around $120K per year with a 10% bonus. My position is remote though, can practically live anywhere in the states. Do you think this is reasonable compensation, or am I being underpaid? I&amp;#39;m just looking for a ballpark figure to discuss with my manager during my next performance review. As a newcomer to the country, l&amp;#39;m not sure how these discussions go here.&lt;/p&gt;\n\n&lt;p&gt;I know one pointer though, a senior DE was hired in my company 2 years ago on $150K base (remote)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12c2b2a", "is_robot_indexable": true, "report_reasons": null, "author": "reflectico", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c2b2a/negotiating_compensation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c2b2a/negotiating_compensation/", "subreddit_subscribers": 96101, "created_utc": 1680654529.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all-\n\nAm currently configuring an EMR cluster. I\u2019m curious how others have approached securing the credentials inside the hive-site.xml file.\n\nI don\u2019t want users to be able to access the credentials inside, but they\u2019ll need to be able to use the file/credentials inside to access hive and run hive queries etc as emr steps.\n\nIve tried jceks- but unfortunately a user can still decrypt whats inside and print things out plaintext", "author_fullname": "t2_zfaau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "hive-site.xml file credentials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c0cye", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680650208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all-&lt;/p&gt;\n\n&lt;p&gt;Am currently configuring an EMR cluster. I\u2019m curious how others have approached securing the credentials inside the hive-site.xml file.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t want users to be able to access the credentials inside, but they\u2019ll need to be able to use the file/credentials inside to access hive and run hive queries etc as emr steps.&lt;/p&gt;\n\n&lt;p&gt;Ive tried jceks- but unfortunately a user can still decrypt whats inside and print things out plaintext&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c0cye", "is_robot_indexable": true, "report_reasons": null, "author": "skrt123", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c0cye/hivesitexml_file_credentials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c0cye/hivesitexml_file_credentials/", "subreddit_subscribers": 96101, "created_utc": 1680650208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently using Redshift and implementing dbt as a transformation framework\u2014 I'm interested in the best way to create a \"development\" structure for code to be tested before being merged into production. \\[This\\]([https://www.datafold.com/blog/how-to-setup-dbt-development-environments](https://www.datafold.com/blog/how-to-setup-dbt-development-environments)) is really the only resource I've found.\n\nIt seems like recommendations are either\n\n* Create a development schema per user\u2014 this seems like it could get out of hand. \n* Create a development database\u2014 not sure how to do this without fully replicating all data that currently exists. Snowflake has zero-copy clones, but there's no Redshift equivalent to my knowledge.\n* Some combination of the above.\n\nI'd love to hear what's worked and what hasn't... And if there's a Redshift/S3 specific way to make this simple and functional.", "author_fullname": "t2_82q10l6i3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to create a development \"environment\" in Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12cvave", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680723255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently using Redshift and implementing dbt as a transformation framework\u2014 I&amp;#39;m interested in the best way to create a &amp;quot;development&amp;quot; structure for code to be tested before being merged into production. [This](&lt;a href=\"https://www.datafold.com/blog/how-to-setup-dbt-development-environments\"&gt;https://www.datafold.com/blog/how-to-setup-dbt-development-environments&lt;/a&gt;) is really the only resource I&amp;#39;ve found.&lt;/p&gt;\n\n&lt;p&gt;It seems like recommendations are either&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Create a development schema per user\u2014 this seems like it could get out of hand. &lt;/li&gt;\n&lt;li&gt;Create a development database\u2014 not sure how to do this without fully replicating all data that currently exists. Snowflake has zero-copy clones, but there&amp;#39;s no Redshift equivalent to my knowledge.&lt;/li&gt;\n&lt;li&gt;Some combination of the above.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear what&amp;#39;s worked and what hasn&amp;#39;t... And if there&amp;#39;s a Redshift/S3 specific way to make this simple and functional.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?auto=webp&amp;v=enabled&amp;s=6dcb69ffc7979d6f51a12b849b4b98abd4770fed", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e6c36f1745e23372f7aedf17dc37a5e29996093", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18f3a85da7997995d953c673ad784930f0d54c66", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a3ebbbf63daff5a2e1b0105765fb822f2ce2961", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cbd9a6dfee840ec4cff0f293c520603bf2a8e8af", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7357cccdb8e7c8ce495705e7157350775d26ba2c", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f49e3c90b978c5d24b63002846b68f86172cc1d", "width": 1080, "height": 567}], "variants": {}, "id": "cn1-9kKU3UI8ewdlfXJ1WUhQfwpOBR1qCe_i6bOWB4s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cvave", "is_robot_indexable": true, "report_reasons": null, "author": "day-tuh-day", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cvave/best_way_to_create_a_development_environment_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cvave/best_way_to_create_a_development_environment_in/", "subreddit_subscribers": 96101, "created_utc": 1680723255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you do dynamic tasks in Airflow? I know there is this concept of dynamic tasks mapping but is this really production ready? \n\nWhat I need is for example to run one task that grabs some file from S3 and based on this file create several tasks (and subsequent tasks as well, so not only 1 task). What we do now is we output the content of the file from step 1 into Airflow variable and then, based on the Variable generate the tasks in for loop. I do not know if this is a good practice? Does not really seem to me so. \n\nAlso, when there are tasks xyz in dag run A, then in the next dag run B there could be different tasks so the tasks from previous run will disappear \u00af\\_(\u30c4)_/\u00af. Is there any elegant solution to this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow dynamic tasks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12ctyar", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680720443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you do dynamic tasks in Airflow? I know there is this concept of dynamic tasks mapping but is this really production ready? &lt;/p&gt;\n\n&lt;p&gt;What I need is for example to run one task that grabs some file from S3 and based on this file create several tasks (and subsequent tasks as well, so not only 1 task). What we do now is we output the content of the file from step 1 into Airflow variable and then, based on the Variable generate the tasks in for loop. I do not know if this is a good practice? Does not really seem to me so. &lt;/p&gt;\n\n&lt;p&gt;Also, when there are tasks xyz in dag run A, then in the next dag run B there could be different tasks so the tasks from previous run will disappear \u00af_(\u30c4)_/\u00af. Is there any elegant solution to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ctyar", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ctyar/airflow_dynamic_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ctyar/airflow_dynamic_tasks/", "subreddit_subscribers": 96101, "created_utc": 1680720443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI may be over thinking this, I've done a POC in Synapse Dedicated Pool and I have a question about hash distribution.\n\nSay I have two tables:\n\nOrder Fact\n- 100 million rows\n- hash distributed on OrderID\n\nCustomer Dimension\n- 1 million rows\n- hash distributed on Account No\n- has an identity column as the surrogate key\n\nI can join the fact to the dimension using the surrogate key.\n\nMy question is, am I correct to hash distribute the dimension on the Account No?  Or should I add in a couple of steps to allow me to hash distribute on the surrogate key?  Will this make the join faster, even though the table distribution would be the same?  What would you recommend as best practice?\n\nTIA \ud83d\ude42", "author_fullname": "t2_9h6gf9pp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to: Surrogate Key as Hash Distribution Key", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ct6ux", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680718877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I may be over thinking this, I&amp;#39;ve done a POC in Synapse Dedicated Pool and I have a question about hash distribution.&lt;/p&gt;\n\n&lt;p&gt;Say I have two tables:&lt;/p&gt;\n\n&lt;p&gt;Order Fact\n- 100 million rows\n- hash distributed on OrderID&lt;/p&gt;\n\n&lt;p&gt;Customer Dimension\n- 1 million rows\n- hash distributed on Account No\n- has an identity column as the surrogate key&lt;/p&gt;\n\n&lt;p&gt;I can join the fact to the dimension using the surrogate key.&lt;/p&gt;\n\n&lt;p&gt;My question is, am I correct to hash distribute the dimension on the Account No?  Or should I add in a couple of steps to allow me to hash distribute on the surrogate key?  Will this make the join faster, even though the table distribution would be the same?  What would you recommend as best practice?&lt;/p&gt;\n\n&lt;p&gt;TIA \ud83d\ude42&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ct6ux", "is_robot_indexable": true, "report_reasons": null, "author": "V10Matt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ct6ux/how_to_surrogate_key_as_hash_distribution_key/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ct6ux/how_to_surrogate_key_as_hash_distribution_key/", "subreddit_subscribers": 96101, "created_utc": 1680718877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to get the columns on which a spark transformation depends without parsing the SQL query. Is there a way to get it from the explain plan or some other method directly in Spark?", "author_fullname": "t2_3yaxcuy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resolve column dependencies in Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cr9tx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680714953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to get the columns on which a spark transformation depends without parsing the SQL query. Is there a way to get it from the explain plan or some other method directly in Spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cr9tx", "is_robot_indexable": true, "report_reasons": null, "author": "midasadim", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cr9tx/resolve_column_dependencies_in_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cr9tx/resolve_column_dependencies_in_spark/", "subreddit_subscribers": 96101, "created_utc": 1680714953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a Glue PySpark job that reads json data from a source S3 bucket with bookmarking enabled, and writes as parquet to a target S3 bucket. I\u2019d like to ensure that the target bucket never contain duplicate records, what\u2019s the best way to achieve that? \nMy json records have a \u201cmessageid\u201d field that\u2019s unique.", "author_fullname": "t2_asncnre1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ensure unique records in S3 bucket populated by Glue PySpark job with bookmarking enabled", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cp9qp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680711057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a Glue PySpark job that reads json data from a source S3 bucket with bookmarking enabled, and writes as parquet to a target S3 bucket. I\u2019d like to ensure that the target bucket never contain duplicate records, what\u2019s the best way to achieve that? \nMy json records have a \u201cmessageid\u201d field that\u2019s unique.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cp9qp", "is_robot_indexable": true, "report_reasons": null, "author": "Jaded_Peanut_2777", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cp9qp/ensure_unique_records_in_s3_bucket_populated_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cp9qp/ensure_unique_records_in_s3_bucket_populated_by/", "subreddit_subscribers": 96101, "created_utc": 1680711057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 Things Our Software Engineers Love About Data Contracts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_12ciisx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1q_KydmJOqiHdjMmblb5E_Z4CEs4OwfLsWEzVVbbuF8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680697235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/gocardless-tech/3-things-our-software-engineers-love-about-data-contracts-3106e1f1602d", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?auto=webp&amp;v=enabled&amp;s=775816d43446db26c90c716581fe3dc597b94762", "width": 961, "height": 676}, "resolutions": [{"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=501db2616d9a5bf721a2471733594f526348c76d", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c1d099b3ef8f6fa13582aa6c3fdf003c9441ddf", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44d5ec524f2ccf0aef1a0b62b6d0dcbc7332ca78", "width": 320, "height": 225}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c713aed6e548975e5069d71bc8ef76967d924e3", "width": 640, "height": 450}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ad2225a3e4219d9536d09077f09910dae6d3d9f", "width": 960, "height": 675}], "variants": {}, "id": "ZhcsTlN1rykHbZmNeR6oO3OyyU2oRrqM49TZJBj2nm8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ciisx", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ciisx/3_things_our_software_engineers_love_about_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/gocardless-tech/3-things-our-software-engineers-love-about-data-contracts-3106e1f1602d", "subreddit_subscribers": 96101, "created_utc": 1680697235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks - This seems like it should be pretty straightforward but I can't seem to find an API or library to help me with this. I want to use Python be able to find out when a new iOS or Android version is released and then I (already have a function for this) write it somewhere. In terms of the automation etc. this will just be integrated to a regularly scheduled ADF job nothing terribly fancy. Is this something that currently exists or at all possible? Would really appreciate any help on this.", "author_fullname": "t2_8fuwkii9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Programmatically Retrieving Android and iOS Versions in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bwd3o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680641950.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks - This seems like it should be pretty straightforward but I can&amp;#39;t seem to find an API or library to help me with this. I want to use Python be able to find out when a new iOS or Android version is released and then I (already have a function for this) write it somewhere. In terms of the automation etc. this will just be integrated to a regularly scheduled ADF job nothing terribly fancy. Is this something that currently exists or at all possible? Would really appreciate any help on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bwd3o", "is_robot_indexable": true, "report_reasons": null, "author": "LiquidSynopsis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12bwd3o/programmatically_retrieving_android_and_ios/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bwd3o/programmatically_retrieving_android_and_ios/", "subreddit_subscribers": 96101, "created_utc": 1680641950.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}