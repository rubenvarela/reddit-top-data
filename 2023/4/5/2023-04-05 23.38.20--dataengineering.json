{"kind": "Listing", "data": {"after": "t3_12ciisx", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "are you a python+sql monkey? are you basically a PM  and a DE as the same time? Share your experience in the comments!", "author_fullname": "t2_82dwrpz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how much business knowledge do you *really* have", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cfomc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680690419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;are you a python+sql monkey? are you basically a PM  and a DE as the same time? Share your experience in the comments!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cfomc", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Ad-8440", "discussion_type": null, "num_comments": 70, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cfomc/how_much_business_knowledge_do_you_really_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cfomc/how_much_business_knowledge_do_you_really_have/", "subreddit_subscribers": 96138, "created_utc": 1680690419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 Helpful extract and load practices for high-quality raw data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": false, "name": "t3_12cbfkr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3IbeqnonkWY6c95i99CP2I6Pi9cMfPoHL07ztkGQz-M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680677757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/towards-data-science/5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?auto=webp&amp;v=enabled&amp;s=d8f939dca3db41615898fce3d5e02edc9f722adb", "width": 1200, "height": 822}, "resolutions": [{"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa41fd85a49ef460db9ee8077a4abfd87558c2c7", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=829c74629b975010398356bf520e93eca2578c4e", "width": 216, "height": 147}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18f23ee8055d21bd26220415934f4ec6a335c900", "width": 320, "height": 219}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7badc32e3c69d46b25a0f6de158584224a17d978", "width": 640, "height": 438}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7aeea8014828d04f94f6bfafb812ba166f37355b", "width": 960, "height": 657}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be89059f76c9ca503a50ed7ad920adf5ea62a864", "width": 1080, "height": 739}], "variants": {}, "id": "IZ72cRWUnIq5zNuKmC1_z1idxm6cWGgUbAFXhn03tdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12cbfkr", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cbfkr/5_helpful_extract_and_load_practices_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/towards-data-science/5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721", "subreddit_subscribers": 96138, "created_utc": 1680677757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a question I'm hoping someone can answer.\n\nIf you have a Type 2 SCD, (customer), and you have an accumulating status fact table (application status) that references customer, and said customer change over the course of the accumulation of statuses on their application, which single customer dim id to you assign to the fact record for the application? Is it the first (so it never changes), or is it the most recent (so you're rebuilding the fact each day)?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kimball approach question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c816i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680668140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a question I&amp;#39;m hoping someone can answer.&lt;/p&gt;\n\n&lt;p&gt;If you have a Type 2 SCD, (customer), and you have an accumulating status fact table (application status) that references customer, and said customer change over the course of the accumulation of statuses on their application, which single customer dim id to you assign to the fact record for the application? Is it the first (so it never changes), or is it the most recent (so you&amp;#39;re rebuilding the fact each day)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c816i", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c816i/kimball_approach_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c816i/kimball_approach_question/", "subreddit_subscribers": 96138, "created_utc": 1680668140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Essentially I run a potentiostat where I get a piece of current/density data every 30 seconds. These run for a 10-day cycle so I save the file as an Excel spreadsheet from time 0s to 999990s in 30-second increments with 66668 different data points. This has been running for 3 months so I have hundreds of thousands of time-stamped data points.\n\nI need to put this info into a nice graph to collate my work but Excel constantly crashes, is very laggy and makes the fan on my laptop sound like an aeroplane is about to take off. \n\nDoes anyone have a suggestion for another programme I can use, please?", "author_fullname": "t2_o1nf1i8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Excel for large data processing, please?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cehg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680686773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Essentially I run a potentiostat where I get a piece of current/density data every 30 seconds. These run for a 10-day cycle so I save the file as an Excel spreadsheet from time 0s to 999990s in 30-second increments with 66668 different data points. This has been running for 3 months so I have hundreds of thousands of time-stamped data points.&lt;/p&gt;\n\n&lt;p&gt;I need to put this info into a nice graph to collate my work but Excel constantly crashes, is very laggy and makes the fan on my laptop sound like an aeroplane is about to take off. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a suggestion for another programme I can use, please?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cehg3", "is_robot_indexable": true, "report_reasons": null, "author": "HanSo-High", "discussion_type": null, "num_comments": 75, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cehg3/alternative_to_excel_for_large_data_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cehg3/alternative_to_excel_for_large_data_processing/", "subreddit_subscribers": 96138, "created_utc": 1680686773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking at some foundational components for a new data platform. Looks like data lake + Spark is just all you need. Spark as a compute engine with it's different APIs seems extremely powerful. \n\nWhat are the drawbacks? If self hosting I guess it's what comes with self hosting anything. If using a managed service like databricks I guess it's cost? \n\nAny insights?", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark/databricks seems amazing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctygq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680720453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at some foundational components for a new data platform. Looks like data lake + Spark is just all you need. Spark as a compute engine with it&amp;#39;s different APIs seems extremely powerful. &lt;/p&gt;\n\n&lt;p&gt;What are the drawbacks? If self hosting I guess it&amp;#39;s what comes with self hosting anything. If using a managed service like databricks I guess it&amp;#39;s cost? &lt;/p&gt;\n\n&lt;p&gt;Any insights?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ctygq", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ctygq/sparkdatabricks_seems_amazing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ctygq/sparkdatabricks_seems_amazing/", "subreddit_subscribers": 96138, "created_utc": 1680720453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nI was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.\n\nAs I've only had experience with RDBM's such as PostgreSQL or MySQL, I'd like to know some _Gotcha's_ before I'm running against a never moving wall.\n\nTherefore, what things that we commonly do in MySQL should we be considerate when switching to BigQuery?", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I know before using BigQuery, having traditional MySQL knowledge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cbbw0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680677439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.&lt;/p&gt;\n\n&lt;p&gt;As I&amp;#39;ve only had experience with RDBM&amp;#39;s such as PostgreSQL or MySQL, I&amp;#39;d like to know some &lt;em&gt;Gotcha&amp;#39;s&lt;/em&gt; before I&amp;#39;m running against a never moving wall.&lt;/p&gt;\n\n&lt;p&gt;Therefore, what things that we commonly do in MySQL should we be considerate when switching to BigQuery?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cbbw0", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/", "subreddit_subscribers": 96138, "created_utc": 1680677439.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've only mostly used SQL and Python in my previous and current job. I'm looking to learn another programming language, which of the 2 is most useful?", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to learn next, Scala or Java?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cn55x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680706849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve only mostly used SQL and Python in my previous and current job. I&amp;#39;m looking to learn another programming language, which of the 2 is most useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cn55x", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cn55x/what_to_learn_next_scala_or_java/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cn55x/what_to_learn_next_scala_or_java/", "subreddit_subscribers": 96138, "created_utc": 1680706849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello chaps,\n\nI'm in a bit of a pickle and could use your expertise. My team and I have been tasked with designing an infrastructure to support our ML Team. Here are the key requirements:\n\n**Database:** Microsoft SQL Server\n\n**Daily data volume:** \\~12 million rows (each file contains data for one day)\n\nOur objective is to accumulate data for an entire year, which will be utilised for constructing inventory predictions across our retail shops.\n\nGiven the substantial volume of data and our reliance on MS SQL Server, I would appreciate any suggestions on how to approach the creation of such a data infrastructure.\n\nOne idea that crossed my mind was to generate a snapshot of the complete data set for day one, and then proceed by gathering only the data that has changed on a daily basis. I'm curious to know if this is a viable approach.\n\nAdditionally, how might we go about concatenating all the data efficiently? And, crucially, what are the potential implications for data consistency in such a scenario?", "author_fullname": "t2_8u34pgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS SQL Server and Big Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cew5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680688195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello chaps,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in a bit of a pickle and could use your expertise. My team and I have been tasked with designing an infrastructure to support our ML Team. Here are the key requirements:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Database:&lt;/strong&gt; Microsoft SQL Server&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Daily data volume:&lt;/strong&gt; ~12 million rows (each file contains data for one day)&lt;/p&gt;\n\n&lt;p&gt;Our objective is to accumulate data for an entire year, which will be utilised for constructing inventory predictions across our retail shops.&lt;/p&gt;\n\n&lt;p&gt;Given the substantial volume of data and our reliance on MS SQL Server, I would appreciate any suggestions on how to approach the creation of such a data infrastructure.&lt;/p&gt;\n\n&lt;p&gt;One idea that crossed my mind was to generate a snapshot of the complete data set for day one, and then proceed by gathering only the data that has changed on a daily basis. I&amp;#39;m curious to know if this is a viable approach.&lt;/p&gt;\n\n&lt;p&gt;Additionally, how might we go about concatenating all the data efficiently? And, crucially, what are the potential implications for data consistency in such a scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cew5o", "is_robot_indexable": true, "report_reasons": null, "author": "Sa1kon", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cew5o/ms_sql_server_and_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cew5o/ms_sql_server_and_big_data/", "subreddit_subscribers": 96138, "created_utc": 1680688195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, for those of you already working with Snowflake, can you please share how you set up the environment for DEV/TEST/PROD?  Do you prefer having individual databases per use-case? How to handle the RBAC properly then? Do you separate the virtual warehouses based on the roles?\nAlso what naming convention did you established for all of it?\nAny suggestions will be welcomed because we are trying to set up our new Snowflake account properly.\nThank you very much!", "author_fullname": "t2_8ouamdf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake environment architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ck24z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680700604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, for those of you already working with Snowflake, can you please share how you set up the environment for DEV/TEST/PROD?  Do you prefer having individual databases per use-case? How to handle the RBAC properly then? Do you separate the virtual warehouses based on the roles?\nAlso what naming convention did you established for all of it?\nAny suggestions will be welcomed because we are trying to set up our new Snowflake account properly.\nThank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ck24z", "is_robot_indexable": true, "report_reasons": null, "author": "adaptrix", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ck24z/snowflake_environment_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ck24z/snowflake_environment_architecture/", "subreddit_subscribers": 96138, "created_utc": 1680700604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Originating from this [SO Answer](https://stackoverflow.com/a/48177344/3673659) lead me down to read BigQuery Quota and Limitations page.\n\nMore specifically, running queries against a table has [a limit of 1500](https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements) operations per day.\n\nBut then, the [DML statements per day is unlimited](https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements), quoting:\n&gt; DML statements count toward the number of table modifications per day (or the number of partitioned table modifications per day for partitioned tables). However, the number of DML statements your project can run per day is unlimited and is not constrained by the table modifications per day quota\n\nHow should I understand these limitations?", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I understand query limitations in BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cicp2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680696840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Originating from this &lt;a href=\"https://stackoverflow.com/a/48177344/3673659\"&gt;SO Answer&lt;/a&gt; lead me down to read BigQuery Quota and Limitations page.&lt;/p&gt;\n\n&lt;p&gt;More specifically, running queries against a table has &lt;a href=\"https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements\"&gt;a limit of 1500&lt;/a&gt; operations per day.&lt;/p&gt;\n\n&lt;p&gt;But then, the &lt;a href=\"https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements\"&gt;DML statements per day is unlimited&lt;/a&gt;, quoting:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;DML statements count toward the number of table modifications per day (or the number of partitioned table modifications per day for partitioned tables). However, the number of DML statements your project can run per day is unlimited and is not constrained by the table modifications per day quota&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;How should I understand these limitations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cicp2", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/", "subreddit_subscribers": 96138, "created_utc": 1680696840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just took on a job that is asking me to build a data mart using data that is all currently being stored in multiple excel files that are manually updated on a weekly basis and being used as the data source for Tableau visualizations.  They are using Keboola for ETL (which I have never heard of before) and snowflake.  Can you help me with the best way to get this data into snowflake and build some sort of data mart / data warehouse that then can be used as the data source for Tableau?  I just need to guidance on best practices and what tools / process you would use.  Data is basically all operational / logistics data.  Thank you for your time.", "author_fullname": "t2_a7qtuvlq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Excel to Snowflake - Best practices ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c717a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680665616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just took on a job that is asking me to build a data mart using data that is all currently being stored in multiple excel files that are manually updated on a weekly basis and being used as the data source for Tableau visualizations.  They are using Keboola for ETL (which I have never heard of before) and snowflake.  Can you help me with the best way to get this data into snowflake and build some sort of data mart / data warehouse that then can be used as the data source for Tableau?  I just need to guidance on best practices and what tools / process you would use.  Data is basically all operational / logistics data.  Thank you for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c717a", "is_robot_indexable": true, "report_reasons": null, "author": "FaithlessnessSea7467", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c717a/excel_to_snowflake_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c717a/excel_to_snowflake_best_practices/", "subreddit_subscribers": 96138, "created_utc": 1680665616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you do dynamic tasks in Airflow? I know there is this concept of dynamic tasks mapping but is this really production ready? \n\nWhat I need is for example to run one task that grabs some file from S3 and based on this file create several tasks (and subsequent tasks as well, so not only 1 task). What we do now is we output the content of the file from step 1 into Airflow variable and then, based on the Variable generate the tasks in for loop. I do not know if this is a good practice? Does not really seem to me so. \n\nAlso, when there are tasks xyz in dag run A, then in the next dag run B there could be different tasks so the tasks from previous run will disappear \u00af\\_(\u30c4)_/\u00af. Is there any elegant solution to this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow dynamic tasks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctyar", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680720443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you do dynamic tasks in Airflow? I know there is this concept of dynamic tasks mapping but is this really production ready? &lt;/p&gt;\n\n&lt;p&gt;What I need is for example to run one task that grabs some file from S3 and based on this file create several tasks (and subsequent tasks as well, so not only 1 task). What we do now is we output the content of the file from step 1 into Airflow variable and then, based on the Variable generate the tasks in for loop. I do not know if this is a good practice? Does not really seem to me so. &lt;/p&gt;\n\n&lt;p&gt;Also, when there are tasks xyz in dag run A, then in the next dag run B there could be different tasks so the tasks from previous run will disappear \u00af_(\u30c4)_/\u00af. Is there any elegant solution to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ctyar", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ctyar/airflow_dynamic_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ctyar/airflow_dynamic_tasks/", "subreddit_subscribers": 96138, "created_utc": 1680720443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Airflow noob and looking for critiques.  \n\nHave a requirement to send data in csv format daily to a handful of clients. In airflow, I have a 'modules' folder containing my custom functions/methods. A 'dag' folder for dags and 'extracts' folder containing the extract scripts. There's one extract script per client and it contains a function to query  our db and send to destination.  The dag i set it up with one @task per client, where it imports the extract script function and executes.  Any issues with that so far?  \n   \n\nI use one script per client because it's only a handful of clients and it'll be easier to quickly adjust to client requirements for now. What would I change or do, to the airflow/python structure, if I needed to scale to like 50 clients?  The extract scripts are generally the same where you import custom modules -&gt; set up connection to db and client destination-&gt; process queries and land in destination.  The queries are the same inasmuch I just have to alter the schema/filters between clients/client_ids.  \n\nIt's fine for now, but struggling to see how I would scale this up quickly while being quick to adjust. Would you loop through a client list in 1 extract script and adjust destination/query settings dynamically? Idk!  \n\n\n\nMuch appreciated all!", "author_fullname": "t2_szv0ygic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctdtc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680719277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Airflow noob and looking for critiques.  &lt;/p&gt;\n\n&lt;p&gt;Have a requirement to send data in csv format daily to a handful of clients. In airflow, I have a &amp;#39;modules&amp;#39; folder containing my custom functions/methods. A &amp;#39;dag&amp;#39; folder for dags and &amp;#39;extracts&amp;#39; folder containing the extract scripts. There&amp;#39;s one extract script per client and it contains a function to query  our db and send to destination.  The dag i set it up with one @task per client, where it imports the extract script function and executes.  Any issues with that so far?  &lt;/p&gt;\n\n&lt;p&gt;I use one script per client because it&amp;#39;s only a handful of clients and it&amp;#39;ll be easier to quickly adjust to client requirements for now. What would I change or do, to the airflow/python structure, if I needed to scale to like 50 clients?  The extract scripts are generally the same where you import custom modules -&amp;gt; set up connection to db and client destination-&amp;gt; process queries and land in destination.  The queries are the same inasmuch I just have to alter the schema/filters between clients/client_ids.  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s fine for now, but struggling to see how I would scale this up quickly while being quick to adjust. Would you loop through a client list in 1 extract script and adjust destination/query settings dynamically? Idk!  &lt;/p&gt;\n\n&lt;p&gt;Much appreciated all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ctdtc", "is_robot_indexable": true, "report_reasons": null, "author": "Hippodick666420", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ctdtc/airflow_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ctdtc/airflow_architecture/", "subreddit_subscribers": 96138, "created_utc": 1680719277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI may be over thinking this, I've done a POC in Synapse Dedicated Pool and I have a question about hash distribution.\n\nSay I have two tables:\n\nOrder Fact\n- 100 million rows\n- hash distributed on OrderID\n\nCustomer Dimension\n- 1 million rows\n- hash distributed on Account No\n- has an identity column as the surrogate key\n\nI can join the fact to the dimension using the surrogate key.\n\nMy question is, am I correct to hash distribute the dimension on the Account No?  Or should I add in a couple of steps to allow me to hash distribute on the surrogate key?  Will this make the join faster, even though the table distribution would be the same?  What would you recommend as best practice?\n\nTIA \ud83d\ude42", "author_fullname": "t2_9h6gf9pp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to: Surrogate Key as Hash Distribution Key", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ct6ux", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680718877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I may be over thinking this, I&amp;#39;ve done a POC in Synapse Dedicated Pool and I have a question about hash distribution.&lt;/p&gt;\n\n&lt;p&gt;Say I have two tables:&lt;/p&gt;\n\n&lt;p&gt;Order Fact\n- 100 million rows\n- hash distributed on OrderID&lt;/p&gt;\n\n&lt;p&gt;Customer Dimension\n- 1 million rows\n- hash distributed on Account No\n- has an identity column as the surrogate key&lt;/p&gt;\n\n&lt;p&gt;I can join the fact to the dimension using the surrogate key.&lt;/p&gt;\n\n&lt;p&gt;My question is, am I correct to hash distribute the dimension on the Account No?  Or should I add in a couple of steps to allow me to hash distribute on the surrogate key?  Will this make the join faster, even though the table distribution would be the same?  What would you recommend as best practice?&lt;/p&gt;\n\n&lt;p&gt;TIA \ud83d\ude42&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ct6ux", "is_robot_indexable": true, "report_reasons": null, "author": "V10Matt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ct6ux/how_to_surrogate_key_as_hash_distribution_key/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ct6ux/how_to_surrogate_key_as_hash_distribution_key/", "subreddit_subscribers": 96138, "created_utc": 1680718877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to get the columns on which a spark transformation depends without parsing the SQL query. Is there a way to get it from the explain plan or some other method directly in Spark?", "author_fullname": "t2_3yaxcuy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resolve column dependencies in Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cr9tx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680714953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to get the columns on which a spark transformation depends without parsing the SQL query. Is there a way to get it from the explain plan or some other method directly in Spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cr9tx", "is_robot_indexable": true, "report_reasons": null, "author": "midasadim", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cr9tx/resolve_column_dependencies_in_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cr9tx/resolve_column_dependencies_in_spark/", "subreddit_subscribers": 96138, "created_utc": 1680714953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently using Redshift and implementing dbt as a transformation framework\u2014 I'm interested in the best way to create a \"development\" structure for code to be tested before being merged into production. [This](https://www.datafold.com/blog/how-to-setup-dbt-development-environments) is really the only resource I've found.\n\nIt seems like recommendations are either\n\n* Create a development schema per user\u2014 this seems like it could get out of hand.\n* Create a development database\u2014 not sure how to do this without fully replicating all data that currently exists. Snowflake has zero-copy clones, but there's no Redshift equivalent to my knowledge.\n* Some combination of the above.\n\nI'd love to hear what's worked and what hasn't... And if there's a Redshift/S3 specific way to make this simple and functional.", "author_fullname": "t2_82q10l6i3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to create a development \"environment\" in Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cvave", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680736218.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680723255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently using Redshift and implementing dbt as a transformation framework\u2014 I&amp;#39;m interested in the best way to create a &amp;quot;development&amp;quot; structure for code to be tested before being merged into production. &lt;a href=\"https://www.datafold.com/blog/how-to-setup-dbt-development-environments\"&gt;This&lt;/a&gt; is really the only resource I&amp;#39;ve found.&lt;/p&gt;\n\n&lt;p&gt;It seems like recommendations are either&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Create a development schema per user\u2014 this seems like it could get out of hand.&lt;/li&gt;\n&lt;li&gt;Create a development database\u2014 not sure how to do this without fully replicating all data that currently exists. Snowflake has zero-copy clones, but there&amp;#39;s no Redshift equivalent to my knowledge.&lt;/li&gt;\n&lt;li&gt;Some combination of the above.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear what&amp;#39;s worked and what hasn&amp;#39;t... And if there&amp;#39;s a Redshift/S3 specific way to make this simple and functional.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?auto=webp&amp;v=enabled&amp;s=6dcb69ffc7979d6f51a12b849b4b98abd4770fed", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e6c36f1745e23372f7aedf17dc37a5e29996093", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18f3a85da7997995d953c673ad784930f0d54c66", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a3ebbbf63daff5a2e1b0105765fb822f2ce2961", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cbd9a6dfee840ec4cff0f293c520603bf2a8e8af", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7357cccdb8e7c8ce495705e7157350775d26ba2c", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f49e3c90b978c5d24b63002846b68f86172cc1d", "width": 1080, "height": 567}], "variants": {}, "id": "cn1-9kKU3UI8ewdlfXJ1WUhQfwpOBR1qCe_i6bOWB4s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cvave", "is_robot_indexable": true, "report_reasons": null, "author": "day-tuh-day", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cvave/best_way_to_create_a_development_environment_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cvave/best_way_to_create_a_development_environment_in/", "subreddit_subscribers": 96138, "created_utc": 1680723255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious on the thoughts here.\n\nIf you were going to create an abstraction layer over your data warehouse to allow transactional systems access, would you be going with FastAPI or [Cube.dev](https://Cube.dev), or something else?\n\nMy first thought here was FastAPI, which is an API after all. It's exactly what I'm looking for. Create the API, then have my transactional systems able to connect via the API and access the data that exists in the warehouse.\n\nHowever [Cube.dev](https://Cube.dev) has both Rest and Graph API access, as well as a SQL style API. This seems like a better option then. I get the Rest API for my transactional systems, and the SQL style API for my reporting and analytics tools.\n\nHave I missed something?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cube.dev vs. FastAPI for an abstraction layer over a data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cfz6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680691195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious on the thoughts here.&lt;/p&gt;\n\n&lt;p&gt;If you were going to create an abstraction layer over your data warehouse to allow transactional systems access, would you be going with FastAPI or &lt;a href=\"https://Cube.dev\"&gt;Cube.dev&lt;/a&gt;, or something else?&lt;/p&gt;\n\n&lt;p&gt;My first thought here was FastAPI, which is an API after all. It&amp;#39;s exactly what I&amp;#39;m looking for. Create the API, then have my transactional systems able to connect via the API and access the data that exists in the warehouse.&lt;/p&gt;\n\n&lt;p&gt;However &lt;a href=\"https://Cube.dev\"&gt;Cube.dev&lt;/a&gt; has both Rest and Graph API access, as well as a SQL style API. This seems like a better option then. I get the Rest API for my transactional systems, and the SQL style API for my reporting and analytics tools.&lt;/p&gt;\n\n&lt;p&gt;Have I missed something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?auto=webp&amp;v=enabled&amp;s=4c2ee9ced32cf7f44c9acfadaf0fc6138d934235", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fc2899acfe1fd7fec2ad6a9c6a16ed630cc31d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb8a7f81c9f0c3327863c615505b600bdd32ead4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0747c7e295fa50f4a169ff2c77b4e38dfe3c70c5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ddb5bc88a05e68b02981b71d6f63b8130ecb7a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd7a3df58c7294394cdfee68c359fedcde4abc6f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=450ad0a42363c74fd12f5edf265b485734b70be3", "width": 1080, "height": 567}], "variants": {}, "id": "CYFlWqFefFx0WAlgFZvtSzIVYhX58H2hKywSvmvXXxw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cfz6t", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cfz6t/cubedev_vs_fastapi_for_an_abstraction_layer_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cfz6t/cubedev_vs_fastapi_for_an_abstraction_layer_over/", "subreddit_subscribers": 96138, "created_utc": 1680691195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to transfer all new ANR events into Redis DB. Is it possibile to trigger Google CF for each new Firebase ANR event?", "author_fullname": "t2_j32k9s23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google CF triggered by Firebase ANR events", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cdal8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680699432.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680683216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to transfer all new ANR events into Redis DB. Is it possibile to trigger Google CF for each new Firebase ANR event?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cdal8", "is_robot_indexable": true, "report_reasons": null, "author": "Away_Efficiency_5837", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cdal8/google_cf_triggered_by_firebase_anr_events/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cdal8/google_cf_triggered_by_firebase_anr_events/", "subreddit_subscribers": 96138, "created_utc": 1680683216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've always had this question: when a company is listed, we usually receive base salary, bonus, and stocks as part of the offer. However, what happens if the company is not listed? What will the company offer instead of stocks?\n\nAlso, as a data engineer with 6 years of experience based in Illinois, l'm currently making around $120K per year with a 10% bonus. My position is remote though, can practically live anywhere in the states. Do you think this is reasonable compensation, or am I being underpaid? I'm just looking for a ballpark figure to discuss with my manager during my next performance review. As a newcomer to the country, l'm not sure how these discussions go here.\n\nI know one pointer though, a senior DE was hired in my company 2 years ago on $150K base (remote)", "author_fullname": "t2_tacjangv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Negotiating Compensation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c2b2a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680654529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve always had this question: when a company is listed, we usually receive base salary, bonus, and stocks as part of the offer. However, what happens if the company is not listed? What will the company offer instead of stocks?&lt;/p&gt;\n\n&lt;p&gt;Also, as a data engineer with 6 years of experience based in Illinois, l&amp;#39;m currently making around $120K per year with a 10% bonus. My position is remote though, can practically live anywhere in the states. Do you think this is reasonable compensation, or am I being underpaid? I&amp;#39;m just looking for a ballpark figure to discuss with my manager during my next performance review. As a newcomer to the country, l&amp;#39;m not sure how these discussions go here.&lt;/p&gt;\n\n&lt;p&gt;I know one pointer though, a senior DE was hired in my company 2 years ago on $150K base (remote)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12c2b2a", "is_robot_indexable": true, "report_reasons": null, "author": "reflectico", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c2b2a/negotiating_compensation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c2b2a/negotiating_compensation/", "subreddit_subscribers": 96138, "created_utc": 1680654529.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all-\n\nAm currently configuring an EMR cluster. I\u2019m curious how others have approached securing the credentials inside the hive-site.xml file.\n\nI don\u2019t want users to be able to access the credentials inside, but they\u2019ll need to be able to use the file/credentials inside to access hive and run hive queries etc as emr steps.\n\nIve tried jceks- but unfortunately a user can still decrypt whats inside and print things out plaintext", "author_fullname": "t2_zfaau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "hive-site.xml file credentials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c0cye", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680650208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all-&lt;/p&gt;\n\n&lt;p&gt;Am currently configuring an EMR cluster. I\u2019m curious how others have approached securing the credentials inside the hive-site.xml file.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t want users to be able to access the credentials inside, but they\u2019ll need to be able to use the file/credentials inside to access hive and run hive queries etc as emr steps.&lt;/p&gt;\n\n&lt;p&gt;Ive tried jceks- but unfortunately a user can still decrypt whats inside and print things out plaintext&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c0cye", "is_robot_indexable": true, "report_reasons": null, "author": "skrt123", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c0cye/hivesitexml_file_credentials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c0cye/hivesitexml_file_credentials/", "subreddit_subscribers": 96138, "created_utc": 1680650208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I\u2019m not sure if this is the right subreddit to post to but I\u2019ve recently received a job offer and was wanted to get some insight on it.\n\nI currently live in DFW and have received an offer for a Data Engineer role for ~80k as my current contract is ending soon. Overall my background is in data analytics and modeling with 2 YoE (undergraduate and masters degree). This is a raise from my current role as an analyst but I have to be honest, I don\u2019t know if I have a ton of knowledge on data pipelining but I feel as though I am a relatively fast learner.\n\nI wanted to know if this is a reasonable offer because upon receiving it I have felt a little underwhelmed and even potentially undervalued. I have a feeling this may also be because of high amounts of socials praising high pay and wanted to know if that\u2019s gotten to my head.", "author_fullname": "t2_1hfbaj39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience &amp; Negotiation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cybtl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680729579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I\u2019m not sure if this is the right subreddit to post to but I\u2019ve recently received a job offer and was wanted to get some insight on it.&lt;/p&gt;\n\n&lt;p&gt;I currently live in DFW and have received an offer for a Data Engineer role for ~80k as my current contract is ending soon. Overall my background is in data analytics and modeling with 2 YoE (undergraduate and masters degree). This is a raise from my current role as an analyst but I have to be honest, I don\u2019t know if I have a ton of knowledge on data pipelining but I feel as though I am a relatively fast learner.&lt;/p&gt;\n\n&lt;p&gt;I wanted to know if this is a reasonable offer because upon receiving it I have felt a little underwhelmed and even potentially undervalued. I have a feeling this may also be because of high amounts of socials praising high pay and wanted to know if that\u2019s gotten to my head.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12cybtl", "is_robot_indexable": true, "report_reasons": null, "author": "UnsureSnake", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cybtl/experience_negotiation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cybtl/experience_negotiation/", "subreddit_subscribers": 96138, "created_utc": 1680729579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have a doubt about how to build my data warehouse's data model. The company I work for is similar to a marketplace that connects sellers and buyers.\n\nHere's a dummy example of how the model of the company works if a buyer buys a product in 3 installments.\n\n&amp;#x200B;\n\ntransaction\\_table (example)\n\nhttps://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725\n\n&amp;#x200B;\n\n(the buyer's amount would be higher because a fee is charged to the buyer)\n\n&amp;#x200B;\n\nWhat would be the best way to model this in my data warehouse? Currently, I don't have a specific data warehouse set up, but rather views of the table in MySQL where I separate the table in the following way:\n\n&amp;#x200B;\n\ntransaction\\_buyer\\_view (example)\n\nhttps://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60\n\n&amp;#x200B;\n\ntransaction\\_seller\\_view (example)\n\nhttps://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75\n\n&amp;#x200B;\n\nSo, when I create a dashboard in Power BI, I retrieve information from either table depending on whether I need to meet a requirement for a team that needs to see information from the buyer's or seller's side. However, I feel that this may not be the most comfortable solution for working with the information. That's why I wanted to know if the best practices for designing a data warehouse would recommend this approach, or if it would be better to keep it in the same table as before, or to have the installments that correspond to the same purchase in the same row (instead of a new row, as it is now), or some other alternative.\n\nAlso, another question I have is what would be the best way to track cases where there an error in the database and the records of the buyers are missing, but the sellers' records are present.\n\n&amp;#x200B;\n\nThank you in advance!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data warehouse design: records of buyers and sellers in same or different table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zffzdz72o4sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2e0f5de5526f44455a37c609ea90b6442938910"}, {"y": 70, "x": 216, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62694771cf40fb8b29211cf3d04d26aca76c2c25"}, {"y": 105, "x": 320, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51782bd6a7b1f2de1a1be2414df68e43ad3906f0"}, {"y": 210, "x": 640, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e3c2a3620f30c59bf7130766c6fbdef1704a068"}, {"y": 315, "x": 960, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8134bb853de479b0f09e0484f85981d1e6be5eb3"}], "s": {"y": 325, "x": 990, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60"}, "id": "zffzdz72o4sa1"}, "9r9hfg1yn4sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0b0805a8ca305bbea5d323be35c74642fb0ffe5"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37d1e84fcea14857b98d112478823013f114bcf9"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b251a36cd5491052d8743ac86d65c83bd2ca834b"}, {"y": 378, "x": 640, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00640cbefe7b03bdd0bab94c7989e28579d74053"}, {"y": 567, "x": 960, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=108b0aa8813753a63ac384ec369a5d0e0886c728"}], "s": {"y": 598, "x": 1012, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725"}, "id": "9r9hfg1yn4sa1"}, "t9k8f7n4o4sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=27dab59246e45df6f8fab4334a2f6c90e96dc350"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0cc48b0b6af66c2f1cb335f2a6be1399144e700"}, {"y": 105, "x": 320, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f936fea5468277f034a58307535798fc4b7980b6"}, {"y": 210, "x": 640, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=023c3f8c1b1a2307f86b81dce273b5b0fd8be07f"}, {"y": 315, "x": 960, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=217b94a79f4b9d903ec0a80ae1aa921c04c39eff"}], "s": {"y": 327, "x": 994, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75"}, "id": "t9k8f7n4o4sa1"}}, "name": "t3_12cx79i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xvaDteTqhhImJzA-CGeD8yEAH51KPN3UhXKI5atxXMw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680727334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a doubt about how to build my data warehouse&amp;#39;s data model. The company I work for is similar to a marketplace that connects sellers and buyers.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a dummy example of how the model of the company works if a buyer buys a product in 3 installments.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;transaction_table (example)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725\"&gt;https://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(the buyer&amp;#39;s amount would be higher because a fee is charged to the buyer)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to model this in my data warehouse? Currently, I don&amp;#39;t have a specific data warehouse set up, but rather views of the table in MySQL where I separate the table in the following way:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;transaction_buyer_view (example)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60\"&gt;https://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;transaction_seller_view (example)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75\"&gt;https://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So, when I create a dashboard in Power BI, I retrieve information from either table depending on whether I need to meet a requirement for a team that needs to see information from the buyer&amp;#39;s or seller&amp;#39;s side. However, I feel that this may not be the most comfortable solution for working with the information. That&amp;#39;s why I wanted to know if the best practices for designing a data warehouse would recommend this approach, or if it would be better to keep it in the same table as before, or to have the installments that correspond to the same purchase in the same row (instead of a new row, as it is now), or some other alternative.&lt;/p&gt;\n\n&lt;p&gt;Also, another question I have is what would be the best way to track cases where there an error in the database and the records of the buyers are missing, but the sellers&amp;#39; records are present.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cx79i", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cx79i/data_warehouse_design_records_of_buyers_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cx79i/data_warehouse_design_records_of_buyers_and/", "subreddit_subscribers": 96138, "created_utc": 1680727334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a BI developer (SQL monkey) and I want to switch to a more technical direction.\n\nI don't use python, so one of logical ways to pursuit DE job is to become Data Analyst who uses SQL and python regularly. \n\nBut I am also considering some sysadmin-like  roles inside my company, that involve working with DHW and cloud network (bash), which seems like a DevOps job.\n\nAfaik, understanding cloud engineering and bash to balance data load/storage is essential for DE.\n\nShall I apply for Data Analyst positions or gain some DevOps knowledge  ?", "author_fullname": "t2_gy4d61n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DevOps knowledge beneficial to switch to DE from BI ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cww0x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680726652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a BI developer (SQL monkey) and I want to switch to a more technical direction.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t use python, so one of logical ways to pursuit DE job is to become Data Analyst who uses SQL and python regularly. &lt;/p&gt;\n\n&lt;p&gt;But I am also considering some sysadmin-like  roles inside my company, that involve working with DHW and cloud network (bash), which seems like a DevOps job.&lt;/p&gt;\n\n&lt;p&gt;Afaik, understanding cloud engineering and bash to balance data load/storage is essential for DE.&lt;/p&gt;\n\n&lt;p&gt;Shall I apply for Data Analyst positions or gain some DevOps knowledge  ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12cww0x", "is_robot_indexable": true, "report_reasons": null, "author": "SolariDoma", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cww0x/is_devops_knowledge_beneficial_to_switch_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cww0x/is_devops_knowledge_beneficial_to_switch_to_de/", "subreddit_subscribers": 96138, "created_utc": 1680726652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a Glue PySpark job that reads json data from a source S3 bucket with bookmarking enabled, and writes as parquet to a target S3 bucket. I\u2019d like to ensure that the target bucket never contain duplicate records, what\u2019s the best way to achieve that? \nMy json records have a \u201cmessageid\u201d field that\u2019s unique.", "author_fullname": "t2_asncnre1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ensure unique records in S3 bucket populated by Glue PySpark job with bookmarking enabled", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cp9qp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680711057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a Glue PySpark job that reads json data from a source S3 bucket with bookmarking enabled, and writes as parquet to a target S3 bucket. I\u2019d like to ensure that the target bucket never contain duplicate records, what\u2019s the best way to achieve that? \nMy json records have a \u201cmessageid\u201d field that\u2019s unique.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cp9qp", "is_robot_indexable": true, "report_reasons": null, "author": "Jaded_Peanut_2777", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cp9qp/ensure_unique_records_in_s3_bucket_populated_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cp9qp/ensure_unique_records_in_s3_bucket_populated_by/", "subreddit_subscribers": 96138, "created_utc": 1680711057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 Things Our Software Engineers Love About Data Contracts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_12ciisx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1q_KydmJOqiHdjMmblb5E_Z4CEs4OwfLsWEzVVbbuF8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680697235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/gocardless-tech/3-things-our-software-engineers-love-about-data-contracts-3106e1f1602d", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?auto=webp&amp;v=enabled&amp;s=775816d43446db26c90c716581fe3dc597b94762", "width": 961, "height": 676}, "resolutions": [{"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=501db2616d9a5bf721a2471733594f526348c76d", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c1d099b3ef8f6fa13582aa6c3fdf003c9441ddf", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44d5ec524f2ccf0aef1a0b62b6d0dcbc7332ca78", "width": 320, "height": 225}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c713aed6e548975e5069d71bc8ef76967d924e3", "width": 640, "height": 450}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ad2225a3e4219d9536d09077f09910dae6d3d9f", "width": 960, "height": 675}], "variants": {}, "id": "ZhcsTlN1rykHbZmNeR6oO3OyyU2oRrqM49TZJBj2nm8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ciisx", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ciisx/3_things_our_software_engineers_love_about_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/gocardless-tech/3-things-our-software-engineers-love-about-data-contracts-3106e1f1602d", "subreddit_subscribers": 96138, "created_utc": 1680697235.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}