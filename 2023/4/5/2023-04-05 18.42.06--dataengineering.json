{"kind": "Listing", "data": {"after": "t3_12ciisx", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \n\nI know projects are not that important but I have a to fun building them and I thought maybe someone else is interested in some of mine. \n\nSo basically this is a very simple Data Lakehouse deployed in Docker containers, which uses Iceberg, Trino, Minio and a Hive Metastore. Since someone maybe directly wants to play with some data I have built an init container which creates an Iceberg table based on a parquet file in the object storage. Furthermore there is a BI Service pre configured to visualize it. \n\nI thought this project might be interesting to some of you who have only worked with traditional Data Warehouses (not that I am an expert with \"new types\" of storages) or want a more real life like storage, without paying a cloud provider, for your own Data projects. \n\nHere is the Github repo: [https://github.com/dominikhei/Local-Data-LakeHouse](https://github.com/dominikhei/Local-Data-LakeHouse)\n\nFeedback is well appreciated :)", "author_fullname": "t2_v219tksh", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Project showcase: sample Data Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12buxtq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680639043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;I know projects are not that important but I have a to fun building them and I thought maybe someone else is interested in some of mine. &lt;/p&gt;\n\n&lt;p&gt;So basically this is a very simple Data Lakehouse deployed in Docker containers, which uses Iceberg, Trino, Minio and a Hive Metastore. Since someone maybe directly wants to play with some data I have built an init container which creates an Iceberg table based on a parquet file in the object storage. Furthermore there is a BI Service pre configured to visualize it. &lt;/p&gt;\n\n&lt;p&gt;I thought this project might be interesting to some of you who have only worked with traditional Data Warehouses (not that I am an expert with &amp;quot;new types&amp;quot; of storages) or want a more real life like storage, without paying a cloud provider, for your own Data projects. &lt;/p&gt;\n\n&lt;p&gt;Here is the Github repo: &lt;a href=\"https://github.com/dominikhei/Local-Data-LakeHouse\"&gt;https://github.com/dominikhei/Local-Data-LakeHouse&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feedback is well appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?auto=webp&amp;v=enabled&amp;s=a58192c664108607ded3daf9478385a01fbd0ecc", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d0d0e069191b2d1de8fceddc2d8d67b6afdcfa6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0a1d064f39318c98ded16c5bb222f58d2359e5a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=583b60681f50e05fd927771f145bb86a48ed662e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f897b46d33f821bc4292cb42076abfee2a38226a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c36c9251e310b24d30ca2ed0c1dabe70c3421f3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=25561e443cddffff0a625175d488742cc40d37ca", "width": 1080, "height": 540}], "variants": {}, "id": "FLYBfmpfBaqdgNeA_3kK487GKlC9BiAetvGC4EzqLpo"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12buxtq", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Hand-577", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12buxtq/project_showcase_sample_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12buxtq/project_showcase_sample_data_lakehouse/", "subreddit_subscribers": 96082, "created_utc": 1680639043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "are you a python+sql monkey? are you basically a PM  and a DE as the same time? Share your experience in the comments!", "author_fullname": "t2_82dwrpz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how much business knowledge do you *really* have", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cfomc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680690419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;are you a python+sql monkey? are you basically a PM  and a DE as the same time? Share your experience in the comments!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cfomc", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Ad-8440", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cfomc/how_much_business_knowledge_do_you_really_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cfomc/how_much_business_knowledge_do_you_really_have/", "subreddit_subscribers": 96082, "created_utc": 1680690419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a question I'm hoping someone can answer.\n\nIf you have a Type 2 SCD, (customer), and you have an accumulating status fact table (application status) that references customer, and said customer change over the course of the accumulation of statuses on their application, which single customer dim id to you assign to the fact record for the application? Is it the first (so it never changes), or is it the most recent (so you're rebuilding the fact each day)?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kimball approach question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c816i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680668140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a question I&amp;#39;m hoping someone can answer.&lt;/p&gt;\n\n&lt;p&gt;If you have a Type 2 SCD, (customer), and you have an accumulating status fact table (application status) that references customer, and said customer change over the course of the accumulation of statuses on their application, which single customer dim id to you assign to the fact record for the application? Is it the first (so it never changes), or is it the most recent (so you&amp;#39;re rebuilding the fact each day)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c816i", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c816i/kimball_approach_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c816i/kimball_approach_question/", "subreddit_subscribers": 96082, "created_utc": 1680668140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 Helpful extract and load practices for high-quality raw data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": false, "name": "t3_12cbfkr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3IbeqnonkWY6c95i99CP2I6Pi9cMfPoHL07ztkGQz-M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680677757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/towards-data-science/5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?auto=webp&amp;v=enabled&amp;s=d8f939dca3db41615898fce3d5e02edc9f722adb", "width": 1200, "height": 822}, "resolutions": [{"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa41fd85a49ef460db9ee8077a4abfd87558c2c7", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=829c74629b975010398356bf520e93eca2578c4e", "width": 216, "height": 147}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18f23ee8055d21bd26220415934f4ec6a335c900", "width": 320, "height": 219}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7badc32e3c69d46b25a0f6de158584224a17d978", "width": 640, "height": 438}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7aeea8014828d04f94f6bfafb812ba166f37355b", "width": 960, "height": 657}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be89059f76c9ca503a50ed7ad920adf5ea62a864", "width": 1080, "height": 739}], "variants": {}, "id": "IZ72cRWUnIq5zNuKmC1_z1idxm6cWGgUbAFXhn03tdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12cbfkr", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cbfkr/5_helpful_extract_and_load_practices_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/towards-data-science/5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721", "subreddit_subscribers": 96082, "created_utc": 1680677757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi people of internet,\n\nI am hopping to get an answer to my question in title. We are a small to medium business and our data amount is smaller than 200gb. So i have googled so much that now I am even confused as to which one to choose in Azure to build our data warehouse. \n\nWe want to create our data warehouse in Azure and choose the right database for that. I have never used Azure portal before so i have been doing some research about it over the last several days to choose a database option for our data warehouse but cannot get my head around as what option is good for us. Can anyone please help me understand which one would be right tool for us?\n\nEdit\n\nTo answer some questions - we will have prob 3 to 5 other users for DWH. At the moment the most pressing reporting need is Sales reporting so we will have usual dims (customer, dates, products etc. ) and a fact table. We will have unstructured (mainly json) and structured data (oracle). We will need to have test and prod instances.\n\nAs I see majority mentions Dedicated SQL Pools as Synapse and I wonder why is that ?? When I search for Dedicated SQL Pools inside Azure Portal, it does not show anything related to Synapse.", "author_fullname": "t2_2b7yft3b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure SQL Database (Single &amp; serverless) or Dedicated SQL Pools (Formerly SQL DW) for Data warehousing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12brosk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680664524.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680632506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people of internet,&lt;/p&gt;\n\n&lt;p&gt;I am hopping to get an answer to my question in title. We are a small to medium business and our data amount is smaller than 200gb. So i have googled so much that now I am even confused as to which one to choose in Azure to build our data warehouse. &lt;/p&gt;\n\n&lt;p&gt;We want to create our data warehouse in Azure and choose the right database for that. I have never used Azure portal before so i have been doing some research about it over the last several days to choose a database option for our data warehouse but cannot get my head around as what option is good for us. Can anyone please help me understand which one would be right tool for us?&lt;/p&gt;\n\n&lt;p&gt;Edit&lt;/p&gt;\n\n&lt;p&gt;To answer some questions - we will have prob 3 to 5 other users for DWH. At the moment the most pressing reporting need is Sales reporting so we will have usual dims (customer, dates, products etc. ) and a fact table. We will have unstructured (mainly json) and structured data (oracle). We will need to have test and prod instances.&lt;/p&gt;\n\n&lt;p&gt;As I see majority mentions Dedicated SQL Pools as Synapse and I wonder why is that ?? When I search for Dedicated SQL Pools inside Azure Portal, it does not show anything related to Synapse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12brosk", "is_robot_indexable": true, "report_reasons": null, "author": "DznFatih", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12brosk/azure_sql_database_single_serverless_or_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12brosk/azure_sql_database_single_serverless_or_dedicated/", "subreddit_subscribers": 96082, "created_utc": 1680632506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Essentially I run a potentiostat where I get a piece of current/density data every 30 seconds. These run for a 10-day cycle so I save the file as an Excel spreadsheet from time 0s to 999990s in 30-second increments with 66668 different data points. This has been running for 3 months so I have hundreds of thousands of time-stamped data points.\n\nI need to put this info into a nice graph to collate my work but Excel constantly crashes, is very laggy and makes the fan on my laptop sound like an aeroplane is about to take off. \n\nDoes anyone have a suggestion for another programme I can use, please?", "author_fullname": "t2_o1nf1i8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Excel for large data processing, please?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cehg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680686773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Essentially I run a potentiostat where I get a piece of current/density data every 30 seconds. These run for a 10-day cycle so I save the file as an Excel spreadsheet from time 0s to 999990s in 30-second increments with 66668 different data points. This has been running for 3 months so I have hundreds of thousands of time-stamped data points.&lt;/p&gt;\n\n&lt;p&gt;I need to put this info into a nice graph to collate my work but Excel constantly crashes, is very laggy and makes the fan on my laptop sound like an aeroplane is about to take off. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a suggestion for another programme I can use, please?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cehg3", "is_robot_indexable": true, "report_reasons": null, "author": "HanSo-High", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cehg3/alternative_to_excel_for_large_data_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cehg3/alternative_to_excel_for_large_data_processing/", "subreddit_subscribers": 96082, "created_utc": 1680686773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nI was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.\n\nAs I've only had experience with RDBM's such as PostgreSQL or MySQL, I'd like to know some _Gotcha's_ before I'm running against a never moving wall.\n\nTherefore, what things that we commonly do in MySQL should we be considerate when switching to BigQuery?", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I know before using BigQuery, having traditional MySQL knowledge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cbbw0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680677439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.&lt;/p&gt;\n\n&lt;p&gt;As I&amp;#39;ve only had experience with RDBM&amp;#39;s such as PostgreSQL or MySQL, I&amp;#39;d like to know some &lt;em&gt;Gotcha&amp;#39;s&lt;/em&gt; before I&amp;#39;m running against a never moving wall.&lt;/p&gt;\n\n&lt;p&gt;Therefore, what things that we commonly do in MySQL should we be considerate when switching to BigQuery?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cbbw0", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/", "subreddit_subscribers": 96082, "created_utc": 1680677439.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m5mja", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Burstable vs non-burstable AWS instance types for data engineering workloads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_12btcue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HVOGVjMNyTgJVlc3NMAQPc-z6iYTS4C5nD9O4LcTULs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680635899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/coiled-hq/burstable-vs-non-burstable-aws-instance-types-for-data-engineering-workloads-540b7f10f6eb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?auto=webp&amp;v=enabled&amp;s=37963b8df59a95702f1521537b89b4c91f11f91b", "width": 600, "height": 371}, "resolutions": [{"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=034ccd70c7da5a52b6c12f37cf8e754abf3d276c", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=990c1441d5c93bc5e69073fc0c7085783709d90d", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20c76277991df844d275216801d8f0ee6f22a551", "width": 320, "height": 197}], "variants": {}, "id": "827n3XVLrcD58rUAY4TDhu4SGblYDmn0dgw0DPUW4ac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12btcue", "is_robot_indexable": true, "report_reasons": null, "author": "dchudz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12btcue/burstable_vs_nonburstable_aws_instance_types_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/coiled-hq/burstable-vs-non-burstable-aws-instance-types-for-data-engineering-workloads-540b7f10f6eb", "subreddit_subscribers": 96082, "created_utc": 1680635899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello chaps,\n\nI'm in a bit of a pickle and could use your expertise. My team and I have been tasked with designing an infrastructure to support our ML Team. Here are the key requirements:\n\n**Database:** Microsoft SQL Server\n\n**Daily data volume:** \\~12 million rows (each file contains data for one day)\n\nOur objective is to accumulate data for an entire year, which will be utilised for constructing inventory predictions across our retail shops.\n\nGiven the substantial volume of data and our reliance on MS SQL Server, I would appreciate any suggestions on how to approach the creation of such a data infrastructure.\n\nOne idea that crossed my mind was to generate a snapshot of the complete data set for day one, and then proceed by gathering only the data that has changed on a daily basis. I'm curious to know if this is a viable approach.\n\nAdditionally, how might we go about concatenating all the data efficiently? And, crucially, what are the potential implications for data consistency in such a scenario?", "author_fullname": "t2_8u34pgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS SQL Server and Big Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cew5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680688195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello chaps,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in a bit of a pickle and could use your expertise. My team and I have been tasked with designing an infrastructure to support our ML Team. Here are the key requirements:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Database:&lt;/strong&gt; Microsoft SQL Server&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Daily data volume:&lt;/strong&gt; ~12 million rows (each file contains data for one day)&lt;/p&gt;\n\n&lt;p&gt;Our objective is to accumulate data for an entire year, which will be utilised for constructing inventory predictions across our retail shops.&lt;/p&gt;\n\n&lt;p&gt;Given the substantial volume of data and our reliance on MS SQL Server, I would appreciate any suggestions on how to approach the creation of such a data infrastructure.&lt;/p&gt;\n\n&lt;p&gt;One idea that crossed my mind was to generate a snapshot of the complete data set for day one, and then proceed by gathering only the data that has changed on a daily basis. I&amp;#39;m curious to know if this is a viable approach.&lt;/p&gt;\n\n&lt;p&gt;Additionally, how might we go about concatenating all the data efficiently? And, crucially, what are the potential implications for data consistency in such a scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cew5o", "is_robot_indexable": true, "report_reasons": null, "author": "Sa1kon", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cew5o/ms_sql_server_and_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cew5o/ms_sql_server_and_big_data/", "subreddit_subscribers": 96082, "created_utc": 1680688195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just took on a job that is asking me to build a data mart using data that is all currently being stored in multiple excel files that are manually updated on a weekly basis and being used as the data source for Tableau visualizations.  They are using Keboola for ETL (which I have never heard of before) and snowflake.  Can you help me with the best way to get this data into snowflake and build some sort of data mart / data warehouse that then can be used as the data source for Tableau?  I just need to guidance on best practices and what tools / process you would use.  Data is basically all operational / logistics data.  Thank you for your time.", "author_fullname": "t2_a7qtuvlq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Excel to Snowflake - Best practices ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c717a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680665616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just took on a job that is asking me to build a data mart using data that is all currently being stored in multiple excel files that are manually updated on a weekly basis and being used as the data source for Tableau visualizations.  They are using Keboola for ETL (which I have never heard of before) and snowflake.  Can you help me with the best way to get this data into snowflake and build some sort of data mart / data warehouse that then can be used as the data source for Tableau?  I just need to guidance on best practices and what tools / process you would use.  Data is basically all operational / logistics data.  Thank you for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c717a", "is_robot_indexable": true, "report_reasons": null, "author": "FaithlessnessSea7467", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c717a/excel_to_snowflake_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c717a/excel_to_snowflake_best_practices/", "subreddit_subscribers": 96082, "created_utc": 1680665616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE's,\n\nJust looking for some advice in terms of the best solutions to ingest the data for every event that is located in the STFP. \n\nThe process i have come up with is setting up a logic app that pulls any new json file that has been dropped into the location, looks every minute for these files and moves them into blob storage. \n\nFrom there using databricks to flatten the JSON into a dataframe and pushing it into another location with the correct dtypes and naming. \n\nIf anyone has a better way of doing that please let me know, or if you've worked with eagleeye before and have a working solution in azure that would be great!", "author_fullname": "t2_eibi1lc4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone in this thread worked with Eagleeye Solutions for a Loyalty Program, basically ingesting the data from an SFTP? {Azure]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bv75l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680639577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE&amp;#39;s,&lt;/p&gt;\n\n&lt;p&gt;Just looking for some advice in terms of the best solutions to ingest the data for every event that is located in the STFP. &lt;/p&gt;\n\n&lt;p&gt;The process i have come up with is setting up a logic app that pulls any new json file that has been dropped into the location, looks every minute for these files and moves them into blob storage. &lt;/p&gt;\n\n&lt;p&gt;From there using databricks to flatten the JSON into a dataframe and pushing it into another location with the correct dtypes and naming. &lt;/p&gt;\n\n&lt;p&gt;If anyone has a better way of doing that please let me know, or if you&amp;#39;ve worked with eagleeye before and have a working solution in azure that would be great!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bv75l", "is_robot_indexable": true, "report_reasons": null, "author": "cosmic-destiny", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bv75l/has_anyone_in_this_thread_worked_with_eagleeye/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bv75l/has_anyone_in_this_thread_worked_with_eagleeye/", "subreddit_subscribers": 96082, "created_utc": 1680639577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your experiences with Debezium? What are common issues you face with it? Do you find it easy to maintain?", "author_fullname": "t2_dhmtsxhx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debezium", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bxvsz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680644986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your experiences with Debezium? What are common issues you face with it? Do you find it easy to maintain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bxvsz", "is_robot_indexable": true, "report_reasons": null, "author": "marioco__", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bxvsz/debezium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bxvsz/debezium/", "subreddit_subscribers": 96082, "created_utc": 1680644986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've only mostly used SQL and Python in my previous and current job. I'm looking to learn another programming language, which of the 2 is most useful?", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to learn next, Scala or Java?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cn55x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680706849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve only mostly used SQL and Python in my previous and current job. I&amp;#39;m looking to learn another programming language, which of the 2 is most useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cn55x", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cn55x/what_to_learn_next_scala_or_java/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cn55x/what_to_learn_next_scala_or_java/", "subreddit_subscribers": 96082, "created_utc": 1680706849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, for those of you already working with Snowflake, can you please share how you set up the environment for DEV/TEST/PROD?  Do you prefer having individual databases per use-case? How to handle the RBAC properly then? Do you separate the virtual warehouses based on the roles?\nAlso what naming convention did you established for all of it?\nAny suggestions will be welcomed because we are trying to set up our new Snowflake account properly.\nThank you very much!", "author_fullname": "t2_8ouamdf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake environment architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ck24z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680700604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, for those of you already working with Snowflake, can you please share how you set up the environment for DEV/TEST/PROD?  Do you prefer having individual databases per use-case? How to handle the RBAC properly then? Do you separate the virtual warehouses based on the roles?\nAlso what naming convention did you established for all of it?\nAny suggestions will be welcomed because we are trying to set up our new Snowflake account properly.\nThank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ck24z", "is_robot_indexable": true, "report_reasons": null, "author": "adaptrix", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ck24z/snowflake_environment_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ck24z/snowflake_environment_architecture/", "subreddit_subscribers": 96082, "created_utc": 1680700604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Originating from this [SO Answer](https://stackoverflow.com/a/48177344/3673659) lead me down to read BigQuery Quota and Limitations page.\n\nMore specifically, running queries against a table has [a limit of 1500](https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements) operations per day.\n\nBut then, the [DML statements per day is unlimited](https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements), quoting:\n&gt; DML statements count toward the number of table modifications per day (or the number of partitioned table modifications per day for partitioned tables). However, the number of DML statements your project can run per day is unlimited and is not constrained by the table modifications per day quota\n\nHow should I understand these limitations?", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I understand query limitations in BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cicp2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680696840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Originating from this &lt;a href=\"https://stackoverflow.com/a/48177344/3673659\"&gt;SO Answer&lt;/a&gt; lead me down to read BigQuery Quota and Limitations page.&lt;/p&gt;\n\n&lt;p&gt;More specifically, running queries against a table has &lt;a href=\"https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements\"&gt;a limit of 1500&lt;/a&gt; operations per day.&lt;/p&gt;\n\n&lt;p&gt;But then, the &lt;a href=\"https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements\"&gt;DML statements per day is unlimited&lt;/a&gt;, quoting:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;DML statements count toward the number of table modifications per day (or the number of partitioned table modifications per day for partitioned tables). However, the number of DML statements your project can run per day is unlimited and is not constrained by the table modifications per day quota&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;How should I understand these limitations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cicp2", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/", "subreddit_subscribers": 96082, "created_utc": 1680696840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, this may seem trivial, but just wanted to clarify and see how you all approach data integration with a data lake.\n\nSo, lets say you are starting from scratch and are tasked with integrating data from a source RDMS (Adventure Works for example) and your data lake. Note that the Adventure Works Database has 68 tables, excluding the dbo schema.\n\nQuestions on how you would approach this:\n\n1. Would you look to integrate all 68 tables from the DB into the Data Lake? So that the Raw and Clean data lake zones would keep the 68 tables in the same normal form that exists in the RDMS. Only in my Curated Zone, I would have business logic and denormalization to create data marts or analytics tables?\n2. Would a folder structure like this be appropriate: *adventure\\_works/schema/table\\_name/partitions/file.snappy.parquet?*\n3. How would you manage upserts in the lake? If an order status changes from open to closed, would you overwrite that record in the data lake with the new status? If not, how would track this change in the lake?\n\nThanks!", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Source Data Integration into a Data Lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12buhgr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680638861.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680638147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, this may seem trivial, but just wanted to clarify and see how you all approach data integration with a data lake.&lt;/p&gt;\n\n&lt;p&gt;So, lets say you are starting from scratch and are tasked with integrating data from a source RDMS (Adventure Works for example) and your data lake. Note that the Adventure Works Database has 68 tables, excluding the dbo schema.&lt;/p&gt;\n\n&lt;p&gt;Questions on how you would approach this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Would you look to integrate all 68 tables from the DB into the Data Lake? So that the Raw and Clean data lake zones would keep the 68 tables in the same normal form that exists in the RDMS. Only in my Curated Zone, I would have business logic and denormalization to create data marts or analytics tables?&lt;/li&gt;\n&lt;li&gt;Would a folder structure like this be appropriate: &lt;em&gt;adventure_works/schema/table_name/partitions/file.snappy.parquet?&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;How would you manage upserts in the lake? If an order status changes from open to closed, would you overwrite that record in the data lake with the new status? If not, how would track this change in the lake?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12buhgr", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12buhgr/source_data_integration_into_a_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12buhgr/source_data_integration_into_a_data_lake/", "subreddit_subscribers": 96082, "created_utc": 1680638147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious on the thoughts here.\n\nIf you were going to create an abstraction layer over your data warehouse to allow transactional systems access, would you be going with FastAPI or [Cube.dev](https://Cube.dev), or something else?\n\nMy first thought here was FastAPI, which is an API after all. It's exactly what I'm looking for. Create the API, then have my transactional systems able to connect via the API and access the data that exists in the warehouse.\n\nHowever [Cube.dev](https://Cube.dev) has both Rest and Graph API access, as well as a SQL style API. This seems like a better option then. I get the Rest API for my transactional systems, and the SQL style API for my reporting and analytics tools.\n\nHave I missed something?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cube.dev vs. FastAPI for an abstraction layer over a data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cfz6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680691195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious on the thoughts here.&lt;/p&gt;\n\n&lt;p&gt;If you were going to create an abstraction layer over your data warehouse to allow transactional systems access, would you be going with FastAPI or &lt;a href=\"https://Cube.dev\"&gt;Cube.dev&lt;/a&gt;, or something else?&lt;/p&gt;\n\n&lt;p&gt;My first thought here was FastAPI, which is an API after all. It&amp;#39;s exactly what I&amp;#39;m looking for. Create the API, then have my transactional systems able to connect via the API and access the data that exists in the warehouse.&lt;/p&gt;\n\n&lt;p&gt;However &lt;a href=\"https://Cube.dev\"&gt;Cube.dev&lt;/a&gt; has both Rest and Graph API access, as well as a SQL style API. This seems like a better option then. I get the Rest API for my transactional systems, and the SQL style API for my reporting and analytics tools.&lt;/p&gt;\n\n&lt;p&gt;Have I missed something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?auto=webp&amp;v=enabled&amp;s=4c2ee9ced32cf7f44c9acfadaf0fc6138d934235", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fc2899acfe1fd7fec2ad6a9c6a16ed630cc31d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb8a7f81c9f0c3327863c615505b600bdd32ead4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0747c7e295fa50f4a169ff2c77b4e38dfe3c70c5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ddb5bc88a05e68b02981b71d6f63b8130ecb7a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd7a3df58c7294394cdfee68c359fedcde4abc6f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=450ad0a42363c74fd12f5edf265b485734b70be3", "width": 1080, "height": 567}], "variants": {}, "id": "CYFlWqFefFx0WAlgFZvtSzIVYhX58H2hKywSvmvXXxw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cfz6t", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cfz6t/cubedev_vs_fastapi_for_an_abstraction_layer_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cfz6t/cubedev_vs_fastapi_for_an_abstraction_layer_over/", "subreddit_subscribers": 96082, "created_utc": 1680691195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to transfer all new ANR events into Redis DB. Is it possibile to trigger Google CF for each new Firebase ANR event?", "author_fullname": "t2_j32k9s23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google CF triggered by Firebase ANR events", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cdal8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680699432.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680683216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to transfer all new ANR events into Redis DB. Is it possibile to trigger Google CF for each new Firebase ANR event?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cdal8", "is_robot_indexable": true, "report_reasons": null, "author": "Away_Efficiency_5837", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cdal8/google_cf_triggered_by_firebase_anr_events/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cdal8/google_cf_triggered_by_firebase_anr_events/", "subreddit_subscribers": 96082, "created_utc": 1680683216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've always had this question: when a company is listed, we usually receive base salary, bonus, and stocks as part of the offer. However, what happens if the company is not listed? What will the company offer instead of stocks?\n\nAlso, as a data engineer with 6 years of experience based in Illinois, l'm currently making around $120K per year with a 10% bonus. My position is remote though, can practically live anywhere in the states. Do you think this is reasonable compensation, or am I being underpaid? I'm just looking for a ballpark figure to discuss with my manager during my next performance review. As a newcomer to the country, l'm not sure how these discussions go here.\n\nI know one pointer though, a senior DE was hired in my company 2 years ago on $150K base (remote)", "author_fullname": "t2_tacjangv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Negotiating Compensation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c2b2a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680654529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve always had this question: when a company is listed, we usually receive base salary, bonus, and stocks as part of the offer. However, what happens if the company is not listed? What will the company offer instead of stocks?&lt;/p&gt;\n\n&lt;p&gt;Also, as a data engineer with 6 years of experience based in Illinois, l&amp;#39;m currently making around $120K per year with a 10% bonus. My position is remote though, can practically live anywhere in the states. Do you think this is reasonable compensation, or am I being underpaid? I&amp;#39;m just looking for a ballpark figure to discuss with my manager during my next performance review. As a newcomer to the country, l&amp;#39;m not sure how these discussions go here.&lt;/p&gt;\n\n&lt;p&gt;I know one pointer though, a senior DE was hired in my company 2 years ago on $150K base (remote)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12c2b2a", "is_robot_indexable": true, "report_reasons": null, "author": "reflectico", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c2b2a/negotiating_compensation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c2b2a/negotiating_compensation/", "subreddit_subscribers": 96082, "created_utc": 1680654529.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all-\n\nAm currently configuring an EMR cluster. I\u2019m curious how others have approached securing the credentials inside the hive-site.xml file.\n\nI don\u2019t want users to be able to access the credentials inside, but they\u2019ll need to be able to use the file/credentials inside to access hive and run hive queries etc as emr steps.\n\nIve tried jceks- but unfortunately a user can still decrypt whats inside and print things out plaintext", "author_fullname": "t2_zfaau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "hive-site.xml file credentials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c0cye", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680650208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all-&lt;/p&gt;\n\n&lt;p&gt;Am currently configuring an EMR cluster. I\u2019m curious how others have approached securing the credentials inside the hive-site.xml file.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t want users to be able to access the credentials inside, but they\u2019ll need to be able to use the file/credentials inside to access hive and run hive queries etc as emr steps.&lt;/p&gt;\n\n&lt;p&gt;Ive tried jceks- but unfortunately a user can still decrypt whats inside and print things out plaintext&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c0cye", "is_robot_indexable": true, "report_reasons": null, "author": "skrt123", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c0cye/hivesitexml_file_credentials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c0cye/hivesitexml_file_credentials/", "subreddit_subscribers": 96082, "created_utc": 1680650208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\nI have a case where I need to stream (or batch every 1 min) new records in BQ stream table into Redis. I think I should use CF gen 2 triggered by BQ event, but I'm not sure how to read only new records in BQ table.\nDo you have idea what's the best way to solve this?", "author_fullname": "t2_j32k9s23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream data from BQ Stream Table into Redis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bsx8t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680635004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,\nI have a case where I need to stream (or batch every 1 min) new records in BQ stream table into Redis. I think I should use CF gen 2 triggered by BQ event, but I&amp;#39;m not sure how to read only new records in BQ table.\nDo you have idea what&amp;#39;s the best way to solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bsx8t", "is_robot_indexable": true, "report_reasons": null, "author": "Away_Efficiency_5837", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bsx8t/stream_data_from_bq_stream_table_into_redis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bsx8t/stream_data_from_bq_stream_table_into_redis/", "subreddit_subscribers": 96082, "created_utc": 1680635004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone ever created a databricks job for dbt using dbx? There is very little documentation/examples by dbx but it does say that a running a dbt job is possible.", "author_fullname": "t2_19klta65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT deployment with dbx", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12brehr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680631911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ever created a databricks job for dbt using dbx? There is very little documentation/examples by dbx but it does say that a running a dbt job is possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12brehr", "is_robot_indexable": true, "report_reasons": null, "author": "DRUKSTOP", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12brehr/dbt_deployment_with_dbx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12brehr/dbt_deployment_with_dbx/", "subreddit_subscribers": 96082, "created_utc": 1680631911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to get the columns on which a spark transformation depends without parsing the SQL query. Is there a way to get it from the explain plan or some other method directly in Spark?", "author_fullname": "t2_3yaxcuy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resolve column dependencies in Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12cr9tx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680714953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to get the columns on which a spark transformation depends without parsing the SQL query. Is there a way to get it from the explain plan or some other method directly in Spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cr9tx", "is_robot_indexable": true, "report_reasons": null, "author": "midasadim", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cr9tx/resolve_column_dependencies_in_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cr9tx/resolve_column_dependencies_in_spark/", "subreddit_subscribers": 96082, "created_utc": 1680714953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a Glue PySpark job that reads json data from a source S3 bucket with bookmarking enabled, and writes as parquet to a target S3 bucket. I\u2019d like to ensure that the target bucket never contain duplicate records, what\u2019s the best way to achieve that? \nMy json records have a \u201cmessageid\u201d field that\u2019s unique.", "author_fullname": "t2_asncnre1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ensure unique records in S3 bucket populated by Glue PySpark job with bookmarking enabled", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cp9qp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680711057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a Glue PySpark job that reads json data from a source S3 bucket with bookmarking enabled, and writes as parquet to a target S3 bucket. I\u2019d like to ensure that the target bucket never contain duplicate records, what\u2019s the best way to achieve that? \nMy json records have a \u201cmessageid\u201d field that\u2019s unique.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cp9qp", "is_robot_indexable": true, "report_reasons": null, "author": "Jaded_Peanut_2777", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cp9qp/ensure_unique_records_in_s3_bucket_populated_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cp9qp/ensure_unique_records_in_s3_bucket_populated_by/", "subreddit_subscribers": 96082, "created_utc": 1680711057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 Things Our Software Engineers Love About Data Contracts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_12ciisx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1q_KydmJOqiHdjMmblb5E_Z4CEs4OwfLsWEzVVbbuF8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680697235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/gocardless-tech/3-things-our-software-engineers-love-about-data-contracts-3106e1f1602d", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?auto=webp&amp;v=enabled&amp;s=775816d43446db26c90c716581fe3dc597b94762", "width": 961, "height": 676}, "resolutions": [{"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=501db2616d9a5bf721a2471733594f526348c76d", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c1d099b3ef8f6fa13582aa6c3fdf003c9441ddf", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44d5ec524f2ccf0aef1a0b62b6d0dcbc7332ca78", "width": 320, "height": 225}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c713aed6e548975e5069d71bc8ef76967d924e3", "width": 640, "height": 450}, {"url": "https://external-preview.redd.it/8yv5088eri3yk-36DAonLiqPV9cjI-EMqURrOXSjvzI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ad2225a3e4219d9536d09077f09910dae6d3d9f", "width": 960, "height": 675}], "variants": {}, "id": "ZhcsTlN1rykHbZmNeR6oO3OyyU2oRrqM49TZJBj2nm8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ciisx", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ciisx/3_things_our_software_engineers_love_about_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/gocardless-tech/3-things-our-software-engineers-love-about-data-contracts-3106e1f1602d", "subreddit_subscribers": 96082, "created_utc": 1680697235.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}