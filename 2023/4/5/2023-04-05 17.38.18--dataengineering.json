{"kind": "Listing", "data": {"after": "t3_12cdal8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \n\nI know projects are not that important but I have a to fun building them and I thought maybe someone else is interested in some of mine. \n\nSo basically this is a very simple Data Lakehouse deployed in Docker containers, which uses Iceberg, Trino, Minio and a Hive Metastore. Since someone maybe directly wants to play with some data I have built an init container which creates an Iceberg table based on a parquet file in the object storage. Furthermore there is a BI Service pre configured to visualize it. \n\nI thought this project might be interesting to some of you who have only worked with traditional Data Warehouses (not that I am an expert with \"new types\" of storages) or want a more real life like storage, without paying a cloud provider, for your own Data projects. \n\nHere is the Github repo: [https://github.com/dominikhei/Local-Data-LakeHouse](https://github.com/dominikhei/Local-Data-LakeHouse)\n\nFeedback is well appreciated :)", "author_fullname": "t2_v219tksh", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Project showcase: sample Data Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12buxtq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680639043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;I know projects are not that important but I have a to fun building them and I thought maybe someone else is interested in some of mine. &lt;/p&gt;\n\n&lt;p&gt;So basically this is a very simple Data Lakehouse deployed in Docker containers, which uses Iceberg, Trino, Minio and a Hive Metastore. Since someone maybe directly wants to play with some data I have built an init container which creates an Iceberg table based on a parquet file in the object storage. Furthermore there is a BI Service pre configured to visualize it. &lt;/p&gt;\n\n&lt;p&gt;I thought this project might be interesting to some of you who have only worked with traditional Data Warehouses (not that I am an expert with &amp;quot;new types&amp;quot; of storages) or want a more real life like storage, without paying a cloud provider, for your own Data projects. &lt;/p&gt;\n\n&lt;p&gt;Here is the Github repo: &lt;a href=\"https://github.com/dominikhei/Local-Data-LakeHouse\"&gt;https://github.com/dominikhei/Local-Data-LakeHouse&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feedback is well appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?auto=webp&amp;v=enabled&amp;s=a58192c664108607ded3daf9478385a01fbd0ecc", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d0d0e069191b2d1de8fceddc2d8d67b6afdcfa6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0a1d064f39318c98ded16c5bb222f58d2359e5a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=583b60681f50e05fd927771f145bb86a48ed662e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f897b46d33f821bc4292cb42076abfee2a38226a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c36c9251e310b24d30ca2ed0c1dabe70c3421f3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=25561e443cddffff0a625175d488742cc40d37ca", "width": 1080, "height": 540}], "variants": {}, "id": "FLYBfmpfBaqdgNeA_3kK487GKlC9BiAetvGC4EzqLpo"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12buxtq", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Hand-577", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12buxtq/project_showcase_sample_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12buxtq/project_showcase_sample_data_lakehouse/", "subreddit_subscribers": 96068, "created_utc": 1680639043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "are you a python+sql monkey? are you basically a PM  and a DE as the same time? Share your experience in the comments!", "author_fullname": "t2_82dwrpz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how much business knowledge do you *really* have", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cfomc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680690419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;are you a python+sql monkey? are you basically a PM  and a DE as the same time? Share your experience in the comments!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cfomc", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Ad-8440", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cfomc/how_much_business_knowledge_do_you_really_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cfomc/how_much_business_knowledge_do_you_really_have/", "subreddit_subscribers": 96068, "created_utc": 1680690419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a question I'm hoping someone can answer.\n\nIf you have a Type 2 SCD, (customer), and you have an accumulating status fact table (application status) that references customer, and said customer change over the course of the accumulation of statuses on their application, which single customer dim id to you assign to the fact record for the application? Is it the first (so it never changes), or is it the most recent (so you're rebuilding the fact each day)?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kimball approach question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c816i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680668140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a question I&amp;#39;m hoping someone can answer.&lt;/p&gt;\n\n&lt;p&gt;If you have a Type 2 SCD, (customer), and you have an accumulating status fact table (application status) that references customer, and said customer change over the course of the accumulation of statuses on their application, which single customer dim id to you assign to the fact record for the application? Is it the first (so it never changes), or is it the most recent (so you&amp;#39;re rebuilding the fact each day)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c816i", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c816i/kimball_approach_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c816i/kimball_approach_question/", "subreddit_subscribers": 96068, "created_utc": 1680668140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 Helpful extract and load practices for high-quality raw data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": false, "name": "t3_12cbfkr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3IbeqnonkWY6c95i99CP2I6Pi9cMfPoHL07ztkGQz-M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680677757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/towards-data-science/5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?auto=webp&amp;v=enabled&amp;s=d8f939dca3db41615898fce3d5e02edc9f722adb", "width": 1200, "height": 822}, "resolutions": [{"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa41fd85a49ef460db9ee8077a4abfd87558c2c7", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=829c74629b975010398356bf520e93eca2578c4e", "width": 216, "height": 147}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18f23ee8055d21bd26220415934f4ec6a335c900", "width": 320, "height": 219}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7badc32e3c69d46b25a0f6de158584224a17d978", "width": 640, "height": 438}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7aeea8014828d04f94f6bfafb812ba166f37355b", "width": 960, "height": 657}, {"url": "https://external-preview.redd.it/B13cJEvR2eMzLUtOyX32pYu0TSPeMxak1cK6d9OeBwU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be89059f76c9ca503a50ed7ad920adf5ea62a864", "width": 1080, "height": 739}], "variants": {}, "id": "IZ72cRWUnIq5zNuKmC1_z1idxm6cWGgUbAFXhn03tdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12cbfkr", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cbfkr/5_helpful_extract_and_load_practices_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/towards-data-science/5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721", "subreddit_subscribers": 96068, "created_utc": 1680677757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi people of internet,\n\nI am hopping to get an answer to my question in title. We are a small to medium business and our data amount is smaller than 200gb. So i have googled so much that now I am even confused as to which one to choose in Azure to build our data warehouse. \n\nWe want to create our data warehouse in Azure and choose the right database for that. I have never used Azure portal before so i have been doing some research about it over the last several days to choose a database option for our data warehouse but cannot get my head around as what option is good for us. Can anyone please help me understand which one would be right tool for us?\n\nEdit\n\nTo answer some questions - we will have prob 3 to 5 other users for DWH. At the moment the most pressing reporting need is Sales reporting so we will have usual dims (customer, dates, products etc. ) and a fact table. We will have unstructured (mainly json) and structured data (oracle). We will need to have test and prod instances.\n\nAs I see majority mentions Dedicated SQL Pools as Synapse and I wonder why is that ?? When I search for Dedicated SQL Pools inside Azure Portal, it does not show anything related to Synapse.", "author_fullname": "t2_2b7yft3b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure SQL Database (Single &amp; serverless) or Dedicated SQL Pools (Formerly SQL DW) for Data warehousing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12brosk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680664524.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680632506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people of internet,&lt;/p&gt;\n\n&lt;p&gt;I am hopping to get an answer to my question in title. We are a small to medium business and our data amount is smaller than 200gb. So i have googled so much that now I am even confused as to which one to choose in Azure to build our data warehouse. &lt;/p&gt;\n\n&lt;p&gt;We want to create our data warehouse in Azure and choose the right database for that. I have never used Azure portal before so i have been doing some research about it over the last several days to choose a database option for our data warehouse but cannot get my head around as what option is good for us. Can anyone please help me understand which one would be right tool for us?&lt;/p&gt;\n\n&lt;p&gt;Edit&lt;/p&gt;\n\n&lt;p&gt;To answer some questions - we will have prob 3 to 5 other users for DWH. At the moment the most pressing reporting need is Sales reporting so we will have usual dims (customer, dates, products etc. ) and a fact table. We will have unstructured (mainly json) and structured data (oracle). We will need to have test and prod instances.&lt;/p&gt;\n\n&lt;p&gt;As I see majority mentions Dedicated SQL Pools as Synapse and I wonder why is that ?? When I search for Dedicated SQL Pools inside Azure Portal, it does not show anything related to Synapse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12brosk", "is_robot_indexable": true, "report_reasons": null, "author": "DznFatih", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12brosk/azure_sql_database_single_serverless_or_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12brosk/azure_sql_database_single_serverless_or_dedicated/", "subreddit_subscribers": 96068, "created_utc": 1680632506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Essentially I run a potentiostat where I get a piece of current/density data every 30 seconds. These run for a 10-day cycle so I save the file as an Excel spreadsheet from time 0s to 999990s in 30-second increments with 66668 different data points. This has been running for 3 months so I have hundreds of thousands of time-stamped data points.\n\nI need to put this info into a nice graph to collate my work but Excel constantly crashes, is very laggy and makes the fan on my laptop sound like an aeroplane is about to take off. \n\nDoes anyone have a suggestion for another programme I can use, please?", "author_fullname": "t2_o1nf1i8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Excel for large data processing, please?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cehg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680686773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Essentially I run a potentiostat where I get a piece of current/density data every 30 seconds. These run for a 10-day cycle so I save the file as an Excel spreadsheet from time 0s to 999990s in 30-second increments with 66668 different data points. This has been running for 3 months so I have hundreds of thousands of time-stamped data points.&lt;/p&gt;\n\n&lt;p&gt;I need to put this info into a nice graph to collate my work but Excel constantly crashes, is very laggy and makes the fan on my laptop sound like an aeroplane is about to take off. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a suggestion for another programme I can use, please?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cehg3", "is_robot_indexable": true, "report_reasons": null, "author": "HanSo-High", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cehg3/alternative_to_excel_for_large_data_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cehg3/alternative_to_excel_for_large_data_processing/", "subreddit_subscribers": 96068, "created_utc": 1680686773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nI was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.\n\nAs I've only had experience with RDBM's such as PostgreSQL or MySQL, I'd like to know some _Gotcha's_ before I'm running against a never moving wall.\n\nTherefore, what things that we commonly do in MySQL should we be considerate when switching to BigQuery?", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I know before using BigQuery, having traditional MySQL knowledge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cbbw0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680677439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.&lt;/p&gt;\n\n&lt;p&gt;As I&amp;#39;ve only had experience with RDBM&amp;#39;s such as PostgreSQL or MySQL, I&amp;#39;d like to know some &lt;em&gt;Gotcha&amp;#39;s&lt;/em&gt; before I&amp;#39;m running against a never moving wall.&lt;/p&gt;\n\n&lt;p&gt;Therefore, what things that we commonly do in MySQL should we be considerate when switching to BigQuery?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cbbw0", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/", "subreddit_subscribers": 96068, "created_utc": 1680677439.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m5mja", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Burstable vs non-burstable AWS instance types for data engineering workloads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_12btcue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HVOGVjMNyTgJVlc3NMAQPc-z6iYTS4C5nD9O4LcTULs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680635899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/coiled-hq/burstable-vs-non-burstable-aws-instance-types-for-data-engineering-workloads-540b7f10f6eb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?auto=webp&amp;v=enabled&amp;s=37963b8df59a95702f1521537b89b4c91f11f91b", "width": 600, "height": 371}, "resolutions": [{"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=034ccd70c7da5a52b6c12f37cf8e754abf3d276c", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=990c1441d5c93bc5e69073fc0c7085783709d90d", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20c76277991df844d275216801d8f0ee6f22a551", "width": 320, "height": 197}], "variants": {}, "id": "827n3XVLrcD58rUAY4TDhu4SGblYDmn0dgw0DPUW4ac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12btcue", "is_robot_indexable": true, "report_reasons": null, "author": "dchudz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12btcue/burstable_vs_nonburstable_aws_instance_types_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/coiled-hq/burstable-vs-non-burstable-aws-instance-types-for-data-engineering-workloads-540b7f10f6eb", "subreddit_subscribers": 96068, "created_utc": 1680635899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are companies still using the licensed ETL tools? like - Informatica, Datastage etc.", "author_fullname": "t2_6kmo2ecy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bq5oe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680629403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are companies still using the licensed ETL tools? like - Informatica, Datastage etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bq5oe", "is_robot_indexable": true, "report_reasons": null, "author": "soujoshi", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bq5oe/etl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bq5oe/etl_tools/", "subreddit_subscribers": 96068, "created_utc": 1680629403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am downloading publicly available provider information from the Centers for Medicare &amp; Medicaid Services. The file is published monthly. Any entity can change names or address without an update to the identifier. I am appending a 'data month' to the data.  \n\n\nI want to have a table (oracle) that helps the user indicate if there has been a change to any of 3 fields that are not the main identifier (address, entity name, business name). In that case I would like to have the most recent information and a second row with the old address, name, ect. The user could use the data download month to see the changes over time on a particular ID.   \n\n\nI can think of 2 ways to do this. Make the key be the identifier plus the other fields I care about. Update the 'downloaded month field' based on the key if it exists otherwise import. Or query the existing data set, append the new data, leverage pandas to check for duplicates on the columns, truncate the existing table, insert the entire table. The monthly files are approx 15k rows.   \n\n\nAnother option would be to just import everything always and then use other methods to filter down to what I want to see. I am learning best practices for table design and ETL and would appreciate some guidance.", "author_fullname": "t2_f1q7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL/Table Design Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bphlz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680628048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am downloading publicly available provider information from the Centers for Medicare &amp;amp; Medicaid Services. The file is published monthly. Any entity can change names or address without an update to the identifier. I am appending a &amp;#39;data month&amp;#39; to the data.  &lt;/p&gt;\n\n&lt;p&gt;I want to have a table (oracle) that helps the user indicate if there has been a change to any of 3 fields that are not the main identifier (address, entity name, business name). In that case I would like to have the most recent information and a second row with the old address, name, ect. The user could use the data download month to see the changes over time on a particular ID.   &lt;/p&gt;\n\n&lt;p&gt;I can think of 2 ways to do this. Make the key be the identifier plus the other fields I care about. Update the &amp;#39;downloaded month field&amp;#39; based on the key if it exists otherwise import. Or query the existing data set, append the new data, leverage pandas to check for duplicates on the columns, truncate the existing table, insert the entire table. The monthly files are approx 15k rows.   &lt;/p&gt;\n\n&lt;p&gt;Another option would be to just import everything always and then use other methods to filter down to what I want to see. I am learning best practices for table design and ETL and would appreciate some guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bphlz", "is_robot_indexable": true, "report_reasons": null, "author": "machinegunke11y", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bphlz/etltable_design_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bphlz/etltable_design_question/", "subreddit_subscribers": 96068, "created_utc": 1680628048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am writing a Spark dataframe to Hive external table using insertInto. The table is partitioned on multiple columns. Something like category/sub_category.\n\nHow can I make sure to write 10 files under each sub_category directory. The data is huge (1 year of data), If I do repartition(10) the job is taking too long. If I repartition on category and sub_category columns together, there is only 1  file getting creating under each sub_category directory.\n\nWhat I am missing? My understanding is if I repartition based on multiple columns and provide numPartition too, it should work.\n\n(df\n.repartion(10, \"category\", \"sub_category\")\n.insertInto(\"schema.tableName\"))\n\nIs this the right way to achieve what I want?", "author_fullname": "t2_gzyg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing Spark dataframe to Hive external table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bqkbw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680630201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am writing a Spark dataframe to Hive external table using insertInto. The table is partitioned on multiple columns. Something like category/sub_category.&lt;/p&gt;\n\n&lt;p&gt;How can I make sure to write 10 files under each sub_category directory. The data is huge (1 year of data), If I do repartition(10) the job is taking too long. If I repartition on category and sub_category columns together, there is only 1  file getting creating under each sub_category directory.&lt;/p&gt;\n\n&lt;p&gt;What I am missing? My understanding is if I repartition based on multiple columns and provide numPartition too, it should work.&lt;/p&gt;\n\n&lt;p&gt;(df\n.repartion(10, &amp;quot;category&amp;quot;, &amp;quot;sub_category&amp;quot;)\n.insertInto(&amp;quot;schema.tableName&amp;quot;))&lt;/p&gt;\n\n&lt;p&gt;Is this the right way to achieve what I want?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bqkbw", "is_robot_indexable": true, "report_reasons": null, "author": "ps2931", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bqkbw/writing_spark_dataframe_to_hive_external_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bqkbw/writing_spark_dataframe_to_hive_external_table/", "subreddit_subscribers": 96068, "created_utc": 1680630201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello chaps,\n\nI'm in a bit of a pickle and could use your expertise. My team and I have been tasked with designing an infrastructure to support our ML Team. Here are the key requirements:\n\n**Database:** Microsoft SQL Server\n\n**Daily data volume:** \\~12 million rows (each file contains data for one day)\n\nOur objective is to accumulate data for an entire year, which will be utilised for constructing inventory predictions across our retail shops.\n\nGiven the substantial volume of data and our reliance on MS SQL Server, I would appreciate any suggestions on how to approach the creation of such a data infrastructure.\n\nOne idea that crossed my mind was to generate a snapshot of the complete data set for day one, and then proceed by gathering only the data that has changed on a daily basis. I'm curious to know if this is a viable approach.\n\nAdditionally, how might we go about concatenating all the data efficiently? And, crucially, what are the potential implications for data consistency in such a scenario?", "author_fullname": "t2_8u34pgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS SQL Server and Big Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cew5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680688195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello chaps,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in a bit of a pickle and could use your expertise. My team and I have been tasked with designing an infrastructure to support our ML Team. Here are the key requirements:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Database:&lt;/strong&gt; Microsoft SQL Server&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Daily data volume:&lt;/strong&gt; ~12 million rows (each file contains data for one day)&lt;/p&gt;\n\n&lt;p&gt;Our objective is to accumulate data for an entire year, which will be utilised for constructing inventory predictions across our retail shops.&lt;/p&gt;\n\n&lt;p&gt;Given the substantial volume of data and our reliance on MS SQL Server, I would appreciate any suggestions on how to approach the creation of such a data infrastructure.&lt;/p&gt;\n\n&lt;p&gt;One idea that crossed my mind was to generate a snapshot of the complete data set for day one, and then proceed by gathering only the data that has changed on a daily basis. I&amp;#39;m curious to know if this is a viable approach.&lt;/p&gt;\n\n&lt;p&gt;Additionally, how might we go about concatenating all the data efficiently? And, crucially, what are the potential implications for data consistency in such a scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cew5o", "is_robot_indexable": true, "report_reasons": null, "author": "Sa1kon", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cew5o/ms_sql_server_and_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cew5o/ms_sql_server_and_big_data/", "subreddit_subscribers": 96068, "created_utc": 1680688195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just took on a job that is asking me to build a data mart using data that is all currently being stored in multiple excel files that are manually updated on a weekly basis and being used as the data source for Tableau visualizations.  They are using Keboola for ETL (which I have never heard of before) and snowflake.  Can you help me with the best way to get this data into snowflake and build some sort of data mart / data warehouse that then can be used as the data source for Tableau?  I just need to guidance on best practices and what tools / process you would use.  Data is basically all operational / logistics data.  Thank you for your time.", "author_fullname": "t2_a7qtuvlq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Excel to Snowflake - Best practices ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12c717a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680665616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just took on a job that is asking me to build a data mart using data that is all currently being stored in multiple excel files that are manually updated on a weekly basis and being used as the data source for Tableau visualizations.  They are using Keboola for ETL (which I have never heard of before) and snowflake.  Can you help me with the best way to get this data into snowflake and build some sort of data mart / data warehouse that then can be used as the data source for Tableau?  I just need to guidance on best practices and what tools / process you would use.  Data is basically all operational / logistics data.  Thank you for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12c717a", "is_robot_indexable": true, "report_reasons": null, "author": "FaithlessnessSea7467", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12c717a/excel_to_snowflake_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12c717a/excel_to_snowflake_best_practices/", "subreddit_subscribers": 96068, "created_utc": 1680665616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE's,\n\nJust looking for some advice in terms of the best solutions to ingest the data for every event that is located in the STFP. \n\nThe process i have come up with is setting up a logic app that pulls any new json file that has been dropped into the location, looks every minute for these files and moves them into blob storage. \n\nFrom there using databricks to flatten the JSON into a dataframe and pushing it into another location with the correct dtypes and naming. \n\nIf anyone has a better way of doing that please let me know, or if you've worked with eagleeye before and have a working solution in azure that would be great!", "author_fullname": "t2_eibi1lc4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone in this thread worked with Eagleeye Solutions for a Loyalty Program, basically ingesting the data from an SFTP? {Azure]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bv75l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680639577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE&amp;#39;s,&lt;/p&gt;\n\n&lt;p&gt;Just looking for some advice in terms of the best solutions to ingest the data for every event that is located in the STFP. &lt;/p&gt;\n\n&lt;p&gt;The process i have come up with is setting up a logic app that pulls any new json file that has been dropped into the location, looks every minute for these files and moves them into blob storage. &lt;/p&gt;\n\n&lt;p&gt;From there using databricks to flatten the JSON into a dataframe and pushing it into another location with the correct dtypes and naming. &lt;/p&gt;\n\n&lt;p&gt;If anyone has a better way of doing that please let me know, or if you&amp;#39;ve worked with eagleeye before and have a working solution in azure that would be great!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bv75l", "is_robot_indexable": true, "report_reasons": null, "author": "cosmic-destiny", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bv75l/has_anyone_in_this_thread_worked_with_eagleeye/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bv75l/has_anyone_in_this_thread_worked_with_eagleeye/", "subreddit_subscribers": 96068, "created_utc": 1680639577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, former data engineer here.\n\nI am working on solving some problems I observed at my last startup related to data pipelines and specifically file/report download and distribution. Are any of the following relatable? If not, something else? (Have nothing to sell...yet)\n\n1. File drops in remote locations are unpredictable but are time critical so processing has to start ASAP.\n2. Failed file downloads must retry or be escalated. Similarly, late file drops become a business/operations risks and must be escalated\n3. Sometimes partners will drop the wrong file (i.e. yesterday's file, wrong format, missing data) and detecting problems with reports/files occurs late in your pipeline or leads to incorrect outcomes\n4. Multiple consumers across the business /engineering depend on the same daily/weekly/monthly reports and all need a copy\n5. Customers want data/reports uploaded to specific storage locations they manage, requiring setup and configuration. Maybe you don't support all storage types? Changing of upload location is not self serve to customer but require support/engineering.\n6. Support/engineering sometimes has to check on file/report content and need to access a particular file in a secure environment\n7. Most of the above become support team challenges before they become engineering challenges\n\nDo any of these seem like relatable challenges? If not, anything else?", "author_fullname": "t2_11or0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do any of these file management problems ring true?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bq0zl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680629143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, former data engineer here.&lt;/p&gt;\n\n&lt;p&gt;I am working on solving some problems I observed at my last startup related to data pipelines and specifically file/report download and distribution. Are any of the following relatable? If not, something else? (Have nothing to sell...yet)&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;File drops in remote locations are unpredictable but are time critical so processing has to start ASAP.&lt;/li&gt;\n&lt;li&gt;Failed file downloads must retry or be escalated. Similarly, late file drops become a business/operations risks and must be escalated&lt;/li&gt;\n&lt;li&gt;Sometimes partners will drop the wrong file (i.e. yesterday&amp;#39;s file, wrong format, missing data) and detecting problems with reports/files occurs late in your pipeline or leads to incorrect outcomes&lt;/li&gt;\n&lt;li&gt;Multiple consumers across the business /engineering depend on the same daily/weekly/monthly reports and all need a copy&lt;/li&gt;\n&lt;li&gt;Customers want data/reports uploaded to specific storage locations they manage, requiring setup and configuration. Maybe you don&amp;#39;t support all storage types? Changing of upload location is not self serve to customer but require support/engineering.&lt;/li&gt;\n&lt;li&gt;Support/engineering sometimes has to check on file/report content and need to access a particular file in a secure environment&lt;/li&gt;\n&lt;li&gt;Most of the above become support team challenges before they become engineering challenges&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Do any of these seem like relatable challenges? If not, anything else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bq0zl", "is_robot_indexable": true, "report_reasons": null, "author": "harryblueberry", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bq0zl/do_any_of_these_file_management_problems_ring_true/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bq0zl/do_any_of_these_file_management_problems_ring_true/", "subreddit_subscribers": 96068, "created_utc": 1680629143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've only mostly used SQL and Python in my previous and current job. I'm looking to learn another programming language, which of the 2 is most useful?", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to learn next, Scala or Java?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cn55x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680706849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve only mostly used SQL and Python in my previous and current job. I&amp;#39;m looking to learn another programming language, which of the 2 is most useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12cn55x", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cn55x/what_to_learn_next_scala_or_java/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cn55x/what_to_learn_next_scala_or_java/", "subreddit_subscribers": 96068, "created_utc": 1680706849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your experiences with Debezium? What are common issues you face with it? Do you find it easy to maintain?", "author_fullname": "t2_dhmtsxhx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debezium", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bxvsz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680644986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your experiences with Debezium? What are common issues you face with it? Do you find it easy to maintain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bxvsz", "is_robot_indexable": true, "report_reasons": null, "author": "marioco__", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bxvsz/debezium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bxvsz/debezium/", "subreddit_subscribers": 96068, "created_utc": 1680644986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is everyone's opinions regarding the crazy rise in demand for data engineering, and the gradual cooling of the market? Has it been a difficult adjustment for anyone?\n\nI'm in my late twenties, and the only thing I've really experienced is an upward momentum in opportunity and wages in this field. Back in early 2022, I was getting thrown opportunities for exciting jobs with high salaries. At least $150k plus TC with just 2-3 years of experience. \n\nNow, it seems like $150k TC is the higher limit of the jobs available, and companies are getting more selective. At least for remote work. \n\nDo you think software engineer/data engineer compensation will hover around the $100-150k range for mid-level? and do you think this is how it's going to stay?\n\nI'm thinking that to get to a higher TC above that, I'll either have to be an even higher level IC or go into management.", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think the job market for the data engineering field will look like in the next 5 years?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12br054", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680631107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is everyone&amp;#39;s opinions regarding the crazy rise in demand for data engineering, and the gradual cooling of the market? Has it been a difficult adjustment for anyone?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in my late twenties, and the only thing I&amp;#39;ve really experienced is an upward momentum in opportunity and wages in this field. Back in early 2022, I was getting thrown opportunities for exciting jobs with high salaries. At least $150k plus TC with just 2-3 years of experience. &lt;/p&gt;\n\n&lt;p&gt;Now, it seems like $150k TC is the higher limit of the jobs available, and companies are getting more selective. At least for remote work. &lt;/p&gt;\n\n&lt;p&gt;Do you think software engineer/data engineer compensation will hover around the $100-150k range for mid-level? and do you think this is how it&amp;#39;s going to stay?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking that to get to a higher TC above that, I&amp;#39;ll either have to be an even higher level IC or go into management.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12br054", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12br054/what_do_you_think_the_job_market_for_the_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12br054/what_do_you_think_the_job_market_for_the_data/", "subreddit_subscribers": 96068, "created_utc": 1680631107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Helloe everyone student here on internship, my subject is about the creation of a data quality framework, and while i am curretnly making a POC i can't help but feel that the subject has no main standards (concerning the process of making it or the general way a data quality should look like), i have created a flow for how the data should go (will probably use airflow for it, but i still don't have the hang on when it is necessary to use airflow and when it's not).\n\nI am curretnly working on an oracle database where i retrieve the data then extract some descriptive statistics, then check for uniqueness (still not well implemented) and the last step are a bunch of rules validation that i just made for the sake of the POC (future plan is to make a business rule engine).\n\nI was thrown a new term called Data Catalog, been searching around and i still don't really understand what does it do (Apache Atlas) and what's the point of it inside of a data quality framework.\n\nCan anyone explain or guide me to ressources to how relevant it is in general ?\n\nN.B : I am still learning what a data quality framework is, it's kinda hard since i find mostly tools like great expectations or pandas-profiling but no general guidelines of how it looks.", "author_fullname": "t2_fcv4rhtp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a data catalog and why would it matter inside a data quality framework ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bqccj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680629756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Helloe everyone student here on internship, my subject is about the creation of a data quality framework, and while i am curretnly making a POC i can&amp;#39;t help but feel that the subject has no main standards (concerning the process of making it or the general way a data quality should look like), i have created a flow for how the data should go (will probably use airflow for it, but i still don&amp;#39;t have the hang on when it is necessary to use airflow and when it&amp;#39;s not).&lt;/p&gt;\n\n&lt;p&gt;I am curretnly working on an oracle database where i retrieve the data then extract some descriptive statistics, then check for uniqueness (still not well implemented) and the last step are a bunch of rules validation that i just made for the sake of the POC (future plan is to make a business rule engine).&lt;/p&gt;\n\n&lt;p&gt;I was thrown a new term called Data Catalog, been searching around and i still don&amp;#39;t really understand what does it do (Apache Atlas) and what&amp;#39;s the point of it inside of a data quality framework.&lt;/p&gt;\n\n&lt;p&gt;Can anyone explain or guide me to ressources to how relevant it is in general ?&lt;/p&gt;\n\n&lt;p&gt;N.B : I am still learning what a data quality framework is, it&amp;#39;s kinda hard since i find mostly tools like great expectations or pandas-profiling but no general guidelines of how it looks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bqccj", "is_robot_indexable": true, "report_reasons": null, "author": "Still-W1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bqccj/what_is_a_data_catalog_and_why_would_it_matter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bqccj/what_is_a_data_catalog_and_why_would_it_matter/", "subreddit_subscribers": 96068, "created_utc": 1680629756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, for those of you already working with Snowflake, can you please share how you set up the environment for DEV/TEST/PROD?  Do you prefer having individual databases per use-case? How to handle the RBAC properly then? Do you separate the virtual warehouses based on the roles?\nAlso what naming convention did you established for all of it?\nAny suggestions will be welcomed because we are trying to set up our new Snowflake account properly.\nThank you very much!", "author_fullname": "t2_8ouamdf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake environment architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ck24z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680700604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, for those of you already working with Snowflake, can you please share how you set up the environment for DEV/TEST/PROD?  Do you prefer having individual databases per use-case? How to handle the RBAC properly then? Do you separate the virtual warehouses based on the roles?\nAlso what naming convention did you established for all of it?\nAny suggestions will be welcomed because we are trying to set up our new Snowflake account properly.\nThank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ck24z", "is_robot_indexable": true, "report_reasons": null, "author": "adaptrix", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ck24z/snowflake_environment_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ck24z/snowflake_environment_architecture/", "subreddit_subscribers": 96068, "created_utc": 1680700604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Originating from this [SO Answer](https://stackoverflow.com/a/48177344/3673659) lead me down to read BigQuery Quota and Limitations page.\n\nMore specifically, running queries against a table has [a limit of 1500](https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements) operations per day.\n\nBut then, the [DML statements per day is unlimited](https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements), quoting:\n&gt; DML statements count toward the number of table modifications per day (or the number of partitioned table modifications per day for partitioned tables). However, the number of DML statements your project can run per day is unlimited and is not constrained by the table modifications per day quota\n\nHow should I understand these limitations?", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I understand query limitations in BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cicp2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680696840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Originating from this &lt;a href=\"https://stackoverflow.com/a/48177344/3673659\"&gt;SO Answer&lt;/a&gt; lead me down to read BigQuery Quota and Limitations page.&lt;/p&gt;\n\n&lt;p&gt;More specifically, running queries against a table has &lt;a href=\"https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements\"&gt;a limit of 1500&lt;/a&gt; operations per day.&lt;/p&gt;\n\n&lt;p&gt;But then, the &lt;a href=\"https://cloud.google.com/bigquery/quotas#data_manipulation_language_statements\"&gt;DML statements per day is unlimited&lt;/a&gt;, quoting:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;DML statements count toward the number of table modifications per day (or the number of partitioned table modifications per day for partitioned tables). However, the number of DML statements your project can run per day is unlimited and is not constrained by the table modifications per day quota&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;How should I understand these limitations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cicp2", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/", "subreddit_subscribers": 96068, "created_utc": 1680696840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, this may seem trivial, but just wanted to clarify and see how you all approach data integration with a data lake.\n\nSo, lets say you are starting from scratch and are tasked with integrating data from a source RDMS (Adventure Works for example) and your data lake. Note that the Adventure Works Database has 68 tables, excluding the dbo schema.\n\nQuestions on how you would approach this:\n\n1. Would you look to integrate all 68 tables from the DB into the Data Lake? So that the Raw and Clean data lake zones would keep the 68 tables in the same normal form that exists in the RDMS. Only in my Curated Zone, I would have business logic and denormalization to create data marts or analytics tables?\n2. Would a folder structure like this be appropriate: *adventure\\_works/schema/table\\_name/partitions/file.snappy.parquet?*\n3. How would you manage upserts in the lake? If an order status changes from open to closed, would you overwrite that record in the data lake with the new status? If not, how would track this change in the lake?\n\nThanks!", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Source Data Integration into a Data Lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12buhgr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680638861.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680638147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, this may seem trivial, but just wanted to clarify and see how you all approach data integration with a data lake.&lt;/p&gt;\n\n&lt;p&gt;So, lets say you are starting from scratch and are tasked with integrating data from a source RDMS (Adventure Works for example) and your data lake. Note that the Adventure Works Database has 68 tables, excluding the dbo schema.&lt;/p&gt;\n\n&lt;p&gt;Questions on how you would approach this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Would you look to integrate all 68 tables from the DB into the Data Lake? So that the Raw and Clean data lake zones would keep the 68 tables in the same normal form that exists in the RDMS. Only in my Curated Zone, I would have business logic and denormalization to create data marts or analytics tables?&lt;/li&gt;\n&lt;li&gt;Would a folder structure like this be appropriate: &lt;em&gt;adventure_works/schema/table_name/partitions/file.snappy.parquet?&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;How would you manage upserts in the lake? If an order status changes from open to closed, would you overwrite that record in the data lake with the new status? If not, how would track this change in the lake?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12buhgr", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12buhgr/source_data_integration_into_a_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12buhgr/source_data_integration_into_a_data_lake/", "subreddit_subscribers": 96068, "created_utc": 1680638147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have 7 slices, a 3.5GB CSV file (uncompressed) with 35m rows and looking into applying AWS recommendations for COPY to data faster into Redshift.\n\nIt seems like when RedShift encounters an uncompressed single file, they'll fan out better than if handed a compressed split files, despite recommending that.\n\n|Files|Compression|Format|STL\\_LOAD\\_COMMITS|Elapsed|File Size|\n|:-|:-|:-|:-|:-|:-|\n|1|UNCOMPRESSED|CSV|55|1m|3.7gb|\n|7|UNCOMPRESSED|CSV|56|2.6m|529mb|\n|60|UNCOMPRESSED|CSV|60|1.9m|61mb|\n|1|GZIP|CSV|1|2.9m|365mb|\n|7|GZIP|CSV|7|2.1m|56mb|\n|60|GZIP|CSV|60|2.5m|7mb|\n|1|ZSTD|CSV|1|3m|450mb|\n|1|UNCOMPRESSED|PARQUET|3|1.57m|319mb|\n|7|UNCOMPRESSED|PARQUET|7|0.9m|44mb|\n|70|UNCOMPRESSED|PARQUET|70|1.7m|4.4mb|\n|1|GZIP|PARQUET|2|1.7m|219mb|\n|7|GZIP|PARQUET|7|0.6m|30mb|\n|70|GZIP|PARQUET|70|1.7m|3mb|\n|1|SNAPPY|PARQUET|2|1.7m||\n|7|SNAPPY|PARQUET|7|1.1m|38mb|\n|1|UNCOMPRESSED|AVRO|1|13.7m|3.65mb|\n\nI would've loved to also test PARQUET with BZ4 compression since RedShift claims that it can do concurrent reads against it, but nothing I'm familiar with seem to be able to write with that.\n\nThe only thing meaningfully outperforming a single UNCOMPRESSED CSV seem to be GZIP PARQUET split to the same number of files as slices.\n\nHas anyone else benchmarked formats and compression on import speed? I would've expected better results switching away from the most naive solution.", "author_fullname": "t2_3bvnd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "COPY to Redshift seems negatively affected by both compression and file splitting with UNCOMPRESSED CSV as peak performance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12br5ab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680674991.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680631382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have 7 slices, a 3.5GB CSV file (uncompressed) with 35m rows and looking into applying AWS recommendations for COPY to data faster into Redshift.&lt;/p&gt;\n\n&lt;p&gt;It seems like when RedShift encounters an uncompressed single file, they&amp;#39;ll fan out better than if handed a compressed split files, despite recommending that.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Files&lt;/th&gt;\n&lt;th align=\"left\"&gt;Compression&lt;/th&gt;\n&lt;th align=\"left\"&gt;Format&lt;/th&gt;\n&lt;th align=\"left\"&gt;STL_LOAD_COMMITS&lt;/th&gt;\n&lt;th align=\"left\"&gt;Elapsed&lt;/th&gt;\n&lt;th align=\"left\"&gt;File Size&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;55&lt;/td&gt;\n&lt;td align=\"left\"&gt;1m&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.7gb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;56&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.6m&lt;/td&gt;\n&lt;td align=\"left\"&gt;529mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.9m&lt;/td&gt;\n&lt;td align=\"left\"&gt;61mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.9m&lt;/td&gt;\n&lt;td align=\"left\"&gt;365mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.1m&lt;/td&gt;\n&lt;td align=\"left\"&gt;56mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.5m&lt;/td&gt;\n&lt;td align=\"left\"&gt;7mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;ZSTD&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;3m&lt;/td&gt;\n&lt;td align=\"left\"&gt;450mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.57m&lt;/td&gt;\n&lt;td align=\"left\"&gt;319mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.9m&lt;/td&gt;\n&lt;td align=\"left\"&gt;44mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.7m&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.4mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.7m&lt;/td&gt;\n&lt;td align=\"left\"&gt;219mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.6m&lt;/td&gt;\n&lt;td align=\"left\"&gt;30mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.7m&lt;/td&gt;\n&lt;td align=\"left\"&gt;3mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;SNAPPY&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.7m&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;SNAPPY&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.1m&lt;/td&gt;\n&lt;td align=\"left\"&gt;38mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;AVRO&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.7m&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.65mb&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I would&amp;#39;ve loved to also test PARQUET with BZ4 compression since RedShift claims that it can do concurrent reads against it, but nothing I&amp;#39;m familiar with seem to be able to write with that.&lt;/p&gt;\n\n&lt;p&gt;The only thing meaningfully outperforming a single UNCOMPRESSED CSV seem to be GZIP PARQUET split to the same number of files as slices.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else benchmarked formats and compression on import speed? I would&amp;#39;ve expected better results switching away from the most naive solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12br5ab", "is_robot_indexable": true, "report_reasons": null, "author": "kitsunde", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12br5ab/copy_to_redshift_seems_negatively_affected_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12br5ab/copy_to_redshift_seems_negatively_affected_by/", "subreddit_subscribers": 96068, "created_utc": 1680631382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sync Autotuner Reduced Our EMR Cost by 25%", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": false, "name": "t3_12bqpb1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ka4pwfiQcQw23bE1Og9iY9mIir1DTA-LFy9iqMx35fQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680630481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/insiderengineering/sync-autotuner-reduced-our-amazon-emr-cost-by-25-percent-2412d168b3e7", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?auto=webp&amp;v=enabled&amp;s=165734a513856dcbcfca1dcedfd75ba343f393af", "width": 1200, "height": 764}, "resolutions": [{"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b740836ea9f7dff60cc2d599ddbd76e848b33b5", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69b26db2206345170f9f91911e8b0e42dfb61129", "width": 216, "height": 137}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75539abe1d1b05761927687a5ab3847b6f5c72fe", "width": 320, "height": 203}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6b463b6933f9b529cb83f6bb404a2ac5db5874a", "width": 640, "height": 407}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=834c8ab069beab301c4a571e1259d95c204f1688", "width": 960, "height": 611}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d7c2f0201e8f2e7e9b238ada9b4a8420354c1b8", "width": 1080, "height": 687}], "variants": {}, "id": "zlUxK1rEQ-lVb4Dfo0VPiK5bI-Z5MBhCuLwGXfR83-M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12bqpb1", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bqpb1/sync_autotuner_reduced_our_emr_cost_by_25/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/insiderengineering/sync-autotuner-reduced-our-amazon-emr-cost-by-25-percent-2412d168b3e7", "subreddit_subscribers": 96068, "created_utc": 1680630481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to transfer all new ANR events into Redis DB. Is it possibile to trigger Google CF for each new Firebase ANR event?", "author_fullname": "t2_j32k9s23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google CF triggered by Firebase ANR events", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cdal8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680699432.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680683216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to transfer all new ANR events into Redis DB. Is it possibile to trigger Google CF for each new Firebase ANR event?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cdal8", "is_robot_indexable": true, "report_reasons": null, "author": "Away_Efficiency_5837", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cdal8/google_cf_triggered_by_firebase_anr_events/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cdal8/google_cf_triggered_by_firebase_anr_events/", "subreddit_subscribers": 96068, "created_utc": 1680683216.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}