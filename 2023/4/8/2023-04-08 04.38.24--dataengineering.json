{"kind": "Listing", "data": {"after": "t3_12epsbd", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hf9ddayj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineers processing data access requests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": false, "name": "t3_12ekdv2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 204, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 204, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PwQFpnI6ohAOk8a2oHExRt5VPPNiNCh0lB3wSKRXgPM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680872041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/fvhpekkylgsa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/fvhpekkylgsa1.png?auto=webp&amp;v=enabled&amp;s=3d488123b490adb09b02b89bf0062491807eb3a7", "width": 1566, "height": 1096}, "resolutions": [{"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e723ffde2298b53be5040ca50cd3fa857d8cddd6", "width": 108, "height": 75}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dad4b60ee7905425f32c05d87fc14462894faa9b", "width": 216, "height": 151}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4acec0efae2a9d17c5d774ba1e561901f29f158e", "width": 320, "height": 223}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2933de1a74c8bd7130551f9d2351a29612fe36d", "width": 640, "height": 447}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c972504627d1b0844729ccf54b5b5183bcc45c04", "width": 960, "height": 671}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba3a61733592d6c7ee91b71476a7fd4e9fb8c9aa", "width": 1080, "height": 755}], "variants": {}, "id": "DY6enitmFwWz8hoYEQaE_nDciegvG14Wmu3A3m0BEUU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12ekdv2", "is_robot_indexable": true, "report_reasons": null, "author": "Bart_Vee", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ekdv2/data_engineers_processing_data_access_requests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/fvhpekkylgsa1.png", "subreddit_subscribers": 96738, "created_utc": 1680872041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The title says everything.Just find Apache Airflow fascinating but I cannot run it locally. Any idea where I can experiment it on the cloud? Almost every other post in this sub is on Airflow/Dagster.   \n\n\nFor example - We have replit to experiment on Python or other programming languages, so is there a similar infra for Airflow/Dagster?", "author_fullname": "t2_6hp3gp78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where can I experiment with Apache Airflow without running it locally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ecrzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680850948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title says everything.Just find Apache Airflow fascinating but I cannot run it locally. Any idea where I can experiment it on the cloud? Almost every other post in this sub is on Airflow/Dagster.   &lt;/p&gt;\n\n&lt;p&gt;For example - We have replit to experiment on Python or other programming languages, so is there a similar infra for Airflow/Dagster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ecrzm", "is_robot_indexable": true, "report_reasons": null, "author": "rohetoric", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ecrzm/where_can_i_experiment_with_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ecrzm/where_can_i_experiment_with_apache_airflow/", "subreddit_subscribers": 96738, "created_utc": 1680850948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've got large file, about 20GB as a parquet and 300GB as an unsorted csv. I'd like to sort by a field and save it as a single csv file. I'm using an ml.r5.12xlarge sagemaker instance to read the parquet to a dataframe, sort, coalesce to a single partition, and save as csv. I thought I might hit problems when I coalesced, but it's actually choking during the sort. I've tried it filtering the dataset to a quarter of the size, but it still chokes.\n\nSuggestions?", "author_fullname": "t2_6fylz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I sort a massive (300 GB) csv with Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12er9ut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680885771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got large file, about 20GB as a parquet and 300GB as an unsorted csv. I&amp;#39;d like to sort by a field and save it as a single csv file. I&amp;#39;m using an ml.r5.12xlarge sagemaker instance to read the parquet to a dataframe, sort, coalesce to a single partition, and save as csv. I thought I might hit problems when I coalesced, but it&amp;#39;s actually choking during the sort. I&amp;#39;ve tried it filtering the dataset to a quarter of the size, but it still chokes.&lt;/p&gt;\n\n&lt;p&gt;Suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12er9ut", "is_robot_indexable": true, "report_reasons": null, "author": "Nooooope", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12er9ut/how_do_i_sort_a_massive_300_gb_csv_with_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12er9ut/how_do_i_sort_a_massive_300_gb_csv_with_spark/", "subreddit_subscribers": 96738, "created_utc": 1680885771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm just an average (perhaps below average) data engineer. I have read many posts in this subreddit daily (other sources also) and have seen many visions from you guys. I want to contribute something, but have nothing to share. So I share this article (I have just read that minutes ago):  \n[https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c](https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c)  \nAt first I was mind-blown, and thought of creating a repo built on top ChatGPT to extract, transform data. My dream collapsed right afterwards, after knowing the existence of AWS Aurora.[https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/](https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/)  \nHowever, i'm still get excited due to this. If the data is relatively simply structured, this would be extremely handy. Otherwise, ETL/ELT pipelines are still needed. Other pros and cons of zero-ETL are also mentioned in the above towardsdatascience article.  \nWhat are your thoughts about this?", "author_fullname": "t2_4exoz3kf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The future of less ETL-data-pipeline building", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12eg4ty", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680862065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just an average (perhaps below average) data engineer. I have read many posts in this subreddit daily (other sources also) and have seen many visions from you guys. I want to contribute something, but have nothing to share. So I share this article (I have just read that minutes ago):&lt;br/&gt;\n&lt;a href=\"https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c\"&gt;https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&lt;/a&gt;&lt;br/&gt;\nAt first I was mind-blown, and thought of creating a repo built on top ChatGPT to extract, transform data. My dream collapsed right afterwards, after knowing the existence of AWS Aurora.&lt;a href=\"https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/\"&gt;https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/&lt;/a&gt;&lt;br/&gt;\nHowever, i&amp;#39;m still get excited due to this. If the data is relatively simply structured, this would be extremely handy. Otherwise, ETL/ELT pipelines are still needed. Other pros and cons of zero-ETL are also mentioned in the above towardsdatascience article.&lt;br/&gt;\nWhat are your thoughts about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?auto=webp&amp;v=enabled&amp;s=ec51b72c1edf8ba4f8dc0eb8bbd0887c5a10bbdd", "width": 1200, "height": 543}, "resolutions": [{"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1e8c9fd48223781443d8c46be48ab920179e2b7", "width": 108, "height": 48}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=382e0f4f2c49bf8cf0e3b3aa4d813e621e21cb1b", "width": 216, "height": 97}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ddedb3f7dfc408418a63076bae0756ac336c924", "width": 320, "height": 144}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e8423f80288e2ce8684080fb040aba5768a2f39", "width": 640, "height": 289}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db543297d3c099db0127a0b2f21eaec818bce132", "width": 960, "height": 434}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3176ee720d2a0710d0cbc971054c316ad001141", "width": 1080, "height": 488}], "variants": {}, "id": "NUV7jTZprOTDG09SohaAm32t7mHuSpgMRI6ybLu37-E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12eg4ty", "is_robot_indexable": true, "report_reasons": null, "author": "duohd", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12eg4ty/the_future_of_less_etldatapipeline_building/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12eg4ty/the_future_of_less_etldatapipeline_building/", "subreddit_subscribers": 96738, "created_utc": 1680862065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m not a data engineer so please don\u2019t derail thread if I\u2019m using the wrong terms\n\n but I am having to re do a large data pipeline that moves data between redshift , databrix, and our front end ui\n\nI basically need a way to check if the data is being pulled thru correctly and the transformations are being done correctly\n\nI\u2019m sure you data engineers have ways to QA this in a structured way.\n\nCould you please either give me an online resource to read or the correct terms to Google so I can find it myself :)?\n\nThanks In advance", "author_fullname": "t2_7jjttbji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resource for creating a QA testing plan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ez3ca", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680901037.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m not a data engineer so please don\u2019t derail thread if I\u2019m using the wrong terms&lt;/p&gt;\n\n&lt;p&gt;but I am having to re do a large data pipeline that moves data between redshift , databrix, and our front end ui&lt;/p&gt;\n\n&lt;p&gt;I basically need a way to check if the data is being pulled thru correctly and the transformations are being done correctly&lt;/p&gt;\n\n&lt;p&gt;I\u2019m sure you data engineers have ways to QA this in a structured way.&lt;/p&gt;\n\n&lt;p&gt;Could you please either give me an online resource to read or the correct terms to Google so I can find it myself :)?&lt;/p&gt;\n\n&lt;p&gt;Thanks In advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ez3ca", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Recipe-38", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ez3ca/resource_for_creating_a_qa_testing_plan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ez3ca/resource_for_creating_a_qa_testing_plan/", "subreddit_subscribers": 96738, "created_utc": 1680901037.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title says it all", "author_fullname": "t2_vm5kbtxr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is dbt + athena/presto a viable stack? how do you orchestrate creation and maintenance of your athena/presto tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ee6bw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680855655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title says it all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ee6bw", "is_robot_indexable": true, "report_reasons": null, "author": "srevolve", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ee6bw/is_dbt_athenapresto_a_viable_stack_how_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ee6bw/is_dbt_athenapresto_a_viable_stack_how_do_you/", "subreddit_subscribers": 96738, "created_utc": 1680855655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What I Don't Want To See In The Data World In 5 Years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 61, "top_awarded_type": null, "hide_score": false, "name": "t3_12el758", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9_HjsHTlAulDskKkZGvs_BmLnOsisdmtkTax-6ZqajA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680873784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "seattledataguy.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://seattledataguy.substack.com/p/what-i-dont-want-to-see-in-the-data", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?auto=webp&amp;v=enabled&amp;s=a6131fd83abd457f466d76cc4cfb5c2cbdcb8757", "width": 1200, "height": 530}, "resolutions": [{"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a405b03b1770f1ed5165c561229970ed093f6ab6", "width": 108, "height": 47}, {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0de727292d78f7c60a0b11a9371f6d7e43037c6b", "width": 216, "height": 95}, {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f3280f08c8675e6c2e2443ac8c9ee0635a28776", "width": 320, "height": 141}, {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a59a467376400f564838cdcab402c593a327bf6e", "width": 640, "height": 282}, {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b4a9a8ffb25ae89a2d7142f3204946bcd630b26", "width": 960, "height": 424}, {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3891ef64193dd18a9f484a17b1b0ce7bbc27bf26", "width": 1080, "height": 477}], "variants": {}, "id": "uA_trWBusPbOF2d9apwPVAwzE_UX45bICkDcvh2Z0Vc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12el758", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12el758/what_i_dont_want_to_see_in_the_data_world_in_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://seattledataguy.substack.com/p/what-i-dont-want-to-see-in-the-data", "subreddit_subscribers": 96738, "created_utc": 1680873784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been a DE now for about 5 years working at startups, FAANG, and medium sized companies. I'm currently a Sr DE at medium sized company and honestly feel I've plateaued in terms of technical skills. I've worked with all the big data frameworks (Kafka, Spark, Airflow, etc) in a managed setting, meaning Databricks spark, Confluence Kafka, Astronomer Airflow. I honestly don't know if it's worth me investing time into Kubernetes to actually deploy all these things internally. My goal is to switch careers and transition into a ML Engineer focusing on infrastructure. I think the best course of action in the short-term would be to transition into a backend Software Engineer. Anyone have thoughts or has gone through a similar situation in their data engineering career?\n\n[View Poll](https://www.reddit.com/poll/12f47e2)", "author_fullname": "t2_hffo35vl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting to plateau", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12f47e2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680911403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been a DE now for about 5 years working at startups, FAANG, and medium sized companies. I&amp;#39;m currently a Sr DE at medium sized company and honestly feel I&amp;#39;ve plateaued in terms of technical skills. I&amp;#39;ve worked with all the big data frameworks (Kafka, Spark, Airflow, etc) in a managed setting, meaning Databricks spark, Confluence Kafka, Astronomer Airflow. I honestly don&amp;#39;t know if it&amp;#39;s worth me investing time into Kubernetes to actually deploy all these things internally. My goal is to switch careers and transition into a ML Engineer focusing on infrastructure. I think the best course of action in the short-term would be to transition into a backend Software Engineer. Anyone have thoughts or has gone through a similar situation in their data engineering career?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/12f47e2\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12f47e2", "is_robot_indexable": true, "report_reasons": null, "author": "domestic_protobuf", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1681343403028, "options": [{"text": "Learn Kubernetes", "id": "22451396"}, {"text": "Switch to Software Engineering", "id": "22451397"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 120, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12f47e2/starting_to_plateau/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/12f47e2/starting_to_plateau/", "subreddit_subscribers": 96738, "created_utc": 1680911403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Context**\n\nWorking at startup, where I own the entire analytics function - it's a blast, but I'm very time constrained at the moment.\n\nI've been using Redshift in an analyst capacity on and off for 10 years and I have considerable experience creating my own Python+S3+Redshift pipelines (e.g. frustrated analyst turning into data engineer).\n\nI don't have as much experience with some of the other AWS DE-related offerings (aside from tinkering here and there).\n\n&amp;#x200B;\n\n**Dilemma**\n\nI have what I thought would be a fairly routine. Perhaps not simple, but nothing unorthodox either and the data is structured well. I thought.\n\nI mainly am looking for some guidance on if I'm off track, making a simple error, using the wrong tool/approach  or anything else? I'm fine doggedly getting to the solution if I know I'm on track, but I'm beginning to wonder if I have some faulty thinking.\n\n&amp;#x200B;\n\n**Details**\n\nWe license a data set which consists of 18 tables.  \nUnless otherwise noted, all \"files\" here are received as gzipped JSONL.   \nWe receive the following:\n\n1. One time snapshot/data dump of each table as a single file (in S3)\n2. Daily updates\n   1. 0-18 delta files each day (1 file/table, but may not be a file the source table hasn't been modified). These files include both new records to create and updates to existing records.\n      1. Fairly standard directory structure where each day has a directory nested as /YYYY/MM/DD/ containng up to 18 data files named using the convention: ***table\\_name***\\_deltas\\_***YYYYMMDD.json.gz***\n      2. The delta files are not materially different from the original snapshot files... there are just a lot fewer rows.\n   2. 1 file containing records to delete plus which table that record is in (e.g. *deleted\\_id,* *deleted\\_from\\_table\\_name).* This also contains records to *merge*, but for reason I'll note in a moment, let's just ignore that for now.\n   3. 1 \"manifest\" file that contains the following for each file in bullet 2.1\n      1. Filename\n      2. \\# total records in file\n      3. \\# new records\n      4. \\# update existing record\n\n&amp;#x200B;\n\nAt the point, I am not trying to deal with any transformation/deletes/merges etc. I simply want to take all of the files an continue appending each daily delta. I was planning to do the same and to create 2 more tables to track the contents of the manifest and delete/merge files. \n\n&amp;#x200B;\n\nMy logic was that if I can just get all the records into Redshift I'll at quickly check facts for certain records or group of records. I don't need all of the updates/merges/deletes neatly reconciled into the final state for each record.\n\nI'd also prefer to keep that logic in dbt/Redshift, with a clear audit trail of any edits.\n\n&amp;#x200B;\n\n**Questions**\n\n1. Given the description of the data and the desire to get the files from S3&gt;Redshift.... what would folks recommend here? Is there anything \"out of the box\" option to simply continue appending the daily updates... then I can sort of the rest later as long as I know the complete delta history is there.\n2. I had envisioned Glue as a solution to create a catalog of the files, which would also make it easy to \"re-run\" the entire job when there is the occasional schema change e.g. new column  (which entails a new snapshot data dump, and then picking up with daily deltas from that new snapshot date. In my experience, I could not even get the crawler to recognize the original 18 files to create the snapshot... never mind appending the new records for each day. Am I totally off base with what I was trying to accomplish? Or does this sound more like user error, which could be solved by a little more effort/patience on my part?", "author_fullname": "t2_657ct", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Daily JSONL files in S3 &gt;&gt; ?? &gt;&gt; Redshift... am I on the right track?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12f6old", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680916738.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Working at startup, where I own the entire analytics function - it&amp;#39;s a blast, but I&amp;#39;m very time constrained at the moment.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using Redshift in an analyst capacity on and off for 10 years and I have considerable experience creating my own Python+S3+Redshift pipelines (e.g. frustrated analyst turning into data engineer).&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have as much experience with some of the other AWS DE-related offerings (aside from tinkering here and there).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Dilemma&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have what I thought would be a fairly routine. Perhaps not simple, but nothing unorthodox either and the data is structured well. I thought.&lt;/p&gt;\n\n&lt;p&gt;I mainly am looking for some guidance on if I&amp;#39;m off track, making a simple error, using the wrong tool/approach  or anything else? I&amp;#39;m fine doggedly getting to the solution if I know I&amp;#39;m on track, but I&amp;#39;m beginning to wonder if I have some faulty thinking.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We license a data set which consists of 18 tables.&lt;br/&gt;\nUnless otherwise noted, all &amp;quot;files&amp;quot; here are received as gzipped JSONL.&lt;br/&gt;\nWe receive the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;One time snapshot/data dump of each table as a single file (in S3)&lt;/li&gt;\n&lt;li&gt;Daily updates\n\n&lt;ol&gt;\n&lt;li&gt;0-18 delta files each day (1 file/table, but may not be a file the source table hasn&amp;#39;t been modified). These files include both new records to create and updates to existing records.\n\n&lt;ol&gt;\n&lt;li&gt;Fairly standard directory structure where each day has a directory nested as /YYYY/MM/DD/ containng up to 18 data files named using the convention: &lt;strong&gt;&lt;em&gt;table_name&lt;/em&gt;&lt;/strong&gt;_deltas_&lt;strong&gt;&lt;em&gt;YYYYMMDD.json.gz&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;The delta files are not materially different from the original snapshot files... there are just a lot fewer rows.&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;1 file containing records to delete plus which table that record is in (e.g. &lt;em&gt;deleted_id,&lt;/em&gt; &lt;em&gt;deleted_from_table_name).&lt;/em&gt; This also contains records to &lt;em&gt;merge&lt;/em&gt;, but for reason I&amp;#39;ll note in a moment, let&amp;#39;s just ignore that for now.&lt;/li&gt;\n&lt;li&gt;1 &amp;quot;manifest&amp;quot; file that contains the following for each file in bullet 2.1\n\n&lt;ol&gt;\n&lt;li&gt;Filename&lt;/li&gt;\n&lt;li&gt;# total records in file&lt;/li&gt;\n&lt;li&gt;# new records&lt;/li&gt;\n&lt;li&gt;# update existing record&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;At the point, I am not trying to deal with any transformation/deletes/merges etc. I simply want to take all of the files an continue appending each daily delta. I was planning to do the same and to create 2 more tables to track the contents of the manifest and delete/merge files. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My logic was that if I can just get all the records into Redshift I&amp;#39;ll at quickly check facts for certain records or group of records. I don&amp;#39;t need all of the updates/merges/deletes neatly reconciled into the final state for each record.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d also prefer to keep that logic in dbt/Redshift, with a clear audit trail of any edits.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Given the description of the data and the desire to get the files from S3&amp;gt;Redshift.... what would folks recommend here? Is there anything &amp;quot;out of the box&amp;quot; option to simply continue appending the daily updates... then I can sort of the rest later as long as I know the complete delta history is there.&lt;/li&gt;\n&lt;li&gt;I had envisioned Glue as a solution to create a catalog of the files, which would also make it easy to &amp;quot;re-run&amp;quot; the entire job when there is the occasional schema change e.g. new column  (which entails a new snapshot data dump, and then picking up with daily deltas from that new snapshot date. In my experience, I could not even get the crawler to recognize the original 18 files to create the snapshot... never mind appending the new records for each day. Am I totally off base with what I was trying to accomplish? Or does this sound more like user error, which could be solved by a little more effort/patience on my part?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12f6old", "is_robot_indexable": true, "report_reasons": null, "author": "jslacks", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12f6old/daily_jsonl_files_in_s3_redshift_am_i_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12f6old/daily_jsonl_files_in_s3_redshift_am_i_on_the/", "subreddit_subscribers": 96738, "created_utc": 1680916738.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ll keep this short. As a DE do you ever build server and client applications? Whether you\u2019re collecting custom logs, dealing with RPCs, or whatever. \n\nMaybe you have platform tools provided, or use existing stacks for queues or Kafka or whatever. I\u2019m looking to see your experiences in this area.", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much server and client building do you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12f2hty", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680907865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ll keep this short. As a DE do you ever build server and client applications? Whether you\u2019re collecting custom logs, dealing with RPCs, or whatever. &lt;/p&gt;\n\n&lt;p&gt;Maybe you have platform tools provided, or use existing stacks for queues or Kafka or whatever. I\u2019m looking to see your experiences in this area.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12f2hty", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12f2hty/how_much_server_and_client_building_do_you_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12f2hty/how_much_server_and_client_building_do_you_do/", "subreddit_subscribers": 96738, "created_utc": 1680907865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've 10 numerical and large datasets where each has 3 generic categories. Each row contains unique data. The end row of each dataset contains the labels for each category. The category is not distinct thus other row may refer to any of the 3 categories.\n\ne.g. Dataset A\n\n&amp;#x200B;\n\n|Date|Value|Category|\n|:-|:-|:-|\n|1/1/2010|1.11111|Alpha|\n|2/1/2010|2.11111|Beta|\n|3/1/2010|2.00009|Alpha|\n|4/1/2010|0.00000|Charlie|\n\nBut the 10 datasets have different volume of data. E.g.  dataset A may have 10K rows, dataset B around 100K, Dataset C 1 million, etc.\n\nI couldn't process all the data as its too large.\n\nWhat would be the best way to sample each dataset? I'd like the sample containing a fair representative of the 3 categories.", "author_fullname": "t2_3z6gqvrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to represent large categorical data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12egjxl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680863159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve 10 numerical and large datasets where each has 3 generic categories. Each row contains unique data. The end row of each dataset contains the labels for each category. The category is not distinct thus other row may refer to any of the 3 categories.&lt;/p&gt;\n\n&lt;p&gt;e.g. Dataset A&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Date&lt;/th&gt;\n&lt;th align=\"left\"&gt;Value&lt;/th&gt;\n&lt;th align=\"left\"&gt;Category&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.11111&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alpha&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.11111&lt;/td&gt;\n&lt;td align=\"left\"&gt;Beta&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.00009&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alpha&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.00000&lt;/td&gt;\n&lt;td align=\"left\"&gt;Charlie&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;But the 10 datasets have different volume of data. E.g.  dataset A may have 10K rows, dataset B around 100K, Dataset C 1 million, etc.&lt;/p&gt;\n\n&lt;p&gt;I couldn&amp;#39;t process all the data as its too large.&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to sample each dataset? I&amp;#39;d like the sample containing a fair representative of the 3 categories.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12egjxl", "is_robot_indexable": true, "report_reasons": null, "author": "runnersgo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12egjxl/how_to_represent_large_categorical_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12egjxl/how_to_represent_large_categorical_data/", "subreddit_subscribers": 96738, "created_utc": 1680863159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The main objective was to scale down redshift nodes.\n\nSo i deleted the data from the cluster to free up one node.\n\nI don't have complete acces to the data warehouse so my task was to just delete the data.\n\nNow we will be scaling it down.\n\nI just want to make sure of some points:\n* Do we have to move data from one node to another to delete. \nI read multiple articles some stated that you would have to redistribute the data from the deleting to other nodes manually.\n\nOthers stated while scaling down the Redshift will take care of redsitributon itself.\n\nI just want to make sure that is there any action require from my side.\n\n\n* Scaling Down :\n\nWe can scale down using queries and UI  both, right?\n\nWhat practice are the best?\n\nI read that we can only scale up and down in a pair i.e we can go 2 up or 2 down. While i read somewhere that we can go as we choose.\n\n* While Scaling Down: \nWhat things to keep in mind while scaling down.\n\nI can stop all the etl pipelines or any transaction that will be happening while scaling down.\n\nI read while scaling down the all transactional queries are put on hold, but it would be better to stop them while scaling down, i think.\n\nPlease let me know any thing i have to keep in mind while scaling down.", "author_fullname": "t2_l38csc3b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scaling Down AWS Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12faj3u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680925921.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The main objective was to scale down redshift nodes.&lt;/p&gt;\n\n&lt;p&gt;So i deleted the data from the cluster to free up one node.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have complete acces to the data warehouse so my task was to just delete the data.&lt;/p&gt;\n\n&lt;p&gt;Now we will be scaling it down.&lt;/p&gt;\n\n&lt;p&gt;I just want to make sure of some points:\n* Do we have to move data from one node to another to delete. \nI read multiple articles some stated that you would have to redistribute the data from the deleting to other nodes manually.&lt;/p&gt;\n\n&lt;p&gt;Others stated while scaling down the Redshift will take care of redsitributon itself.&lt;/p&gt;\n\n&lt;p&gt;I just want to make sure that is there any action require from my side.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Scaling Down :&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We can scale down using queries and UI  both, right?&lt;/p&gt;\n\n&lt;p&gt;What practice are the best?&lt;/p&gt;\n\n&lt;p&gt;I read that we can only scale up and down in a pair i.e we can go 2 up or 2 down. While i read somewhere that we can go as we choose.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;While Scaling Down: \nWhat things to keep in mind while scaling down.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I can stop all the etl pipelines or any transaction that will be happening while scaling down.&lt;/p&gt;\n\n&lt;p&gt;I read while scaling down the all transactional queries are put on hold, but it would be better to stop them while scaling down, i think.&lt;/p&gt;\n\n&lt;p&gt;Please let me know any thing i have to keep in mind while scaling down.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12faj3u", "is_robot_indexable": true, "report_reasons": null, "author": "AdSure744", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12faj3u/scaling_down_aws_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12faj3u/scaling_down_aws_redshift/", "subreddit_subscribers": 96738, "created_utc": 1680925921.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nJust wondering what the general consensus is about strings in fact tables when it comes to things like Dedicated Pool or Redshift.\n\nTraditionally I would set the Order Type, Order Status etc. to the FK's that join to the dimension.  When it comes to MPP I'm wondering if I should store those fields as strings instead.  My reasons being:\n\n1) Reduce the need for users to do 2/3/4 extra joins to get descriptions (improving query speed)\n2) Following on from 1, reducing users filtering on the above join table fields, rather than on the fact directly\n3) When doing live queries from Tableau I'd hope the estimates would be more accurate\n\nIs this reasonable?  Or am I best just to test it thoroughly?\n\nTIA \ud83d\ude42", "author_fullname": "t2_9h6gf9pp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dedicated Pool Fact Table: Do you allow strings?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12etz55", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680890982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Just wondering what the general consensus is about strings in fact tables when it comes to things like Dedicated Pool or Redshift.&lt;/p&gt;\n\n&lt;p&gt;Traditionally I would set the Order Type, Order Status etc. to the FK&amp;#39;s that join to the dimension.  When it comes to MPP I&amp;#39;m wondering if I should store those fields as strings instead.  My reasons being:&lt;/p&gt;\n\n&lt;p&gt;1) Reduce the need for users to do 2/3/4 extra joins to get descriptions (improving query speed)\n2) Following on from 1, reducing users filtering on the above join table fields, rather than on the fact directly\n3) When doing live queries from Tableau I&amp;#39;d hope the estimates would be more accurate&lt;/p&gt;\n\n&lt;p&gt;Is this reasonable?  Or am I best just to test it thoroughly?&lt;/p&gt;\n\n&lt;p&gt;TIA \ud83d\ude42&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12etz55", "is_robot_indexable": true, "report_reasons": null, "author": "V10Matt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12etz55/dedicated_pool_fact_table_do_you_allow_strings/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12etz55/dedicated_pool_fact_table_do_you_allow_strings/", "subreddit_subscribers": 96738, "created_utc": 1680890982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nHello guys,\nCan anyone point me to a place where I can learn how to extract specie row-data with row level security from SQL Db to storage container  using data factory ?", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Row level security in ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ergr0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680886138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,\nCan anyone point me to a place where I can learn how to extract specie row-data with row level security from SQL Db to storage container  using data factory ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ergr0", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ergr0/row_level_security_in_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ergr0/row_level_security_in_adf/", "subreddit_subscribers": 96738, "created_utc": 1680886138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Good day, I am a data engineer and have been asked to be a part of panel for interviewing director level candidates. They will oversee the data engineering and devsecops teams. The candidate will be given a technical prompt and they will be required to discuss a recent project they led. This is the first time I will be interviewing a senior level candidate. What questions can I ask the candidate for a meaningful assessment and experience for all of us?", "author_fullname": "t2_77mz0n8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions to ask a leadership candidate as a DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12erf91", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680886057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good day, I am a data engineer and have been asked to be a part of panel for interviewing director level candidates. They will oversee the data engineering and devsecops teams. The candidate will be given a technical prompt and they will be required to discuss a recent project they led. This is the first time I will be interviewing a senior level candidate. What questions can I ask the candidate for a meaningful assessment and experience for all of us?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12erf91", "is_robot_indexable": true, "report_reasons": null, "author": "Programmer_Virtual", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12erf91/questions_to_ask_a_leadership_candidate_as_a_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12erf91/questions_to_ask_a_leadership_candidate_as_a_de/", "subreddit_subscribers": 96738, "created_utc": 1680886057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've created a solution based on these articles:\n\n* Theory: [https://www.redhat.com/en/blog/why-spark-ceph-part-1-3](https://www.redhat.com/en/blog/why-spark-ceph-part-1-3)\n* Implementation: [https://radanalytics.io/examples/ceph-source-example](https://radanalytics.io/examples/ceph-source-example)\n\nIt has been a messy journey, but so far it seems like I've gotten to the last step - I can query txt, csv and other files, but I'm still struggling querying the actual sqlite3 file on Ceph.\n\nHas anyone done anything similar in this regard to give some general tips and tricks, best practices and so on?\n\n*Note: This is more of a generic discussion thread, but if you'd like to help me the error I'm facing, I have a post on Stackoverflow:* [*https://stackoverflow.com/questions/75959520/querying-an-sqlite3-file-on-ceph-via-s3a-using-pyspark-requirement-failed-the*](https://stackoverflow.com/questions/75959520/querying-an-sqlite3-file-on-ceph-via-s3a-using-pyspark-requirement-failed-the)", "author_fullname": "t2_sy57joj0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Spark to query Sqlite3 files stored on Ceph", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12erdiz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680885968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve created a solution based on these articles:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Theory: &lt;a href=\"https://www.redhat.com/en/blog/why-spark-ceph-part-1-3\"&gt;https://www.redhat.com/en/blog/why-spark-ceph-part-1-3&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Implementation: &lt;a href=\"https://radanalytics.io/examples/ceph-source-example\"&gt;https://radanalytics.io/examples/ceph-source-example&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It has been a messy journey, but so far it seems like I&amp;#39;ve gotten to the last step - I can query txt, csv and other files, but I&amp;#39;m still struggling querying the actual sqlite3 file on Ceph.&lt;/p&gt;\n\n&lt;p&gt;Has anyone done anything similar in this regard to give some general tips and tricks, best practices and so on?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Note: This is more of a generic discussion thread, but if you&amp;#39;d like to help me the error I&amp;#39;m facing, I have a post on Stackoverflow:&lt;/em&gt; &lt;a href=\"https://stackoverflow.com/questions/75959520/querying-an-sqlite3-file-on-ceph-via-s3a-using-pyspark-requirement-failed-the\"&gt;&lt;em&gt;https://stackoverflow.com/questions/75959520/querying-an-sqlite3-file-on-ceph-via-s3a-using-pyspark-requirement-failed-the&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aJKUjlRZOQ5zCI_oiLgRfvnrBPEJ42leZbocgCpUCNY.jpg?auto=webp&amp;v=enabled&amp;s=05c8470dbdcfd7cd0dfd7e4527cdd4602cc774cf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/aJKUjlRZOQ5zCI_oiLgRfvnrBPEJ42leZbocgCpUCNY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2137cbf420f60b1525ac7e09bfa9a8a3f4c22186", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/aJKUjlRZOQ5zCI_oiLgRfvnrBPEJ42leZbocgCpUCNY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd4a05a9750b0dc811783b83485cb4dafaea0f80", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/aJKUjlRZOQ5zCI_oiLgRfvnrBPEJ42leZbocgCpUCNY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=107428a18928923d74b2c4855e3a7b4667e6edf7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/aJKUjlRZOQ5zCI_oiLgRfvnrBPEJ42leZbocgCpUCNY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db5cd310b5df37671aadef3563fb49bca6fe0a3d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/aJKUjlRZOQ5zCI_oiLgRfvnrBPEJ42leZbocgCpUCNY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=676043fceccc799c8bc827d7c767343abad586c9", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/aJKUjlRZOQ5zCI_oiLgRfvnrBPEJ42leZbocgCpUCNY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=804865a5b207b480f68a1161cf196cd208f881d6", "width": 1080, "height": 567}], "variants": {}, "id": "rpz7XRvvl4-yX8tMXzyfnMqpwkzXT4I2njPuoYdvEls"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12erdiz", "is_robot_indexable": true, "report_reasons": null, "author": "pioneeringwork", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12erdiz/using_spark_to_query_sqlite3_files_stored_on_ceph/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12erdiz/using_spark_to_query_sqlite3_files_stored_on_ceph/", "subreddit_subscribers": 96738, "created_utc": 1680885968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all - does using Delta Sharing cost anything as a Databricks customer? I would want to know the answer for multiple use cases (DBX-to-DBX internal, DBX-to-DBX external or another organization but same VPC, and DBX-to-DBX external different VPC or CSP altogether).   \n\n\nThanks so much.", "author_fullname": "t2_4oqqusfb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Sharing Cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12epuyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680883088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all - does using Delta Sharing cost anything as a Databricks customer? I would want to know the answer for multiple use cases (DBX-to-DBX internal, DBX-to-DBX external or another organization but same VPC, and DBX-to-DBX external different VPC or CSP altogether).   &lt;/p&gt;\n\n&lt;p&gt;Thanks so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12epuyd", "is_robot_indexable": true, "report_reasons": null, "author": "Foreign_Magician_429", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12epuyd/delta_sharing_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12epuyd/delta_sharing_cost/", "subreddit_subscribers": 96738, "created_utc": 1680883088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering! I've been contributing to an open-source project in the data lineage domain, and I'd really like to know more about the challenges you face in your day-to-day work.\n\nSo, let's discuss! What do you find most troublesome about data lineage? Are you struggling with tracking data dependencies, compliance, testing, or maybe something else entirely?", "author_fullname": "t2_i0qyphvw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fellow Data Engineers, Share Your Data Lineage Struggles!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ep3pl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680881592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;! I&amp;#39;ve been contributing to an open-source project in the data lineage domain, and I&amp;#39;d really like to know more about the challenges you face in your day-to-day work.&lt;/p&gt;\n\n&lt;p&gt;So, let&amp;#39;s discuss! What do you find most troublesome about data lineage? Are you struggling with tracking data dependencies, compliance, testing, or maybe something else entirely?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ep3pl", "is_robot_indexable": true, "report_reasons": null, "author": "ProfessionalHorse707", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ep3pl/fellow_data_engineers_share_your_data_lineage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ep3pl/fellow_data_engineers_share_your_data_lineage/", "subreddit_subscribers": 96738, "created_utc": 1680881592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data scientist within a small data science team. We're currently in the midst of a transition to cloud, but due to regulatory mess we cannot actually switch within the next three years. We're kind of an oddball out of the bunch, as the other IT teams focus in making microservices for the organization and don't care much about what we do. 90% of the organization is not IT, so it's not like they hand us the best tools.\n\nMeanwhile, we kind of need a robust development environment that data scientists can use to make more 'production-ready' products. Right now, DEs kind of have to make major adjustments to fit everything in docker containers and have to deal with the issue of debugging things in different environments than it was developed in (Windows Server 2019 vs Linux in the container). Also there's an issue where if someone is running a model (let's say XGB) it literally freezes the session of other colleagues logged into the remote desktop session (yikes!). \n\nWhat's currently a 'good' practice in terms of setting up a development environment? I've looked into Hyper-V or something like Proxmox  and starting up a simple CLI debian per project and using remote explorer and/or dev containers using VSCode. It's a major requirement to have RBAC (Role based access control) per project. If it's indeed not a crazy solution, how would it work so that a DBA can quickly give access to tables for that project in that environment?", "author_fullname": "t2_607hu6ywi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On premise development environment for data scientists. Best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12eozr9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680881394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data scientist within a small data science team. We&amp;#39;re currently in the midst of a transition to cloud, but due to regulatory mess we cannot actually switch within the next three years. We&amp;#39;re kind of an oddball out of the bunch, as the other IT teams focus in making microservices for the organization and don&amp;#39;t care much about what we do. 90% of the organization is not IT, so it&amp;#39;s not like they hand us the best tools.&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, we kind of need a robust development environment that data scientists can use to make more &amp;#39;production-ready&amp;#39; products. Right now, DEs kind of have to make major adjustments to fit everything in docker containers and have to deal with the issue of debugging things in different environments than it was developed in (Windows Server 2019 vs Linux in the container). Also there&amp;#39;s an issue where if someone is running a model (let&amp;#39;s say XGB) it literally freezes the session of other colleagues logged into the remote desktop session (yikes!). &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s currently a &amp;#39;good&amp;#39; practice in terms of setting up a development environment? I&amp;#39;ve looked into Hyper-V or something like Proxmox  and starting up a simple CLI debian per project and using remote explorer and/or dev containers using VSCode. It&amp;#39;s a major requirement to have RBAC (Role based access control) per project. If it&amp;#39;s indeed not a crazy solution, how would it work so that a DBA can quickly give access to tables for that project in that environment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12eozr9", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial_Score757", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12eozr9/on_premise_development_environment_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12eozr9/on_premise_development_environment_for_data/", "subreddit_subscribers": 96738, "created_utc": 1680881394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wanted to try out `pyarrow`'s `S3FileSystem` by converting a `boto3` snippet to read gzipped jsonline files (a dump from DynamoDB). Compared to the original, the `pyarrow` one is terribly slow *well before data starts reading JSON into memory*.\n\nThe snippet core logic is this:\n\n```python\nfrom pyarrow import fs\ns3 = fs.S3FileSystem()\n\nfrom mymodule import yield_as_json\n\nmyfile = \"bucket/path/to/dump.json.gz\"\n\nwith s3.open_input_stream(myfile) as stream:\n    data = list(yield_as_json(stream))\n```\n\nWhat's inside `yield_as_json` does not really matter - I believe it works, because the second snippet (below) works. My suspicion is that `pyarrow` reads the whole data in memory, decompresses it and then starts iterating over lines.\n\nBy comparison, the `boto3` snippet is:\n\n```python\nimport boto3\nimport gzip\n\nfrom mymodule import yield_as_json\n\ns3 = boto3.session.Session(**session_kwargs).client(\"s3\")\n\nbucket = \"bucket\"\nkey = \"path/to/dump.json.gz\"\nstream = s3.get_object(Bucket=bucket, Key=key).get(\"Body\")\n\nwith gzip.open(stream, \"rb\") as zipped:\n    data = list(yield_as_json(zipped))\n```\n\nDoes anyone know what's happening, or how I could improve the code?", "author_fullname": "t2_329zuj1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "pyarrow S3FS: reading zipped files is slow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12eoc5a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "245217ea-ac9d-11eb-a81a-0e03519a5d4b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680880088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to try out &lt;code&gt;pyarrow&lt;/code&gt;&amp;#39;s &lt;code&gt;S3FileSystem&lt;/code&gt; by converting a &lt;code&gt;boto3&lt;/code&gt; snippet to read gzipped jsonline files (a dump from DynamoDB). Compared to the original, the &lt;code&gt;pyarrow&lt;/code&gt; one is terribly slow &lt;em&gt;well before data starts reading JSON into memory&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;The snippet core logic is this:&lt;/p&gt;\n\n&lt;p&gt;```python\nfrom pyarrow import fs\ns3 = fs.S3FileSystem()&lt;/p&gt;\n\n&lt;p&gt;from mymodule import yield_as_json&lt;/p&gt;\n\n&lt;p&gt;myfile = &amp;quot;bucket/path/to/dump.json.gz&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;with s3.open_input_stream(myfile) as stream:\n    data = list(yield_as_json(stream))\n```&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s inside &lt;code&gt;yield_as_json&lt;/code&gt; does not really matter - I believe it works, because the second snippet (below) works. My suspicion is that &lt;code&gt;pyarrow&lt;/code&gt; reads the whole data in memory, decompresses it and then starts iterating over lines.&lt;/p&gt;\n\n&lt;p&gt;By comparison, the &lt;code&gt;boto3&lt;/code&gt; snippet is:&lt;/p&gt;\n\n&lt;p&gt;```python\nimport boto3\nimport gzip&lt;/p&gt;\n\n&lt;p&gt;from mymodule import yield_as_json&lt;/p&gt;\n\n&lt;p&gt;s3 = boto3.session.Session(**session_kwargs).client(&amp;quot;s3&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;bucket = &amp;quot;bucket&amp;quot;\nkey = &amp;quot;path/to/dump.json.gz&amp;quot;\nstream = s3.get_object(Bucket=bucket, Key=key).get(&amp;quot;Body&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;with gzip.open(stream, &amp;quot;rb&amp;quot;) as zipped:\n    data = list(yield_as_json(zipped))\n```&lt;/p&gt;\n\n&lt;p&gt;Does anyone know what&amp;#39;s happening, or how I could improve the code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Scientist", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12eoc5a", "is_robot_indexable": true, "report_reasons": null, "author": "BaggiPonte", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12eoc5a/pyarrow_s3fs_reading_zipped_files_is_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12eoc5a/pyarrow_s3fs_reading_zipped_files_is_slow/", "subreddit_subscribers": 96738, "created_utc": 1680880088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We used to use traditional syntax in all our Airflow dags, however, some of the tasks would be much easier to write with the TaskFlow API but on the other hand that would mean mixing two different styles. \n\nWhat is your view on this? Would you go with easier path to develop (mixing both syntax) or cleaner (use exactly one flavor)?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Combining TaskFlow API and Traditional syntax in Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12emkq6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680876642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We used to use traditional syntax in all our Airflow dags, however, some of the tasks would be much easier to write with the TaskFlow API but on the other hand that would mean mixing two different styles. &lt;/p&gt;\n\n&lt;p&gt;What is your view on this? Would you go with easier path to develop (mixing both syntax) or cleaner (use exactly one flavor)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12emkq6", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12emkq6/combining_taskflow_api_and_traditional_syntax_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12emkq6/combining_taskflow_api_and_traditional_syntax_in/", "subreddit_subscribers": 96738, "created_utc": 1680876642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello  \nI am working at consulting company as BI developer (mainly PowerBI but also a Qliksense), however recently I have been roll off from the project due to project freeze and now siting on a bench. I've been told that I'll be having an interview next week Tue/Wed on data engineering role with AWS. My cloud \"stack\" is Azure - AZ-900, DP-900, PL-300. I used a little bit of Azure Data Studio and Synapse. Thats pretty much it. I know SQL to the extend I can freely make a view myself with some transformations or to answer some business question but never been \"deeper\" and I am afraid not that I'll not land in that position - it is guaranteed, but that I'll make fool of myself. I know core concepts of DE, but never really used the tools na AWS itself.   \n\n\nAny protips for me? What to prepare if we're talking about bare minimum, to not make a fool out of myself?", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "help me in not making fool of myself", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ekb6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680871879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;br/&gt;\nI am working at consulting company as BI developer (mainly PowerBI but also a Qliksense), however recently I have been roll off from the project due to project freeze and now siting on a bench. I&amp;#39;ve been told that I&amp;#39;ll be having an interview next week Tue/Wed on data engineering role with AWS. My cloud &amp;quot;stack&amp;quot; is Azure - AZ-900, DP-900, PL-300. I used a little bit of Azure Data Studio and Synapse. Thats pretty much it. I know SQL to the extend I can freely make a view myself with some transformations or to answer some business question but never been &amp;quot;deeper&amp;quot; and I am afraid not that I&amp;#39;ll not land in that position - it is guaranteed, but that I&amp;#39;ll make fool of myself. I know core concepts of DE, but never really used the tools na AWS itself.   &lt;/p&gt;\n\n&lt;p&gt;Any protips for me? What to prepare if we&amp;#39;re talking about bare minimum, to not make a fool out of myself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ekb6i", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ekb6i/help_me_in_not_making_fool_of_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ekb6i/help_me_in_not_making_fool_of_myself/", "subreddit_subscribers": 96738, "created_utc": 1680871879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1jkhpl2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to run dynamic Github Action workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_12ejuno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/FZ-EQeupBO47ozfZ4Yhsp-mzoSYWcEYoFigcz_6ap38.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680870879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/35692957ef94", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?auto=webp&amp;v=enabled&amp;s=eef5caba0c98e8021ccceef2330b82bb0c4606f5", "width": 1200, "height": 750}, "resolutions": [{"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a850d3475130a02050eaace31f251748b905d792", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adef17c44871e980d9b4e4473cdd9d8ef5f4a484", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b74f8c19297e4c723220debde72294891846b15b", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd5f85dd858cd8d54a191d6499ade97cfa1dac48", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=242170b34d9b9b44c47ac539d4498063edcfd29e", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=770b2e3cc0ccc9d1a5c21f271a323605672be27d", "width": 1080, "height": 675}], "variants": {}, "id": "corEyKiyBN7xmaYdlI-4JDM9ATZVUIIp5xW9IRgtJB8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ejuno", "is_robot_indexable": true, "report_reasons": null, "author": "Luxi36", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ejuno/how_to_run_dynamic_github_action_workflows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/35692957ef94", "subreddit_subscribers": 96738, "created_utc": 1680870879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!\n\nI've been searching for a while an installer for SpectX (the log analytics tool). Since the company was bought by Dynatrace, it has become impossible to find it online.\n\nI was wondering if anyone has an installer, or even an installed version from which we can extract the specx.jar.\n\nI happen to have a license for it, but no way to use it.", "author_fullname": "t2_8q939c1mp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SpectX", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12eg82n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680862305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been searching for a while an installer for SpectX (the log analytics tool). Since the company was bought by Dynatrace, it has become impossible to find it online.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone has an installer, or even an installed version from which we can extract the specx.jar.&lt;/p&gt;\n\n&lt;p&gt;I happen to have a license for it, but no way to use it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12eg82n", "is_robot_indexable": true, "report_reasons": null, "author": "AdditionalDLL", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12eg82n/spectx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12eg82n/spectx/", "subreddit_subscribers": 96738, "created_utc": 1680862305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vuozxz2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workflow orchestrators: Comparing Metaflow, Kedro, Luigi, Airflow, Flyte, Prefect and others", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_12epsbd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7O2Raqq31tVgokW1i_9qxmlOv59uD82kjm35PDTk6ug.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680882946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dsdaily.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dsdaily.substack.com/p/workflow-orchestrators-metaflow-kedro", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?auto=webp&amp;v=enabled&amp;s=a6a802bb2d7efde54ff0ef2559e66e363bf49a5a", "width": 1018, "height": 598}, "resolutions": [{"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a90eb229993f8276fb3f03b5a2de1277134e270", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7481dd837331da965c18dc15b5976422a51163e", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe6d2538e603e9ff23432a09fc2833ac7a295d1b", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9afb7d19e43b264aa71a446da5da02ffb669588a", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e0c11dd6aa271307b19698104a9c2ca7db2cc184", "width": 960, "height": 563}], "variants": {}, "id": "k_OJwzSNhRaOxxkROZWgnREqC7dSqVJTRM4y-8YAGys"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12epsbd", "is_robot_indexable": true, "report_reasons": null, "author": "RAFisherman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12epsbd/workflow_orchestrators_comparing_metaflow_kedro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dsdaily.substack.com/p/workflow-orchestrators-metaflow-kedro", "subreddit_subscribers": 96738, "created_utc": 1680882946.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}