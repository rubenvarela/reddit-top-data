{"kind": "Listing", "data": {"after": "t3_12qax7w", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My new org is entirely MS/Azure based, and I am pretty much a data team of one. They have no real data warehouse to speak of, just a few MS applications in the cloud and PowerBI. \n\nData volume is quite small (we have around 10k orders per year) and unlikely to grow significantly for at least a couple of years. Beyond Dynamics we have the normal suspects, socials, Google analytics and third party APIs.\n\nAny suggestions for a good solution for a DWH? Budget is tight so I would like to use meltano to dump raw data into the DWH and DBT core to build some gold level tables to serve into PowerBI.\n\nWould a simple MSSQL database do it? Or would a entry tier Synapse instance be a better place to start? Or perhaps even Snowflake?\n\nAny experiences very welcome.\n\nEDIT: amazed by the quality responses here, thanks so much folks.", "author_fullname": "t2_clh5r1ln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for a small DWH on Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12pwc42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681806417.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681764699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My new org is entirely MS/Azure based, and I am pretty much a data team of one. They have no real data warehouse to speak of, just a few MS applications in the cloud and PowerBI. &lt;/p&gt;\n\n&lt;p&gt;Data volume is quite small (we have around 10k orders per year) and unlikely to grow significantly for at least a couple of years. Beyond Dynamics we have the normal suspects, socials, Google analytics and third party APIs.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions for a good solution for a DWH? Budget is tight so I would like to use meltano to dump raw data into the DWH and DBT core to build some gold level tables to serve into PowerBI.&lt;/p&gt;\n\n&lt;p&gt;Would a simple MSSQL database do it? Or would a entry tier Synapse instance be a better place to start? Or perhaps even Snowflake?&lt;/p&gt;\n\n&lt;p&gt;Any experiences very welcome.&lt;/p&gt;\n\n&lt;p&gt;EDIT: amazed by the quality responses here, thanks so much folks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12pwc42", "is_robot_indexable": true, "report_reasons": null, "author": "Far-Restaurant-9691", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12pwc42/recommendations_for_a_small_dwh_on_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12pwc42/recommendations_for_a_small_dwh_on_azure/", "subreddit_subscribers": 100180, "created_utc": 1681764699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve used PySpark for years. I\u2019m just getting into Scala and I already understand functional programming. What are good resources for going deep into Scala and extending my Spark capabilities via Scala?", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From PySpark to Scala", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12q5bi3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681781549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve used PySpark for years. I\u2019m just getting into Scala and I already understand functional programming. What are good resources for going deep into Scala and extending my Spark capabilities via Scala?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12q5bi3", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12q5bi3/from_pyspark_to_scala/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12q5bi3/from_pyspark_to_scala/", "subreddit_subscribers": 100180, "created_utc": 1681781549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently a DE and interested in building a basic working knowledge of machine learning/data science. I don't plan to make a switch to a DS role but see value in adding a basic working knowledge to my skill set.\n\nWhat are some good free/paid resources for this?\n\nThanks!", "author_fullname": "t2_2vuapfhl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for Current DE Interested in Learning Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12pd2hk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681732098.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently a DE and interested in building a basic working knowledge of machine learning/data science. I don&amp;#39;t plan to make a switch to a DS role but see value in adding a basic working knowledge to my skill set.&lt;/p&gt;\n\n&lt;p&gt;What are some good free/paid resources for this?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12pd2hk", "is_robot_indexable": true, "report_reasons": null, "author": "TheShitStorms92", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12pd2hk/resources_for_current_de_interested_in_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12pd2hk/resources_for_current_de_interested_in_learning/", "subreddit_subscribers": 100180, "created_utc": 1681732098.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any use case where we would want to implement multiprocessing or multithreading within an Airflow DAG? If there is such a use case, why can't we simply instantiate a different task instance under the same DAG which runs in parallel? Does this choice also depend on the type of executor you are running Airflow on?\n\nI am asking because I see a lot of multiprocessing logic written in many airflow DAGs in my company (the task logic splits, say, N items among, say, K processes, and then each subprocess processes N/K data items) and I have never understood why it was implemented in the first place. We use the Kubernetes ~~operator~~ executor at our company and the CPU and memory requirements to execute the DAG (with just one task with multiprocessing implemented) are huge. Since only one task is executed, I am assuming the task gets executed on one node/instance with those huge requirements (I have limited knowledge of Kubernetes). My point is why can't we instantiate different task instances and let them run on smaller instances?\n\nOne reason that comes to my mind is that it makes the DAG script cleaner and assigns all the responsibility of splitting the data items to one task. But I am not able to think of anything else apart from this reason.\n\nIt would be great if someone can help me understand this. Thanks for reading.\n\nEDIT: Corrected operator to executor (please see the striked out word).", "author_fullname": "t2_uqx8q0b4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiprocessing/multithreading in an Airflow DAG", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12pc16r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681735088.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681729762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any use case where we would want to implement multiprocessing or multithreading within an Airflow DAG? If there is such a use case, why can&amp;#39;t we simply instantiate a different task instance under the same DAG which runs in parallel? Does this choice also depend on the type of executor you are running Airflow on?&lt;/p&gt;\n\n&lt;p&gt;I am asking because I see a lot of multiprocessing logic written in many airflow DAGs in my company (the task logic splits, say, N items among, say, K processes, and then each subprocess processes N/K data items) and I have never understood why it was implemented in the first place. We use the Kubernetes &lt;del&gt;operator&lt;/del&gt; executor at our company and the CPU and memory requirements to execute the DAG (with just one task with multiprocessing implemented) are huge. Since only one task is executed, I am assuming the task gets executed on one node/instance with those huge requirements (I have limited knowledge of Kubernetes). My point is why can&amp;#39;t we instantiate different task instances and let them run on smaller instances?&lt;/p&gt;\n\n&lt;p&gt;One reason that comes to my mind is that it makes the DAG script cleaner and assigns all the responsibility of splitting the data items to one task. But I am not able to think of anything else apart from this reason.&lt;/p&gt;\n\n&lt;p&gt;It would be great if someone can help me understand this. Thanks for reading.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Corrected operator to executor (please see the striked out word).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12pc16r", "is_robot_indexable": true, "report_reasons": null, "author": "the-fake-me", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12pc16r/multiprocessingmultithreading_in_an_airflow_dag/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12pc16r/multiprocessingmultithreading_in_an_airflow_dag/", "subreddit_subscribers": 100180, "created_utc": 1681729762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHey /r/dataengineering!\n\nRecently retired as a data scientist/engineer.  I figured i ought to at least try to give back to the opensource community.  I've created a Docker app to help data scientists build data projects with pipelines in mind \\*from the start\\*, rather than as Jupyter notebook afterthoughts:\n\n[**https://github.com/jason-brian-anderson/pipeline\\_gen**](https://github.com/jason-brian-anderson/pipeline_gen)\n\nI suspect I wasn't the only one with no clue about good pipeline design principles.  I customized the base Airflow docker-compose to run data pipelines and on a dedicated GPU pytorch container. The repo is a git template, and was hoping ot might be useful to others beyond just my personal projects.\n\nI'd love to hear your thoughts. is it a good thing to encourage the data  science community to develop from the start with pipelines in mind?", "author_fullname": "t2_9g8u21p9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline_gen: a project for simplified data pipeline design targeted at promoting good pipeline design principles among data scientists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12q1ph9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681774539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;Recently retired as a data scientist/engineer.  I figured i ought to at least try to give back to the opensource community.  I&amp;#39;ve created a Docker app to help data scientists build data projects with pipelines in mind *from the start*, rather than as Jupyter notebook afterthoughts:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/jason-brian-anderson/pipeline_gen\"&gt;&lt;strong&gt;https://github.com/jason-brian-anderson/pipeline_gen&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I suspect I wasn&amp;#39;t the only one with no clue about good pipeline design principles.  I customized the base Airflow docker-compose to run data pipelines and on a dedicated GPU pytorch container. The repo is a git template, and was hoping ot might be useful to others beyond just my personal projects.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear your thoughts. is it a good thing to encourage the data  science community to develop from the start with pipelines in mind?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qKizHP8ZahfJgXgg0AxjnWykExz_j3sOgNj78AiuO00.jpg?auto=webp&amp;v=enabled&amp;s=c1c932cebe7ad6b7da6c3a8ac758b4c015430ecf", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qKizHP8ZahfJgXgg0AxjnWykExz_j3sOgNj78AiuO00.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c2796740d1577705d18ae732340ace1a8c9df09", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qKizHP8ZahfJgXgg0AxjnWykExz_j3sOgNj78AiuO00.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8958047e87ccd35586585709ff12371974101434", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qKizHP8ZahfJgXgg0AxjnWykExz_j3sOgNj78AiuO00.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=80c63f6455dc4248592a3c23198d64640ea66dc4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qKizHP8ZahfJgXgg0AxjnWykExz_j3sOgNj78AiuO00.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53a58b1646243881e0d699983def42c4fa03a4f8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qKizHP8ZahfJgXgg0AxjnWykExz_j3sOgNj78AiuO00.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c3b8887f6642e7774c502dbcb2e23d19f6fa96e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qKizHP8ZahfJgXgg0AxjnWykExz_j3sOgNj78AiuO00.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5047f210faead4fc8e96855a837e4d910a6cd89e", "width": 1080, "height": 540}], "variants": {}, "id": "i_Png3iGB-feerTRQyTJZMNWOeqXHZunl3kMQORqODs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "12q1ph9", "is_robot_indexable": true, "report_reasons": null, "author": "airflowy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12q1ph9/pipeline_gen_a_project_for_simplified_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12q1ph9/pipeline_gen_a_project_for_simplified_data/", "subreddit_subscribers": 100180, "created_utc": 1681774539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, I\u00b4m currently interested in pursuing a master's degree around related to data engineering and analytics. I'd prefer an online program since I'd rather not wreck my bank account and destabilize my family life at the moment. Do you have any experience, recommendations or warnings towards any specific programs? I'm a Latin-American and know there are certain institutions that provide some sort of support, scholarships and/or related programs to fulfil a certain quota and, well, I wouldn't like to waste any opportunity at reach.\n\nThank you!", "author_fullname": "t2_wah5l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering and analytics Masters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12plhfd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681745436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, I\u00b4m currently interested in pursuing a master&amp;#39;s degree around related to data engineering and analytics. I&amp;#39;d prefer an online program since I&amp;#39;d rather not wreck my bank account and destabilize my family life at the moment. Do you have any experience, recommendations or warnings towards any specific programs? I&amp;#39;m a Latin-American and know there are certain institutions that provide some sort of support, scholarships and/or related programs to fulfil a certain quota and, well, I wouldn&amp;#39;t like to waste any opportunity at reach.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12plhfd", "is_robot_indexable": true, "report_reasons": null, "author": "reborndu", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12plhfd/data_engineering_and_analytics_masters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12plhfd/data_engineering_and_analytics_masters/", "subreddit_subscribers": 100180, "created_utc": 1681745436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks!\n\nI started a newsletter called Simetrique about Data Engineering and Analytics Engineering. I am trying to make it not about latest news and trends (however it's impossible to avoid), but rather to write about topics that are interesting to myself. For example, interesting tools, approaches or articles. Also, sometimes I'm going to write about my personal experience with data and analytics.\n\nI'm currently an Analytics Engineer, and held a lot of analytical titles (such as data analyst, BI engineer and even a head of BI) for the past 10 year. So the data is my passion, especially its engineering part.\n\nFirst 4 issues are already published so you can go and check the type of content I'm going to post. Planning to make about 1 newsletter a week, so not going to spam you.\n\n[https://simetrique.substack.com/](https://simetrique.substack.com/)", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I started a newsletter about DE and AE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12pyojl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681768904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!&lt;/p&gt;\n\n&lt;p&gt;I started a newsletter called Simetrique about Data Engineering and Analytics Engineering. I am trying to make it not about latest news and trends (however it&amp;#39;s impossible to avoid), but rather to write about topics that are interesting to myself. For example, interesting tools, approaches or articles. Also, sometimes I&amp;#39;m going to write about my personal experience with data and analytics.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently an Analytics Engineer, and held a lot of analytical titles (such as data analyst, BI engineer and even a head of BI) for the past 10 year. So the data is my passion, especially its engineering part.&lt;/p&gt;\n\n&lt;p&gt;First 4 issues are already published so you can go and check the type of content I&amp;#39;m going to post. Planning to make about 1 newsletter a week, so not going to spam you.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://simetrique.substack.com/\"&gt;https://simetrique.substack.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AC0LWTaobZ0ozdKWOeI1XsHzyDYe9xbHmEp5oF2YNEU.jpg?auto=webp&amp;v=enabled&amp;s=eb7bdadcab12495fa1b1d88da290fa1a6840f15c", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/AC0LWTaobZ0ozdKWOeI1XsHzyDYe9xbHmEp5oF2YNEU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc3907b05bab8847a3f7f3c88ba9b13bcad063d5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AC0LWTaobZ0ozdKWOeI1XsHzyDYe9xbHmEp5oF2YNEU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=239d599db4fd3341980de2498027e7f38769095f", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/AC0LWTaobZ0ozdKWOeI1XsHzyDYe9xbHmEp5oF2YNEU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e76eb1c53656e1b9161d28d095c21cbb2f89012", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/AC0LWTaobZ0ozdKWOeI1XsHzyDYe9xbHmEp5oF2YNEU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6edc9bfaa20abf1d71ef7ddf516df53075da38a8", "width": 640, "height": 333}], "variants": {}, "id": "h-2msyp46eyAeZx54uovglwbzRki6DsxoUVoQUBZbGk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12pyojl", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12pyojl/i_started_a_newsletter_about_de_and_ae/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12pyojl/i_started_a_newsletter_about_de_and_ae/", "subreddit_subscribers": 100180, "created_utc": 1681768904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! I work on a very small Data team and have spent the last few months setting up a warehouse environment using Fivetran, dbt, and Snowflake. I'm very happy with these tools, and we're already seeing returns on them. Currently I just have a free account on dbt Cloud that is just running 'dbt run all' every night. I'm looking at setting up Dagster or something similar to handle the orchestration. On paper, it seems like it would integrate with everything and give me one place to handle all of these tools and catch errors, but I'm worried that it's unnecessary complexity at this point. We're not running ML models or anything like that at this point, just building data sources that feed BI tools. I am also looking for a way to generate dbt docs that our team can easily view and I think an orchestrator would help with that although we'd have to host them somewhere.\n\nI would appreciate anyone's thoughts on this if anyone has been in a similar situation. Thanks!", "author_fullname": "t2_7ljys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is an orchestrator like Dagster overkill?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12pm6g0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681746347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I work on a very small Data team and have spent the last few months setting up a warehouse environment using Fivetran, dbt, and Snowflake. I&amp;#39;m very happy with these tools, and we&amp;#39;re already seeing returns on them. Currently I just have a free account on dbt Cloud that is just running &amp;#39;dbt run all&amp;#39; every night. I&amp;#39;m looking at setting up Dagster or something similar to handle the orchestration. On paper, it seems like it would integrate with everything and give me one place to handle all of these tools and catch errors, but I&amp;#39;m worried that it&amp;#39;s unnecessary complexity at this point. We&amp;#39;re not running ML models or anything like that at this point, just building data sources that feed BI tools. I am also looking for a way to generate dbt docs that our team can easily view and I think an orchestrator would help with that although we&amp;#39;d have to host them somewhere.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate anyone&amp;#39;s thoughts on this if anyone has been in a similar situation. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12pm6g0", "is_robot_indexable": true, "report_reasons": null, "author": "tydor73", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12pm6g0/is_an_orchestrator_like_dagster_overkill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12pm6g0/is_an_orchestrator_like_dagster_overkill/", "subreddit_subscribers": 100180, "created_utc": 1681746347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you think are some of the problems that the current Modern Data Stack (ETL + Analytics) tools are not able to solve?\n\nIs there a need for industry specific ETL tools so that they have a great deal of depth with data extraction tools specific to that industry? Example: Stitch solving for the fintech industry.", "author_fullname": "t2_a4d2j3fh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern Data Stack tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12q9am5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681789753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you think are some of the problems that the current Modern Data Stack (ETL + Analytics) tools are not able to solve?&lt;/p&gt;\n\n&lt;p&gt;Is there a need for industry specific ETL tools so that they have a great deal of depth with data extraction tools specific to that industry? Example: Stitch solving for the fintech industry.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12q9am5", "is_robot_indexable": true, "report_reasons": null, "author": "Living-Nobody-2727", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12q9am5/modern_data_stack_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12q9am5/modern_data_stack_tools/", "subreddit_subscribers": 100180, "created_utc": 1681789753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Greetings,\n\nI'm a DE currently working on a project to set up a data lake architecture. My job is to create data pipelines using AWS Step Functions, Lambda functions, Glue jobs, and SQS for error notification. I'm facing a challenge of coding an extractor process using PySpark on Glue that can be reusable for different data sources.\n\nI initially tried using the ***spark.read.format(\"jdbc\").options*** method, which worked well, but it also meant setting up configuration files and jar files for each data source and apply some logic to fetch them.Then, I looked into using ***create\\_dynamic\\_frame\\_from\\_catalog*** or  ***create\\_dynamic\\_frame\\_from\\_options*** to let Glue manage the connections, and only pass the database and table as parameters. However, I couldn't filter the data before extraction since the source tables are not partitioned, so I couldn't use ***push\\_down\\_predicate*** either.\n\nShould I go back to the first approach?If anyone has experience with this kind of issue or suggestions on how to approach it, I would appreciate your help. Thank you!", "author_fullname": "t2_dpj60sgl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best approach for extracting data of a JDBC for a given date on Pyspark/AWS Glue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12pj52l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681742915.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681742540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a DE currently working on a project to set up a data lake architecture. My job is to create data pipelines using AWS Step Functions, Lambda functions, Glue jobs, and SQS for error notification. I&amp;#39;m facing a challenge of coding an extractor process using PySpark on Glue that can be reusable for different data sources.&lt;/p&gt;\n\n&lt;p&gt;I initially tried using the &lt;strong&gt;&lt;em&gt;spark.read.format(&amp;quot;jdbc&amp;quot;).options&lt;/em&gt;&lt;/strong&gt; method, which worked well, but it also meant setting up configuration files and jar files for each data source and apply some logic to fetch them.Then, I looked into using &lt;strong&gt;&lt;em&gt;create_dynamic_frame_from_catalog&lt;/em&gt;&lt;/strong&gt; or  &lt;strong&gt;&lt;em&gt;create_dynamic_frame_from_options&lt;/em&gt;&lt;/strong&gt; to let Glue manage the connections, and only pass the database and table as parameters. However, I couldn&amp;#39;t filter the data before extraction since the source tables are not partitioned, so I couldn&amp;#39;t use &lt;strong&gt;&lt;em&gt;push_down_predicate&lt;/em&gt;&lt;/strong&gt; either.&lt;/p&gt;\n\n&lt;p&gt;Should I go back to the first approach?If anyone has experience with this kind of issue or suggestions on how to approach it, I would appreciate your help. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12pj52l", "is_robot_indexable": true, "report_reasons": null, "author": "_unwin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12pj52l/best_approach_for_extracting_data_of_a_jdbc_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12pj52l/best_approach_for_extracting_data_of_a_jdbc_for_a/", "subreddit_subscribers": 100180, "created_utc": 1681742540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking for a solution for bi-directional, continuous replication of data between Snowflake tables &amp; Databricks.\n\nSo far, I've found some SaaS solutions like CData &amp; Qlik which seem like they'd fit the bill.  I wanted to get some buy-in from you fine folks to see if you have faced the same requirement, if there were any build-vs-buy discussions, and the solution(s) you ultimately went with.", "author_fullname": "t2_u5xaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solution for Continuous, bi-directional Data Replication between Snowflake &amp; Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12piwdp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681742263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a solution for bi-directional, continuous replication of data between Snowflake tables &amp;amp; Databricks.&lt;/p&gt;\n\n&lt;p&gt;So far, I&amp;#39;ve found some SaaS solutions like CData &amp;amp; Qlik which seem like they&amp;#39;d fit the bill.  I wanted to get some buy-in from you fine folks to see if you have faced the same requirement, if there were any build-vs-buy discussions, and the solution(s) you ultimately went with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12piwdp", "is_robot_indexable": true, "report_reasons": null, "author": "trevcatdangerous", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12piwdp/solution_for_continuous_bidirectional_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12piwdp/solution_for_continuous_bidirectional_data/", "subreddit_subscribers": 100180, "created_utc": 1681742263.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Last month, dbt somewhat quietly released a pretty important feature for dbt Cloud: Webhooks. Although seemingly small, this is a big deal for data teams for simplifying their stack. The number of tools you need to maintain is dropping slowly and things are consolidating under dbt Cloud.  \n\n\nhere is an introduction how to make use of webhooks using fal-serverless - [https://blog.fal.ai/dbt-cloud-webhooks/](https://blog.fal.ai/dbt-cloud-webhooks/)", "author_fullname": "t2_y15lw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One step closer to the orchestrator-less data stack - dbt cloud webhooks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12q9zaj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681791258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last month, dbt somewhat quietly released a pretty important feature for dbt Cloud: Webhooks. Although seemingly small, this is a big deal for data teams for simplifying their stack. The number of tools you need to maintain is dropping slowly and things are consolidating under dbt Cloud.  &lt;/p&gt;\n\n&lt;p&gt;here is an introduction how to make use of webhooks using fal-serverless - &lt;a href=\"https://blog.fal.ai/dbt-cloud-webhooks/\"&gt;https://blog.fal.ai/dbt-cloud-webhooks/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/y0j21BcBcPDByOOViYEnHviW3D7LsCFoljvx9QuNvAQ.jpg?auto=webp&amp;v=enabled&amp;s=1c4feafb757f2cb9e8d3a4cb3c030c124cb6d0bd", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/y0j21BcBcPDByOOViYEnHviW3D7LsCFoljvx9QuNvAQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ba966ec42959de788a534643fc24345151e6205", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/y0j21BcBcPDByOOViYEnHviW3D7LsCFoljvx9QuNvAQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0ff8a1211f60937c5915adedd301a82c6dfeb71", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/y0j21BcBcPDByOOViYEnHviW3D7LsCFoljvx9QuNvAQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75af832ff586da1d9d45fa5f996662522ad40ba3", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/y0j21BcBcPDByOOViYEnHviW3D7LsCFoljvx9QuNvAQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf08962ac3886060a9da97a43f3dd0f78d9f56ef", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/y0j21BcBcPDByOOViYEnHviW3D7LsCFoljvx9QuNvAQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e77b96d945ea259661a3f6a2cd0e33a4494ff5bf", "width": 960, "height": 960}], "variants": {}, "id": "_l4XhOa77zXw6MNF803areU5rwgdSbLMetldn_jVdb8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12q9zaj", "is_robot_indexable": true, "report_reasons": null, "author": "gorkemyurt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12q9zaj/one_step_closer_to_the_orchestratorless_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12q9zaj/one_step_closer_to_the_orchestratorless_data/", "subreddit_subscribers": 100180, "created_utc": 1681791258.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am developing an app that will store that json that represents the state of an object, it will also record modifications made to that object -- think of it as an accounting ledger. It will be write heavy, but I would like to be able to read from it in a reasonable time. I think queries will use two - three columns for filtering\n\nThinking in terms of db columns, id need:\n\n    id &lt;uuid&gt;\n    type &lt;string&gt; -- the thing that was modified. user, location, noun, etc. I will need to be able to filter on this\n    type_id &lt;uuid&gt; -- luckily we use uuid everywhere. I will need to be able to filter on this\n    change &lt;string&gt; -- formatted json representing the modifications. I will not need to search in this (would be nice to in the future maybe)\n    timestamp &lt;time&gt; -- time that the modification happened. I will need to do some bounding based on this.\n\nI figured this would be simple as a single table that supported multiple types. \n\nMy original plan was a Postgres db, tune it as necessary and worry about it when I need to worry about it. Now im wondering if there are better solutions for what I want to do \n\nThanks", "author_fullname": "t2_c2o44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendation on potential storage for a changeling-type app", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ppqcy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681752179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am developing an app that will store that json that represents the state of an object, it will also record modifications made to that object -- think of it as an accounting ledger. It will be write heavy, but I would like to be able to read from it in a reasonable time. I think queries will use two - three columns for filtering&lt;/p&gt;\n\n&lt;p&gt;Thinking in terms of db columns, id need:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;id &amp;lt;uuid&amp;gt;\ntype &amp;lt;string&amp;gt; -- the thing that was modified. user, location, noun, etc. I will need to be able to filter on this\ntype_id &amp;lt;uuid&amp;gt; -- luckily we use uuid everywhere. I will need to be able to filter on this\nchange &amp;lt;string&amp;gt; -- formatted json representing the modifications. I will not need to search in this (would be nice to in the future maybe)\ntimestamp &amp;lt;time&amp;gt; -- time that the modification happened. I will need to do some bounding based on this.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I figured this would be simple as a single table that supported multiple types. &lt;/p&gt;\n\n&lt;p&gt;My original plan was a Postgres db, tune it as necessary and worry about it when I need to worry about it. Now im wondering if there are better solutions for what I want to do &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ppqcy", "is_robot_indexable": true, "report_reasons": null, "author": "needed_an_account", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ppqcy/recommendation_on_potential_storage_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ppqcy/recommendation_on_potential_storage_for_a/", "subreddit_subscribers": 100180, "created_utc": 1681752179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, what's the best way to build streaming pipeline for video and audio files in gcp?\nThese files came via api for now. I want to move these files continuously to cloud storage. size of one file ranges from kb to mb.\nHas anyone ever done something similar to this. Pls help.", "author_fullname": "t2_vkmvzdm7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video streaming pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12pgl8v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681739264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, what&amp;#39;s the best way to build streaming pipeline for video and audio files in gcp?\nThese files came via api for now. I want to move these files continuously to cloud storage. size of one file ranges from kb to mb.\nHas anyone ever done something similar to this. Pls help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12pgl8v", "is_robot_indexable": true, "report_reasons": null, "author": "shaikh21", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12pgl8v/video_streaming_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12pgl8v/video_streaming_pipeline/", "subreddit_subscribers": 100180, "created_utc": 1681739264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been tasked to start a CoE team for data within my organisation. Seems like I have  a starting problem \ud83d\ude15.\n\nHow does it usually start??\nBuild a vision for CoE\nStart building a team\nBest practices and architecture\nConfused where do I begin!", "author_fullname": "t2_6kmo2ecy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data CoE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12q99n4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681789691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been tasked to start a CoE team for data within my organisation. Seems like I have  a starting problem \ud83d\ude15.&lt;/p&gt;\n\n&lt;p&gt;How does it usually start??\nBuild a vision for CoE\nStart building a team\nBest practices and architecture\nConfused where do I begin!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12q99n4", "is_robot_indexable": true, "report_reasons": null, "author": "soujoshi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12q99n4/data_coe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12q99n4/data_coe/", "subreddit_subscribers": 100180, "created_utc": 1681789691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Newbie here and apologize if this question has already been answered, but I\u2019m in a new role where Databricks autoloader is used for all data ingestion tasks\u2014whether the data is batch or streaming. We use the medallion architecture to load files from azure blob  to bronze, silver, and gold tables. I\u2019ve found that autoloader works great for appending data to bronze tables and works fine when simple merge operations are done from bronze to silver. But when we start to have edge cases, such as restated data, that requires a selective delete and append, we run into problems. Additionally, when trying to use autoloader to write business logic (aggregations, joins, filters, etc.) it quickly becomes a nightmare. \n\nCurious if anyone else has run into similar problems, and if so, what did you do to solve it? Does it even make sense to use autoloader for batch data processing?", "author_fullname": "t2_883tcd85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Autoloader for batch processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12q8wp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681788892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Newbie here and apologize if this question has already been answered, but I\u2019m in a new role where Databricks autoloader is used for all data ingestion tasks\u2014whether the data is batch or streaming. We use the medallion architecture to load files from azure blob  to bronze, silver, and gold tables. I\u2019ve found that autoloader works great for appending data to bronze tables and works fine when simple merge operations are done from bronze to silver. But when we start to have edge cases, such as restated data, that requires a selective delete and append, we run into problems. Additionally, when trying to use autoloader to write business logic (aggregations, joins, filters, etc.) it quickly becomes a nightmare. &lt;/p&gt;\n\n&lt;p&gt;Curious if anyone else has run into similar problems, and if so, what did you do to solve it? Does it even make sense to use autoloader for batch data processing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12q8wp3", "is_robot_indexable": true, "report_reasons": null, "author": "Basic_Cucumber_165", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12q8wp3/autoloader_for_batch_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12q8wp3/autoloader_for_batch_processing/", "subreddit_subscribers": 100180, "created_utc": 1681788892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to play around with some online tools such as secoda, mozart data etc for which i need a postgres instance. Rather than using any cloud provider i thought of using either my local postgres or postgres using docker. I am able to connect to the database using dbeaver in my local but not able to understand how to open it for remote access or have these third party tools access it. \n\nIt would be great for me to learn how to setup a postgres instance locally and use it for any development activities i wish. \n\nAdditionally if there is any website that i can allows a easy postgres/mysql db setup on cloud without much overhead and almost free then that would be great too. Few options i tried- neon.tech, elephantsql.", "author_fullname": "t2_81zlbrs6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help in setting up local postgres instance + allowing remote connections", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12q6k8f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681784076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to play around with some online tools such as secoda, mozart data etc for which i need a postgres instance. Rather than using any cloud provider i thought of using either my local postgres or postgres using docker. I am able to connect to the database using dbeaver in my local but not able to understand how to open it for remote access or have these third party tools access it. &lt;/p&gt;\n\n&lt;p&gt;It would be great for me to learn how to setup a postgres instance locally and use it for any development activities i wish. &lt;/p&gt;\n\n&lt;p&gt;Additionally if there is any website that i can allows a easy postgres/mysql db setup on cloud without much overhead and almost free then that would be great too. Few options i tried- neon.tech, elephantsql.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12q6k8f", "is_robot_indexable": true, "report_reasons": null, "author": "SnooPeanuts5237", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12q6k8f/need_help_in_setting_up_local_postgres_instance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12q6k8f/need_help_in_setting_up_local_postgres_instance/", "subreddit_subscribers": 100180, "created_utc": 1681784076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We store the queries that create tableau extracts in Github - we have a different folder for each query. \n\nI have started creating a subdirectory in the folder called tests where I put validation tests testing things like - the max\\_date year for this id should be no bigger than 2020. Or a report like \"10% of items are null\" for this result. \n\nThe problem is that these queries cannot run as is because they depend on the extract query and sql does not have file imports for instance. additionally it would be cool to run it as a test suite where I can determine if all 10 tests pass for instance after each change. Finally, the query sometimes takes a long time to run and I wish there was a way to use a temporary table. \n\nIm thinking of creating a python script to basically run this query and create a temporary table and then run each query as a test and output the results in a readable way. But is there something out there like this that I can just use? I know about dbt tests but I think its overkill just for each new tableau data source .", "author_fullname": "t2_b3n9i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tool Recommendation for testing queries in a file?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ptpdx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681759687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We store the queries that create tableau extracts in Github - we have a different folder for each query. &lt;/p&gt;\n\n&lt;p&gt;I have started creating a subdirectory in the folder called tests where I put validation tests testing things like - the max_date year for this id should be no bigger than 2020. Or a report like &amp;quot;10% of items are null&amp;quot; for this result. &lt;/p&gt;\n\n&lt;p&gt;The problem is that these queries cannot run as is because they depend on the extract query and sql does not have file imports for instance. additionally it would be cool to run it as a test suite where I can determine if all 10 tests pass for instance after each change. Finally, the query sometimes takes a long time to run and I wish there was a way to use a temporary table. &lt;/p&gt;\n\n&lt;p&gt;Im thinking of creating a python script to basically run this query and create a temporary table and then run each query as a test and output the results in a readable way. But is there something out there like this that I can just use? I know about dbt tests but I think its overkill just for each new tableau data source .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ptpdx", "is_robot_indexable": true, "report_reasons": null, "author": "third_dude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ptpdx/tool_recommendation_for_testing_queries_in_a_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ptpdx/tool_recommendation_for_testing_queries_in_a_file/", "subreddit_subscribers": 100180, "created_utc": 1681759687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://medium.com/gastromatic/synchronizing-data-using-memphis-dev-a-case-study-2e6e9a7b5512](https://medium.com/gastromatic/synchronizing-data-using-memphis-dev-a-case-study-2e6e9a7b5512)", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synchronizing data using a new message broker: A case study.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12plo5v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681745684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/gastromatic/synchronizing-data-using-memphis-dev-a-case-study-2e6e9a7b5512\"&gt;https://medium.com/gastromatic/synchronizing-data-using-memphis-dev-a-case-study-2e6e9a7b5512&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8Gj87tbeVCk9Y6gv1XrD2JCIhSQPYF-TiAufDrQjRpo.jpg?auto=webp&amp;v=enabled&amp;s=e0dcf65745f8f81ca40c6232048eba40042bbf0a", "width": 1200, "height": 685}, "resolutions": [{"url": "https://external-preview.redd.it/8Gj87tbeVCk9Y6gv1XrD2JCIhSQPYF-TiAufDrQjRpo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5dd2916bdbf4df71087bc2fd4a81cfe569b85627", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/8Gj87tbeVCk9Y6gv1XrD2JCIhSQPYF-TiAufDrQjRpo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c4e2e0bee9f7fe0ef25a3c7c898f283313a17d4", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/8Gj87tbeVCk9Y6gv1XrD2JCIhSQPYF-TiAufDrQjRpo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0c1d8204e15115c3870303acdc8ce061c110c1e", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/8Gj87tbeVCk9Y6gv1XrD2JCIhSQPYF-TiAufDrQjRpo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55b7a318d94772320df353222cbe01861bf19c33", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/8Gj87tbeVCk9Y6gv1XrD2JCIhSQPYF-TiAufDrQjRpo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10b85f5b8d4c343988c05c7fa7c79f5c754af917", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/8Gj87tbeVCk9Y6gv1XrD2JCIhSQPYF-TiAufDrQjRpo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1183da065c5cf2cc73a5ff0fe7fe2e74be20a4a", "width": 1080, "height": 616}], "variants": {}, "id": "a2iUTeqWmkO2QfQFQh8Uti-nGge_UVR1rRhDsRSJ5sk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12plo5v", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12plo5v/synchronizing_data_using_a_new_message_broker_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12plo5v/synchronizing_data_using_a_new_message_broker_a/", "subreddit_subscribers": 100180, "created_utc": 1681745684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hot Takes on the Modern Data Stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_12qi81h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zZ-7CbC36WBUJqr3_HKyJNv3nkcWm-2lBw9OZsBeOQc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681812434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mattpalmer.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://mattpalmer.io/posts/hot-takes/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qSFsxKq_BlFRgIjKDPWSACdVRac4Sw9FpPgru3bFijQ.jpg?auto=webp&amp;v=enabled&amp;s=3b7b32b87771d38919e831492129a5a56d632014", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/qSFsxKq_BlFRgIjKDPWSACdVRac4Sw9FpPgru3bFijQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a07fd1ba6c6c77efc849d049c68146606eed77e5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qSFsxKq_BlFRgIjKDPWSACdVRac4Sw9FpPgru3bFijQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d29efe0934ceec736da4e103b08796f8a4d58cfd", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qSFsxKq_BlFRgIjKDPWSACdVRac4Sw9FpPgru3bFijQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d6fdea8d58567bedb3e4876156f9598d74ef52b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qSFsxKq_BlFRgIjKDPWSACdVRac4Sw9FpPgru3bFijQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0846248532357b4b940eb1f7b45654015796268", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qSFsxKq_BlFRgIjKDPWSACdVRac4Sw9FpPgru3bFijQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31876b0f3c007e46a2f696e6288d6859f1f509be", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qSFsxKq_BlFRgIjKDPWSACdVRac4Sw9FpPgru3bFijQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b264ce591b3e7e0aa032b439ec549196a9499aa", "width": 1080, "height": 567}], "variants": {}, "id": "A1mOJ6oZeDUrTFa7TvwsIw_R8wZDDIod3BQIdmWhuGw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12qi81h", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12qi81h/hot_takes_on_the_modern_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://mattpalmer.io/posts/hot-takes/", "subreddit_subscribers": 100180, "created_utc": 1681812434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, i have started preparing for gcp dat engineer exam. I want to clear this exam at any cost in my first attempt since the fees of exam is quite high. Does anyone has the practice tests available and can share with me?\nThanks in advance.", "author_fullname": "t2_vkmvzdm7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need practice papers for gcp data engineer exam.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12qi654", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681812296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, i have started preparing for gcp dat engineer exam. I want to clear this exam at any cost in my first attempt since the fees of exam is quite high. Does anyone has the practice tests available and can share with me?\nThanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12qi654", "is_robot_indexable": true, "report_reasons": null, "author": "shaikh21", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12qi654/need_practice_papers_for_gcp_data_engineer_exam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12qi654/need_practice_papers_for_gcp_data_engineer_exam/", "subreddit_subscribers": 100180, "created_utc": 1681812296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What was the most challenging or the weirdest unstructured data preparation you've ever performed? What did you use for parsing/cleansing?", "author_fullname": "t2_1z5jdh5h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The most challenging data preparation/ingestion of unstructured data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12pxyjy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681767619.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What was the most challenging or the weirdest unstructured data preparation you&amp;#39;ve ever performed? What did you use for parsing/cleansing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12pxyjy", "is_robot_indexable": true, "report_reasons": null, "author": "Greg_Z_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12pxyjy/the_most_challenging_data_preparationingestion_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12pxyjy/the_most_challenging_data_preparationingestion_of/", "subreddit_subscribers": 100180, "created_utc": 1681767619.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_81yu8ev", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Webinar - Running dbt core on Airflow in production - learnings from 2 years of battle scars", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12pqntp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.44, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/v2cUNUuhL7O_rZWmg5pE4fJiGn_4K5TFCgXNw08wn-w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681753949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "operationalanalytics.club", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.operationalanalytics.club/events/running-dbt-core-on-airflow-in-production-learnings-from-2-years-of-battle-scars?ref=35499551", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-5BBXhg-IdWkcKReEbkNtJFYlkG3hoBP7R-o9hg9c1A.jpg?auto=webp&amp;v=enabled&amp;s=4ddf4c74086580fd6f7c3abe03787341c335f48e", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/-5BBXhg-IdWkcKReEbkNtJFYlkG3hoBP7R-o9hg9c1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b0a737b3456b0c9617b5e5296ab4ad103d2a37d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-5BBXhg-IdWkcKReEbkNtJFYlkG3hoBP7R-o9hg9c1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8cc5bde8c1647cafccb5622c81f13dc182ba64b", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-5BBXhg-IdWkcKReEbkNtJFYlkG3hoBP7R-o9hg9c1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1539678836218a354e83b3e6a78d14dbca0b811", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/-5BBXhg-IdWkcKReEbkNtJFYlkG3hoBP7R-o9hg9c1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb38b9569dba052d500a37f287d47bea38aea21f", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/-5BBXhg-IdWkcKReEbkNtJFYlkG3hoBP7R-o9hg9c1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2ffe9efaf51575ddd7e55625ff43de6f303d244", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/-5BBXhg-IdWkcKReEbkNtJFYlkG3hoBP7R-o9hg9c1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98a8ddb78fafa82694c74c46f1afdbe6b8eeb72b", "width": 1080, "height": 565}], "variants": {}, "id": "zFQ3N50ROx6yVa7lCSV-VqnE2EVJ8YES1JgT95yllIU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12pqntp", "is_robot_indexable": true, "report_reasons": null, "author": "parzival1984", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12pqntp/webinar_running_dbt_core_on_airflow_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.operationalanalytics.club/events/running-dbt-core-on-airflow-in-production-learnings-from-2-years-of-battle-scars?ref=35499551", "subreddit_subscribers": 100180, "created_utc": 1681753949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, I have a data engineer panel interview at Merck tomorrow. Any help or advise would be greatly appreciated\u2026", "author_fullname": "t2_u1p9g2g4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merck Data Engineer Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qf6m1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681804040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, I have a data engineer panel interview at Merck tomorrow. Any help or advise would be greatly appreciated\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12qf6m1", "is_robot_indexable": true, "report_reasons": null, "author": "Active-Ask-3524", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12qf6m1/merck_data_engineer_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12qf6m1/merck_data_engineer_interview/", "subreddit_subscribers": 100180, "created_utc": 1681804040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://jobs.cvshealth.com/job/17447961/lead-cloud-engineer-gcp-sql-python-devops-ai-ml-remote/\n\n\nHeard CVS (us employer) is spark / sql heavy so easy / medium leet code? Or coderpad.? Do they also ask extensive hour long case scenarios to solve(panel)?\n\nAnyone whose in this position or a similar one like sr DE? Tips?\n\nDoes the JD sound like they need devops guy too or assume devops knowledge should be there as it relates to ci cd cloud aws gcp etc?\n\nThanks \ud83d\ude0a", "author_fullname": "t2_1411ira9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to prepare for this position? See below", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qax7w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681793397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://jobs.cvshealth.com/job/17447961/lead-cloud-engineer-gcp-sql-python-devops-ai-ml-remote/\"&gt;https://jobs.cvshealth.com/job/17447961/lead-cloud-engineer-gcp-sql-python-devops-ai-ml-remote/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Heard CVS (us employer) is spark / sql heavy so easy / medium leet code? Or coderpad.? Do they also ask extensive hour long case scenarios to solve(panel)?&lt;/p&gt;\n\n&lt;p&gt;Anyone whose in this position or a similar one like sr DE? Tips?&lt;/p&gt;\n\n&lt;p&gt;Does the JD sound like they need devops guy too or assume devops knowledge should be there as it relates to ci cd cloud aws gcp etc?&lt;/p&gt;\n\n&lt;p&gt;Thanks \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Cx90UrVAA_Ce-prjOBJrVavrHPW3LHOoqjWxkoIq1VU.jpg?auto=webp&amp;v=enabled&amp;s=35cfd8278c5dc027d9e84502947f9d25bb572563", "width": 307, "height": 301}, "resolutions": [{"url": "https://external-preview.redd.it/Cx90UrVAA_Ce-prjOBJrVavrHPW3LHOoqjWxkoIq1VU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=52fce6abaad0ef58c32fc60fd65b1667e720cf31", "width": 108, "height": 105}, {"url": "https://external-preview.redd.it/Cx90UrVAA_Ce-prjOBJrVavrHPW3LHOoqjWxkoIq1VU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd028943f5dc2a163ed867393c9af5f8270523b7", "width": 216, "height": 211}], "variants": {}, "id": "ISz3uhpPErEo_NVnFGH1tzrSJ9_t-uEnKKfJkw3EfEI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12qax7w", "is_robot_indexable": true, "report_reasons": null, "author": "wisegeek57", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12qax7w/how_to_prepare_for_this_position_see_below/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12qax7w/how_to_prepare_for_this_position_see_below/", "subreddit_subscribers": 100180, "created_utc": 1681793397.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}