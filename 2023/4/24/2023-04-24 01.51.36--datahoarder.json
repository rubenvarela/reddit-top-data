{"kind": "Listing", "data": {"after": "t3_12wml3v", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've already archived some, but I wanted to know if anyone's aware of an existing list of subs.", "author_fullname": "t2_fxa6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a list of nsfw-subreddits? Given the upcoming Imgur purge I'd like to archive some of them.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vrrdf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 148, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 148, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682215551.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve already archived some, but I wanted to know if anyone&amp;#39;s aware of an existing list of subs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12vrrdf", "is_robot_indexable": true, "report_reasons": null, "author": "onlytoask", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12vrrdf/is_there_a_list_of_nsfwsubreddits_given_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12vrrdf/is_there_a_list_of_nsfwsubreddits_given_the/", "subreddit_subscribers": 679589, "created_utc": 1682215551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have two music libraries. One of them is formatted in a very ugly way but that is necessary to leave them seeding, the other looks nice and scans perfectly into my software, but cannot be seeded.\n\nI wonder how hard it would be to back them up as essentially two slight variations of the same file, such as the way Restic can store snapshots of a modified file only taking up a small fraction of the space of a whole replacement file. I'm especially interested in finding out if this is possible without rebuilding the entire library with hardlinks, as it takes about 14 hours of continuous tagging to set up the library.\n\nIs this doable?\n\nedit: SOLVED. It turns out that Restic does take care of this, not just from incremental changes to one file, but to large quantities of similar files, which is exactly what I was looking for!", "author_fullname": "t2_cc458uu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How hard is it to backup slightly different versions of the same file without using double space?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wh5i7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 105, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 105, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682287890.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682267380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have two music libraries. One of them is formatted in a very ugly way but that is necessary to leave them seeding, the other looks nice and scans perfectly into my software, but cannot be seeded.&lt;/p&gt;\n\n&lt;p&gt;I wonder how hard it would be to back them up as essentially two slight variations of the same file, such as the way Restic can store snapshots of a modified file only taking up a small fraction of the space of a whole replacement file. I&amp;#39;m especially interested in finding out if this is possible without rebuilding the entire library with hardlinks, as it takes about 14 hours of continuous tagging to set up the library.&lt;/p&gt;\n\n&lt;p&gt;Is this doable?&lt;/p&gt;\n\n&lt;p&gt;edit: SOLVED. It turns out that Restic does take care of this, not just from incremental changes to one file, but to large quantities of similar files, which is exactly what I was looking for!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wh5i7", "is_robot_indexable": true, "report_reasons": null, "author": "SleepingAndy", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wh5i7/how_hard_is_it_to_backup_slightly_different/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wh5i7/how_hard_is_it_to_backup_slightly_different/", "subreddit_subscribers": 679589, "created_utc": 1682267380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seems like something you guys might be interested in", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_12vt2s7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_2xuixd1m", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EImly_6u3icb-YS1oaJU2FGtmA9JOD7k-Rb9ddeoEfg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "", "author_fullname": "t2_owff7qyq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[P] I built a tool that auto-generates scrapers for any website with GPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "four", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_12v0vda", "quarantine": false, "link_flair_text_color": null, "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 926, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/tgl8gqowoeva1/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/tgl8gqowoeva1/DASH_96.mp4", "dash_url": "https://v.redd.it/tgl8gqowoeva1/DASHPlaylist.mpd?a=1684893095%2CYTA0MjcyNWIzYTEwZjdmNDkyMDE4NzhkOGMxZDA1YTljZGE5MDJjOWEwNmI2MmE5MjU1OTdmZDRkODc3YjZmOA%3D%3D&amp;v=1&amp;f=sd", "duration": 52, "hls_url": "https://v.redd.it/tgl8gqowoeva1/HLSPlaylist.m3u8?a=1684893095%2CNTViZTQ4Njg1YTdlNjY3ZjcyYTc5YWU1MTFiZTE2Njc2ZDkyOTU3Mzg0NTY5NTAzZWRjMzdkYjczNDUyZjE3ZQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Project", "can_mod_post": false, "score": 926, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/EImly_6u3icb-YS1oaJU2FGtmA9JOD7k-Rb9ddeoEfg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "mod_note": null, "created": 1682156612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/tgl8gqowoeva1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=55cec3ae3044f6c64c43d0f712dfb0ed61b836ec", "width": 1280, "height": 769}, "resolutions": [{"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8d28acb9227e49f26482a3bd6dd0c97d51c6d44b", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6f81024b75c9a04f51460d6a75a18a883c42bdb9", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d41bb7eb400eb423cf17175b4ed755ff97c9eb3e", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=81271b0de0cfe1b3c74dc4a4d1d9f4b5ab409d41", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=238c5527e599826a33034726ec4cd8e2173cda00", "width": 960, "height": 576}, {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=4d25f46139649ebb2056b070bb4d97f0236996ee", "width": 1080, "height": 648}], "variants": {}, "id": "sa4Kc7bNerFyGsRQeSV7YVar_YftNWYuTLy8b6ol4go"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 20, "id": "award_abb865cf-620b-4219-8777-3658cf9091fb", "penny_donate": null, "award_sub_type": "PREMIUM", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Starstruck_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Starstruck_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Starstruck_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Starstruck_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Starstruck_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Starstruck_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Can't stop seeing stars", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Starstruck", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=33eba03e0a6defb8abde795039b68e67e88b065e", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=c78ffa7024982f9d4734f29e69627ec6c4c5484f", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=4e130b357daf70f0a9cc7f8bf8b0e70f0e1e17fd", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=931bd1908b603e61ec971fb9c882d221eb800cc5", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=ed034a40c86c89af6308aca761ec27b46e6207f0", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 512, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/snotiq9vxyn51_Starstruck.png"}], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "12v0vda", "is_robot_indexable": true, "report_reasons": null, "author": "madredditscientist", "discussion_type": null, "num_comments": 76, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/12v0vda/p_i_built_a_tool_that_autogenerates_scrapers_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/tgl8gqowoeva1", "subreddit_subscribers": 2639267, "created_utc": 1682156612.0, "num_crossposts": 3, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/tgl8gqowoeva1/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/tgl8gqowoeva1/DASH_96.mp4", "dash_url": "https://v.redd.it/tgl8gqowoeva1/DASHPlaylist.mpd?a=1684893095%2CYTA0MjcyNWIzYTEwZjdmNDkyMDE4NzhkOGMxZDA1YTljZGE5MDJjOWEwNmI2MmE5MjU1OTdmZDRkODc3YjZmOA%3D%3D&amp;v=1&amp;f=sd", "duration": 52, "hls_url": "https://v.redd.it/tgl8gqowoeva1/HLSPlaylist.m3u8?a=1684893095%2CNTViZTQ4Njg1YTdlNjY3ZjcyYTc5YWU1MTFiZTE2Njc2ZDkyOTU3Mzg0NTY5NTAzZWRjMzdkYjczNDUyZjE3ZQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}], "created": 1682218443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/tgl8gqowoeva1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?auto=webp&amp;v=enabled&amp;s=35c96016939320f35161c3e60385d50a00576114", "width": 1280, "height": 769}, "resolutions": [{"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10cedb7bf8a2b37555f43c81b28adb05f142af27", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f25a882c44078fcf2ea169c937abb7667bf738a", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7cb93d2af3a14770140ff60ad1ace4c23a36c95", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ee0545a0c41453e62ce02b9c033829fd4957f66", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4165e6dbba3273ff01b5dda70a18fa6a2d4bb3d", "width": 960, "height": 576}, {"url": "https://external-preview.redd.it/8tdkmqGXRC9T-6eizcMvCCu-0b67tFizWXU82QkHyNE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99f93a3efcf6c2be326204772972495bb657d89a", "width": 1080, "height": 648}], "variants": {}, "id": "sa4Kc7bNerFyGsRQeSV7YVar_YftNWYuTLy8b6ol4go"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12vt2s7", "is_robot_indexable": true, "report_reasons": null, "author": "SigmaSixShooter", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12v0vda", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12vt2s7/seems_like_something_you_guys_might_be_interested/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/tgl8gqowoeva1", "subreddit_subscribers": 679589, "created_utc": 1682218443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_14me0d9r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am trying to find the source of this image I have from an old hard drive, was widely used in the late 90's and early 2000's as an avatar and on personal websites. Is it from a PC game? Trying to find the earliest source.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 52, "top_awarded_type": null, "hide_score": false, "name": "t3_12wejd8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 56, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 70, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 56, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/R8T36vWDt0Hq6V5qe-12pKJdhzPhJ9u112hCkrXMlX8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682263635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/u96wa0vyinva1.gif", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/u96wa0vyinva1.gif?format=png8&amp;v=enabled&amp;s=9d97a6f8953bc69e23fa724fc308c8ea723088c4", "width": 100, "height": 75}, "resolutions": [], "variants": {"gif": {"source": {"url": "https://preview.redd.it/u96wa0vyinva1.gif?s=fc2767a6b4245cf0e37622ae8df1b90c33ed4097", "width": 100, "height": 75}, "resolutions": []}, "mp4": {"source": {"url": "https://preview.redd.it/u96wa0vyinva1.gif?format=mp4&amp;v=enabled&amp;s=4facdb1204240aa2784b14170954be6d856ce8b2", "width": 100, "height": 75}, "resolutions": []}}, "id": "96RaTbF_1Hz36G7afDCI8iijxHMcvq9wZWPSbW0K5iQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wejd8", "is_robot_indexable": true, "report_reasons": null, "author": "thingumajig13", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wejd8/i_am_trying_to_find_the_source_of_this_image_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/u96wa0vyinva1.gif", "subreddit_subscribers": 679589, "created_utc": 1682263635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellow hoarders,\n\nI've been fighting with a big collection of video files, which do not have any uniform default track selection, and I was sick of always  changing tracks in the beginning of a movie or episode. Updating them manually was never an option. So I developed a tool changing default audio and subtitle tracks of matroska (.mkv) files. \n\nI just released a new version featuring some new improvements, like only updating files since the last successful execution (works great with a cron job), it's now usable without a config file and it can match the same configuration for a whole directory / series / season.\n\nCheck it out: [MKVAudioSubtitleChanger v3.0](https://github.com/RatzzFatzz/MKVAudioSubtitleChanger)\nand [its release](https://github.com/RatzzFatzz/MKVAudioSubtitleChanger/releases/tag/v3.0)\n\nI hope you folks can save some time with it :)", "author_fullname": "t2_trarh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Change default audio &amp; subtitle tracks of mkv files without rewriting files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w55h7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682265971.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682248630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow hoarders,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been fighting with a big collection of video files, which do not have any uniform default track selection, and I was sick of always  changing tracks in the beginning of a movie or episode. Updating them manually was never an option. So I developed a tool changing default audio and subtitle tracks of matroska (.mkv) files. &lt;/p&gt;\n\n&lt;p&gt;I just released a new version featuring some new improvements, like only updating files since the last successful execution (works great with a cron job), it&amp;#39;s now usable without a config file and it can match the same configuration for a whole directory / series / season.&lt;/p&gt;\n\n&lt;p&gt;Check it out: &lt;a href=\"https://github.com/RatzzFatzz/MKVAudioSubtitleChanger\"&gt;MKVAudioSubtitleChanger v3.0&lt;/a&gt;\nand &lt;a href=\"https://github.com/RatzzFatzz/MKVAudioSubtitleChanger/releases/tag/v3.0\"&gt;its release&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I hope you folks can save some time with it :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wpj6S5ByAv-UviQf0wSWXf-zBy6a8q_DLmsGxK60j2s.jpg?auto=webp&amp;v=enabled&amp;s=05084365ee5b653e58bb87b9c2f746694d4ee4c7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/wpj6S5ByAv-UviQf0wSWXf-zBy6a8q_DLmsGxK60j2s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=424b9845a843406c7a9ef029d7bcd452ba7a99fe", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/wpj6S5ByAv-UviQf0wSWXf-zBy6a8q_DLmsGxK60j2s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f59fab837b4379323e9de84f53f758eb940b11e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/wpj6S5ByAv-UviQf0wSWXf-zBy6a8q_DLmsGxK60j2s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a4f86e031542499620ddbe4395695f977a494f1", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/wpj6S5ByAv-UviQf0wSWXf-zBy6a8q_DLmsGxK60j2s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a66c346a9b79b6fac4f72c4b36aeff13438a37ed", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/wpj6S5ByAv-UviQf0wSWXf-zBy6a8q_DLmsGxK60j2s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea66e1e83229540e6ae06293a79e35ac352281a9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/wpj6S5ByAv-UviQf0wSWXf-zBy6a8q_DLmsGxK60j2s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43aa8515e8aaac8feb71dc9a714c0df9bd63221d", "width": 1080, "height": 540}], "variants": {}, "id": "GPsbwZ7ToYx7em89lM54PrLBCf0qhPBI_f3ep_P4PkQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "88TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12w55h7", "is_robot_indexable": true, "report_reasons": null, "author": "RatzzFatzz", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12w55h7/change_default_audio_subtitle_tracks_of_mkv_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12w55h7/change_default_audio_subtitle_tracks_of_mkv_files/", "subreddit_subscribers": 679589, "created_utc": 1682248630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Previously posted in January 2023, but the purge is next week so thought a reminder would be helpful given the level of comments on the New Year's post...  \n\n\nOriginal post: [https://www.reddit.com/r/DataHoarder/comments/10da7a9/official\\_synology\\_download\\_site\\_closing\\_legacy/](https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/)\n\n*The Synology Archive (their official \"not-quite-the-latest\" firmware and package installation file site for the NAS / Surveillance / Router / etc. range of products) is being shut down by 1st May 2023 due to licensing agreements.*\n\n*Only the latest versions will remain going forward.*\n\n[*https://archive.synology.com/download/*](https://archive.synology.com/download/)", "author_fullname": "t2_8vh7jb4m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Reminder) Synology legacy DSM firmware / package install files to be deleted by 1st May 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wp46q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682281939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Previously posted in January 2023, but the purge is next week so thought a reminder would be helpful given the level of comments on the New Year&amp;#39;s post...  &lt;/p&gt;\n\n&lt;p&gt;Original post: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/\"&gt;https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;The Synology Archive (their official &amp;quot;not-quite-the-latest&amp;quot; firmware and package installation file site for the NAS / Surveillance / Router / etc. range of products) is being shut down by 1st May 2023 due to licensing agreements.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Only the latest versions will remain going forward.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://archive.synology.com/download/\"&gt;&lt;em&gt;https://archive.synology.com/download/&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wp46q", "is_robot_indexable": true, "report_reasons": null, "author": "enchantedspring", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/", "subreddit_subscribers": 679589, "created_utc": 1682281939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I really like the illustrations by Ernst Haeckel: [https://www.rawpixel.com/search/Ernst%20Haeckel?page=1&amp;sort=curated&amp;topic\\_group=\\_topics](https://www.rawpixel.com/search/Ernst%20Haeckel?page=1&amp;sort=curated&amp;topic_group=_topics) They're public domain and free to download form rawpixel, however there's almost 600 of them an manually clicking through all of them is a lot of hassle. Is there a way to bulk download these?", "author_fullname": "t2_mtwvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to bulk download public domain images from rawpixel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wmua9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682277682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really like the illustrations by Ernst Haeckel: &lt;a href=\"https://www.rawpixel.com/search/Ernst%20Haeckel?page=1&amp;amp;sort=curated&amp;amp;topic_group=_topics\"&gt;https://www.rawpixel.com/search/Ernst%20Haeckel?page=1&amp;amp;sort=curated&amp;amp;topic_group=_topics&lt;/a&gt; They&amp;#39;re public domain and free to download form rawpixel, however there&amp;#39;s almost 600 of them an manually clicking through all of them is a lot of hassle. Is there a way to bulk download these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?auto=webp&amp;v=enabled&amp;s=1af3b5a64226b5aea4c938d0240a5b52557c7f0b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5541b0f2230e1e06628cf221b6eaa6e729c88580", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fe6f131b1c18a1cf2771cad5139c452799b6514", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b14361b294cff32078277d4073801004e45232c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=684622a9ad1c8d2c1ad0ced5f7852ece33db3d35", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d0428d9308767ac79b9167cf01f3fad8ff6bdf1", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e59cbab567b4d17e4915ddc6fa7f2a01c722fc34", "width": 1080, "height": 567}], "variants": {}, "id": "HIrJldySEIpGoDvol5hbo5YRljcp3DpVl9CTF9sRf1k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wmua9", "is_robot_indexable": true, "report_reasons": null, "author": "meowchin", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wmua9/how_to_bulk_download_public_domain_images_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wmua9/how_to_bulk_download_public_domain_images_from/", "subreddit_subscribers": 679589, "created_utc": 1682277682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With the purge upcoming, I want to a copy of everything. I found [this one](https://github.com/MonkeyMaster64/Reddit-User-Media-Downloader-Public) but it consistently dies at ~100 downloads.", "author_fullname": "t2_884ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I mass download all posts to imgur from my reddit account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wq0qp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682283676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the purge upcoming, I want to a copy of everything. I found &lt;a href=\"https://github.com/MonkeyMaster64/Reddit-User-Media-Downloader-Public\"&gt;this one&lt;/a&gt; but it consistently dies at ~100 downloads.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?auto=webp&amp;v=enabled&amp;s=d4d2a3fdf3099d8b4d11472eb6a298ac7e783837", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d2f414b99b9d8ab75cf57491ca6473aa8fc6e17", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=307344e8957630decb73a3f244a87d6e00d91ecf", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64b80e6047d05571555e7ee61454feb0c14909ec", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c1caded37338b9ceb1db0793e24980dcfece233", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c500dc0177cdd4202f660d816947e2bac0130810", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77ae399b8237e43afb40d793bd7b23ef19ca30fb", "width": 1080, "height": 540}], "variants": {}, "id": "bmywCok8x0Uer_ayc3_mT334c-neK8CHtbS6u1V2VGw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wq0qp", "is_robot_indexable": true, "report_reasons": null, "author": "Dudwithacake", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wq0qp/how_can_i_mass_download_all_posts_to_imgur_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wq0qp/how_can_i_mass_download_all_posts_to_imgur_from/", "subreddit_subscribers": 679589, "created_utc": 1682283676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was thinking about storing a backup on an additional drive and sync it once a week. Once a month I'd maybe try to access everything once via DD, but how do I check for corruption?\n\nThanks in advance!\n\nEdit: Thank you very much everyone, I am enchanted by the wonders of ZFS and running my final backup on an external ZFS drive with parity, now.\ud83d\ude0a", "author_fullname": "t2_fcazho3f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you protect yourself from failing drives and data corruption?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vz7j7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682275697.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682233246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking about storing a backup on an additional drive and sync it once a week. Once a month I&amp;#39;d maybe try to access everything once via DD, but how do I check for corruption?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;Edit: Thank you very much everyone, I am enchanted by the wonders of ZFS and running my final backup on an external ZFS drive with parity, now.\ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12vz7j7", "is_robot_indexable": true, "report_reasons": null, "author": "Zivilisationsmuede", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12vz7j7/how_do_you_protect_yourself_from_failing_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12vz7j7/how_do_you_protect_yourself_from_failing_drives/", "subreddit_subscribers": 679589, "created_utc": 1682233246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What is good program to organize and view pictures?in only one program (Plex, Jellyfin).\n\nWhat is good program to organize and view pictures? So that it's possible to see images from location within some date range and with selected peoples.", "author_fullname": "t2_p4j7gr9n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to organize and view pictures?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w3tic", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682245067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is good program to organize and view pictures?in only one program (Plex, Jellyfin).&lt;/p&gt;\n\n&lt;p&gt;What is good program to organize and view pictures? So that it&amp;#39;s possible to see images from location within some date range and with selected peoples.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12w3tic", "is_robot_indexable": true, "report_reasons": null, "author": "Beneficial-Car-3959", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12w3tic/how_to_organize_and_view_pictures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12w3tic/how_to_organize_and_view_pictures/", "subreddit_subscribers": 679589, "created_utc": 1682245067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got my data back, but it was annoying. The clone switch was not turned on and the start clone button also didn't get pressed. Somehow must have glitched and both disks got raw partitioned, no clone process actually happened. The data was recovered with r-studio and EaseUS without too much hassle.\n\nUsed this multireader for years with no issues, then boom.", "author_fullname": "t2_tqilsle9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Heads up if you are using a multi HDD read that has clone function it can trigger randomly and wipe all the disks to a raw state.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vq3or", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682211864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got my data back, but it was annoying. The clone switch was not turned on and the start clone button also didn&amp;#39;t get pressed. Somehow must have glitched and both disks got raw partitioned, no clone process actually happened. The data was recovered with r-studio and EaseUS without too much hassle.&lt;/p&gt;\n\n&lt;p&gt;Used this multireader for years with no issues, then boom.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12vq3or", "is_robot_indexable": true, "report_reasons": null, "author": "If_I_was_Lycurgus", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12vq3or/heads_up_if_you_are_using_a_multi_hdd_read_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12vq3or/heads_up_if_you_are_using_a_multi_hdd_read_that/", "subreddit_subscribers": 679589, "created_utc": 1682211864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have a collection of thousands of CDs.  I re-ripped them at 320kbps into MusicBee (approx. 2-3 yrs ago) with their onboard ripping software.  I initially didn't bother with their capability to verify with accurip, but now am regretting that decision as some discs do seem to have some popping/background noise.\n\nIs there a way to verify these files with accurip after the fact, or would I have to re-rip all my CDs to do this?\n\nThanks in advance for the input.", "author_fullname": "t2_8jcci1qb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you verify the rip accuracy of a CD after ripping is completed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wtpvi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682290990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a collection of thousands of CDs.  I re-ripped them at 320kbps into MusicBee (approx. 2-3 yrs ago) with their onboard ripping software.  I initially didn&amp;#39;t bother with their capability to verify with accurip, but now am regretting that decision as some discs do seem to have some popping/background noise.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to verify these files with accurip after the fact, or would I have to re-rip all my CDs to do this?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for the input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wtpvi", "is_robot_indexable": true, "report_reasons": null, "author": "MKdebunker", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wtpvi/can_you_verify_the_rip_accuracy_of_a_cd_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wtpvi/can_you_verify_the_rip_accuracy_of_a_cd_after/", "subreddit_subscribers": 679589, "created_utc": 1682290990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There s a website storing all the very important scanned pdf files of photography booklets magazines posters and brochures i want to back up this entire website and pdf files.\n\nhttps://www.pacificrimcamera.com/rl/rlrindex.htm\n\nhow can i auto backup site website?", "author_fullname": "t2_8c04f90yy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to download the entire pdf library from a website?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wrp6y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682286944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There s a website storing all the very important scanned pdf files of photography booklets magazines posters and brochures i want to back up this entire website and pdf files.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pacificrimcamera.com/rl/rlrindex.htm\"&gt;https://www.pacificrimcamera.com/rl/rlrindex.htm&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;how can i auto backup site website?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wrp6y", "is_robot_indexable": true, "report_reasons": null, "author": "donerfucker39", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wrp6y/how_to_download_the_entire_pdf_library_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wrp6y/how_to_download_the_entire_pdf_library_from_a/", "subreddit_subscribers": 679589, "created_utc": 1682286944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "TL;DR Is it possible to partition the 8TB drive so I can use it as storage?\n\nI have avoided setting up an NAS, maybe it is time.\n\nHere is what I have at work.\n\nBackground. My wife and I run a newspaper and an insurance agency out of the office that is next door to our house in a town of 495 population. Been doing Ins for 22 yrs and newspaper since 2015. Me and my wife are the only people that work in our office, no employees. We do have three chihuahuas that stay at work with us most days. In the front office, (one big room), we have two pcs. In the news room we have two pc's. We have desks side by side and build newspaper pages together every week. Everything is networked, hardwire, ethernet.\n\nNot that you had to know any of that, really.\n\nThe two news computers have the most data to store. Both are Dell T5500 Towers, each running Raid 1 on 1TB drives, running Win 7 Pro. Why don't we upgrade to Win 10 you ask? We are running legacy Adobe software -- Indesign CS3 with forever license. We don't want to be on Adobe's subscription services. That's the Why we are using Win 7. Bought two 8TB internal HDDs to put in these two machines, for storage only. Now you have the background.\n\nI installed the 8TB HDD but Win 7 says it is 1.3TB. My Bios is A15 version.\n\nIs it possible to partition the 8TB drive so I can use it as storage?\n\nEdit: added TL;DR at top", "author_fullname": "t2_ab522p09", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using 8TB HDD for storage on Win 7 Pro Dell T5500, Boot Drive is Raid 1 - Two 1TB HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wso3w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682289957.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682288869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR Is it possible to partition the 8TB drive so I can use it as storage?&lt;/p&gt;\n\n&lt;p&gt;I have avoided setting up an NAS, maybe it is time.&lt;/p&gt;\n\n&lt;p&gt;Here is what I have at work.&lt;/p&gt;\n\n&lt;p&gt;Background. My wife and I run a newspaper and an insurance agency out of the office that is next door to our house in a town of 495 population. Been doing Ins for 22 yrs and newspaper since 2015. Me and my wife are the only people that work in our office, no employees. We do have three chihuahuas that stay at work with us most days. In the front office, (one big room), we have two pcs. In the news room we have two pc&amp;#39;s. We have desks side by side and build newspaper pages together every week. Everything is networked, hardwire, ethernet.&lt;/p&gt;\n\n&lt;p&gt;Not that you had to know any of that, really.&lt;/p&gt;\n\n&lt;p&gt;The two news computers have the most data to store. Both are Dell T5500 Towers, each running Raid 1 on 1TB drives, running Win 7 Pro. Why don&amp;#39;t we upgrade to Win 10 you ask? We are running legacy Adobe software -- Indesign CS3 with forever license. We don&amp;#39;t want to be on Adobe&amp;#39;s subscription services. That&amp;#39;s the Why we are using Win 7. Bought two 8TB internal HDDs to put in these two machines, for storage only. Now you have the background.&lt;/p&gt;\n\n&lt;p&gt;I installed the 8TB HDD but Win 7 says it is 1.3TB. My Bios is A15 version.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to partition the 8TB drive so I can use it as storage?&lt;/p&gt;\n\n&lt;p&gt;Edit: added TL;DR at top&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wso3w", "is_robot_indexable": true, "report_reasons": null, "author": "Glass-Result-2739", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wso3w/using_8tb_hdd_for_storage_on_win_7_pro_dell_t5500/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wso3w/using_8tb_hdd_for_storage_on_win_7_pro_dell_t5500/", "subreddit_subscribers": 679589, "created_utc": 1682288869.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nI'm in a bit of a conundrum. I'm trying to download a folder from a Shared Drive in Google Drive using JDownloader 2. However, JDownloader is unable to scrape private GDrive folders only files \ud83d\ude25. I cannot share the content as a public link (user restriction applied by the Google Group that the shared drive is located in).\n\nA Discord friend recommended that I utilise AirExplorer, however, I don't trust it since it seems to be both closed source and proprietary, but it does the job (for now).\n\nI like to use applications that have both a GUI and CMD capability, with Open Source being an important factor for me. I did check the r/FREEMEDIAHECKYEAH wiki as well.\n\nWhat FOSS program would you guys recommend for me?", "author_fullname": "t2_29rfr2cc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AirExplorer Alternatives that are FOSS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vs2tl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682216248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in a bit of a conundrum. I&amp;#39;m trying to download a folder from a Shared Drive in Google Drive using JDownloader 2. However, JDownloader is unable to scrape private GDrive folders only files \ud83d\ude25. I cannot share the content as a public link (user restriction applied by the Google Group that the shared drive is located in).&lt;/p&gt;\n\n&lt;p&gt;A Discord friend recommended that I utilise AirExplorer, however, I don&amp;#39;t trust it since it seems to be both closed source and proprietary, but it does the job (for now).&lt;/p&gt;\n\n&lt;p&gt;I like to use applications that have both a GUI and CMD capability, with Open Source being an important factor for me. I did check the &lt;a href=\"/r/FREEMEDIAHECKYEAH\"&gt;r/FREEMEDIAHECKYEAH&lt;/a&gt; wiki as well.&lt;/p&gt;\n\n&lt;p&gt;What FOSS program would you guys recommend for me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "5TB. Windows and Android.", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12vs2tl", "is_robot_indexable": true, "report_reasons": null, "author": "Noobgamer0111", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12vs2tl/airexplorer_alternatives_that_are_foss/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12vs2tl/airexplorer_alternatives_that_are_foss/", "subreddit_subscribers": 679589, "created_utc": 1682216248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got two mapped drives and I'd like Macrium to be able to backup one of the volumes on my NAS to another volume on my NAS. I can select the backup volume in Macrium, but the source mapped drive isn't showing up on MR, only the \"physical\" drives are seen and able to be ticked.\n\nQNAP's various backup software offerings that I've tried so far have been ridiculous, and MR just works. I've been using it for about a year before I got my NAS and I'd like to keep using it, since I paid for it.\n\nIt's the simplest thing, really. I have a volume on my NAS with all my data, and I have another volume on the NAS that I want to use for backups of that data. I just want something that will copy all of the data over from the data volume to the backup volume, once every few days, and update the backup to be the same as the data volume. \n\nHow can I get MR to do this?", "author_fullname": "t2_96z9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Macrium Reflect: Ability to backup from a mapped NAS source to a mapped NAS destination?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12wwlvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682296707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got two mapped drives and I&amp;#39;d like Macrium to be able to backup one of the volumes on my NAS to another volume on my NAS. I can select the backup volume in Macrium, but the source mapped drive isn&amp;#39;t showing up on MR, only the &amp;quot;physical&amp;quot; drives are seen and able to be ticked.&lt;/p&gt;\n\n&lt;p&gt;QNAP&amp;#39;s various backup software offerings that I&amp;#39;ve tried so far have been ridiculous, and MR just works. I&amp;#39;ve been using it for about a year before I got my NAS and I&amp;#39;d like to keep using it, since I paid for it.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s the simplest thing, really. I have a volume on my NAS with all my data, and I have another volume on the NAS that I want to use for backups of that data. I just want something that will copy all of the data over from the data volume to the backup volume, once every few days, and update the backup to be the same as the data volume. &lt;/p&gt;\n\n&lt;p&gt;How can I get MR to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "16TB RAID0 SSD / 16TB RAID1", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wwlvo", "is_robot_indexable": true, "report_reasons": null, "author": "ultranothing", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12wwlvo/macrium_reflect_ability_to_backup_from_a_mapped/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wwlvo/macrium_reflect_ability_to_backup_from_a_mapped/", "subreddit_subscribers": 679589, "created_utc": 1682296707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "They got taken down a few years ago due to a cease and desist from the copyright holder. They\u2019re still out and about here and there, but i was wondering if someone had the full batches they used to release. Any help\u2019s appreciated!", "author_fullname": "t2_ly9uj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have a collection of Kamen Rider series subbed by TV-Nihon?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12ww3d8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682295664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They got taken down a few years ago due to a cease and desist from the copyright holder. They\u2019re still out and about here and there, but i was wondering if someone had the full batches they used to release. Any help\u2019s appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12ww3d8", "is_robot_indexable": true, "report_reasons": null, "author": "legobrak", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12ww3d8/does_anyone_have_a_collection_of_kamen_rider/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12ww3d8/does_anyone_have_a_collection_of_kamen_rider/", "subreddit_subscribers": 679589, "created_utc": 1682295664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a ton of shucked white label drives 10-16tb, a bunch of red pros and a bunch of Exos and iron wolf pro drives.\n\nLately I\u2019ve been leaning towards the iron wolf pro drives because of warranty and pricing.  Although WD did warranty my dead shucked drive.  \n\nBut I\u2019ve seen a bunch of Toshiba DC and MDD drives online.  They are much cheaper.  Are they worth going with?  I know Toshiba is a big storage name but MDD?\n\nI\u2019ve seen them on this price watching website.\n\nhttps://diskprices.com/?locale=us&amp;condition=new&amp;disk_types=external_hdd,internal_hdd\n\nEdit: upon further research, it appears that the Toshiba and WD DC drives have no warranty.  So that rules them out.\n\nThose MDD drives are mighty tempting though.", "author_fullname": "t2_ytkgh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Toshiba DC or Max Digital Data MDD drives worth considering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wrlb6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682287052.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682286735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a ton of shucked white label drives 10-16tb, a bunch of red pros and a bunch of Exos and iron wolf pro drives.&lt;/p&gt;\n\n&lt;p&gt;Lately I\u2019ve been leaning towards the iron wolf pro drives because of warranty and pricing.  Although WD did warranty my dead shucked drive.  &lt;/p&gt;\n\n&lt;p&gt;But I\u2019ve seen a bunch of Toshiba DC and MDD drives online.  They are much cheaper.  Are they worth going with?  I know Toshiba is a big storage name but MDD?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen them on this price watching website.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://diskprices.com/?locale=us&amp;amp;condition=new&amp;amp;disk_types=external_hdd,internal_hdd\"&gt;https://diskprices.com/?locale=us&amp;amp;condition=new&amp;amp;disk_types=external_hdd,internal_hdd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Edit: upon further research, it appears that the Toshiba and WD DC drives have no warranty.  So that rules them out.&lt;/p&gt;\n\n&lt;p&gt;Those MDD drives are mighty tempting though.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "600TB Unraid", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wrlb6", "is_robot_indexable": true, "report_reasons": null, "author": "sittingmongoose", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12wrlb6/are_toshiba_dc_or_max_digital_data_mdd_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wrlb6/are_toshiba_dc_or_max_digital_data_mdd_drives/", "subreddit_subscribers": 679589, "created_utc": 1682286735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey y'all,\n\nJust found this sub and gave it a follow. Basically, I'm sick &amp; tired of not being properly compensated for my data, or having any dignity over its use. Years of ad-supported socials where my data is the product has left me feeling drained and unfocused while consuming content, and I'm tired of it.\n\nI'm taking it back. I'm on a mission to stop using these services as if my data doesn't matter, download my data from each social that I've used, and create my own repository for each. Is anyone else doing this? Do you have any advice?\n\nFun fact, I already have my reddit data. I made some data art and a visualization out of it, which made me feel so connected to myself, more than any social ever offers.", "author_fullname": "t2_rvdz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on data pulling, data management, best practices, etc.?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wnwhu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682279669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y&amp;#39;all,&lt;/p&gt;\n\n&lt;p&gt;Just found this sub and gave it a follow. Basically, I&amp;#39;m sick &amp;amp; tired of not being properly compensated for my data, or having any dignity over its use. Years of ad-supported socials where my data is the product has left me feeling drained and unfocused while consuming content, and I&amp;#39;m tired of it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m taking it back. I&amp;#39;m on a mission to stop using these services as if my data doesn&amp;#39;t matter, download my data from each social that I&amp;#39;ve used, and create my own repository for each. Is anyone else doing this? Do you have any advice?&lt;/p&gt;\n\n&lt;p&gt;Fun fact, I already have my reddit data. I made some data art and a visualization out of it, which made me feel so connected to myself, more than any social ever offers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wnwhu", "is_robot_indexable": true, "report_reasons": null, "author": "eukaryote_machine", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wnwhu/advice_on_data_pulling_data_management_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wnwhu/advice_on_data_pulling_data_management_best/", "subreddit_subscribers": 679589, "created_utc": 1682279669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m new to all of this sort of thing. Doing some research, I see they charge for various api calls and classes(whatever the classes mean). Anyway, is there a way to avoid api calls? Is that a thing?", "author_fullname": "t2_4f5t898d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backblaze api calls", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vuhzj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682221671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m new to all of this sort of thing. Doing some research, I see they charge for various api calls and classes(whatever the classes mean). Anyway, is there a way to avoid api calls? Is that a thing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12vuhzj", "is_robot_indexable": true, "report_reasons": null, "author": "Steeler_Train", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12vuhzj/backblaze_api_calls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12vuhzj/backblaze_api_calls/", "subreddit_subscribers": 679589, "created_utc": 1682221671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to connect three 2.5 external HDDs at once to M1 MacBook Air. I was going to buy a powered USB hub that is 5v/4a, but was wondering if there would be any problems with doing this? Despite all the power that the hub may offer, can the Mac\u2019s usb c port handle all of it? Is a 5v/4a hub the right amount of voltage/power for simultaneous use of 3 HDDs?\n\nThis is the hub I was thinking of getting: [https://www.amazon.com/Powered-Port-Data-Card-Readers/dp/B085Q2HD97/ref=sr\\_1\\_6?crid=AIP6QTL36JFR&amp;keywords=powered%2Busb-c%2Bhub&amp;qid=1682117938&amp;s=electronics&amp;sprefix=powered%2Busb-c%2Bhub%2Celectronics%2C96&amp;sr=1-6&amp;th=1](https://www.amazon.com/Powered-Port-Data-Card-Readers/dp/B085Q2HD97/ref=sr_1_6?crid=AIP6QTL36JFR&amp;keywords=powered%2Busb-c%2Bhub&amp;qid=1682117938&amp;s=electronics&amp;sprefix=powered%2Busb-c%2Bhub%2Celectronics%2C96&amp;sr=1-6&amp;th=1)\n\nThank you!", "author_fullname": "t2_vo9ypism", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about USB hub for HDDs (MacBook Air)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vu8br", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682221054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to connect three 2.5 external HDDs at once to M1 MacBook Air. I was going to buy a powered USB hub that is 5v/4a, but was wondering if there would be any problems with doing this? Despite all the power that the hub may offer, can the Mac\u2019s usb c port handle all of it? Is a 5v/4a hub the right amount of voltage/power for simultaneous use of 3 HDDs?&lt;/p&gt;\n\n&lt;p&gt;This is the hub I was thinking of getting: &lt;a href=\"https://www.amazon.com/Powered-Port-Data-Card-Readers/dp/B085Q2HD97/ref=sr_1_6?crid=AIP6QTL36JFR&amp;amp;keywords=powered%2Busb-c%2Bhub&amp;amp;qid=1682117938&amp;amp;s=electronics&amp;amp;sprefix=powered%2Busb-c%2Bhub%2Celectronics%2C96&amp;amp;sr=1-6&amp;amp;th=1\"&gt;https://www.amazon.com/Powered-Port-Data-Card-Readers/dp/B085Q2HD97/ref=sr_1_6?crid=AIP6QTL36JFR&amp;amp;keywords=powered%2Busb-c%2Bhub&amp;amp;qid=1682117938&amp;amp;s=electronics&amp;amp;sprefix=powered%2Busb-c%2Bhub%2Celectronics%2C96&amp;amp;sr=1-6&amp;amp;th=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12vu8br", "is_robot_indexable": true, "report_reasons": null, "author": "sleepingtiger2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12vu8br/questions_about_usb_hub_for_hdds_macbook_air/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12vu8br/questions_about_usb_hub_for_hdds_macbook_air/", "subreddit_subscribers": 679589, "created_utc": 1682221054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 4 Google Drive accounts and each of them is 1-2 TB in size, total max of 8TB.\n\nI am thinking about setting up a Win 2022 Server, 32GB RAM, using Storage Space and the ReFS file system, then doing backups with CrashPlan.\n\nWhat kind of problems should I consider with this setup? My goal is to have live syncing of the drives to this system so that the files can be versioned to CrashPlan, and also use for long-term storage of this data as well.\n\nI have a couple of TB of other misc data that I've just had for many years that I might toss on there as well.\n\nThanks for any feedback!", "author_fullname": "t2_aabovddw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Win 2022 Server + ReFS + Storage Space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vrfym", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682214858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 4 Google Drive accounts and each of them is 1-2 TB in size, total max of 8TB.&lt;/p&gt;\n\n&lt;p&gt;I am thinking about setting up a Win 2022 Server, 32GB RAM, using Storage Space and the ReFS file system, then doing backups with CrashPlan.&lt;/p&gt;\n\n&lt;p&gt;What kind of problems should I consider with this setup? My goal is to have live syncing of the drives to this system so that the files can be versioned to CrashPlan, and also use for long-term storage of this data as well.&lt;/p&gt;\n\n&lt;p&gt;I have a couple of TB of other misc data that I&amp;#39;ve just had for many years that I might toss on there as well.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12vrfym", "is_robot_indexable": true, "report_reasons": null, "author": "Mr-JT", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12vrfym/win_2022_server_refs_storage_space/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12vrfym/win_2022_server_refs_storage_space/", "subreddit_subscribers": 679589, "created_utc": 1682214858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for high capacity drives on eBay and I stumbled across [this listing ](https://www.ebay.co.uk/itm/195565439832?mkcid=16&amp;mkevt=1&amp;mkrid=711-127632-2357-0&amp;ssspo=btss1z-rtvw&amp;sssrc=2349624&amp;ssuid=IEOFjMpERfW&amp;var=&amp;widget_ver=artemis&amp;media=COPY) for a new 12tb exos x drive for \u00a3150. It seems too good to be true but everything checks out. Seller has good feedback and nothing seems especially out of the ordinary apart from the price.\n\nAny reason why I shouldn't go for it?", "author_fullname": "t2_gv61xsx7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does this listing look legit? Should I buy it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wrfon", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682286849.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682286423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for high capacity drives on eBay and I stumbled across &lt;a href=\"https://www.ebay.co.uk/itm/195565439832?mkcid=16&amp;amp;mkevt=1&amp;amp;mkrid=711-127632-2357-0&amp;amp;ssspo=btss1z-rtvw&amp;amp;sssrc=2349624&amp;amp;ssuid=IEOFjMpERfW&amp;amp;var=&amp;amp;widget_ver=artemis&amp;amp;media=COPY\"&gt;this listing &lt;/a&gt; for a new 12tb exos x drive for \u00a3150. It seems too good to be true but everything checks out. Seller has good feedback and nothing seems especially out of the ordinary apart from the price.&lt;/p&gt;\n\n&lt;p&gt;Any reason why I shouldn&amp;#39;t go for it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4PdkmAnw2DTtORea6I22MTbWyKA1vAsHTo58aCr3Ttg.jpg?auto=webp&amp;v=enabled&amp;s=c7fd045d83c5f6ddcaf97211f5696d4b8b0b9996", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/4PdkmAnw2DTtORea6I22MTbWyKA1vAsHTo58aCr3Ttg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0bd7c84bdb57116ffbf8cca6b01da3bd86dd321a", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/4PdkmAnw2DTtORea6I22MTbWyKA1vAsHTo58aCr3Ttg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da4e65a62d6bfcd34c2adc388fc2e30e1b32bbbd", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/4PdkmAnw2DTtORea6I22MTbWyKA1vAsHTo58aCr3Ttg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=140ce27938d065c5e630176d1f740693a2564232", "width": 320, "height": 320}], "variants": {}, "id": "YE58jdHd3h-2MW7LcL2a6qjwFK6BOrI-dtcedgj7Mp4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wrfon", "is_robot_indexable": true, "report_reasons": null, "author": "quetzalv2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wrfon/does_this_listing_look_legit_should_i_buy_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wrfon/does_this_listing_look_legit_should_i_buy_it/", "subreddit_subscribers": 679589, "created_utc": 1682286423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have a Collection on Archive.org with thousands of items, and I want to get a list of all the direct download links of every item in the collection. They are all .mp4 videos. I do not need to download them, I just would like a .txt file or something with the direct link (where the URL ends in .mp4) with one link per line. I've been looking at their CLI for IA and it seems like this is possible with the 'ia download' option using the -d argument, but I'm having trouble getting it to work. Is this possible?", "author_fullname": "t2_9pw95", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to export a list of every URL of a file inside of an Archive.org Collection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wplj2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682282862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have a Collection on Archive.org with thousands of items, and I want to get a list of all the direct download links of every item in the collection. They are all .mp4 videos. I do not need to download them, I just would like a .txt file or something with the direct link (where the URL ends in .mp4) with one link per line. I&amp;#39;ve been looking at their CLI for IA and it seems like this is possible with the &amp;#39;ia download&amp;#39; option using the -d argument, but I&amp;#39;m having trouble getting it to work. Is this possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wplj2", "is_robot_indexable": true, "report_reasons": null, "author": "lnvis", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wplj2/is_it_possible_to_export_a_list_of_every_url_of_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wplj2/is_it_possible_to_export_a_list_of_every_url_of_a/", "subreddit_subscribers": 679589, "created_utc": 1682282862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys! I am trying to optimize my search skills to find websites that may not be archived. To do this, I am trying to search inside the robots.txt file for something related to\n\n`'site:*.[Country Code]` `filetype:txt (Noindex AND *follow)'`\n\nI have tried several search engines such as DuckDuckGo, Neeva, and others. Here are some of them for you to try. Enjoy! Some of them may be less indexed in Wayback Machine, but they are still worth it.\n\n&amp;#x200B;\n\n* [**https://rsync.rediris.es/sites/es.tld.org/LuCAS-web/**](https://rsync.rediris.es/sites/es.tld.org/LuCAS-web/) \\- A Spanish website dedicated to open-source software documentation and various IT resources.\n   * Last updated: 2007 (Surprisingly outdated!)\n\n&amp;#x200B;\n\n* [**https://www.dzexams.com/**](https://www.dzexams.com/) \\- An Algerian website offering exam resources for school children, available in French, Arabic, and Berber languages. The site features a wealth of downloadable materials.\n* &amp;#x200B;\n* [**https://blog.naver.com/ins\\_soul80**](https://blog.naver.com/ins_soul80) \\- A personal Korean blog covering IT and technology topics.", "author_fullname": "t2_8ztqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking the noindex grial websites - 001 Week - Browsing internet aimlessly.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wml3v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": " Internet Aimlessly", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682285436.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682277210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys! I am trying to optimize my search skills to find websites that may not be archived. To do this, I am trying to search inside the robots.txt file for something related to&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;#39;site:*.[Country Code]&lt;/code&gt; &lt;code&gt;filetype:txt (Noindex AND *follow)&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I have tried several search engines such as DuckDuckGo, Neeva, and others. Here are some of them for you to try. Enjoy! Some of them may be less indexed in Wayback Machine, but they are still worth it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://rsync.rediris.es/sites/es.tld.org/LuCAS-web/\"&gt;&lt;strong&gt;https://rsync.rediris.es/sites/es.tld.org/LuCAS-web/&lt;/strong&gt;&lt;/a&gt; - A Spanish website dedicated to open-source software documentation and various IT resources.\n\n&lt;ul&gt;\n&lt;li&gt;Last updated: 2007 (Surprisingly outdated!)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.dzexams.com/\"&gt;&lt;strong&gt;https://www.dzexams.com/&lt;/strong&gt;&lt;/a&gt; - An Algerian website offering exam resources for school children, available in French, Arabic, and Berber languages. The site features a wealth of downloadable materials.&lt;/li&gt;\n&lt;li&gt;&amp;#x200B;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://blog.naver.com/ins_soul80\"&gt;&lt;strong&gt;https://blog.naver.com/ins_soul80&lt;/strong&gt;&lt;/a&gt; - A personal Korean blog covering IT and technology topics.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12wml3v", "is_robot_indexable": true, "report_reasons": null, "author": "peliciego", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wml3v/looking_the_noindex_grial_websites_001_week/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wml3v/looking_the_noindex_grial_websites_001_week/", "subreddit_subscribers": 679589, "created_utc": 1682277210.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}