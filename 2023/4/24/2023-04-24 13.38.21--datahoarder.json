{"kind": "Listing", "data": {"after": "t3_12wrp6y", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have two music libraries. One of them is formatted in a very ugly way but that is necessary to leave them seeding, the other looks nice and scans perfectly into my software, but cannot be seeded.\n\nI wonder how hard it would be to back them up as essentially two slight variations of the same file, such as the way Restic can store snapshots of a modified file only taking up a small fraction of the space of a whole replacement file. I'm especially interested in finding out if this is possible without rebuilding the entire library with hardlinks, as it takes about 14 hours of continuous tagging to set up the library.\n\nIs this doable?\n\nedit: SOLVED. It turns out that Restic does take care of this, not just from incremental changes to one file, but to large quantities of similar files, which is exactly what I was looking for!", "author_fullname": "t2_cc458uu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How hard is it to backup slightly different versions of the same file without using double space?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wh5i7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 235, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 235, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682287890.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682267380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have two music libraries. One of them is formatted in a very ugly way but that is necessary to leave them seeding, the other looks nice and scans perfectly into my software, but cannot be seeded.&lt;/p&gt;\n\n&lt;p&gt;I wonder how hard it would be to back them up as essentially two slight variations of the same file, such as the way Restic can store snapshots of a modified file only taking up a small fraction of the space of a whole replacement file. I&amp;#39;m especially interested in finding out if this is possible without rebuilding the entire library with hardlinks, as it takes about 14 hours of continuous tagging to set up the library.&lt;/p&gt;\n\n&lt;p&gt;Is this doable?&lt;/p&gt;\n\n&lt;p&gt;edit: SOLVED. It turns out that Restic does take care of this, not just from incremental changes to one file, but to large quantities of similar files, which is exactly what I was looking for!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wh5i7", "is_robot_indexable": true, "report_reasons": null, "author": "SleepingAndy", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wh5i7/how_hard_is_it_to_backup_slightly_different/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wh5i7/how_hard_is_it_to_backup_slightly_different/", "subreddit_subscribers": 679633, "created_utc": 1682267380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Previously posted in January 2023, but the purge is next week so thought a reminder would be helpful given the level of comments on the New Year's post...  \n\n\nOriginal post: [https://www.reddit.com/r/DataHoarder/comments/10da7a9/official\\_synology\\_download\\_site\\_closing\\_legacy/](https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/)\n\n*The Synology Archive (their official \"not-quite-the-latest\" firmware and package installation file site for the NAS / Surveillance / Router / etc. range of products) is being shut down by 1st May 2023 due to licensing agreements.*\n\n*Only the latest versions will remain going forward.*\n\n[*https://archive.synology.com/download/*](https://archive.synology.com/download/)", "author_fullname": "t2_8vh7jb4m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Reminder) Synology legacy DSM firmware / package install files to be deleted by 1st May 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wp46q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 72, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 72, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682281939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Previously posted in January 2023, but the purge is next week so thought a reminder would be helpful given the level of comments on the New Year&amp;#39;s post...  &lt;/p&gt;\n\n&lt;p&gt;Original post: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/\"&gt;https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;The Synology Archive (their official &amp;quot;not-quite-the-latest&amp;quot; firmware and package installation file site for the NAS / Surveillance / Router / etc. range of products) is being shut down by 1st May 2023 due to licensing agreements.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Only the latest versions will remain going forward.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://archive.synology.com/download/\"&gt;&lt;em&gt;https://archive.synology.com/download/&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wp46q", "is_robot_indexable": true, "report_reasons": null, "author": "enchantedspring", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/", "subreddit_subscribers": 679633, "created_utc": 1682281939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I really like the illustrations by Ernst Haeckel: [https://www.rawpixel.com/search/Ernst%20Haeckel?page=1&amp;sort=curated&amp;topic\\_group=\\_topics](https://www.rawpixel.com/search/Ernst%20Haeckel?page=1&amp;sort=curated&amp;topic_group=_topics) They're public domain and free to download form rawpixel, however there's almost 600 of them an manually clicking through all of them is a lot of hassle. Is there a way to bulk download these?", "author_fullname": "t2_mtwvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to bulk download public domain images from rawpixel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wmua9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682277682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really like the illustrations by Ernst Haeckel: &lt;a href=\"https://www.rawpixel.com/search/Ernst%20Haeckel?page=1&amp;amp;sort=curated&amp;amp;topic_group=_topics\"&gt;https://www.rawpixel.com/search/Ernst%20Haeckel?page=1&amp;amp;sort=curated&amp;amp;topic_group=_topics&lt;/a&gt; They&amp;#39;re public domain and free to download form rawpixel, however there&amp;#39;s almost 600 of them an manually clicking through all of them is a lot of hassle. Is there a way to bulk download these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?auto=webp&amp;v=enabled&amp;s=1af3b5a64226b5aea4c938d0240a5b52557c7f0b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5541b0f2230e1e06628cf221b6eaa6e729c88580", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fe6f131b1c18a1cf2771cad5139c452799b6514", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b14361b294cff32078277d4073801004e45232c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=684622a9ad1c8d2c1ad0ced5f7852ece33db3d35", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d0428d9308767ac79b9167cf01f3fad8ff6bdf1", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/V_bJzSkgiA3Nb5EIZrlXn_eg07G7xSuMNPxdFplhVrs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e59cbab567b4d17e4915ddc6fa7f2a01c722fc34", "width": 1080, "height": 567}], "variants": {}, "id": "HIrJldySEIpGoDvol5hbo5YRljcp3DpVl9CTF9sRf1k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wmua9", "is_robot_indexable": true, "report_reasons": null, "author": "meowchin", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wmua9/how_to_bulk_download_public_domain_images_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wmua9/how_to_bulk_download_public_domain_images_from/", "subreddit_subscribers": 679633, "created_utc": 1682277682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With the purge upcoming, I want to a copy of everything. I found [this one](https://github.com/MonkeyMaster64/Reddit-User-Media-Downloader-Public) but it consistently dies at ~100 downloads.", "author_fullname": "t2_884ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I mass download all posts to imgur from my reddit account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wq0qp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682283676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the purge upcoming, I want to a copy of everything. I found &lt;a href=\"https://github.com/MonkeyMaster64/Reddit-User-Media-Downloader-Public\"&gt;this one&lt;/a&gt; but it consistently dies at ~100 downloads.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?auto=webp&amp;v=enabled&amp;s=d4d2a3fdf3099d8b4d11472eb6a298ac7e783837", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d2f414b99b9d8ab75cf57491ca6473aa8fc6e17", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=307344e8957630decb73a3f244a87d6e00d91ecf", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64b80e6047d05571555e7ee61454feb0c14909ec", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c1caded37338b9ceb1db0793e24980dcfece233", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c500dc0177cdd4202f660d816947e2bac0130810", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/WZ3dtpUsOEIbQeGifv8ZGdAbEYt_nUAWEE6oqjRPHHU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77ae399b8237e43afb40d793bd7b23ef19ca30fb", "width": 1080, "height": 540}], "variants": {}, "id": "bmywCok8x0Uer_ayc3_mT334c-neK8CHtbS6u1V2VGw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wq0qp", "is_robot_indexable": true, "report_reasons": null, "author": "Dudwithacake", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wq0qp/how_can_i_mass_download_all_posts_to_imgur_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wq0qp/how_can_i_mass_download_all_posts_to_imgur_from/", "subreddit_subscribers": 679633, "created_utc": 1682283676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I haven't seen such a product but I was wondering if there was some integrated automatic seamless solution to my requirements. \n\nI have baby pictures of my daughter that are worth all the treasures of the world that I want to protect. \n\nI am looking for something like an external hard-drive, which has two seperate hard-drives within it. One hard-drive automatically back-ups onto the second hard-drive with a mirror image. The idea being if one hard-drive fails, the second is good to go. \n\nI really want this to be easy and automatic. I don't really want to set up a NAS. Not really looking to overcomplicate things. I just want something that is consumer grade, works out of the box and idiot proof with minimal set up or ongoing maintenance. Ideally, it is using some reputable company that stands by their product. I am not too worried about budget, as long as it achieves these requirements. \n\nFrom some research, seems like NAS/DAS solutions are still a little too involved for my liking. \n\nI don't need a huge amount of storage, no more than 4TB for each mirror (8TB total?)", "author_fullname": "t2_7r9ya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consumer grade external hard-drive with multiple seperate internal hard-drives that automatically create mirror back-ups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12x1cil", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682306642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I haven&amp;#39;t seen such a product but I was wondering if there was some integrated automatic seamless solution to my requirements. &lt;/p&gt;\n\n&lt;p&gt;I have baby pictures of my daughter that are worth all the treasures of the world that I want to protect. &lt;/p&gt;\n\n&lt;p&gt;I am looking for something like an external hard-drive, which has two seperate hard-drives within it. One hard-drive automatically back-ups onto the second hard-drive with a mirror image. The idea being if one hard-drive fails, the second is good to go. &lt;/p&gt;\n\n&lt;p&gt;I really want this to be easy and automatic. I don&amp;#39;t really want to set up a NAS. Not really looking to overcomplicate things. I just want something that is consumer grade, works out of the box and idiot proof with minimal set up or ongoing maintenance. Ideally, it is using some reputable company that stands by their product. I am not too worried about budget, as long as it achieves these requirements. &lt;/p&gt;\n\n&lt;p&gt;From some research, seems like NAS/DAS solutions are still a little too involved for my liking. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t need a huge amount of storage, no more than 4TB for each mirror (8TB total?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12x1cil", "is_robot_indexable": true, "report_reasons": null, "author": "andrew_bolkonski", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12x1cil/consumer_grade_external_harddrive_with_multiple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12x1cil/consumer_grade_external_harddrive_with_multiple/", "subreddit_subscribers": 679633, "created_utc": 1682306642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "At 8th of January of 2023, the Pra\u00e7a dos Tr\u00eas Poderes (the political nuclei of the capital of Brazil) suffered an invasion by terrorists.\n\nSkipping (maybe) unwanted details, **the part of the government for the buildings and it's recordings, released the relative ones as a OneDrive folder (here: https://drive.presidencia.gov.br/public/615ba7), as stated in [this Reddit post](https://www.reddit.com/r/brasil/comments/12wcheo/ricardo_cappelli_chefe_interino_do_gsi_compatilha/).**\n\nSo, I ask for help keeping these files alive, as the Brazilian government is knowingly prone to hacker attacks in order to share private information, or **destroy public data**.\n\nIn my part, I downloaded some of the folders (just what my SSD can hold besides my normal use, as it's all I have) and created a torrent file, and will be sharing it.\n\nFrom your part, I ask to **teach me** on **what else can I do** to preserve these recordings, as this community knows how to do it.\n\nThank you all for your time.\n\n\\*The file can be found at https://file.io/lABKMFAZCwL6\n\nEditing log: Fixed File.io link. I should search for another file hoster...", "author_fullname": "t2_5lsk2df0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oficial recordings of the invasion of the Brazilian capital at 8th of January of 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12x0oqo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682336686.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682305236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At 8th of January of 2023, the Pra\u00e7a dos Tr\u00eas Poderes (the political nuclei of the capital of Brazil) suffered an invasion by terrorists.&lt;/p&gt;\n\n&lt;p&gt;Skipping (maybe) unwanted details, &lt;strong&gt;the part of the government for the buildings and it&amp;#39;s recordings, released the relative ones as a OneDrive folder (here: &lt;a href=\"https://drive.presidencia.gov.br/public/615ba7\"&gt;https://drive.presidencia.gov.br/public/615ba7&lt;/a&gt;), as stated in &lt;a href=\"https://www.reddit.com/r/brasil/comments/12wcheo/ricardo_cappelli_chefe_interino_do_gsi_compatilha/\"&gt;this Reddit post&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So, I ask for help keeping these files alive, as the Brazilian government is knowingly prone to hacker attacks in order to share private information, or &lt;strong&gt;destroy public data&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;In my part, I downloaded some of the folders (just what my SSD can hold besides my normal use, as it&amp;#39;s all I have) and created a torrent file, and will be sharing it.&lt;/p&gt;\n\n&lt;p&gt;From your part, I ask to &lt;strong&gt;teach me&lt;/strong&gt; on &lt;strong&gt;what else can I do&lt;/strong&gt; to preserve these recordings, as this community knows how to do it.&lt;/p&gt;\n\n&lt;p&gt;Thank you all for your time.&lt;/p&gt;\n\n&lt;p&gt;*The file can be found at &lt;a href=\"https://file.io/lABKMFAZCwL6\"&gt;https://file.io/lABKMFAZCwL6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Editing log: Fixed File.io link. I should search for another file hoster...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Blockchains? I save my data on torrents.", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12x0oqo", "is_robot_indexable": true, "report_reasons": null, "author": "zekkious", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12x0oqo/oficial_recordings_of_the_invasion_of_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12x0oqo/oficial_recordings_of_the_invasion_of_the/", "subreddit_subscribers": 679633, "created_utc": 1682305236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was trying to get RedditDownloader to work which seemed promising: https://shadowmoose.github.io/RedditDownloader/Getting_Started/Sources/\n\nI want to scrape the content from each post. I tried getting this to work in the command line but I'm having some difficulties because the `psaw` python package is deprecated and the downloader repo seems to be unmaintained. I tried replacing the `psaw` packages with the `pmaw` ones but no luck.", "author_fullname": "t2_594701l5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solution for mass download of GDPR saved_posts.csv?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xats9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682331534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to get RedditDownloader to work which seemed promising: &lt;a href=\"https://shadowmoose.github.io/RedditDownloader/Getting_Started/Sources/\"&gt;https://shadowmoose.github.io/RedditDownloader/Getting_Started/Sources/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I want to scrape the content from each post. I tried getting this to work in the command line but I&amp;#39;m having some difficulties because the &lt;code&gt;psaw&lt;/code&gt; python package is deprecated and the downloader repo seems to be unmaintained. I tried replacing the &lt;code&gt;psaw&lt;/code&gt; packages with the &lt;code&gt;pmaw&lt;/code&gt; ones but no luck.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12xats9", "is_robot_indexable": true, "report_reasons": null, "author": "agw_sommelier", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12xats9/solution_for_mass_download_of_gdpr_saved_postscsv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12xats9/solution_for_mass_download_of_gdpr_saved_postscsv/", "subreddit_subscribers": 679633, "created_utc": 1682331534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have a collection of thousands of CDs.  I re-ripped them at 320kbps into MusicBee (approx. 2-3 yrs ago) with their onboard ripping software.  I initially didn't bother with their capability to verify with accurip, but now am regretting that decision as some discs do seem to have some popping/background noise.\n\nIs there a way to verify these files with accurip after the fact, or would I have to re-rip all my CDs to do this?\n\nThanks in advance for the input.", "author_fullname": "t2_8jcci1qb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you verify the rip accuracy of a CD after ripping is completed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wtpvi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682290990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a collection of thousands of CDs.  I re-ripped them at 320kbps into MusicBee (approx. 2-3 yrs ago) with their onboard ripping software.  I initially didn&amp;#39;t bother with their capability to verify with accurip, but now am regretting that decision as some discs do seem to have some popping/background noise.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to verify these files with accurip after the fact, or would I have to re-rip all my CDs to do this?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for the input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wtpvi", "is_robot_indexable": true, "report_reasons": null, "author": "MKdebunker", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wtpvi/can_you_verify_the_rip_accuracy_of_a_cd_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wtpvi/can_you_verify_the_rip_accuracy_of_a_cd_after/", "subreddit_subscribers": 679633, "created_utc": 1682290990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "TL;DR Is it possible to partition the 8TB drive so I can use it as storage?\n\nI have avoided setting up an NAS, maybe it is time.\n\nHere is what I have at work.\n\nBackground. My wife and I run a newspaper and an insurance agency out of the office that is next door to our house in a town of 495 population. Been doing Ins for 22 yrs and newspaper since 2015. Me and my wife are the only people that work in our office, no employees. We do have three chihuahuas that stay at work with us most days. In the front office, (one big room), we have two pcs. In the news room we have two pc's. We have desks side by side and build newspaper pages together every week. Everything is networked, hardwire, ethernet.\n\nNot that you had to know any of that, really.\n\nThe two news computers have the most data to store. Both are Dell T5500 Towers, each running Raid 1 on 1TB drives, running Win 7 Pro. Why don't we upgrade to Win 10 you ask? We are running legacy Adobe software -- Indesign CS3 with forever license. We don't want to be on Adobe's subscription services. That's the Why we are using Win 7. Bought two 8TB internal HDDs to put in these two machines, for storage only. Now you have the background.\n\nI installed the 8TB HDD but Win 7 says it is 1.3TB. My Bios is A15 version.\n\nIs it possible to partition the 8TB drive so I can use it as storage?\n\nEdit: added TL;DR at top", "author_fullname": "t2_ab522p09", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using 8TB HDD for storage on Win 7 Pro Dell T5500, Boot Drive is Raid 1 - Two 1TB HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wso3w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682289957.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682288869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR Is it possible to partition the 8TB drive so I can use it as storage?&lt;/p&gt;\n\n&lt;p&gt;I have avoided setting up an NAS, maybe it is time.&lt;/p&gt;\n\n&lt;p&gt;Here is what I have at work.&lt;/p&gt;\n\n&lt;p&gt;Background. My wife and I run a newspaper and an insurance agency out of the office that is next door to our house in a town of 495 population. Been doing Ins for 22 yrs and newspaper since 2015. Me and my wife are the only people that work in our office, no employees. We do have three chihuahuas that stay at work with us most days. In the front office, (one big room), we have two pcs. In the news room we have two pc&amp;#39;s. We have desks side by side and build newspaper pages together every week. Everything is networked, hardwire, ethernet.&lt;/p&gt;\n\n&lt;p&gt;Not that you had to know any of that, really.&lt;/p&gt;\n\n&lt;p&gt;The two news computers have the most data to store. Both are Dell T5500 Towers, each running Raid 1 on 1TB drives, running Win 7 Pro. Why don&amp;#39;t we upgrade to Win 10 you ask? We are running legacy Adobe software -- Indesign CS3 with forever license. We don&amp;#39;t want to be on Adobe&amp;#39;s subscription services. That&amp;#39;s the Why we are using Win 7. Bought two 8TB internal HDDs to put in these two machines, for storage only. Now you have the background.&lt;/p&gt;\n\n&lt;p&gt;I installed the 8TB HDD but Win 7 says it is 1.3TB. My Bios is A15 version.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to partition the 8TB drive so I can use it as storage?&lt;/p&gt;\n\n&lt;p&gt;Edit: added TL;DR at top&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wso3w", "is_robot_indexable": true, "report_reasons": null, "author": "Glass-Result-2739", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wso3w/using_8tb_hdd_for_storage_on_win_7_pro_dell_t5500/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wso3w/using_8tb_hdd_for_storage_on_win_7_pro_dell_t5500/", "subreddit_subscribers": 679633, "created_utc": 1682288869.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Basic question about HDD specifications:\n\nI'm about to buy HDDs for a NAS. The NAS I'm looking to buy is 10GbE with a link aggregation possibility of 20 GbE. I want to utilise this to its maximum potential.\n\nSo, my question is about the HDD I'm looking to buy, if it says SATA 6 GB/s interface, does it mean it can only transfer up to 6 GB/s and not able to use the 10 GbE or 20 GbE? \n\nAlso, a cache of 512MB is good, right? Better than 256MB I have seen on other drives.", "author_fullname": "t2_ffjtxxr5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Clarification on Specifications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12xeq06", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682340734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basic question about HDD specifications:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m about to buy HDDs for a NAS. The NAS I&amp;#39;m looking to buy is 10GbE with a link aggregation possibility of 20 GbE. I want to utilise this to its maximum potential.&lt;/p&gt;\n\n&lt;p&gt;So, my question is about the HDD I&amp;#39;m looking to buy, if it says SATA 6 GB/s interface, does it mean it can only transfer up to 6 GB/s and not able to use the 10 GbE or 20 GbE? &lt;/p&gt;\n\n&lt;p&gt;Also, a cache of 512MB is good, right? Better than 256MB I have seen on other drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12xeq06", "is_robot_indexable": true, "report_reasons": null, "author": "101az", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12xeq06/clarification_on_specifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12xeq06/clarification_on_specifications/", "subreddit_subscribers": 679633, "created_utc": 1682340734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_24u5cpux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The ESA archive in Italy. Didn't know that they had 1TB \"DVD\" in the '80!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_12xe9vw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/2t0LvMn8_G4?start=1213&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"L&amp;#39;incredibile evoluzione tecnologica dei dati satellitari\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "L'incredibile evoluzione tecnologica dei dati satellitari", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/2t0LvMn8_G4?start=1213&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"L&amp;#39;incredibile evoluzione tecnologica dei dati satellitari\"&gt;&lt;/iframe&gt;", "author_name": "Barbascura eXtra", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/2t0LvMn8_G4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@BarbascuraEXtra"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/2t0LvMn8_G4?start=1213&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"L&amp;#39;incredibile evoluzione tecnologica dei dati satellitari\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12xe9vw", "height": 200}, "link_flair_text": "News", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/El-dBFQ1gkEkGXB0eauJ1aBJFs8iZI8u09WE8USnw7A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682339808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=2t0LvMn8_G4&amp;t=1213s", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nVBQpuaS9IFLpJLo-a5oOamPN0le3nAq9GaNKrp_9K8.jpg?auto=webp&amp;v=enabled&amp;s=9798b06123fd4227b595a28ffa5da560c8e25f8e", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/nVBQpuaS9IFLpJLo-a5oOamPN0le3nAq9GaNKrp_9K8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4077e1b71231e314e3359d6fa6929d8564b2b087", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/nVBQpuaS9IFLpJLo-a5oOamPN0le3nAq9GaNKrp_9K8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b08e427d78b01370123b5dec86fcec424e9dc4c4", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/nVBQpuaS9IFLpJLo-a5oOamPN0le3nAq9GaNKrp_9K8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7448adf522399626cfd62cd049ed4cc39611f6d8", "width": 320, "height": 240}], "variants": {}, "id": "oXfq-fGx6Ivp58OHX9jpZpSYBSoZ-AvvyVhTT6GIxzM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12xe9vw", "is_robot_indexable": true, "report_reasons": null, "author": "TopdeckIsSkill", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12xe9vw/the_esa_archive_in_italy_didnt_know_that_they_had/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=2t0LvMn8_G4&amp;t=1213s", "subreddit_subscribers": 679633, "created_utc": 1682339808.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "L'incredibile evoluzione tecnologica dei dati satellitari", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/2t0LvMn8_G4?start=1213&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"L&amp;#39;incredibile evoluzione tecnologica dei dati satellitari\"&gt;&lt;/iframe&gt;", "author_name": "Barbascura eXtra", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/2t0LvMn8_G4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@BarbascuraEXtra"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Got an old WD My Cloud Home Duo 16TB from my dad awhile ago, have used it for ages, and need help. \n\nInitially my dad wanted to use it, as a hard drive, but refused to understand/accept what a NAS is, what its purpose/use is and how to properly use it. He got frustrated so rather than take it back to the store for a refund or get something proper, he gave it to me. \n\nFor the past 18 months I have been able to use it as my primary drive (we were moving house) but I had no other choice but to keep it connected to our modem, and in a cupboard not properly vented. Since he at least passed the setup phase, his email address is still attached to it, so every time it overheats and shuts down, he gives me shit for it, even though it was never stored correctly/looked after properly or vented correctly out in the open. \n\nNow I have finally moved into my new house, and want to start using it again but I am worried that if I connect it into the modem and it overheats, he will get an email and i'll be up shit creek again.\n\nIs there anyway I can shuck it, or remove the drives and put it, into another case/storage option or find a way around it at all? Can I use this NAS any other way, then it's intended purpose.", "author_fullname": "t2_d7675", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Change a WD My Cloud Home Duo.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12xdrb4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682338728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got an old WD My Cloud Home Duo 16TB from my dad awhile ago, have used it for ages, and need help. &lt;/p&gt;\n\n&lt;p&gt;Initially my dad wanted to use it, as a hard drive, but refused to understand/accept what a NAS is, what its purpose/use is and how to properly use it. He got frustrated so rather than take it back to the store for a refund or get something proper, he gave it to me. &lt;/p&gt;\n\n&lt;p&gt;For the past 18 months I have been able to use it as my primary drive (we were moving house) but I had no other choice but to keep it connected to our modem, and in a cupboard not properly vented. Since he at least passed the setup phase, his email address is still attached to it, so every time it overheats and shuts down, he gives me shit for it, even though it was never stored correctly/looked after properly or vented correctly out in the open. &lt;/p&gt;\n\n&lt;p&gt;Now I have finally moved into my new house, and want to start using it again but I am worried that if I connect it into the modem and it overheats, he will get an email and i&amp;#39;ll be up shit creek again.&lt;/p&gt;\n\n&lt;p&gt;Is there anyway I can shuck it, or remove the drives and put it, into another case/storage option or find a way around it at all? Can I use this NAS any other way, then it&amp;#39;s intended purpose.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12xdrb4", "is_robot_indexable": true, "report_reasons": null, "author": "DrWho345", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12xdrb4/change_a_wd_my_cloud_home_duo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12xdrb4/change_a_wd_my_cloud_home_duo/", "subreddit_subscribers": 679633, "created_utc": 1682338728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Was looking on Amazon UK and found the one touch hub on a really good deal, \u00a3160 for the 10tb model, making it \u00a350 cheaper than the cheapest available bare drive.\n\nI just wanted to check if the drive inside was fit for purpose. I'm looking for a 7200 rpm drive, ideally CMR. Anyone know which drives Seagate uses in them?\n\nHas anyone had any experience with them?", "author_fullname": "t2_gv61xsx7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate one touch hub 10tb shuck", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xa2ut", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682329359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was looking on Amazon UK and found the one touch hub on a really good deal, \u00a3160 for the 10tb model, making it \u00a350 cheaper than the cheapest available bare drive.&lt;/p&gt;\n\n&lt;p&gt;I just wanted to check if the drive inside was fit for purpose. I&amp;#39;m looking for a 7200 rpm drive, ideally CMR. Anyone know which drives Seagate uses in them?&lt;/p&gt;\n\n&lt;p&gt;Has anyone had any experience with them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12xa2ut", "is_robot_indexable": true, "report_reasons": null, "author": "quetzalv2", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12xa2ut/seagate_one_touch_hub_10tb_shuck/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12xa2ut/seagate_one_touch_hub_10tb_shuck/", "subreddit_subscribers": 679633, "created_utc": 1682329359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For anyone who is about to lose their Netflix DVD subscription, we have found some partial alternatives.\n\n[https://www.reddit.com/r/NetflixDVDRevival](https://www.reddit.com/r/NetflixDVDRevival)", "author_fullname": "t2_6h5l9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives to Netflix DVD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wz8dk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682302264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For anyone who is about to lose their Netflix DVD subscription, we have found some partial alternatives.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/NetflixDVDRevival\"&gt;https://www.reddit.com/r/NetflixDVDRevival&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12wz8dk", "is_robot_indexable": true, "report_reasons": null, "author": "CALIGVLA", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wz8dk/alternatives_to_netflix_dvd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wz8dk/alternatives_to_netflix_dvd/", "subreddit_subscribers": 679633, "created_utc": 1682302264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone. I was considering zfs but discovered OpenZFS for Windows. Can I get a sanity check on my upgrade path?\n___\n###Currently\n\n* Jellyfin on Windows 11 (Latitude 7300)\n* 8TB primary, 18TB backing up vs [FreeFileSync](https://freefilesync.org/)\n* Mediasonic Probox 4-bay (S3) DAS, via USB\n\nPreviously had the 8TB in a UASP enclosure, but monthly resets and growing storage needs means I needed some intermediate. Got the Mediasonic for basic JBOD over the next few months while I plan/shop/configure the end-goal. If I fill the 8TB, I'll just switch to the 18TB for primary and shopping more diligently. \n\nI don't really want to switch from Windows either, since I'm comfortable with it and Dell includes battery and power management features I'm not sure I could implement in whatever distro I'd go with. I bought the [business half of a laptop for $100](https://i.imgur.com/eCKfNcT.jpg) and it transcodes well.\n\n___\n\n###End-goal\n\n* Mini-ITX based NAS, 4-drives, 1 NVME cache (prob unnecessary)\n* Same Jellyfin server, just pointing to NAS (maybe still connected as DAS, who knows)\n* Some kind of 3-4 drive zRAID with 1 drive tolerance\n\nI want to separate my storage from my media server. Idk, I need to start thinking more about transitioning to Home Assistant. It'll be a lot of work since I have tons of different devices across ecosystems (Kasa, Philips, Ecobee, Samsung, etc). Still, I'd prefer some kind of central home management that includes storage and media delivery. I haven't even begun to plan out surveillance and storage, ugh. Can I do that with ZFS too? Just all in one box, but some purple drives that will only take surveillance footage. \n\n___\n\nI'm getting ahead of myself. I want to trial ZFS first. My drives are NTFS so I'll just format the new one, copy over, format the old one, copy back; proceed? I intend to run ZFS on Windows first with JBOD, and just set up a regular job to sync the two drives. When I actually fill up the 8TB, I'll buy one or two more 18TBs stay JBOD for a while until I build a system.", "author_fullname": "t2_12uui09p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feedback: Media Storage solution path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wybeu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682300623.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682300238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I was considering zfs but discovered OpenZFS for Windows. Can I get a sanity check on my upgrade path?&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Currently&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Jellyfin on Windows 11 (Latitude 7300)&lt;/li&gt;\n&lt;li&gt;8TB primary, 18TB backing up vs &lt;a href=\"https://freefilesync.org/\"&gt;FreeFileSync&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Mediasonic Probox 4-bay (S3) DAS, via USB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Previously had the 8TB in a UASP enclosure, but monthly resets and growing storage needs means I needed some intermediate. Got the Mediasonic for basic JBOD over the next few months while I plan/shop/configure the end-goal. If I fill the 8TB, I&amp;#39;ll just switch to the 18TB for primary and shopping more diligently. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t really want to switch from Windows either, since I&amp;#39;m comfortable with it and Dell includes battery and power management features I&amp;#39;m not sure I could implement in whatever distro I&amp;#39;d go with. I bought the &lt;a href=\"https://i.imgur.com/eCKfNcT.jpg\"&gt;business half of a laptop for $100&lt;/a&gt; and it transcodes well.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;End-goal&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Mini-ITX based NAS, 4-drives, 1 NVME cache (prob unnecessary)&lt;/li&gt;\n&lt;li&gt;Same Jellyfin server, just pointing to NAS (maybe still connected as DAS, who knows)&lt;/li&gt;\n&lt;li&gt;Some kind of 3-4 drive zRAID with 1 drive tolerance&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I want to separate my storage from my media server. Idk, I need to start thinking more about transitioning to Home Assistant. It&amp;#39;ll be a lot of work since I have tons of different devices across ecosystems (Kasa, Philips, Ecobee, Samsung, etc). Still, I&amp;#39;d prefer some kind of central home management that includes storage and media delivery. I haven&amp;#39;t even begun to plan out surveillance and storage, ugh. Can I do that with ZFS too? Just all in one box, but some purple drives that will only take surveillance footage. &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I&amp;#39;m getting ahead of myself. I want to trial ZFS first. My drives are NTFS so I&amp;#39;ll just format the new one, copy over, format the old one, copy back; proceed? I intend to run ZFS on Windows first with JBOD, and just set up a regular job to sync the two drives. When I actually fill up the 8TB, I&amp;#39;ll buy one or two more 18TBs stay JBOD for a while until I build a system.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vjv6RPOWJxSOiRYPynTwPtAQXMAMStdr31FcRFd6whk.jpg?auto=webp&amp;v=enabled&amp;s=2f53df899952d5a272b65ecf9b832bb2c915bc4f", "width": 2604, "height": 2690}, "resolutions": [{"url": "https://external-preview.redd.it/vjv6RPOWJxSOiRYPynTwPtAQXMAMStdr31FcRFd6whk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa5a0bebc82704ff4916a085bcd68562f7df3af5", "width": 108, "height": 111}, {"url": "https://external-preview.redd.it/vjv6RPOWJxSOiRYPynTwPtAQXMAMStdr31FcRFd6whk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=686beaaba92cd5849f415912dfc5ed7e6535bdd7", "width": 216, "height": 223}, {"url": "https://external-preview.redd.it/vjv6RPOWJxSOiRYPynTwPtAQXMAMStdr31FcRFd6whk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aac5482e9db31f0cc6af503bf807239a117e4f4a", "width": 320, "height": 330}, {"url": "https://external-preview.redd.it/vjv6RPOWJxSOiRYPynTwPtAQXMAMStdr31FcRFd6whk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1523ea5033722b85f3703add72ae4aba5b86c505", "width": 640, "height": 661}, {"url": "https://external-preview.redd.it/vjv6RPOWJxSOiRYPynTwPtAQXMAMStdr31FcRFd6whk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2dd6d829eee1c5554c0f3ecd3525ae1f938ba3ab", "width": 960, "height": 991}, {"url": "https://external-preview.redd.it/vjv6RPOWJxSOiRYPynTwPtAQXMAMStdr31FcRFd6whk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9982c7c6ff14d68a694d72c7bafd528d55f7c437", "width": 1080, "height": 1115}], "variants": {}, "id": "ugNsyX2h_6HDRZ-xDOKtezm3URm3uBlh7J0YcN4K9sI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wybeu", "is_robot_indexable": true, "report_reasons": null, "author": "Hung_L", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wybeu/feedback_media_storage_solution_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wybeu/feedback_media_storage_solution_path/", "subreddit_subscribers": 679633, "created_utc": 1682300238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got two mapped drives and I'd like Macrium to be able to backup one of the volumes on my NAS to another volume on my NAS. I can select the backup volume in Macrium, but the source mapped drive isn't showing up on MR, only the \"physical\" drives are seen and able to be ticked.\n\nQNAP's various backup software offerings that I've tried so far have been ridiculous, and MR just works. I've been using it for about a year before I got my NAS and I'd like to keep using it, since I paid for it.\n\nIt's the simplest thing, really. I have a volume on my NAS with all my data, and I have another volume on the NAS that I want to use for backups of that data. I just want something that will copy all of the data over from the data volume to the backup volume, once every few days, and update the backup to be the same as the data volume. \n\nHow can I get MR to do this?", "author_fullname": "t2_96z9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Macrium Reflect: Ability to backup from a mapped NAS source to a mapped NAS destination?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wwlvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682296707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got two mapped drives and I&amp;#39;d like Macrium to be able to backup one of the volumes on my NAS to another volume on my NAS. I can select the backup volume in Macrium, but the source mapped drive isn&amp;#39;t showing up on MR, only the &amp;quot;physical&amp;quot; drives are seen and able to be ticked.&lt;/p&gt;\n\n&lt;p&gt;QNAP&amp;#39;s various backup software offerings that I&amp;#39;ve tried so far have been ridiculous, and MR just works. I&amp;#39;ve been using it for about a year before I got my NAS and I&amp;#39;d like to keep using it, since I paid for it.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s the simplest thing, really. I have a volume on my NAS with all my data, and I have another volume on the NAS that I want to use for backups of that data. I just want something that will copy all of the data over from the data volume to the backup volume, once every few days, and update the backup to be the same as the data volume. &lt;/p&gt;\n\n&lt;p&gt;How can I get MR to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "16TB RAID0 SSD / 16TB RAID1", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wwlvo", "is_robot_indexable": true, "report_reasons": null, "author": "ultranothing", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12wwlvo/macrium_reflect_ability_to_backup_from_a_mapped/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wwlvo/macrium_reflect_ability_to_backup_from_a_mapped/", "subreddit_subscribers": 679633, "created_utc": 1682296707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "About 2 months ago, I RMA\u2019d 2 New Exos Hard Drives -18 TB &amp; 16 TB. I had them unused for a while because I assumed my USB devices couldn\u2019t handle it. I finally RMA\u2019d them with the assumption that they were DOA. Return was easy and I received replacement HDD\u2019s. I know they replace with refurbished drives. Now the 16TB drive is showing 7000+ reallocated sectors so looks like I will have to RMA this drive. Has anyone had a replacement under warranty fail so quickly? Do they require SeaTools test? I know there was nothing about requiring test when I RMA\u2019d drives the first time.", "author_fullname": "t2_4crjceau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate RMA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wnufz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682279557.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About 2 months ago, I RMA\u2019d 2 New Exos Hard Drives -18 TB &amp;amp; 16 TB. I had them unused for a while because I assumed my USB devices couldn\u2019t handle it. I finally RMA\u2019d them with the assumption that they were DOA. Return was easy and I received replacement HDD\u2019s. I know they replace with refurbished drives. Now the 16TB drive is showing 7000+ reallocated sectors so looks like I will have to RMA this drive. Has anyone had a replacement under warranty fail so quickly? Do they require SeaTools test? I know there was nothing about requiring test when I RMA\u2019d drives the first time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wnufz", "is_robot_indexable": true, "report_reasons": null, "author": "Socialdis99", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wnufz/seagate_rma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wnufz/seagate_rma/", "subreddit_subscribers": 679633, "created_utc": 1682279557.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys! I am trying to optimize my search skills to find websites that may not be archived. To do this, I am trying to search inside the robots.txt file for something related to\n\n`'site:*.[Country Code]` `filetype:txt (Noindex AND *follow)'`\n\nI have tried several search engines such as DuckDuckGo, Neeva, and others. Here are some of them for you to try. Enjoy! Some of them may be less indexed in Wayback Machine, but they are still worth it.\n\n&amp;#x200B;\n\n* [**https://rsync.rediris.es/sites/es.tld.org/LuCAS-web/**](https://rsync.rediris.es/sites/es.tld.org/LuCAS-web/) \\- A Spanish website dedicated to open-source software documentation and various IT resources.\n   * Last updated: 2007 (Surprisingly outdated!)\n\n&amp;#x200B;\n\n* [**https://www.dzexams.com/**](https://www.dzexams.com/) \\- An Algerian website offering exam resources for school children, available in French, Arabic, and Berber languages. The site features a wealth of downloadable materials.\n* &amp;#x200B;\n* [**https://blog.naver.com/ins\\_soul80**](https://blog.naver.com/ins_soul80) \\- A personal Korean blog covering IT and technology topics.", "author_fullname": "t2_8ztqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking the noindex grial websites - 001 Week - Browsing internet aimlessly.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wml3v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": " Internet Aimlessly", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682285436.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682277210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys! I am trying to optimize my search skills to find websites that may not be archived. To do this, I am trying to search inside the robots.txt file for something related to&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;#39;site:*.[Country Code]&lt;/code&gt; &lt;code&gt;filetype:txt (Noindex AND *follow)&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I have tried several search engines such as DuckDuckGo, Neeva, and others. Here are some of them for you to try. Enjoy! Some of them may be less indexed in Wayback Machine, but they are still worth it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://rsync.rediris.es/sites/es.tld.org/LuCAS-web/\"&gt;&lt;strong&gt;https://rsync.rediris.es/sites/es.tld.org/LuCAS-web/&lt;/strong&gt;&lt;/a&gt; - A Spanish website dedicated to open-source software documentation and various IT resources.\n\n&lt;ul&gt;\n&lt;li&gt;Last updated: 2007 (Surprisingly outdated!)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.dzexams.com/\"&gt;&lt;strong&gt;https://www.dzexams.com/&lt;/strong&gt;&lt;/a&gt; - An Algerian website offering exam resources for school children, available in French, Arabic, and Berber languages. The site features a wealth of downloadable materials.&lt;/li&gt;\n&lt;li&gt;&amp;#x200B;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://blog.naver.com/ins_soul80\"&gt;&lt;strong&gt;https://blog.naver.com/ins_soul80&lt;/strong&gt;&lt;/a&gt; - A personal Korean blog covering IT and technology topics.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12wml3v", "is_robot_indexable": true, "report_reasons": null, "author": "peliciego", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wml3v/looking_the_noindex_grial_websites_001_week/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wml3v/looking_the_noindex_grial_websites_001_week/", "subreddit_subscribers": 679633, "created_utc": 1682277210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How do I get full resolution pictures from a Wix hosted page like [this](https://www.bodylog.net/portfolio2)?\n\nThe image URLs look like this:\n\n[https://static.wixstatic.com/media/a9e41c\\_c89a35d3964f45e0bd924f48fa390cfd\\~mv2.jpg/v1/fill/w\\_2495,h\\_3120,q\\_90/a9e41c\\_c89a35d3964f45e0bd924f48fa390cfd\\~mv2.webp](https://static.wixstatic.com/media/a9e41c_c89a35d3964f45e0bd924f48fa390cfd~mv2.jpg/v1/fill/w_2495,h_3120,q_90/a9e41c_c89a35d3964f45e0bd924f48fa390cfd~mv2.webp)\n\nAnd I can change this part of the URL\n\n    w_2495,h3120,q_90\n\nTo get a higher resolution. Is there a way to get the maximum resolution? Does anyone know how this works?", "author_fullname": "t2_mj7wn4y9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get full resolution images from Wix pages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wlplr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682275606.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do I get full resolution pictures from a Wix hosted page like &lt;a href=\"https://www.bodylog.net/portfolio2\"&gt;this&lt;/a&gt;?&lt;/p&gt;\n\n&lt;p&gt;The image URLs look like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://static.wixstatic.com/media/a9e41c_c89a35d3964f45e0bd924f48fa390cfd%7Emv2.jpg/v1/fill/w_2495,h_3120,q_90/a9e41c_c89a35d3964f45e0bd924f48fa390cfd%7Emv2.webp\"&gt;https://static.wixstatic.com/media/a9e41c_c89a35d3964f45e0bd924f48fa390cfd~mv2.jpg/v1/fill/w_2495,h_3120,q_90/a9e41c_c89a35d3964f45e0bd924f48fa390cfd~mv2.webp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And I can change this part of the URL&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;w_2495,h3120,q_90\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;To get a higher resolution. Is there a way to get the maximum resolution? Does anyone know how this works?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wlplr", "is_robot_indexable": true, "report_reasons": null, "author": "xnbxb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wlplr/get_full_resolution_images_from_wix_pages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wlplr/get_full_resolution_images_from_wix_pages/", "subreddit_subscribers": 679633, "created_utc": 1682275606.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So in light of the recent news, I wanted to go back and archive all my saved posts from my accounts. Problem is one of them is busted and has hidden almost all the saved posts(still saved, but doesn't show up in the \"saved\" tab). I requested all the data and got it all as CSV files. Is it possible to scrape from that? If so, what would be the best way?\n\nEdit: I think scraping would be more appropriate than download", "author_fullname": "t2_vobhonql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download FROM a CSV file?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wbhpw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682263334.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682260858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So in light of the recent news, I wanted to go back and archive all my saved posts from my accounts. Problem is one of them is busted and has hidden almost all the saved posts(still saved, but doesn&amp;#39;t show up in the &amp;quot;saved&amp;quot; tab). I requested all the data and got it all as CSV files. Is it possible to scrape from that? If so, what would be the best way?&lt;/p&gt;\n\n&lt;p&gt;Edit: I think scraping would be more appropriate than download&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wbhpw", "is_robot_indexable": true, "report_reasons": null, "author": "AsAccThrow1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wbhpw/download_from_a_csv_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wbhpw/download_from_a_csv_file/", "subreddit_subscribers": 679633, "created_utc": 1682260858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ci9n1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who said old ST3000DM's couldn't last...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "name": "t3_12wacxg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.52, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/SCF-cuvg6XJU3unuKVxJ8u8M1VK14ZjXt_WJwxZ1Mv8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682259767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/fbu4fhdb8nva1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/fbu4fhdb8nva1.jpg?auto=webp&amp;v=enabled&amp;s=cfd0f3e753c009e181b5c5ebeee934728f0343a7", "width": 758, "height": 478}, "resolutions": [{"url": "https://preview.redd.it/fbu4fhdb8nva1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f382b90f68f65b1a1b8bb919614161deaf07337b", "width": 108, "height": 68}, {"url": "https://preview.redd.it/fbu4fhdb8nva1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ad49c2a0d530b7490bdfb7f6b3b8e22e6b90736", "width": 216, "height": 136}, {"url": "https://preview.redd.it/fbu4fhdb8nva1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d58e603351312c3cc84a3f8ec655a5148aba673", "width": 320, "height": 201}, {"url": "https://preview.redd.it/fbu4fhdb8nva1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec455851745ac9607fb6706651964b85279160be", "width": 640, "height": 403}], "variants": {}, "id": "BX8IJkzxIWgB6IwdhVycw2npb47IZc-SsPAuEfiFb38"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wacxg", "is_robot_indexable": true, "report_reasons": null, "author": "PimpSLAYER187", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wacxg/who_said_old_st3000dms_couldnt_last/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/fbu4fhdb8nva1.jpg", "subreddit_subscribers": 679633, "created_utc": 1682259767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I create a file called files.txt, and inside I put some reddit usernames.  \n\n\nwhen i run   \n\n\n* gallery-dl -i files.txt  \n\n\nIt downloads everything to the same folder. I would like it to create a subfolder with the username, and then download everything from that user into that location without splitting it.   \n\n\nis there a way to do this?  \n\n\nI could write a python script that runs one by one and moves the files after they are done, but then if I re-run it it would redownload everything again.  \n\n\nThanks!", "author_fullname": "t2_bmc3ngo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with gallery-dl for downloading", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12x012d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682303860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I create a file called files.txt, and inside I put some reddit usernames.  &lt;/p&gt;\n\n&lt;p&gt;when i run   &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;gallery-dl -i files.txt&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It downloads everything to the same folder. I would like it to create a subfolder with the username, and then download everything from that user into that location without splitting it.   &lt;/p&gt;\n\n&lt;p&gt;is there a way to do this?  &lt;/p&gt;\n\n&lt;p&gt;I could write a python script that runs one by one and moves the files after they are done, but then if I re-run it it would redownload everything again.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12x012d", "is_robot_indexable": true, "report_reasons": null, "author": "ThrowAwayButYouKnew", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12x012d/need_help_with_gallerydl_for_downloading/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12x012d/need_help_with_gallerydl_for_downloading/", "subreddit_subscribers": 679633, "created_utc": 1682303860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Lots of social media sites (e.g. Twitter, Reddit, etc.) have a scrolling feature where the content out of the viewport gets erased. So even if your scroll bar is quite far down, all the stuff above is blank. This is an issue when I try to Save-As or Print a webpage from these websites to archive it, since I'll get this long HTML page or long PDF which is 95% blank, except for where I happen to be viewing the page when I hit the save/print commands. \n\nHow can I save/print a complete webpage if it has this behavior? For a test, you can try to get a complete download of the r/popular front page (e.g. produce a &gt;20 page pdf with all the text/photos visible if one were to scroll through by hand).", "author_fullname": "t2_3tj54e1x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saving the complete contents of a scrolling social media webpage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wygue", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682300578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lots of social media sites (e.g. Twitter, Reddit, etc.) have a scrolling feature where the content out of the viewport gets erased. So even if your scroll bar is quite far down, all the stuff above is blank. This is an issue when I try to Save-As or Print a webpage from these websites to archive it, since I&amp;#39;ll get this long HTML page or long PDF which is 95% blank, except for where I happen to be viewing the page when I hit the save/print commands. &lt;/p&gt;\n\n&lt;p&gt;How can I save/print a complete webpage if it has this behavior? For a test, you can try to get a complete download of the &lt;a href=\"/r/popular\"&gt;r/popular&lt;/a&gt; front page (e.g. produce a &amp;gt;20 page pdf with all the text/photos visible if one were to scroll through by hand).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wygue", "is_robot_indexable": true, "report_reasons": null, "author": "dnrlk", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wygue/saving_the_complete_contents_of_a_scrolling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wygue/saving_the_complete_contents_of_a_scrolling/", "subreddit_subscribers": 679633, "created_utc": 1682300578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently bought a 16tb Seagate Expansion drive. It passed the long self test through Seatools. I wasn't sure if that's enough. I've heard about Badblocks, but I've seen people say it's not useful for really big drives and to be honest it looks like it takes more tech savvy to use than I have.", "author_fullname": "t2_fxa6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a passed Seatools longtest sufficient for determining the quality of of a drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wxqvj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682299026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought a 16tb Seagate Expansion drive. It passed the long self test through Seatools. I wasn&amp;#39;t sure if that&amp;#39;s enough. I&amp;#39;ve heard about Badblocks, but I&amp;#39;ve seen people say it&amp;#39;s not useful for really big drives and to be honest it looks like it takes more tech savvy to use than I have.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wxqvj", "is_robot_indexable": true, "report_reasons": null, "author": "onlytoask", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wxqvj/is_a_passed_seatools_longtest_sufficient_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wxqvj/is_a_passed_seatools_longtest_sufficient_for/", "subreddit_subscribers": 679633, "created_utc": 1682299026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There s a website storing all the very important scanned pdf files of photography booklets magazines posters and brochures i want to back up this entire website and pdf files.\n\nhttps://www.pacificrimcamera.com/rl/rlrindex.htm\n\nhow can i auto backup site website?", "author_fullname": "t2_8c04f90yy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to download the entire pdf library from a website?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wrp6y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682286944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There s a website storing all the very important scanned pdf files of photography booklets magazines posters and brochures i want to back up this entire website and pdf files.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pacificrimcamera.com/rl/rlrindex.htm\"&gt;https://www.pacificrimcamera.com/rl/rlrindex.htm&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;how can i auto backup site website?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12wrp6y", "is_robot_indexable": true, "report_reasons": null, "author": "donerfucker39", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12wrp6y/how_to_download_the_entire_pdf_library_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12wrp6y/how_to_download_the_entire_pdf_library_from_a/", "subreddit_subscribers": 679633, "created_utc": 1682286944.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}