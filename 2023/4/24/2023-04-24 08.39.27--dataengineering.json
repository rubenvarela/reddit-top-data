{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background\n\nI am a 2 YOE data analyst from a finance background but I am doing lots of data engineering work, so I want to make a move on to a data engineer role. \n\nI am certified with AWS SAA and CCP, and I am going to get the Data Analytics Specialty certification. I have built some data pipelines with AWS services with python scripting.\n\nMy question is: Is it a data structure and algorithm a must for data engineering roles?  \nI had a few interviews and there were coding interviews, but I failed so badly due to the data structure and algorithm questions. It sometimes feels like a software engineer role - perhaps data engineers are really software engineers who focus on data ?\n\nAdditionally, some role require ML knowledge in the coding interview, and again I did it not very well. \n\nI am not so sure if I should slightly leave AWS certifications and focus more on data structure and algorithm knowledge.   \nIt would be highly appreciated if anyone could share your experience, please?\n\np.s. I am seeking ways to improve myself or succeed in the interview.   \nThank you so much", "author_fullname": "t2_pd2piq1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a data structure and algorithm a must for data engineering roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w49y1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 91, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 91, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682246220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background&lt;/p&gt;\n\n&lt;p&gt;I am a 2 YOE data analyst from a finance background but I am doing lots of data engineering work, so I want to make a move on to a data engineer role. &lt;/p&gt;\n\n&lt;p&gt;I am certified with AWS SAA and CCP, and I am going to get the Data Analytics Specialty certification. I have built some data pipelines with AWS services with python scripting.&lt;/p&gt;\n\n&lt;p&gt;My question is: Is it a data structure and algorithm a must for data engineering roles?&lt;br/&gt;\nI had a few interviews and there were coding interviews, but I failed so badly due to the data structure and algorithm questions. It sometimes feels like a software engineer role - perhaps data engineers are really software engineers who focus on data ?&lt;/p&gt;\n\n&lt;p&gt;Additionally, some role require ML knowledge in the coding interview, and again I did it not very well. &lt;/p&gt;\n\n&lt;p&gt;I am not so sure if I should slightly leave AWS certifications and focus more on data structure and algorithm knowledge.&lt;br/&gt;\nIt would be highly appreciated if anyone could share your experience, please?&lt;/p&gt;\n\n&lt;p&gt;p.s. I am seeking ways to improve myself or succeed in the interview.&lt;br/&gt;\nThank you so much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12w49y1", "is_robot_indexable": true, "report_reasons": null, "author": "uk_dataguy", "discussion_type": null, "num_comments": 83, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w49y1/is_it_a_data_structure_and_algorithm_a_must_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w49y1/is_it_a_data_structure_and_algorithm_a_must_for/", "subreddit_subscribers": 101984, "created_utc": 1682246220.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand that Delta Lake is 100% an OSS, but is it *really*? Is anyone using Delta Lake as their storage format, but not using Databricks? It almost seems that Delta Lake is coupled with Databricks (or at the very least, Spark). Is it even possible to leverage the benefits of using Delta Lake without using Databricks or Spark?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake without Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wsatu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682288131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand that Delta Lake is 100% an OSS, but is it &lt;em&gt;really&lt;/em&gt;? Is anyone using Delta Lake as their storage format, but not using Databricks? It almost seems that Delta Lake is coupled with Databricks (or at the very least, Spark). Is it even possible to leverage the benefits of using Delta Lake without using Databricks or Spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wsatu", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wsatu/delta_lake_without_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wsatu/delta_lake_without_databricks/", "subreddit_subscribers": 101984, "created_utc": 1682288131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI'm new to dbt and I'm looking for some guidance on implementing Kimball-style Slowly Changing Dimension (SCD) Type 2 dimensions and Time Span Accumulating Facts using dbt. I have come across some examples that use the snapshot feature to track the history of source tables, but I'm specifically interested in pure SCD2 or time span accumulating facts.\n\n&gt;In the dbt best practices, it's recommended to snapshot the source data in its raw form and use downstream models to clean up the data.   \n\"Snapshot source data.[\u200b](https://docs.getdbt.com/docs/build/snapshots#snapshot-source-data)  \n&gt;  \n&gt;Your models should then select from these snapshots, treating them like regular data sources. As much as possible, snapshot your source data in its raw form and use downstream models to clean up the data\"\n\n[https://docs.getdbt.com/docs/build/snapshots#snapshot-query-best-practices](https://docs.getdbt.com/docs/build/snapshots#snapshot-query-best-practices)\n\nI'm hoping someone can point me in the right direction for a pure SCD2 or time span accumulating facts implementation.\n\nThank you in advance for your help!", "author_fullname": "t2_1v4h09lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt SCD type2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w5kn1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682249823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to dbt and I&amp;#39;m looking for some guidance on implementing Kimball-style Slowly Changing Dimension (SCD) Type 2 dimensions and Time Span Accumulating Facts using dbt. I have come across some examples that use the snapshot feature to track the history of source tables, but I&amp;#39;m specifically interested in pure SCD2 or time span accumulating facts.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;In the dbt best practices, it&amp;#39;s recommended to snapshot the source data in its raw form and use downstream models to clean up the data.&lt;br/&gt;\n&amp;quot;Snapshot source data.&lt;a href=\"https://docs.getdbt.com/docs/build/snapshots#snapshot-source-data\"&gt;\u200b&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Your models should then select from these snapshots, treating them like regular data sources. As much as possible, snapshot your source data in its raw form and use downstream models to clean up the data&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.getdbt.com/docs/build/snapshots#snapshot-query-best-practices\"&gt;https://docs.getdbt.com/docs/build/snapshots#snapshot-query-best-practices&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping someone can point me in the right direction for a pure SCD2 or time span accumulating facts implementation.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?auto=webp&amp;v=enabled&amp;s=bfe5e9b2927d016e953dc1100d04aa7edae028b8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac2470355dc3f9626c6f35140e0ec423549da50e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6800fda7fcc0abb4bd00f0af3c485a21b9befd60", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ff6e290ecb9d197e6339baf74f37ad4bcf47da0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59c6c188d4255d45db867f741fbf4a6fe1161f4a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56be5b52655ecb694651e6bf1bf5a025673b7cc3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8a7d892a9bafd02790b2d76b5fbbe76e9d0857f", "width": 1080, "height": 567}], "variants": {}, "id": "KBohsdqrfvkRxfqADmI_uqtotFtqgZjYu8NQbRpJlaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12w5kn1", "is_robot_indexable": true, "report_reasons": null, "author": "mrcool444", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w5kn1/dbt_scd_type2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w5kn1/dbt_scd_type2/", "subreddit_subscribers": 101984, "created_utc": 1682249823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been hearing more and more about data contracts, but the implementation seems to focus on kafka and pub/sub. Do you need to be implementing your own extractors in order to use them? What about for tools like fivetran, meltano, or other extract/load tools. It doesn't seem like data contracts are feasible unless you have more hands on architecture in place", "author_fullname": "t2_sbb17r9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Implementing data contracts with ELT tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wozxq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682281727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been hearing more and more about data contracts, but the implementation seems to focus on kafka and pub/sub. Do you need to be implementing your own extractors in order to use them? What about for tools like fivetran, meltano, or other extract/load tools. It doesn&amp;#39;t seem like data contracts are feasible unless you have more hands on architecture in place&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wozxq", "is_robot_indexable": true, "report_reasons": null, "author": "sir-camaris", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wozxq/implementing_data_contracts_with_elt_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wozxq/implementing_data_contracts_with_elt_tools/", "subreddit_subscribers": 101984, "created_utc": 1682281727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://www.makingmeaning.info/post/llms-generative-ai-comes-to-databricks", "author_fullname": "t2_a1z6eog1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLMs and Generative AI comes to Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w2gww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682241642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.makingmeaning.info/post/llms-generative-ai-comes-to-databricks\"&gt;https://www.makingmeaning.info/post/llms-generative-ai-comes-to-databricks&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?auto=webp&amp;v=enabled&amp;s=c6c011d5cad9c522dcddd45aa1e7d61f56bb04e8", "width": 1000, "height": 606}, "resolutions": [{"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=42979bf884023d945787bca37677f7c39ed6579c", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c36d10dba0aaea2fc85ddf22624574ee953ee6a3", "width": 216, "height": 130}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=123348506a8c3dbfdf495ff63edbc5298080a89e", "width": 320, "height": 193}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ce7df8f8208c50a669d3d9d0cc715fb3ebd95d0", "width": 640, "height": 387}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa5f0d78536edc3a4d47d2a7687e8b13a858abff", "width": 960, "height": 581}], "variants": {}, "id": "-oI9nzQ8WeRrnbIoYsG4Ofr6SE3w0haEM2R3dsV27-Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12w2gww", "is_robot_indexable": true, "report_reasons": null, "author": "DataAnalyticsDude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w2gww/llms_and_generative_ai_comes_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w2gww/llms_and_generative_ai_comes_to_databricks/", "subreddit_subscribers": 101984, "created_utc": 1682241642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am starting to learn IaC and found out to options for AWS infra. wanted to get opinion from the community what they think about it", "author_fullname": "t2_i8mtb986", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS CDK vs Terraform ? Any thoughts on pros and cons of each ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w8q38", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682256580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am starting to learn IaC and found out to options for AWS infra. wanted to get opinion from the community what they think about it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12w8q38", "is_robot_indexable": true, "report_reasons": null, "author": "BhauNibba", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w8q38/aws_cdk_vs_terraform_any_thoughts_on_pros_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w8q38/aws_cdk_vs_terraform_any_thoughts_on_pros_and/", "subreddit_subscribers": 101984, "created_utc": 1682256580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1fixi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How DuckDB compares with Athena when querying Parquet files, and how to connect it to Cloudflare R2 for free egress", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wvhx6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682294474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "fet.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://fet.dev/posts/throwing-lots-of-data-on-duckdb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12wvhx6", "is_robot_indexable": true, "report_reasons": null, "author": "pilt", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wvhx6/how_duckdb_compares_with_athena_when_querying/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://fet.dev/posts/throwing-lots-of-data-on-duckdb", "subreddit_subscribers": 101984, "created_utc": 1682294474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am curious to hear of interesting and useful projects in the data engineering and modeling space that use large language models. Please do share if you know if such open source projects or tools", "author_fullname": "t2_jcps4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interesting projects in data engineering using LLMs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wn7xs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682278392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curious to hear of interesting and useful projects in the data engineering and modeling space that use large language models. Please do share if you know if such open source projects or tools&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wn7xs", "is_robot_indexable": true, "report_reasons": null, "author": "sonalg", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wn7xs/interesting_projects_in_data_engineering_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wn7xs/interesting_projects_in_data_engineering_using/", "subreddit_subscribers": 101984, "created_utc": 1682278392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "To move data from sql server --&gt; snowflake\n\nOne that either has a self hosted agent , or can be self hosted. So ot can access our on premise sql server? Ideally something reliable and easy, cheap to setup.\n\nI'm currently using data factory, but it's getting a bit expensive to run this hourly. And requires a lot of setup.\n\nI tried airbyte but that was terrible snf buggy. E.g. Can't connect to db with more than 100 tables.", "author_fullname": "t2_1tabu63j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know a good data load tool for sql server - cdc", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wuio4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682292901.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682292543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To move data from sql server --&amp;gt; snowflake&lt;/p&gt;\n\n&lt;p&gt;One that either has a self hosted agent , or can be self hosted. So ot can access our on premise sql server? Ideally something reliable and easy, cheap to setup.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using data factory, but it&amp;#39;s getting a bit expensive to run this hourly. And requires a lot of setup.&lt;/p&gt;\n\n&lt;p&gt;I tried airbyte but that was terrible snf buggy. E.g. Can&amp;#39;t connect to db with more than 100 tables.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wuio4", "is_robot_indexable": true, "report_reasons": null, "author": "jkp69", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wuio4/anyone_know_a_good_data_load_tool_for_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wuio4/anyone_know_a_good_data_load_tool_for_sql_server/", "subreddit_subscribers": 101984, "created_utc": 1682292543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data scientist on our team has been exploring ML models. I don\u2019t have any experience in this arena, but team is looking to me (DE) to put together a framework for how these models should be managed/deployed. wondering what the best options are for deploying a ML model? What parts should be owned by the data scientist vs data engineering?\n\nOur current tech stack:\n\nLinux server, Azure Devops for CI/CD and script deployments, Redshift for data warehouse, and then model built in Python\n\nWe are also beginning to explore AWS Sagemaker but no guarantee company will let us bring that on.", "author_fullname": "t2_8qi1s4iv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best options for deploying ML model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wqxiy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682285446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data scientist on our team has been exploring ML models. I don\u2019t have any experience in this arena, but team is looking to me (DE) to put together a framework for how these models should be managed/deployed. wondering what the best options are for deploying a ML model? What parts should be owned by the data scientist vs data engineering?&lt;/p&gt;\n\n&lt;p&gt;Our current tech stack:&lt;/p&gt;\n\n&lt;p&gt;Linux server, Azure Devops for CI/CD and script deployments, Redshift for data warehouse, and then model built in Python&lt;/p&gt;\n\n&lt;p&gt;We are also beginning to explore AWS Sagemaker but no guarantee company will let us bring that on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wqxiy", "is_robot_indexable": true, "report_reasons": null, "author": "fancyfanch", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wqxiy/best_options_for_deploying_ml_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wqxiy/best_options_for_deploying_ml_model/", "subreddit_subscribers": 101984, "created_utc": 1682285446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why would you create an iceberg table vs moving the data to an internal snowflake table? Is the benefit of creating an iceberg tables so it can be used by different compute like spark or trino? Cost savings? In my mind, iceberg is like schema on write vs schema on read. If someone makes a change to an upstream file like removing a field, it would break my iceberg load process.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Iceberg Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wo6pk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682280198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why would you create an iceberg table vs moving the data to an internal snowflake table? Is the benefit of creating an iceberg tables so it can be used by different compute like spark or trino? Cost savings? In my mind, iceberg is like schema on write vs schema on read. If someone makes a change to an upstream file like removing a field, it would break my iceberg load process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wo6pk", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wo6pk/snowflake_iceberg_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wo6pk/snowflake_iceberg_tables/", "subreddit_subscribers": 101984, "created_utc": 1682280198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently we are using a Postgres database where all of the data for our web application is stored. Analytics has become a big part of our product offering and currently we are running analytics from the same database.\n\nThis is working perfectly fine since the main table for the analytics is heavily indexed but it takes ages to refresh that table and we can currently only refresh our data once per day.\n\nI would like to implement an OLAP database for our analytics that basically contains a live copy of our public schema from our OLTP database (The data is mutable) and then join those tables in the OLAP db to create our flat analytics table in as little time as possible (preferably refreshed every 5min).\n\nI have tried using clickhouse and their experimental materialized postgressql database engine to create a replica of my public schema, this has worked great but clickhouse is having a lot of trouble joining our large tables (constant running out of memory errors)\n\nCan anyone recommend a better way to implement what I\u2019m trying to achieve ?\n\nFeel free to recommend other tools or DB\u2019s that i can use. (Bonus points if its open source).\n\nThanks in advance.\nA junior Data Engineer", "author_fullname": "t2_crdnm9xe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OLAP Database advise needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w5t7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682254109.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682250408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently we are using a Postgres database where all of the data for our web application is stored. Analytics has become a big part of our product offering and currently we are running analytics from the same database.&lt;/p&gt;\n\n&lt;p&gt;This is working perfectly fine since the main table for the analytics is heavily indexed but it takes ages to refresh that table and we can currently only refresh our data once per day.&lt;/p&gt;\n\n&lt;p&gt;I would like to implement an OLAP database for our analytics that basically contains a live copy of our public schema from our OLTP database (The data is mutable) and then join those tables in the OLAP db to create our flat analytics table in as little time as possible (preferably refreshed every 5min).&lt;/p&gt;\n\n&lt;p&gt;I have tried using clickhouse and their experimental materialized postgressql database engine to create a replica of my public schema, this has worked great but clickhouse is having a lot of trouble joining our large tables (constant running out of memory errors)&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a better way to implement what I\u2019m trying to achieve ?&lt;/p&gt;\n\n&lt;p&gt;Feel free to recommend other tools or DB\u2019s that i can use. (Bonus points if its open source).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.\nA junior Data Engineer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12w5t7y", "is_robot_indexable": true, "report_reasons": null, "author": "Agile_Trash_470", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w5t7y/olap_database_advise_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w5t7y/olap_database_advise_needed/", "subreddit_subscribers": 101984, "created_utc": 1682250408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_983tug55s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End-to-end Machine Learning modelling in BigQuery \u2014 Google Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_12wonl7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HZm20YvaY--DYP4PDbJ6Vjsd9pb6DXnn4030qSTpAPI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682281077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/artificial-corner/end-to-end-machine-learning-modelling-in-bigquery-google-cloud-a0d9e7eca20b", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?auto=webp&amp;v=enabled&amp;s=b59424aa4430f0984f32696bceea9e1ea1de97a0", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7331fbabd443814aeb6ba95dfab4b2450809461b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed226ce9932c1ff8d71c4a1dda55cf3624dbb2c5", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=443678e26ad4d8f23ee32e5c34b5bccec41978c6", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38ff374fae1be790984292ab10858f6c0409dff4", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e3414e945c111c3505150d278d0fc7dc2e05678d", "width": 960, "height": 960}], "variants": {}, "id": "_9geqsbDIsWAAy4ufcfZ4iRR1RlvaEJ5bZTKufjB3tw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12wonl7", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum247", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wonl7/endtoend_machine_learning_modelling_in_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/artificial-corner/end-to-end-machine-learning-modelling-in-bigquery-google-cloud-a0d9e7eca20b", "subreddit_subscribers": 101984, "created_utc": 1682281077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Want information from Data Engineers who use Google Cloud:** \n\nIs it your responsibility to create VM instances and perform network and HTTP load balancing? I am prep. for the GC PDE cert; following the kickstart cert\u2014program for partners by Google. The program includes creating and managing cloud resources. Should I spend a lot of time learning about infrastructure/ compute services?", "author_fullname": "t2_eo907yrs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GC Data Engineers, help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wrmwt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682286822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Want information from Data Engineers who use Google Cloud:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;Is it your responsibility to create VM instances and perform network and HTTP load balancing? I am prep. for the GC PDE cert; following the kickstart cert\u2014program for partners by Google. The program includes creating and managing cloud resources. Should I spend a lot of time learning about infrastructure/ compute services?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wrmwt", "is_robot_indexable": true, "report_reasons": null, "author": "avg_ali", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wrmwt/gc_data_engineers_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wrmwt/gc_data_engineers_help/", "subreddit_subscribers": 101984, "created_utc": 1682286822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI'm working to create a data pipeline that is near real-time but also scalable and cost effective. If I can have the data move from source to BigQuery in under 60 seconds that will be fine. Pipeline is in GCP.\n\nRight now, I have a Cloud Run instance that is listening for HTTP requests and then sending the data to GCS. I'm wondering the most effective, scalable, and easy way to move that data from GCS to BigQuery. Two options \n\n* When new data is loaded into GCS, I'll have a pub/sub notification sent out and that will trigger a Cloud Function to move the data. Once it's in a BigQuery staging dataset, I can do some transformations and create some reporting tables. \n* When new data is loaded into GCS, I'll I can use the pull pub/sub message with a DAG and use airflow to move the data from GCS to BigQuery\n\nThinking through the pro's cons of each. First option seems easiest, should be fairly scalable, will be cheap. I'm probably looking at 1k events / hour so nothing crazy. Second option allows me to kick off a DAG that could incorporate other transformation logic but not sure how scalable it will be and airflow on GCP can get expensive. \n\nThanks for the help!", "author_fullname": "t2_9u8negv8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most effective way to move data from GCS to BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wip0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682270182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working to create a data pipeline that is near real-time but also scalable and cost effective. If I can have the data move from source to BigQuery in under 60 seconds that will be fine. Pipeline is in GCP.&lt;/p&gt;\n\n&lt;p&gt;Right now, I have a Cloud Run instance that is listening for HTTP requests and then sending the data to GCS. I&amp;#39;m wondering the most effective, scalable, and easy way to move that data from GCS to BigQuery. Two options &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When new data is loaded into GCS, I&amp;#39;ll have a pub/sub notification sent out and that will trigger a Cloud Function to move the data. Once it&amp;#39;s in a BigQuery staging dataset, I can do some transformations and create some reporting tables. &lt;/li&gt;\n&lt;li&gt;When new data is loaded into GCS, I&amp;#39;ll I can use the pull pub/sub message with a DAG and use airflow to move the data from GCS to BigQuery&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thinking through the pro&amp;#39;s cons of each. First option seems easiest, should be fairly scalable, will be cheap. I&amp;#39;m probably looking at 1k events / hour so nothing crazy. Second option allows me to kick off a DAG that could incorporate other transformation logic but not sure how scalable it will be and airflow on GCP can get expensive. &lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wip0o", "is_robot_indexable": true, "report_reasons": null, "author": "Proper_Ad_7836", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wip0o/most_effective_way_to_move_data_from_gcs_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wip0o/most_effective_way_to_move_data_from_gcs_to/", "subreddit_subscribers": 101984, "created_utc": 1682270182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does everyone handle upstream data changes that break your data models? For example, we use DMS to replicate our data real time into Redshift raw schema. The upstream teams always make changes and then it breaks our downstream fact tables and staging views. Some of the changes are like removing columns, deprecating an existing column, and adding a new column. I have seen some people say create dbt tests on your sources and make sure everything is build from the staging schema so there is only one location to fix a downstream schema instead of many views. Right now, one small upstream change requires us to fix tons of dbt models. This isn\u2019t really scalable. Any advice?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upstream schema changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wdcfl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682262550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does everyone handle upstream data changes that break your data models? For example, we use DMS to replicate our data real time into Redshift raw schema. The upstream teams always make changes and then it breaks our downstream fact tables and staging views. Some of the changes are like removing columns, deprecating an existing column, and adding a new column. I have seen some people say create dbt tests on your sources and make sure everything is build from the staging schema so there is only one location to fix a downstream schema instead of many views. Right now, one small upstream change requires us to fix tons of dbt models. This isn\u2019t really scalable. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wdcfl", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wdcfl/upstream_schema_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wdcfl/upstream_schema_changes/", "subreddit_subscribers": 101984, "created_utc": 1682262550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here is a dumb question:\n\n**How should I compare Prometheus-like DBs with Snowflake when choosing a target storage time-series data?**\n\nLet me elaborate.\n\nI have a lot of experience in the analytics field for different types of data. That's not the case for time-series data where my \"bat-suite\" is not well equipped.\n\nWe have this use case where servers are generating tons of time-series data. This data has its use for operational purposes. That is not where I come in. I come in for the business analytics goals. We want to make this data available to people that will make business decisions on where to improve, what to explore, etc, based on these server metrics.\n\nToday, Snowflake is the analytics interface in the company. However, if the data is time-series, why would I send them to Snowflake? Considering the company already has a Phrometheus infrastructure in place, with Logstash and Graphana, wouldn't be more interesting to have this specific data ingested to a time-series DB like Phrometheus?\n\nWhen thinking about why not sending to Phrometheus I find the following idea popping up in my head:\n\n1. Better to have only one interface for analytics in the company (cultural reasons).\n2. All data in one place = easier joins (technical reason).\n3. Snowflake is SQL-queryable (that's a cultural and a technical reason).\n\nReasons 1 and 2 by themselves would be enough to make the decision. So I am trying to think without them (Also, maybe the other data to be joined is also time-series and should be moved to a time-series DB? Who knows...).\n\nPlease share your toughs. :)", "author_fullname": "t2_vd6ewwka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time-series data: Prometheus-like DB or Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w71xc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682253178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is a dumb question:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How should I compare Prometheus-like DBs with Snowflake when choosing a target storage time-series data?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me elaborate.&lt;/p&gt;\n\n&lt;p&gt;I have a lot of experience in the analytics field for different types of data. That&amp;#39;s not the case for time-series data where my &amp;quot;bat-suite&amp;quot; is not well equipped.&lt;/p&gt;\n\n&lt;p&gt;We have this use case where servers are generating tons of time-series data. This data has its use for operational purposes. That is not where I come in. I come in for the business analytics goals. We want to make this data available to people that will make business decisions on where to improve, what to explore, etc, based on these server metrics.&lt;/p&gt;\n\n&lt;p&gt;Today, Snowflake is the analytics interface in the company. However, if the data is time-series, why would I send them to Snowflake? Considering the company already has a Phrometheus infrastructure in place, with Logstash and Graphana, wouldn&amp;#39;t be more interesting to have this specific data ingested to a time-series DB like Phrometheus?&lt;/p&gt;\n\n&lt;p&gt;When thinking about why not sending to Phrometheus I find the following idea popping up in my head:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Better to have only one interface for analytics in the company (cultural reasons).&lt;/li&gt;\n&lt;li&gt;All data in one place = easier joins (technical reason).&lt;/li&gt;\n&lt;li&gt;Snowflake is SQL-queryable (that&amp;#39;s a cultural and a technical reason).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Reasons 1 and 2 by themselves would be enough to make the decision. So I am trying to think without them (Also, maybe the other data to be joined is also time-series and should be moved to a time-series DB? Who knows...).&lt;/p&gt;\n\n&lt;p&gt;Please share your toughs. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12w71xc", "is_robot_indexable": true, "report_reasons": null, "author": "yfeltz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w71xc/timeseries_data_prometheuslike_db_or_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w71xc/timeseries_data_prometheuslike_db_or_snowflake/", "subreddit_subscribers": 101984, "created_utc": 1682253178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I found this an odd assessment\n\n [Google a Leader in 2023 Forrester Wave Data Management for Analytics | Google Cloud Blog](https://cloud.google.com/blog/products/data-analytics/google-a-leader-in-2023-forrester-wave-data-management-for-analytics) \n\nNo AWS ?\n\nInformatica in leader quadrant for analytics?\n\nWho is intersystems, that is ahead of Microsoft and Snowflake?", "author_fullname": "t2_5lv81gkx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forrester wave on Data management for analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12x3c86", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682311160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found this an odd assessment&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://cloud.google.com/blog/products/data-analytics/google-a-leader-in-2023-forrester-wave-data-management-for-analytics\"&gt;Google a Leader in 2023 Forrester Wave Data Management for Analytics | Google Cloud Blog&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;No AWS ?&lt;/p&gt;\n\n&lt;p&gt;Informatica in leader quadrant for analytics?&lt;/p&gt;\n\n&lt;p&gt;Who is intersystems, that is ahead of Microsoft and Snowflake?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?auto=webp&amp;v=enabled&amp;s=2bb423580e49d6864e3715f6654632d558a7d49c", "width": 2500, "height": 1232}, "resolutions": [{"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48579d8f753ce69b5f1b8ae0af753906892694f5", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1386d91cb1dd393b161dbfe57a97c27025c38f1", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c6c56995d10243a604293adda8ea07608cc4e59", "width": 320, "height": 157}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf7551884fe67c86f8908209089a05249b1c74ee", "width": 640, "height": 315}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07368c03b392acb2c54be0f5f3abda104f59aff0", "width": 960, "height": 473}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d94a8dee7f54cbfd6516eef96347cfa5a61ec285", "width": 1080, "height": 532}], "variants": {}, "id": "wcoegTgls8vDT7sU2cRo3smh6WlMonA1_OjrS3a-3OQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12x3c86", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical_Wish_4358", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12x3c86/forrester_wave_on_data_management_for_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12x3c86/forrester_wave_on_data_management_for_analytics/", "subreddit_subscribers": 101984, "created_utc": 1682311160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know druid and rockset are typically used for OLAP, but both support SQL and relational (row based) queries... \n\nCurrently we use Mongo (our data is all JSON) and not that happy with it.  Would either of those be a good alternative for our OLTP write once and read many use case?  We typically don't do transactional updates... mostly queries across different document types.", "author_fullname": "t2_bluzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Apache Druid (or Rockset) for OLTP use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wxez0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682298343.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know druid and rockset are typically used for OLAP, but both support SQL and relational (row based) queries... &lt;/p&gt;\n\n&lt;p&gt;Currently we use Mongo (our data is all JSON) and not that happy with it.  Would either of those be a good alternative for our OLTP write once and read many use case?  We typically don&amp;#39;t do transactional updates... mostly queries across different document types.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wxez0", "is_robot_indexable": true, "report_reasons": null, "author": "Beertimeanytime", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wxez0/using_apache_druid_or_rockset_for_oltp_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wxez0/using_apache_druid_or_rockset_for_oltp_use_cases/", "subreddit_subscribers": 101984, "created_utc": 1682298343.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a Synapse workspace and I'd like to track the pipelines I run in it using Log Analytics. No Spark pool or SQL data, just that Synapse Pipelines run, and some details of those. \n\nI've created a separate resource group, a log analytics workspace, and connected the Synapse workspace to that via diagnostic settings. I've chosen the three pipeline log options available and to write logs to the log analytics workspace I configured.\n\nIve read everything I can but still, if I run a pipeline manually or with a trigger, no entries are shown in log analytics, even after a wait time. Log analytics shows no data and it's tables are empty. \n\nI'm running these pipelines from live mode. \nCan anyone shed some light on what I'm missing. It seems like log analytics should be connecting pretty easily. Doing this from a visual studio subscription. \n\nThanks.", "author_fullname": "t2_lmanh1xe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse and Log Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ws3qi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682287742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Synapse workspace and I&amp;#39;d like to track the pipelines I run in it using Log Analytics. No Spark pool or SQL data, just that Synapse Pipelines run, and some details of those. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve created a separate resource group, a log analytics workspace, and connected the Synapse workspace to that via diagnostic settings. I&amp;#39;ve chosen the three pipeline log options available and to write logs to the log analytics workspace I configured.&lt;/p&gt;\n\n&lt;p&gt;Ive read everything I can but still, if I run a pipeline manually or with a trigger, no entries are shown in log analytics, even after a wait time. Log analytics shows no data and it&amp;#39;s tables are empty. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running these pipelines from live mode. \nCan anyone shed some light on what I&amp;#39;m missing. It seems like log analytics should be connecting pretty easily. Doing this from a visual studio subscription. &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ws3qi", "is_robot_indexable": true, "report_reasons": null, "author": "TheFirstGlassPilot", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ws3qi/synapse_and_log_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ws3qi/synapse_and_log_analytics/", "subreddit_subscribers": 101984, "created_utc": 1682287742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have any tips, strategies, or resources on how to forecast and compare cost of workloads run on Databricks?  Example: how do you anticipate how much you'll be spending on compute for a project and if that amount is too much? \n\n[https://www.databricks.com/product/pricing/product-pricing/instance-types](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n\nare you just using this pricing calculator and picking the compute that will do it in the time that you're willing to wait?", "author_fullname": "t2_tsg6kr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pricing Databricks workloads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wmzsr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682277969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any tips, strategies, or resources on how to forecast and compare cost of workloads run on Databricks?  Example: how do you anticipate how much you&amp;#39;ll be spending on compute for a project and if that amount is too much? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.databricks.com/product/pricing/product-pricing/instance-types\"&gt;https://www.databricks.com/product/pricing/product-pricing/instance-types&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;are you just using this pricing calculator and picking the compute that will do it in the time that you&amp;#39;re willing to wait?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?auto=webp&amp;v=enabled&amp;s=81fadd2b039e6a77769e188d2cccb3b86ef3f685", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b760d8074e07ac99dc78ec15e1a38c06a7dbdad7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75a6072b4cf7509e972f24aa9138ed6afb5cecf0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=996d4c3bcc7ac13a9c25e3af33852e1246449b71", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74db19b833cba064dc927289bb9e603c40649f85", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7088c1762337d5e51f359cd97bd80ea57fe9c7a1", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bd75be4500b08cb818d69e661bb10c1f8fa6a71", "width": 1080, "height": 567}], "variants": {}, "id": "chr5A0PJvePhRJ7lI3sFaxKWyQXhWy3BRSTRcBAQKNs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wmzsr", "is_robot_indexable": true, "report_reasons": null, "author": "lifec0ach", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wmzsr/pricing_databricks_workloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wmzsr/pricing_databricks_workloads/", "subreddit_subscribers": 101984, "created_utc": 1682277969.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}