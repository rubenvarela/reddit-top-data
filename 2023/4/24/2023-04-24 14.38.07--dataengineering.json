{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, I have 6+ years of exp in data engineering. Started with sql and python, moved to hadoop ecosystem , learned and worked on Spark (on premise), Also worked on AWS not much though, used big query and GCS for a project.  \nNow I feel anxious by never ending technology stack. When you start getting deeper into one, something new comes up.    \nFor jobs, everyone wants the expertise on that new tech like, specific cloud service, snowflake, databricks, dbt, kafka etc. I know what there tools are and what they do but not in detail since haven't work on these.   \nA lot of time I feel imposter syndrome, jack of all master of none, like I wont be able to do it long enough, And Its impacting my mental health.  \nNeed honest advice and how you guys think about it.  \n.\n\nI thought I may not be the only one facing this issue, may be worth to have a discussion.", "author_fullname": "t2_u1vbo568", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting overwhelmed by wide and ever-changing tech stack.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12x895j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 77, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 77, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682323806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I have 6+ years of exp in data engineering. Started with sql and python, moved to hadoop ecosystem , learned and worked on Spark (on premise), Also worked on AWS not much though, used big query and GCS for a project.&lt;br/&gt;\nNow I feel anxious by never ending technology stack. When you start getting deeper into one, something new comes up.&lt;br/&gt;\nFor jobs, everyone wants the expertise on that new tech like, specific cloud service, snowflake, databricks, dbt, kafka etc. I know what there tools are and what they do but not in detail since haven&amp;#39;t work on these.&lt;br/&gt;\nA lot of time I feel imposter syndrome, jack of all master of none, like I wont be able to do it long enough, And Its impacting my mental health.&lt;br/&gt;\nNeed honest advice and how you guys think about it.&lt;br/&gt;\n.&lt;/p&gt;\n\n&lt;p&gt;I thought I may not be the only one facing this issue, may be worth to have a discussion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12x895j", "is_robot_indexable": true, "report_reasons": null, "author": "manu13891", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12x895j/getting_overwhelmed_by_wide_and_everchanging_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12x895j/getting_overwhelmed_by_wide_and_everchanging_tech/", "subreddit_subscribers": 102040, "created_utc": 1682323806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand that Delta Lake is 100% an OSS, but is it *really*? Is anyone using Delta Lake as their storage format, but not using Databricks? It almost seems that Delta Lake is coupled with Databricks (or at the very least, Spark). Is it even possible to leverage the benefits of using Delta Lake without using Databricks or Spark?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake without Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wsatu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682288131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand that Delta Lake is 100% an OSS, but is it &lt;em&gt;really&lt;/em&gt;? Is anyone using Delta Lake as their storage format, but not using Databricks? It almost seems that Delta Lake is coupled with Databricks (or at the very least, Spark). Is it even possible to leverage the benefits of using Delta Lake without using Databricks or Spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wsatu", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wsatu/delta_lake_without_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wsatu/delta_lake_without_databricks/", "subreddit_subscribers": 102040, "created_utc": 1682288131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been hearing more and more about data contracts, but the implementation seems to focus on kafka and pub/sub. Do you need to be implementing your own extractors in order to use them? What about for tools like fivetran, meltano, or other extract/load tools. It doesn't seem like data contracts are feasible unless you have more hands on architecture in place\n\nSince it seems unclear - this is the sort of thing I am talking about: https://www.striim.com/blog/a-guide-to-data-contracts/#:~:text=A%20data%20contract%20is%20a,be%20transformed%20for%20end%20users.", "author_fullname": "t2_sbb17r9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Implementing data contracts with ELT tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wozxq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682345431.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682281727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been hearing more and more about data contracts, but the implementation seems to focus on kafka and pub/sub. Do you need to be implementing your own extractors in order to use them? What about for tools like fivetran, meltano, or other extract/load tools. It doesn&amp;#39;t seem like data contracts are feasible unless you have more hands on architecture in place&lt;/p&gt;\n\n&lt;p&gt;Since it seems unclear - this is the sort of thing I am talking about: &lt;a href=\"https://www.striim.com/blog/a-guide-to-data-contracts/#:%7E:text=A%20data%20contract%20is%20a,be%20transformed%20for%20end%20users\"&gt;https://www.striim.com/blog/a-guide-to-data-contracts/#:~:text=A%20data%20contract%20is%20a,be%20transformed%20for%20end%20users&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LGkzF-y2IN0-rSoj3lkT_WrOHH2dVQH4r2hLj-JheCA.jpg?auto=webp&amp;v=enabled&amp;s=962803ea4b890a2934feb42f3a721af720e26332", "width": 1999, "height": 1705}, "resolutions": [{"url": "https://external-preview.redd.it/LGkzF-y2IN0-rSoj3lkT_WrOHH2dVQH4r2hLj-JheCA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17735a4e8fac0f0afeb0f8671b2608f9f72e2b7e", "width": 108, "height": 92}, {"url": "https://external-preview.redd.it/LGkzF-y2IN0-rSoj3lkT_WrOHH2dVQH4r2hLj-JheCA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff151cdbe2983ad131e2222ff3ac99728ad0d987", "width": 216, "height": 184}, {"url": "https://external-preview.redd.it/LGkzF-y2IN0-rSoj3lkT_WrOHH2dVQH4r2hLj-JheCA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2e91c3dcb2c58280bfb5f89724f125556239efb", "width": 320, "height": 272}, {"url": "https://external-preview.redd.it/LGkzF-y2IN0-rSoj3lkT_WrOHH2dVQH4r2hLj-JheCA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af6d275b06193a713acda990084d1ed6fd4bcb90", "width": 640, "height": 545}, {"url": "https://external-preview.redd.it/LGkzF-y2IN0-rSoj3lkT_WrOHH2dVQH4r2hLj-JheCA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb01109f26ae11221f8a4416bd09a26a957c5af4", "width": 960, "height": 818}, {"url": "https://external-preview.redd.it/LGkzF-y2IN0-rSoj3lkT_WrOHH2dVQH4r2hLj-JheCA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51e8037be7a083dbe0a11ad444144b3741af88f7", "width": 1080, "height": 921}], "variants": {}, "id": "UpplUEy-AVpgYf5KGt_4ODZXdVsOv2XC8IwES5VLu6M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wozxq", "is_robot_indexable": true, "report_reasons": null, "author": "sir-camaris", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wozxq/implementing_data_contracts_with_elt_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wozxq/implementing_data_contracts_with_elt_tools/", "subreddit_subscribers": 102040, "created_utc": 1682281727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1fixi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How DuckDB compares with Athena when querying Parquet files, and how to connect it to Cloudflare R2 for free egress", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wvhx6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682294474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "fet.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://fet.dev/posts/throwing-lots-of-data-on-duckdb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12wvhx6", "is_robot_indexable": true, "report_reasons": null, "author": "pilt", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wvhx6/how_duckdb_compares_with_athena_when_querying/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://fet.dev/posts/throwing-lots-of-data-on-duckdb", "subreddit_subscribers": 102040, "created_utc": 1682294474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am curious to hear of interesting and useful projects in the data engineering and modeling space that use large language models. Please do share if you know if such open source projects or tools", "author_fullname": "t2_jcps4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interesting projects in data engineering using LLMs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wn7xs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682278392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curious to hear of interesting and useful projects in the data engineering and modeling space that use large language models. Please do share if you know if such open source projects or tools&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wn7xs", "is_robot_indexable": true, "report_reasons": null, "author": "sonalg", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wn7xs/interesting_projects_in_data_engineering_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wn7xs/interesting_projects_in_data_engineering_using/", "subreddit_subscribers": 102040, "created_utc": 1682278392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "To move data from sql server --&gt; snowflake\n\nOne that either has a self hosted agent , or can be self hosted. So ot can access our on premise sql server? Ideally something reliable and easy, cheap to setup.\n\nI'm currently using data factory, but it's getting a bit expensive to run this hourly. And requires a lot of setup.\n\nI tried airbyte but that was terrible snf buggy. E.g. Can't connect to db with more than 100 tables.", "author_fullname": "t2_1tabu63j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know a good data load tool for sql server - cdc", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wuio4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682292901.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682292543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To move data from sql server --&amp;gt; snowflake&lt;/p&gt;\n\n&lt;p&gt;One that either has a self hosted agent , or can be self hosted. So ot can access our on premise sql server? Ideally something reliable and easy, cheap to setup.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using data factory, but it&amp;#39;s getting a bit expensive to run this hourly. And requires a lot of setup.&lt;/p&gt;\n\n&lt;p&gt;I tried airbyte but that was terrible snf buggy. E.g. Can&amp;#39;t connect to db with more than 100 tables.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wuio4", "is_robot_indexable": true, "report_reasons": null, "author": "jkp69", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wuio4/anyone_know_a_good_data_load_tool_for_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wuio4/anyone_know_a_good_data_load_tool_for_sql_server/", "subreddit_subscribers": 102040, "created_utc": 1682292543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Z-Ordering is a powerful optimization technique for improving data skipping and query performance in Delta Lake tables. Understanding key concepts like predicates and cardinality, along with real-world examples, can help you make informed decisions when implementing Z-Ordering in your project. In this article, we will discuss these concepts in more detail and provide guidelines for choosing the frequency of Z-Ordering based on your specific use case and data characteristics.\n\n# Key Concepts\n\n**Predicates and Cardinality Predicates:**\n\nIn SQL, predicates are conditions used in the WHERE clause to filter rows based on specific criteria. For example, a query might have a predicate such as WHERE age &gt; 30 to filter out rows where the age is greater than 30. Z-Ordering can improve query performance when columns used in predicates have high cardinality.\n\n**Cardinality:**\n\nCardinality refers to the number of distinct values in a column. A column with high cardinality has a large number of unique values, whereas a column with low cardinality has a smaller number of unique values. For example, a \"gender\" column has low cardinality (typically two values: male and female), while an \"email\" column has high cardinality (unique values for each individual).\n\n**Real-World Scenario**\n\nA retail company has a large Delta Lake table containing daily sales data with millions of rows. The table has columns like store\\_id, product\\_id, sale\\_date, and revenue. The company often runs queries to analyze sales performance for specific products in certain stores over different periods. To optimize these queries, the company decides to implement Z-Ordering on the table, focusing on the store\\_id, product\\_id, and sale\\_date columns. As these columns have high cardinality and are commonly used in query predicates, Z-Ordering can help colocate related information in the same set of files, improving data skipping and query performance. Guidelines for Choosing the Frequency of Z-Ordering: Large or complex data and selective or frequent queries: Z-order your table more often to improve query performance and resource utilization. Small or simple data and less selective or infrequent queries: Z-order your table less often or not at all to avoid unnecessary overhead and complexity. Frequent or concurrent data updates: Z-order your table less often or only on specific partitions to avoid conflicts or delays with other operations.\n\n**Drawbacks of Z-Ordering and Possible Solutions**\n\nWhile Z-Ordering can provide significant benefits in terms of query performance and resource utilization, it also comes with certain drawbacks:\n\nNon-idempotency and incremental operation:\n\nZ-Ordering is not idempotent and aims to be an incremental operation. The time it takes for Z-Ordering is not guaranteed to reduce over multiple runs. However, if no new data is added to a partition that was just Z-ordered, another Z-ordering of that partition will not have any effect.\n\n**Solution:**\n\nCarefully plan and schedule Z-Ordering operations based on your data update frequency and specific use case. Monitor the performance improvements after each Z-Ordering operation to avoid unnecessary repetitions.\n\n**Impact on data freshness and Concurrency**\n\nZ-Ordering rewrites files based on the column values, which may cause conflicts or delays with other concurrent operations on the table. This can impact data freshness and concurrency in your Delta Lake table.\n\n**Solution:**\n\nPrioritize Z-Ordering on specific partitions or during periods of low data update activity to minimize conflicts and delays. Also, consider using Delta Lake's optimistic concurrency control feature to manage concurrent transactions effectively.\n\n**Increased storage and compute costs:**\n\nZ-Ordering creates new files and removes old files, potentially increasing storage and compute costs. You may need to run the VACUUM command to reclaim storage space and reduce storage costs.\n\n**Solution:**\n\nRegularly monitor your storage and compute costs and adjust your Z-Ordering frequency based on the observed benefits and costs. Use the VACUUM command to clean up old files and reduce storage costs.\n\n**Conclusion**\n\nUnderstanding predicates, cardinality, real-world scenarios, and the drawbacks of Z-Ordering can help you make informed decisions when implementing Z-Ordering in your Delta Lake project. By considering your specific use case, data characteristics, and potential drawbacks, you can optimize query performance and resource utilization while minimizing potential issues. Regularly monitor and troubleshoot your tables and queries to ensure they are running efficiently and effectively.\n\nSources: [Delta Lake Data Skipping](https://learn.microsoft.com/en-us/azure/databricks/delta/data-skipping?ssp=1&amp;setlang=en-GB&amp;safesearch=moderate) [Z-Ordering Optimizations](https://docs.delta.io/latest/optimizations-oss.html#:~:text=Z%2DOrdering%20is%20a%20technique,Lake%20in%20data%2Dskipping%20algorithms)", "author_fullname": "t2_azqk02tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Z-Ordering in Delta Lake: Key Considerations and Guidelines for Implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xbw64", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682344480.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682334419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Z-Ordering is a powerful optimization technique for improving data skipping and query performance in Delta Lake tables. Understanding key concepts like predicates and cardinality, along with real-world examples, can help you make informed decisions when implementing Z-Ordering in your project. In this article, we will discuss these concepts in more detail and provide guidelines for choosing the frequency of Z-Ordering based on your specific use case and data characteristics.&lt;/p&gt;\n\n&lt;h1&gt;Key Concepts&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Predicates and Cardinality Predicates:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In SQL, predicates are conditions used in the WHERE clause to filter rows based on specific criteria. For example, a query might have a predicate such as WHERE age &amp;gt; 30 to filter out rows where the age is greater than 30. Z-Ordering can improve query performance when columns used in predicates have high cardinality.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cardinality:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Cardinality refers to the number of distinct values in a column. A column with high cardinality has a large number of unique values, whereas a column with low cardinality has a smaller number of unique values. For example, a &amp;quot;gender&amp;quot; column has low cardinality (typically two values: male and female), while an &amp;quot;email&amp;quot; column has high cardinality (unique values for each individual).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Real-World Scenario&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A retail company has a large Delta Lake table containing daily sales data with millions of rows. The table has columns like store_id, product_id, sale_date, and revenue. The company often runs queries to analyze sales performance for specific products in certain stores over different periods. To optimize these queries, the company decides to implement Z-Ordering on the table, focusing on the store_id, product_id, and sale_date columns. As these columns have high cardinality and are commonly used in query predicates, Z-Ordering can help colocate related information in the same set of files, improving data skipping and query performance. Guidelines for Choosing the Frequency of Z-Ordering: Large or complex data and selective or frequent queries: Z-order your table more often to improve query performance and resource utilization. Small or simple data and less selective or infrequent queries: Z-order your table less often or not at all to avoid unnecessary overhead and complexity. Frequent or concurrent data updates: Z-order your table less often or only on specific partitions to avoid conflicts or delays with other operations.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Drawbacks of Z-Ordering and Possible Solutions&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;While Z-Ordering can provide significant benefits in terms of query performance and resource utilization, it also comes with certain drawbacks:&lt;/p&gt;\n\n&lt;p&gt;Non-idempotency and incremental operation:&lt;/p&gt;\n\n&lt;p&gt;Z-Ordering is not idempotent and aims to be an incremental operation. The time it takes for Z-Ordering is not guaranteed to reduce over multiple runs. However, if no new data is added to a partition that was just Z-ordered, another Z-ordering of that partition will not have any effect.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Carefully plan and schedule Z-Ordering operations based on your data update frequency and specific use case. Monitor the performance improvements after each Z-Ordering operation to avoid unnecessary repetitions.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Impact on data freshness and Concurrency&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Z-Ordering rewrites files based on the column values, which may cause conflicts or delays with other concurrent operations on the table. This can impact data freshness and concurrency in your Delta Lake table.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Prioritize Z-Ordering on specific partitions or during periods of low data update activity to minimize conflicts and delays. Also, consider using Delta Lake&amp;#39;s optimistic concurrency control feature to manage concurrent transactions effectively.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Increased storage and compute costs:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Z-Ordering creates new files and removes old files, potentially increasing storage and compute costs. You may need to run the VACUUM command to reclaim storage space and reduce storage costs.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Regularly monitor your storage and compute costs and adjust your Z-Ordering frequency based on the observed benefits and costs. Use the VACUUM command to clean up old files and reduce storage costs.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Understanding predicates, cardinality, real-world scenarios, and the drawbacks of Z-Ordering can help you make informed decisions when implementing Z-Ordering in your Delta Lake project. By considering your specific use case, data characteristics, and potential drawbacks, you can optimize query performance and resource utilization while minimizing potential issues. Regularly monitor and troubleshoot your tables and queries to ensure they are running efficiently and effectively.&lt;/p&gt;\n\n&lt;p&gt;Sources: &lt;a href=\"https://learn.microsoft.com/en-us/azure/databricks/delta/data-skipping?ssp=1&amp;amp;setlang=en-GB&amp;amp;safesearch=moderate\"&gt;Delta Lake Data Skipping&lt;/a&gt; &lt;a href=\"https://docs.delta.io/latest/optimizations-oss.html#:%7E:text=Z%2DOrdering%20is%20a%20technique,Lake%20in%20data%2Dskipping%20algorithms\"&gt;Z-Ordering Optimizations&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?auto=webp&amp;v=enabled&amp;s=8e9dd29e55be5f9d4be03f8a2ca6dc669418c160", "width": 400, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c681715b246565ff9e7dfd8843f18ead842afeb2", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b70c2ce2562a35082ef9647f1b3db65e880827dd", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=448b5cac6633b9e14f4209e70d3996bf0d0f97b6", "width": 320, "height": 320}], "variants": {}, "id": "LVmzWMJU1UZwRubzQYJZSar-z-Rq8ntUH65yhQyfxB8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12xbw64", "is_robot_indexable": true, "report_reasons": null, "author": "adatascientistSl", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xbw64/zordering_in_delta_lake_key_considerations_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xbw64/zordering_in_delta_lake_key_considerations_and/", "subreddit_subscribers": 102040, "created_utc": 1682334419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data scientist on our team has been exploring ML models. I don\u2019t have any experience in this arena, but team is looking to me (DE) to put together a framework for how these models should be managed/deployed. wondering what the best options are for deploying a ML model? What parts should be owned by the data scientist vs data engineering?\n\nOur current tech stack:\n\nLinux server, Azure Devops for CI/CD and script deployments, Redshift for data warehouse, and then model built in Python\n\nWe are also beginning to explore AWS Sagemaker but no guarantee company will let us bring that on.", "author_fullname": "t2_8qi1s4iv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best options for deploying ML model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wqxiy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682285446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data scientist on our team has been exploring ML models. I don\u2019t have any experience in this arena, but team is looking to me (DE) to put together a framework for how these models should be managed/deployed. wondering what the best options are for deploying a ML model? What parts should be owned by the data scientist vs data engineering?&lt;/p&gt;\n\n&lt;p&gt;Our current tech stack:&lt;/p&gt;\n\n&lt;p&gt;Linux server, Azure Devops for CI/CD and script deployments, Redshift for data warehouse, and then model built in Python&lt;/p&gt;\n\n&lt;p&gt;We are also beginning to explore AWS Sagemaker but no guarantee company will let us bring that on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wqxiy", "is_robot_indexable": true, "report_reasons": null, "author": "fancyfanch", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wqxiy/best_options_for_deploying_ml_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wqxiy/best_options_for_deploying_ml_model/", "subreddit_subscribers": 102040, "created_utc": 1682285446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why would you create an iceberg table vs moving the data to an internal snowflake table? Is the benefit of creating an iceberg tables so it can be used by different compute like spark or trino? Cost savings? In my mind, iceberg is like schema on write vs schema on read. If someone makes a change to an upstream file like removing a field, it would break my iceberg load process.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Iceberg Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wo6pk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682280198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why would you create an iceberg table vs moving the data to an internal snowflake table? Is the benefit of creating an iceberg tables so it can be used by different compute like spark or trino? Cost savings? In my mind, iceberg is like schema on write vs schema on read. If someone makes a change to an upstream file like removing a field, it would break my iceberg load process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wo6pk", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wo6pk/snowflake_iceberg_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wo6pk/snowflake_iceberg_tables/", "subreddit_subscribers": 102040, "created_utc": 1682280198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently creating PySpark jobs on AWS/Glue environment. Moving forward with my team we decided to make our \"catalog\" -&gt; \"transactional\"  using one between Apache Hudi and Delta Lake. I have been spiking both technologies (using from a few MBs to TBs of dummy data) and they both fulfil our requirements. However a decision has not been taken yet so I've extended my research online and I wrote down a few bullet points that look more \"significative\":\n\n* Both well maintained projects based on Github stats\n* Apache Hudi has more features and support for integration with external tools than Delta Lake\n* Delta Lake is generally \"faster\" than Apache Hudi\n* Both are supported by AWS/Glue (for my case I refer only to AWS/Glue V4)\n* Not major differences for the syntax and code produced\n\nI guess all those points are arguably. Do you have any point to help the decision? Thank you!", "author_fullname": "t2_utk7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Indecision of choosing between Apache Hudi and Delta Lake on AWS/Glue environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12x8u1i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682325902.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682325555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently creating PySpark jobs on AWS/Glue environment. Moving forward with my team we decided to make our &amp;quot;catalog&amp;quot; -&amp;gt; &amp;quot;transactional&amp;quot;  using one between Apache Hudi and Delta Lake. I have been spiking both technologies (using from a few MBs to TBs of dummy data) and they both fulfil our requirements. However a decision has not been taken yet so I&amp;#39;ve extended my research online and I wrote down a few bullet points that look more &amp;quot;significative&amp;quot;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Both well maintained projects based on Github stats&lt;/li&gt;\n&lt;li&gt;Apache Hudi has more features and support for integration with external tools than Delta Lake&lt;/li&gt;\n&lt;li&gt;Delta Lake is generally &amp;quot;faster&amp;quot; than Apache Hudi&lt;/li&gt;\n&lt;li&gt;Both are supported by AWS/Glue (for my case I refer only to AWS/Glue V4)&lt;/li&gt;\n&lt;li&gt;Not major differences for the syntax and code produced&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I guess all those points are arguably. Do you have any point to help the decision? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12x8u1i", "is_robot_indexable": true, "report_reasons": null, "author": "df016", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12x8u1i/indecision_of_choosing_between_apache_hudi_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12x8u1i/indecision_of_choosing_between_apache_hudi_and/", "subreddit_subscribers": 102040, "created_utc": 1682325555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Want information from Data Engineers who use Google Cloud:** \n\nIs it your responsibility to create VM instances and perform network and HTTP load balancing? I am prep. for the GC PDE cert; following the kickstart cert\u2014program for partners by Google. The program includes creating and managing cloud resources. Should I spend a lot of time learning about infrastructure/ compute services?", "author_fullname": "t2_eo907yrs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GC Data Engineers, help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wrmwt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682286822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Want information from Data Engineers who use Google Cloud:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;Is it your responsibility to create VM instances and perform network and HTTP load balancing? I am prep. for the GC PDE cert; following the kickstart cert\u2014program for partners by Google. The program includes creating and managing cloud resources. Should I spend a lot of time learning about infrastructure/ compute services?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wrmwt", "is_robot_indexable": true, "report_reasons": null, "author": "avg_ali", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wrmwt/gc_data_engineers_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wrmwt/gc_data_engineers_help/", "subreddit_subscribers": 102040, "created_utc": 1682286822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_983tug55s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End-to-end Machine Learning modelling in BigQuery \u2014 Google Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_12wonl7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HZm20YvaY--DYP4PDbJ6Vjsd9pb6DXnn4030qSTpAPI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682281077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/artificial-corner/end-to-end-machine-learning-modelling-in-bigquery-google-cloud-a0d9e7eca20b", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?auto=webp&amp;v=enabled&amp;s=b59424aa4430f0984f32696bceea9e1ea1de97a0", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7331fbabd443814aeb6ba95dfab4b2450809461b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed226ce9932c1ff8d71c4a1dda55cf3624dbb2c5", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=443678e26ad4d8f23ee32e5c34b5bccec41978c6", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38ff374fae1be790984292ab10858f6c0409dff4", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/nxWZU3uoWGiuBAPLByO2Ffw9wcsL1iB8defJ-apQ020.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e3414e945c111c3505150d278d0fc7dc2e05678d", "width": 960, "height": 960}], "variants": {}, "id": "_9geqsbDIsWAAy4ufcfZ4iRR1RlvaEJ5bZTKufjB3tw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12wonl7", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum247", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wonl7/endtoend_machine_learning_modelling_in_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/artificial-corner/end-to-end-machine-learning-modelling-in-bigquery-google-cloud-a0d9e7eca20b", "subreddit_subscribers": 102040, "created_utc": 1682281077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI'm working to create a data pipeline that is near real-time but also scalable and cost effective. If I can have the data move from source to BigQuery in under 60 seconds that will be fine. Pipeline is in GCP.\n\nRight now, I have a Cloud Run instance that is listening for HTTP requests and then sending the data to GCS. I'm wondering the most effective, scalable, and easy way to move that data from GCS to BigQuery. Two options \n\n* When new data is loaded into GCS, I'll have a pub/sub notification sent out and that will trigger a Cloud Function to move the data. Once it's in a BigQuery staging dataset, I can do some transformations and create some reporting tables. \n* When new data is loaded into GCS, I'll I can use the pull pub/sub message with a DAG and use airflow to move the data from GCS to BigQuery\n\nThinking through the pro's cons of each. First option seems easiest, should be fairly scalable, will be cheap. I'm probably looking at 1k events / hour so nothing crazy. Second option allows me to kick off a DAG that could incorporate other transformation logic but not sure how scalable it will be and airflow on GCP can get expensive. \n\nThanks for the help!", "author_fullname": "t2_9u8negv8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most effective way to move data from GCS to BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wip0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682270182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working to create a data pipeline that is near real-time but also scalable and cost effective. If I can have the data move from source to BigQuery in under 60 seconds that will be fine. Pipeline is in GCP.&lt;/p&gt;\n\n&lt;p&gt;Right now, I have a Cloud Run instance that is listening for HTTP requests and then sending the data to GCS. I&amp;#39;m wondering the most effective, scalable, and easy way to move that data from GCS to BigQuery. Two options &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When new data is loaded into GCS, I&amp;#39;ll have a pub/sub notification sent out and that will trigger a Cloud Function to move the data. Once it&amp;#39;s in a BigQuery staging dataset, I can do some transformations and create some reporting tables. &lt;/li&gt;\n&lt;li&gt;When new data is loaded into GCS, I&amp;#39;ll I can use the pull pub/sub message with a DAG and use airflow to move the data from GCS to BigQuery&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thinking through the pro&amp;#39;s cons of each. First option seems easiest, should be fairly scalable, will be cheap. I&amp;#39;m probably looking at 1k events / hour so nothing crazy. Second option allows me to kick off a DAG that could incorporate other transformation logic but not sure how scalable it will be and airflow on GCP can get expensive. &lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wip0o", "is_robot_indexable": true, "report_reasons": null, "author": "Proper_Ad_7836", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wip0o/most_effective_way_to_move_data_from_gcs_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wip0o/most_effective_way_to_move_data_from_gcs_to/", "subreddit_subscribers": 102040, "created_utc": 1682270182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does everyone handle upstream data changes that break your data models? For example, we use DMS to replicate our data real time into Redshift raw schema. The upstream teams always make changes and then it breaks our downstream fact tables and staging views. Some of the changes are like removing columns, deprecating an existing column, and adding a new column. I have seen some people say create dbt tests on your sources and make sure everything is build from the staging schema so there is only one location to fix a downstream schema instead of many views. Right now, one small upstream change requires us to fix tons of dbt models. This isn\u2019t really scalable. Any advice?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upstream schema changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wdcfl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682262550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does everyone handle upstream data changes that break your data models? For example, we use DMS to replicate our data real time into Redshift raw schema. The upstream teams always make changes and then it breaks our downstream fact tables and staging views. Some of the changes are like removing columns, deprecating an existing column, and adding a new column. I have seen some people say create dbt tests on your sources and make sure everything is build from the staging schema so there is only one location to fix a downstream schema instead of many views. Right now, one small upstream change requires us to fix tons of dbt models. This isn\u2019t really scalable. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wdcfl", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wdcfl/upstream_schema_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wdcfl/upstream_schema_changes/", "subreddit_subscribers": 102040, "created_utc": 1682262550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to use solutions that work on our own servers and I can\u2019t introduce new tools or DB types. \n\nWhat Python related tools should I look into to make our stack as modern as possible? I want our team to end up with marketable skills and work experience. My own career is toast but I want to help the younger ones. \n\nWe use airflow, Python, sql, sql alchemy, linting of Python and SQL. I\u2019m looking into introducing DBT core, great expectations. Git/pull requests for collaboration.", "author_fullname": "t2_7gpue71ag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Locked on onPrem, best stack possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12x8jw3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682324697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to use solutions that work on our own servers and I can\u2019t introduce new tools or DB types. &lt;/p&gt;\n\n&lt;p&gt;What Python related tools should I look into to make our stack as modern as possible? I want our team to end up with marketable skills and work experience. My own career is toast but I want to help the younger ones. &lt;/p&gt;\n\n&lt;p&gt;We use airflow, Python, sql, sql alchemy, linting of Python and SQL. I\u2019m looking into introducing DBT core, great expectations. Git/pull requests for collaboration.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12x8jw3", "is_robot_indexable": true, "report_reasons": null, "author": "Annual_Anxiety_4457", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12x8jw3/locked_on_onprem_best_stack_possible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12x8jw3/locked_on_onprem_best_stack_possible/", "subreddit_subscribers": 102040, "created_utc": 1682324697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I found this an odd assessment\n\n [Google a Leader in 2023 Forrester Wave Data Management for Analytics | Google Cloud Blog](https://cloud.google.com/blog/products/data-analytics/google-a-leader-in-2023-forrester-wave-data-management-for-analytics) \n\nNo AWS ?\n\nInformatica in leader quadrant for analytics?\n\nWho is intersystems, that is ahead of Microsoft and Snowflake?", "author_fullname": "t2_5lv81gkx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forrester wave on Data management for analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12x3c86", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682311160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found this an odd assessment&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://cloud.google.com/blog/products/data-analytics/google-a-leader-in-2023-forrester-wave-data-management-for-analytics\"&gt;Google a Leader in 2023 Forrester Wave Data Management for Analytics | Google Cloud Blog&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;No AWS ?&lt;/p&gt;\n\n&lt;p&gt;Informatica in leader quadrant for analytics?&lt;/p&gt;\n\n&lt;p&gt;Who is intersystems, that is ahead of Microsoft and Snowflake?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?auto=webp&amp;v=enabled&amp;s=2bb423580e49d6864e3715f6654632d558a7d49c", "width": 2500, "height": 1232}, "resolutions": [{"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48579d8f753ce69b5f1b8ae0af753906892694f5", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1386d91cb1dd393b161dbfe57a97c27025c38f1", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c6c56995d10243a604293adda8ea07608cc4e59", "width": 320, "height": 157}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf7551884fe67c86f8908209089a05249b1c74ee", "width": 640, "height": 315}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07368c03b392acb2c54be0f5f3abda104f59aff0", "width": 960, "height": 473}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d94a8dee7f54cbfd6516eef96347cfa5a61ec285", "width": 1080, "height": 532}], "variants": {}, "id": "wcoegTgls8vDT7sU2cRo3smh6WlMonA1_OjrS3a-3OQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12x3c86", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical_Wish_4358", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12x3c86/forrester_wave_on_data_management_for_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12x3c86/forrester_wave_on_data_management_for_analytics/", "subreddit_subscribers": 102040, "created_utc": 1682311160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to use DBT authenticating via OAuth method using a service account to be able to run queries on an external table in BigQuery that pulls data from a google sheet. DBT runs via a cloud build but receives a \u201cpermission denied while getting Drive credentials\u201d error. \n\nFurther info:\n- the sheet is shared with the service account I am using. \n- I understand scopes would be the way to do this from the CLI but I don\u2019t know how to do this or similar via a service account on cloud build", "author_fullname": "t2_3fxv004y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I set up a build on Google Cloud Build running DBT via a service account, so that DBT is able to perform a query on an external table that gets data from GSheets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12xg4qv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682343523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use DBT authenticating via OAuth method using a service account to be able to run queries on an external table in BigQuery that pulls data from a google sheet. DBT runs via a cloud build but receives a \u201cpermission denied while getting Drive credentials\u201d error. &lt;/p&gt;\n\n&lt;p&gt;Further info:\n- the sheet is shared with the service account I am using. \n- I understand scopes would be the way to do this from the CLI but I don\u2019t know how to do this or similar via a service account on cloud build&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12xg4qv", "is_robot_indexable": true, "report_reasons": null, "author": "J1010H", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xg4qv/how_can_i_set_up_a_build_on_google_cloud_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xg4qv/how_can_i_set_up_a_build_on_google_cloud_build/", "subreddit_subscribers": 102040, "created_utc": 1682343523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is quite poor practice, no? I received a job offer from a health tech company and I\u2019d be working on pipelines touching PHI data. They do not supply a laptop and I\u2019d have to use a personal device. \n\nI\u2019m quite surprised in general that a tech company wouldn\u2019t provide computers but especially shocked that a health tech company would not. Just want to sanity check that this is unacceptable, or if I\u2019m missing something (coming from a bigger company and looking for a startup role so I know there will be changes, but was not expecting this).", "author_fullname": "t2_odulp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal laptop required at health tech company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12xfvnc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682343029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is quite poor practice, no? I received a job offer from a health tech company and I\u2019d be working on pipelines touching PHI data. They do not supply a laptop and I\u2019d have to use a personal device. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m quite surprised in general that a tech company wouldn\u2019t provide computers but especially shocked that a health tech company would not. Just want to sanity check that this is unacceptable, or if I\u2019m missing something (coming from a bigger company and looking for a startup role so I know there will be changes, but was not expecting this).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12xfvnc", "is_robot_indexable": true, "report_reasons": null, "author": "ihaveanideer", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xfvnc/personal_laptop_required_at_health_tech_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xfvnc/personal_laptop_required_at_health_tech_company/", "subreddit_subscribers": 102040, "created_utc": 1682343029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been writing a lot of SQL lately but I get errors like mismatching no. of columns in a row during an insert more so because I'm dealing with huge tables and the editors never show me an error like that. I understand that it's like a compile time error but is there any workaround that you guys use? TIA.", "author_fullname": "t2_q0hjyvx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What editors do you use for SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12xfoqw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682342644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been writing a lot of SQL lately but I get errors like mismatching no. of columns in a row during an insert more so because I&amp;#39;m dealing with huge tables and the editors never show me an error like that. I understand that it&amp;#39;s like a compile time error but is there any workaround that you guys use? TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12xfoqw", "is_robot_indexable": true, "report_reasons": null, "author": "Ready--Aim--Fire", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xfoqw/what_editors_do_you_use_for_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xfoqw/what_editors_do_you_use_for_sql/", "subreddit_subscribers": 102040, "created_utc": 1682342644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_20a1cwjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Map Visualizations with Wikidata and Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_12x8nqb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/udnkcjVCosMgwiUtKFTLcxOMhtsAJIjzVj-DNombFxs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682325017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/kKIkuldPfzb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?auto=webp&amp;v=enabled&amp;s=def798be933b7245c1866fe9ccf6539b62f83a7e", "width": 1200, "height": 604}, "resolutions": [{"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2583932a77a8a88dfdda4833e2efa4e4c89beaa6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56f0ee48ea86326d8a6c050c63ef55013be21f90", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85a5ef80305143c3b934503f6b9d453a1321f977", "width": 320, "height": 161}, {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=afa9792f954872cd445f477f7631d966f803725a", "width": 640, "height": 322}, {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df665efdc0663f406bd22f9b3c0e621e26ae1dc9", "width": 960, "height": 483}, {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e35a9e14de5f7cda63d26ebd5539f85a55c2412", "width": 1080, "height": 543}], "variants": {}, "id": "rjVs8rT6pWH7ForqHakf1ssj7iplJ1LxDWZHnVDWbQk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12x8nqb", "is_robot_indexable": true, "report_reasons": null, "author": "jkspiderdog", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12x8nqb/creating_map_visualizations_with_wikidata_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/kKIkuldPfzb", "subreddit_subscribers": 102040, "created_utc": 1682325017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know druid and rockset are typically used for OLAP, but both support SQL and relational (row based) queries... \n\nCurrently we use Mongo (our data is all JSON) and not that happy with it.  Would either of those be a good alternative for our OLTP write once and read many use case?  We typically don't do transactional updates... mostly queries across different document types.", "author_fullname": "t2_bluzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Apache Druid (or Rockset) for OLTP use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wxez0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682298343.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know druid and rockset are typically used for OLAP, but both support SQL and relational (row based) queries... &lt;/p&gt;\n\n&lt;p&gt;Currently we use Mongo (our data is all JSON) and not that happy with it.  Would either of those be a good alternative for our OLTP write once and read many use case?  We typically don&amp;#39;t do transactional updates... mostly queries across different document types.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wxez0", "is_robot_indexable": true, "report_reasons": null, "author": "Beertimeanytime", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wxez0/using_apache_druid_or_rockset_for_oltp_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wxez0/using_apache_druid_or_rockset_for_oltp_use_cases/", "subreddit_subscribers": 102040, "created_utc": 1682298343.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a Synapse workspace and I'd like to track the pipelines I run in it using Log Analytics. No Spark pool or SQL data, just that Synapse Pipelines run, and some details of those. \n\nI've created a separate resource group, a log analytics workspace, and connected the Synapse workspace to that via diagnostic settings. I've chosen the three pipeline log options available and to write logs to the log analytics workspace I configured.\n\nIve read everything I can but still, if I run a pipeline manually or with a trigger, no entries are shown in log analytics, even after a wait time. Log analytics shows no data and it's tables are empty. \n\nI'm running these pipelines from live mode. \nCan anyone shed some light on what I'm missing. It seems like log analytics should be connecting pretty easily. Doing this from a visual studio subscription. \n\nThanks.", "author_fullname": "t2_lmanh1xe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse and Log Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ws3qi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682287742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Synapse workspace and I&amp;#39;d like to track the pipelines I run in it using Log Analytics. No Spark pool or SQL data, just that Synapse Pipelines run, and some details of those. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve created a separate resource group, a log analytics workspace, and connected the Synapse workspace to that via diagnostic settings. I&amp;#39;ve chosen the three pipeline log options available and to write logs to the log analytics workspace I configured.&lt;/p&gt;\n\n&lt;p&gt;Ive read everything I can but still, if I run a pipeline manually or with a trigger, no entries are shown in log analytics, even after a wait time. Log analytics shows no data and it&amp;#39;s tables are empty. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running these pipelines from live mode. \nCan anyone shed some light on what I&amp;#39;m missing. It seems like log analytics should be connecting pretty easily. Doing this from a visual studio subscription. &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ws3qi", "is_robot_indexable": true, "report_reasons": null, "author": "TheFirstGlassPilot", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ws3qi/synapse_and_log_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ws3qi/synapse_and_log_analytics/", "subreddit_subscribers": 102040, "created_utc": 1682287742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have any tips, strategies, or resources on how to forecast and compare cost of workloads run on Databricks?  Example: how do you anticipate how much you'll be spending on compute for a project and if that amount is too much? \n\n[https://www.databricks.com/product/pricing/product-pricing/instance-types](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n\nare you just using this pricing calculator and picking the compute that will do it in the time that you're willing to wait?", "author_fullname": "t2_tsg6kr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pricing Databricks workloads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wmzsr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682277969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any tips, strategies, or resources on how to forecast and compare cost of workloads run on Databricks?  Example: how do you anticipate how much you&amp;#39;ll be spending on compute for a project and if that amount is too much? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.databricks.com/product/pricing/product-pricing/instance-types\"&gt;https://www.databricks.com/product/pricing/product-pricing/instance-types&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;are you just using this pricing calculator and picking the compute that will do it in the time that you&amp;#39;re willing to wait?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?auto=webp&amp;v=enabled&amp;s=81fadd2b039e6a77769e188d2cccb3b86ef3f685", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b760d8074e07ac99dc78ec15e1a38c06a7dbdad7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75a6072b4cf7509e972f24aa9138ed6afb5cecf0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=996d4c3bcc7ac13a9c25e3af33852e1246449b71", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74db19b833cba064dc927289bb9e603c40649f85", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7088c1762337d5e51f359cd97bd80ea57fe9c7a1", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bd75be4500b08cb818d69e661bb10c1f8fa6a71", "width": 1080, "height": 567}], "variants": {}, "id": "chr5A0PJvePhRJ7lI3sFaxKWyQXhWy3BRSTRcBAQKNs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wmzsr", "is_robot_indexable": true, "report_reasons": null, "author": "lifec0ach", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wmzsr/pricing_databricks_workloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wmzsr/pricing_databricks_workloads/", "subreddit_subscribers": 102040, "created_utc": 1682277969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some developed or coming Reverse ETL frameworks in market right now??\n\nGrouproo was one which has been acquired by Airbyte recently and feels like this feature/framework will closed sourced.", "author_fullname": "t2_1pye2bsf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Reverse ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xbq2r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682334024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some developed or coming Reverse ETL frameworks in market right now??&lt;/p&gt;\n\n&lt;p&gt;Grouproo was one which has been acquired by Airbyte recently and feels like this feature/framework will closed sourced.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12xbq2r", "is_robot_indexable": true, "report_reasons": null, "author": "piyushsingariya", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xbq2r/open_source_reverse_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xbq2r/open_source_reverse_etl/", "subreddit_subscribers": 102040, "created_utc": 1682334024.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}