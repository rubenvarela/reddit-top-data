{"kind": "Listing", "data": {"after": "t3_12irh9h", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have experience in DE for about 7 years and currently promoted to Data Architect role. I understand my responsibilities which include laying out data models, architecture diagrams, suggesting the best tools for a particular scenario. I have worked majorly in Azure and Databricks .\n\nPlease suggest the learning path / thinking methodology for me to be better at my current role.", "author_fullname": "t2_2xxs9nne", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have been recently promoted to Data Architect Role. I need help on learning resources/pointers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12iaaka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 62, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 62, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681199121.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681192407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have experience in DE for about 7 years and currently promoted to Data Architect role. I understand my responsibilities which include laying out data models, architecture diagrams, suggesting the best tools for a particular scenario. I have worked majorly in Azure and Databricks .&lt;/p&gt;\n\n&lt;p&gt;Please suggest the learning path / thinking methodology for me to be better at my current role.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12iaaka", "is_robot_indexable": true, "report_reasons": null, "author": "inglocines", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12iaaka/i_have_been_recently_promoted_to_data_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12iaaka/i_have_been_recently_promoted_to_data_architect/", "subreddit_subscribers": 98130, "created_utc": 1681192407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_975og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to use dbt source freshness tests to detect stale data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_12inq6z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/noCUQVwfgAxpdD300vlHP-w_Vk1xBMS_Ttf-9Ny8RNw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681227499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datafold.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datafold.com/blog/dbt-source-freshness", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WxpGs6QZU6CXiLkU96qO8RJjw4_4kfZ7qRcgOwFtsso.jpg?auto=webp&amp;v=enabled&amp;s=4a2e7bf440d31e813fbfed118f2002466730cdef", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/WxpGs6QZU6CXiLkU96qO8RJjw4_4kfZ7qRcgOwFtsso.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd999aa101b6b02ad7b9401b6512c65d55b7d934", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/WxpGs6QZU6CXiLkU96qO8RJjw4_4kfZ7qRcgOwFtsso.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e543df1eb9d1e1b52ec25a320c6494e1e6eb4ba", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/WxpGs6QZU6CXiLkU96qO8RJjw4_4kfZ7qRcgOwFtsso.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c8ac3945ef71a946231120973bab1279083550a", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/WxpGs6QZU6CXiLkU96qO8RJjw4_4kfZ7qRcgOwFtsso.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a64035e5ef1f38e6d55725ff6d4e337d9d60bee", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/WxpGs6QZU6CXiLkU96qO8RJjw4_4kfZ7qRcgOwFtsso.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32af2cff24ddc953b8c0d4f53d80d44fd01cd7ab", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/WxpGs6QZU6CXiLkU96qO8RJjw4_4kfZ7qRcgOwFtsso.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db2d5a8b7088d3f01673eab7d94b8ca156d2b999", "width": 1080, "height": 607}], "variants": {}, "id": "Hk_DWgM9iMOzRYbc08qVJTco8z-aNttuxoSZwBKbciQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12inq6z", "is_robot_indexable": true, "report_reasons": null, "author": "arimbr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12inq6z/how_to_use_dbt_source_freshness_tests_to_detect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datafold.com/blog/dbt-source-freshness", "subreddit_subscribers": 98130, "created_utc": 1681227499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've made a simple ETL project orchestrated by Airflow. The process is basically download &gt;&gt; transform &gt;&gt; load into Postgres.\n\n    with DAG(...):\n        task1 = PythonOperator(\n            task_id = \"DownloadData\",\n            python_callable = download_task\n        \u00a0 \u00a0 )\n        task2 = PythonOperator(\n            task_id = \"TransformData\",\n            python_callable = transform_task\n        \u00a0 \u00a0 )\n        ...\n        task1 &gt;&gt; task2 &gt;&gt; task3\n\nFirst task downloads more than 10k text-based files containing weather data. Its speed is okay (4 minutes w/ multithreading).\n\nThe problem is the second task. I used pandas to transform text-based files into clean data in .tsv format.\n\n    def _read_file(filename):\n        with open(filename, \"r\", encoding=\"UTF-8\") as f:\n            for line in f:           \n                line_content = [float(i) for i in line.split()]\n                yield {\n                    \"station_id\": filename,\n                    \"date\": f\"line_content[0]-line_content[1]-line_content[2]\"\n                    \"air_temperature\": line_content[4],\n                    ...\n                }\n\nI use `_read_file()` to read the content of the text-based file and make it a pandas dataframe to transform it.\n\n[raw text-based file \\(weather hourly data\\)](https://preview.redd.it/vritpkald8ta1.png?width=867&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c0bd6140837ad95c0bb1e21554326086803ea5c2)\n\n&amp;#x200B;\n\n    def transform(filename):        \n        # Create a dataframe out of file data\n        file_data = _read_file(filename)\n        df = pd.DataFrame(file_data)\n    \n        # Get the summarization of data (min, mean, max)\n        df = df.groupby(['station_id', 'date']).agg(\n            air_temperature_avg = ('air_temperature', 'mean'),\n            air_temperature_min = ('air_temperature', 'min'),\n            air_temperature_max = ('air_temperature', 'max'),\n            ...\n        )\n    \n        df.to_csv(f\"{filename}.tsv\", sep = \"\\t\")\n\n&amp;#x200B;\n\n[transformed data \\(daily summary: mean, min, and max\\)](https://preview.redd.it/06xqm23od8ta1.png?width=890&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1748787299abfd1424251c2c3aaa1c2fae69a4b0)\n\nThen i use the transform function in transform\\_task to summarize 10k+ raw text-based files one by one\n\n    def transform_task():\n        for filename in glob.glob(\"raw_directory/*\"): \u00a0 \u00a0 \u00a0 \u00a0 \n            transform(filename)\n\n&amp;#x200B;\n\nNote: I'm doing this locally in my laptop, not in cloud\n\nTransformation of each file is quick, about 0.15-0.20 seconds each. However, there are more than 10k files to transform so it takes about 40 minutes to accomplish the task. What do you think can be done to make this process faster?  Thanks in advance!", "author_fullname": "t2_3j0efkbj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[ETL Project] Transformation with Python pandas too slow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 69, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vritpkald8ta1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/vritpkald8ta1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d107e03c8260dff820045e7e6b041408e268535"}, {"y": 107, "x": 216, "u": "https://preview.redd.it/vritpkald8ta1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=305ecb53d459cda96dc5281d32a37c1d34e2191d"}, {"y": 159, "x": 320, "u": "https://preview.redd.it/vritpkald8ta1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7efb5852a4692e1ef85dbbd54b5e6c9f1e7fd6c9"}, {"y": 318, "x": 640, "u": "https://preview.redd.it/vritpkald8ta1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c4f60aaf2291325653e15b89ac6d89ad0170331"}], "s": {"y": 432, "x": 867, "u": "https://preview.redd.it/vritpkald8ta1.png?width=867&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c0bd6140837ad95c0bb1e21554326086803ea5c2"}, "id": "vritpkald8ta1"}, "06xqm23od8ta1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 34, "x": 108, "u": "https://preview.redd.it/06xqm23od8ta1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93511b5de9624e32b16b8c2cffa927a77321a831"}, {"y": 68, "x": 216, "u": "https://preview.redd.it/06xqm23od8ta1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=828c0732137bab2a055716c074977acf9fb5a5da"}, {"y": 101, "x": 320, "u": "https://preview.redd.it/06xqm23od8ta1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=782d8bd64676f91b572919eafda1b15c4c9eb021"}, {"y": 202, "x": 640, "u": "https://preview.redd.it/06xqm23od8ta1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61f15eef99db8331ce810c947905e612bba073c8"}], "s": {"y": 281, "x": 890, "u": "https://preview.redd.it/06xqm23od8ta1.png?width=890&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1748787299abfd1424251c2c3aaa1c2fae69a4b0"}, "id": "06xqm23od8ta1"}}, "name": "t3_12ie2fq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wf4RNpLcW-lz-jrhuDtUGh4NumBHQWJQDlF-mGfc-uA.jpg", "edited": 1681211557.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681204509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve made a simple ETL project orchestrated by Airflow. The process is basically download &amp;gt;&amp;gt; transform &amp;gt;&amp;gt; load into Postgres.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;with DAG(...):\n    task1 = PythonOperator(\n        task_id = &amp;quot;DownloadData&amp;quot;,\n        python_callable = download_task\n    \u00a0 \u00a0 )\n    task2 = PythonOperator(\n        task_id = &amp;quot;TransformData&amp;quot;,\n        python_callable = transform_task\n    \u00a0 \u00a0 )\n    ...\n    task1 &amp;gt;&amp;gt; task2 &amp;gt;&amp;gt; task3\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;First task downloads more than 10k text-based files containing weather data. Its speed is okay (4 minutes w/ multithreading).&lt;/p&gt;\n\n&lt;p&gt;The problem is the second task. I used pandas to transform text-based files into clean data in .tsv format.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def _read_file(filename):\n    with open(filename, &amp;quot;r&amp;quot;, encoding=&amp;quot;UTF-8&amp;quot;) as f:\n        for line in f:           \n            line_content = [float(i) for i in line.split()]\n            yield {\n                &amp;quot;station_id&amp;quot;: filename,\n                &amp;quot;date&amp;quot;: f&amp;quot;line_content[0]-line_content[1]-line_content[2]&amp;quot;\n                &amp;quot;air_temperature&amp;quot;: line_content[4],\n                ...\n            }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I use &lt;code&gt;_read_file()&lt;/code&gt; to read the content of the text-based file and make it a pandas dataframe to transform it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vritpkald8ta1.png?width=867&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c0bd6140837ad95c0bb1e21554326086803ea5c2\"&gt;raw text-based file (weather hourly data)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def transform(filename):        \n    # Create a dataframe out of file data\n    file_data = _read_file(filename)\n    df = pd.DataFrame(file_data)\n\n    # Get the summarization of data (min, mean, max)\n    df = df.groupby([&amp;#39;station_id&amp;#39;, &amp;#39;date&amp;#39;]).agg(\n        air_temperature_avg = (&amp;#39;air_temperature&amp;#39;, &amp;#39;mean&amp;#39;),\n        air_temperature_min = (&amp;#39;air_temperature&amp;#39;, &amp;#39;min&amp;#39;),\n        air_temperature_max = (&amp;#39;air_temperature&amp;#39;, &amp;#39;max&amp;#39;),\n        ...\n    )\n\n    df.to_csv(f&amp;quot;{filename}.tsv&amp;quot;, sep = &amp;quot;\\t&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/06xqm23od8ta1.png?width=890&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1748787299abfd1424251c2c3aaa1c2fae69a4b0\"&gt;transformed data (daily summary: mean, min, and max)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Then i use the transform function in transform_task to summarize 10k+ raw text-based files one by one&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def transform_task():\n    for filename in glob.glob(&amp;quot;raw_directory/*&amp;quot;): \u00a0 \u00a0 \u00a0 \u00a0 \n        transform(filename)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Note: I&amp;#39;m doing this locally in my laptop, not in cloud&lt;/p&gt;\n\n&lt;p&gt;Transformation of each file is quick, about 0.15-0.20 seconds each. However, there are more than 10k files to transform so it takes about 40 minutes to accomplish the task. What do you think can be done to make this process faster?  Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ie2fq", "is_robot_indexable": true, "report_reasons": null, "author": "Pervert_Spongebob", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ie2fq/etl_project_transformation_with_python_pandas_too/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ie2fq/etl_project_transformation_with_python_pandas_too/", "subreddit_subscribers": 98130, "created_utc": 1681204509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Project with Airflow, DuckDB, MinIO, Streamlit and the AstroSDK", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12io51t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ir3H1xOoIb8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Quickstart Project with DuckDB, MinIO and Streamlit\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Quickstart Project with DuckDB, MinIO and Streamlit", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ir3H1xOoIb8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Quickstart Project with DuckDB, MinIO and Streamlit\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/ir3H1xOoIb8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ir3H1xOoIb8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Quickstart Project with DuckDB, MinIO and Streamlit\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12io51t", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/C1naVPMCf-NVQ2t-oeXHMl4fPtikScSkrLYr21lQT7E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681228317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/ir3H1xOoIb8", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ncIe23vBi-MwJHABiGjw0JSN8zu2i_6GCRasr2wttP4.jpg?auto=webp&amp;v=enabled&amp;s=6a0b7647651ce6f2c4af2fce28200cdcfc7c73f3", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ncIe23vBi-MwJHABiGjw0JSN8zu2i_6GCRasr2wttP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b290915d178f1ee75ceff5f7e053dc7566fd4c3a", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ncIe23vBi-MwJHABiGjw0JSN8zu2i_6GCRasr2wttP4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=138ae22da4e6f615e245d1293fccfb34a57665cf", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ncIe23vBi-MwJHABiGjw0JSN8zu2i_6GCRasr2wttP4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b8bf888802f94138883b0e352a8feb2a5456274", "width": 320, "height": 240}], "variants": {}, "id": "JA9sMYD-6aEO4BCBc0r4btyF20uJ4WxAJhbu-4uefp8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12io51t", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12io51t/data_engineering_project_with_airflow_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/ir3H1xOoIb8", "subreddit_subscribers": 98130, "created_utc": 1681228317.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Quickstart Project with DuckDB, MinIO and Streamlit", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ir3H1xOoIb8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Quickstart Project with DuckDB, MinIO and Streamlit\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/ir3H1xOoIb8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nI'm making my way through the data engineering course on Databricks Academy and it's alright but I feel like every lesson is just someone reading from a notebook and executing SQL/ Python in the notebook. Feels like there's a lot of context missing (e.g there's course setup code but we never see this code).\n\nIs there any alternatives you prefer when learning Databricks?", "author_fullname": "t2_56o0g58i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Databricks academy for learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ivqk8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681243414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m making my way through the data engineering course on Databricks Academy and it&amp;#39;s alright but I feel like every lesson is just someone reading from a notebook and executing SQL/ Python in the notebook. Feels like there&amp;#39;s a lot of context missing (e.g there&amp;#39;s course setup code but we never see this code).&lt;/p&gt;\n\n&lt;p&gt;Is there any alternatives you prefer when learning Databricks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ivqk8", "is_robot_indexable": true, "report_reasons": null, "author": "IG-55", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ivqk8/alternative_to_databricks_academy_for_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ivqk8/alternative_to_databricks_academy_for_learning/", "subreddit_subscribers": 98130, "created_utc": 1681243414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team currently have our data sitting in a delta lake in an Azure storage account. We use Databricks on Azure to ingest and process data.\n\nWe have gotten to the stage where we would like to create an API to act as an interface to output data to web applications run by other teams as we don't want to tie their applications directly to the use of delta. We want them to be able to pass in parameters, we would then do some final steps of processing using the parameters provided, and then we want to send the output data directly back to the calling application. The final output of data will be a time-series and we expect that in the future the time-series could have in excess of one million items. The tables that will be queried are fairly large and growing so we want to minimise the compute time of the calculations by using Spark or some other method that is as fast. Ideally, we would want to do this without a wrapper layer that we would need to host external to Databricks, at least for now.\n\nWe realise that our setup is fairly similar to what other teams seem to have and were wondering how anyone else has managed to achieve what we are trying to.", "author_fullname": "t2_n937n0g6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake/Databricks egress via API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12iodei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681228788.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team currently have our data sitting in a delta lake in an Azure storage account. We use Databricks on Azure to ingest and process data.&lt;/p&gt;\n\n&lt;p&gt;We have gotten to the stage where we would like to create an API to act as an interface to output data to web applications run by other teams as we don&amp;#39;t want to tie their applications directly to the use of delta. We want them to be able to pass in parameters, we would then do some final steps of processing using the parameters provided, and then we want to send the output data directly back to the calling application. The final output of data will be a time-series and we expect that in the future the time-series could have in excess of one million items. The tables that will be queried are fairly large and growing so we want to minimise the compute time of the calculations by using Spark or some other method that is as fast. Ideally, we would want to do this without a wrapper layer that we would need to host external to Databricks, at least for now.&lt;/p&gt;\n\n&lt;p&gt;We realise that our setup is fairly similar to what other teams seem to have and were wondering how anyone else has managed to achieve what we are trying to.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12iodei", "is_robot_indexable": true, "report_reasons": null, "author": "piri9825", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12iodei/delta_lakedatabricks_egress_via_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12iodei/delta_lakedatabricks_egress_via_api/", "subreddit_subscribers": 98130, "created_utc": 1681228788.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \nAbout me: I joined the corporate world as an Data engineer just after graduating in mid 2021 (but my current work revolves around Data analytics, SQL and Python). Total exp: 1.5yrs\n\nI am currently in a dilemma: whether I should continue as a Data Engineer by expanding my skills (learning Spark etc) or switch to SDE roles (safe evergreen option).\n\nI have few concerns about DE role which is kinda stopping me from fully deep diving into it. \n\n1. Are data engineers considered 2nd class employees in a company? I have read that DE is a role that supports business functions and Data scientist and are generally not profit generating employees (away from business) hence their efforts can sometimes go unnoticed.\n\n2. How does day in a life looks like? Work life balance and work environment?\n\n3. Growth prospects: Can a DE transition/grow into leadership positions?\n\n\nLooking for some guidance. Thanks in advance!", "author_fullname": "t2_75txeq7v4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DE a good role in terms of work and work life balance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12idd74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681204269.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681202190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, \nAbout me: I joined the corporate world as an Data engineer just after graduating in mid 2021 (but my current work revolves around Data analytics, SQL and Python). Total exp: 1.5yrs&lt;/p&gt;\n\n&lt;p&gt;I am currently in a dilemma: whether I should continue as a Data Engineer by expanding my skills (learning Spark etc) or switch to SDE roles (safe evergreen option).&lt;/p&gt;\n\n&lt;p&gt;I have few concerns about DE role which is kinda stopping me from fully deep diving into it. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Are data engineers considered 2nd class employees in a company? I have read that DE is a role that supports business functions and Data scientist and are generally not profit generating employees (away from business) hence their efforts can sometimes go unnoticed.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How does day in a life looks like? Work life balance and work environment?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Growth prospects: Can a DE transition/grow into leadership positions?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Looking for some guidance. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12idd74", "is_robot_indexable": true, "report_reasons": null, "author": "Skrirraa", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12idd74/is_de_a_good_role_in_terms_of_work_and_work_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12idd74/is_de_a_good_role_in_terms_of_work_and_work_life/", "subreddit_subscribers": 98130, "created_utc": 1681202190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen a few people mention trino + data lake. I see in the trino docs that it's designed to query different sources directly, ie without prior etl/elt. \n\nSo why bother with a data lake? \n\nI can imagine it's to make reads faster, for better organization, and to avoid noisy neighbor type issues on source systems. \n\nAnyone have ideas on those or other reasons?", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why use trino on a data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ip77n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681230527.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen a few people mention trino + data lake. I see in the trino docs that it&amp;#39;s designed to query different sources directly, ie without prior etl/elt. &lt;/p&gt;\n\n&lt;p&gt;So why bother with a data lake? &lt;/p&gt;\n\n&lt;p&gt;I can imagine it&amp;#39;s to make reads faster, for better organization, and to avoid noisy neighbor type issues on source systems. &lt;/p&gt;\n\n&lt;p&gt;Anyone have ideas on those or other reasons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ip77n", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ip77n/why_use_trino_on_a_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ip77n/why_use_trino_on_a_data_lake/", "subreddit_subscribers": 98130, "created_utc": 1681230527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello DE friends.\n\nMy current company uses Apache Airflow for scheduling, and we are quite happy with it. But giving technically gifted PMs the ability to make simple DAGs from a visual interface would be amazing.\n\nFor this we love the N8N interface. \n\nHowever the native N8N scheduler and workflows are too lacking and overcomplicated to use for our workflows.\n\nHence my question.\n\nHas anyone connected N8N or any no-code solution to Airflow? \n\nLet me be specific. We aren't looking to trigger a DAG from N8N. We'd like to build it there, task by task, and then have the resulting DAG deployed on Airflow.\n\nIf you have any hints, ideas or projects that you think are doing this, I'd be super interested.\n\nThanks for your help \ud83d\ude4f", "author_fullname": "t2_85uwiihz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using no-code to create simple Airflow Dags", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12icsvl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681200327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello DE friends.&lt;/p&gt;\n\n&lt;p&gt;My current company uses Apache Airflow for scheduling, and we are quite happy with it. But giving technically gifted PMs the ability to make simple DAGs from a visual interface would be amazing.&lt;/p&gt;\n\n&lt;p&gt;For this we love the N8N interface. &lt;/p&gt;\n\n&lt;p&gt;However the native N8N scheduler and workflows are too lacking and overcomplicated to use for our workflows.&lt;/p&gt;\n\n&lt;p&gt;Hence my question.&lt;/p&gt;\n\n&lt;p&gt;Has anyone connected N8N or any no-code solution to Airflow? &lt;/p&gt;\n\n&lt;p&gt;Let me be specific. We aren&amp;#39;t looking to trigger a DAG from N8N. We&amp;#39;d like to build it there, task by task, and then have the resulting DAG deployed on Airflow.&lt;/p&gt;\n\n&lt;p&gt;If you have any hints, ideas or projects that you think are doing this, I&amp;#39;d be super interested.&lt;/p&gt;\n\n&lt;p&gt;Thanks for your help \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12icsvl", "is_robot_indexable": true, "report_reasons": null, "author": "GeekyTricky", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12icsvl/using_nocode_to_create_simple_airflow_dags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12icsvl/using_nocode_to_create_simple_airflow_dags/", "subreddit_subscribers": 98130, "created_utc": 1681200327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9wrfjdsj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Lakehouse by the sea: Migrating Seafowl storage layer to delta-rs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12ivoi5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/rESLgC8lkPhmLkFb5qvPHsEMV5HAbJcWWjzDSMWoV_4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681243302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "splitgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.splitgraph.com/blog/seafowl-delta-storage-layer", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cbi3Dc--2pOvkY1v9YYEvc-dHCC71jiiWQNeScaoxgA.jpg?auto=webp&amp;v=enabled&amp;s=0e71ea909937e503f7c833b044201cba0a51063c", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/cbi3Dc--2pOvkY1v9YYEvc-dHCC71jiiWQNeScaoxgA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65db8ae9f1540d295f00950b12174d7be3508625", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/cbi3Dc--2pOvkY1v9YYEvc-dHCC71jiiWQNeScaoxgA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef63e238179153a4a97e6b2aeb3be247c0fab18a", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/cbi3Dc--2pOvkY1v9YYEvc-dHCC71jiiWQNeScaoxgA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=114619e03e3054a5fa0cae59378077ea601c4948", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/cbi3Dc--2pOvkY1v9YYEvc-dHCC71jiiWQNeScaoxgA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b01ec1e97553be30eeef904cd70222598fa4dbac", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/cbi3Dc--2pOvkY1v9YYEvc-dHCC71jiiWQNeScaoxgA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b850d0b1d23c39f8132a7f3d26e2ae978ddea7b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/cbi3Dc--2pOvkY1v9YYEvc-dHCC71jiiWQNeScaoxgA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=726ad6e29157a5a4b019406464a60331cf91597c", "width": 1080, "height": 567}], "variants": {}, "id": "kwl5UPry6AuAlyMdIEIWUnQ47W6ryiRkRihcsErskV4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ivoi5", "is_robot_indexable": true, "report_reasons": null, "author": "gruuya", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ivoi5/a_lakehouse_by_the_sea_migrating_seafowl_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.splitgraph.com/blog/seafowl-delta-storage-layer", "subreddit_subscribers": 98130, "created_utc": 1681243302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I have a question, probably very strange one, but I have a task on this which is killing me.\nThe problem is that I have a lot of JSON files that need to be processed. The files have different structure schema, but the requester wants to have the data flattened in a table with a very general structure, to cover all formats. The data will be used then for reporting.\n Is there any possible way to do this? \nI would appreciate any suggestion on how the final table may look like and how to flatten the data. The implementation should be done in Java, but I would not focus now on this.\n Thank you very much!", "author_fullname": "t2_8ouamdf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generic JSON schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12irysl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681236123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I have a question, probably very strange one, but I have a task on this which is killing me.\nThe problem is that I have a lot of JSON files that need to be processed. The files have different structure schema, but the requester wants to have the data flattened in a table with a very general structure, to cover all formats. The data will be used then for reporting.\n Is there any possible way to do this? \nI would appreciate any suggestion on how the final table may look like and how to flatten the data. The implementation should be done in Java, but I would not focus now on this.\n Thank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12irysl", "is_robot_indexable": true, "report_reasons": null, "author": "adaptrix", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12irysl/generic_json_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12irysl/generic_json_schema/", "subreddit_subscribers": 98130, "created_utc": 1681236123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious how everyone is feeling.\n\n[View Poll](https://www.reddit.com/poll/12ir3f6)", "author_fullname": "t2_rdm9w1f0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How burnt out are you? (Non Students)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ir3f6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681234429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious how everyone is feeling.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/12ir3f6\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ir3f6", "is_robot_indexable": true, "report_reasons": null, "author": "keeney_arcadia", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1681493629835, "options": [{"text": "I love my job!", "id": "22511488"}, {"text": "It comes in waves", "id": "22511489"}, {"text": "I'm regularly stressed", "id": "22511490"}, {"text": "I loathe going to work", "id": "22511491"}, {"text": "I'm buried in work and see no light at the end of the tunnel", "id": "22511492"}, {"text": "It's just a job, I put my 8 hours in and dip", "id": "22511493"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 725, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ir3f6/how_burnt_out_are_you_non_students/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/12ir3f6/how_burnt_out_are_you_non_students/", "subreddit_subscribers": 98130, "created_utc": 1681234429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are you familiar with Data Quality and Great Expectations? \n\nI recently started using this library on a data pipeline. As a junior Data Engineer, I found the documentation quite overwhelming and unsuitable for Databricks. However, I was able to create a workflow for my team: \n\n1. **Fill a form to create an expectation suite**\n2. **run / schedule a data factory**\n3. **get mail /notification with report**\n\nI'm also looking for ways to automate data exploration so I can better understand the values associated with each expectation. I know Ydata exists, but my data scientist doesn't seem to like it.   \n\nGreat Expectations is a library that runs tests without the need for manual coding them.\n\nIf anyone is experienced with this library and wants to provide me with some tips, feel free to reach out! I'm also eager to discuss my ideas for a future post.", "author_fullname": "t2_7clw5kel", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monitoring data with Great Expectations - Junior Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12j5ltv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681263762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are you familiar with Data Quality and Great Expectations? &lt;/p&gt;\n\n&lt;p&gt;I recently started using this library on a data pipeline. As a junior Data Engineer, I found the documentation quite overwhelming and unsuitable for Databricks. However, I was able to create a workflow for my team: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Fill a form to create an expectation suite&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;run / schedule a data factory&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;get mail /notification with report&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m also looking for ways to automate data exploration so I can better understand the values associated with each expectation. I know Ydata exists, but my data scientist doesn&amp;#39;t seem to like it.   &lt;/p&gt;\n\n&lt;p&gt;Great Expectations is a library that runs tests without the need for manual coding them.&lt;/p&gt;\n\n&lt;p&gt;If anyone is experienced with this library and wants to provide me with some tips, feel free to reach out! I&amp;#39;m also eager to discuss my ideas for a future post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12j5ltv", "is_robot_indexable": true, "report_reasons": null, "author": "Independent_Ad6023", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12j5ltv/monitoring_data_with_great_expectations_junior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12j5ltv/monitoring_data_with_great_expectations_junior/", "subreddit_subscribers": 98130, "created_utc": 1681263762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have experience with these (cloud or open source) and can either recommend or warn against using them?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Estuary or Airbyte for binlog/WAL CDC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12j3jzu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681259250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have experience with these (cloud or open source) and can either recommend or warn against using them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12j3jzu", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12j3jzu/estuary_or_airbyte_for_binlogwal_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12j3jzu/estuary_or_airbyte_for_binlogwal_cdc/", "subreddit_subscribers": 98130, "created_utc": 1681259250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since each company uses different tools for their environment how do we manage to learn the one's we don't know when applying for a role? How in-depth should I go when learning a new tool since I might not use that tool for too long before having to switch to something new at a different company?", "author_fullname": "t2_76fvluuq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with learning the many changes of tools in this field.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12iyemq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681248637.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since each company uses different tools for their environment how do we manage to learn the one&amp;#39;s we don&amp;#39;t know when applying for a role? How in-depth should I go when learning a new tool since I might not use that tool for too long before having to switch to something new at a different company?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12iyemq", "is_robot_indexable": true, "report_reasons": null, "author": "notGaruda1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12iyemq/how_to_deal_with_learning_the_many_changes_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12iyemq/how_to_deal_with_learning_the_many_changes_of/", "subreddit_subscribers": 98130, "created_utc": 1681248637.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It's been 3 months since I started working at a started, as a Junior Data Engineer. Currently, I am in the final year of CIS Undergraduate program.  The boss wants me to attempt the GC Prof. DE exam and is giving me one month for preparation. Is one month enough time? \nThe company will bear all the expenses. Its certainly a good opportunity to get an industry-recognised credential, but I am worried whether I will be able to pull this off. \nHave worked on BigQuery, proficient with python and SQL, sound knowledge of crucial DS/ DE concepts(foundational and advanced). Never used DataFlow or Dataproc, though.", "author_fullname": "t2_eo907yrs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Cloud Prof. Data Engineer certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ikdue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681220744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s been 3 months since I started working at a started, as a Junior Data Engineer. Currently, I am in the final year of CIS Undergraduate program.  The boss wants me to attempt the GC Prof. DE exam and is giving me one month for preparation. Is one month enough time? \nThe company will bear all the expenses. Its certainly a good opportunity to get an industry-recognised credential, but I am worried whether I will be able to pull this off. \nHave worked on BigQuery, proficient with python and SQL, sound knowledge of crucial DS/ DE concepts(foundational and advanced). Never used DataFlow or Dataproc, though.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12ikdue", "is_robot_indexable": true, "report_reasons": null, "author": "avg_ali", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ikdue/google_cloud_prof_data_engineer_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ikdue/google_cloud_prof_data_engineer_certification/", "subreddit_subscribers": 98130, "created_utc": 1681220744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7idw1yfm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The database inside out with event streams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 61, "top_awarded_type": null, "hide_score": false, "name": "t3_12ih4dp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b-6fjQm3Jj02KsbW6pCoFxI7xmg1wQorUJ0tT_uF0wM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681213375.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@hugo.oliveira.rocha/the-database-inside-out-with-event-streams-86d4a54192eb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FK2aKV_Zmm73XT_WBi7YAIU3ZvECCLmfmj_94ivkAPE.jpg?auto=webp&amp;v=enabled&amp;s=e08d1b7c1918c19adb4f232f59c8b09c12447f7e", "width": 675, "height": 295}, "resolutions": [{"url": "https://external-preview.redd.it/FK2aKV_Zmm73XT_WBi7YAIU3ZvECCLmfmj_94ivkAPE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=439e1fdf2dfa6e664df9b1d530e5ee3e3f69daf6", "width": 108, "height": 47}, {"url": "https://external-preview.redd.it/FK2aKV_Zmm73XT_WBi7YAIU3ZvECCLmfmj_94ivkAPE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d127b0df7284249c7496901a227819ab91d9d14e", "width": 216, "height": 94}, {"url": "https://external-preview.redd.it/FK2aKV_Zmm73XT_WBi7YAIU3ZvECCLmfmj_94ivkAPE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e83383852aeae4e29f66cd0177a00b25f7013c3b", "width": 320, "height": 139}, {"url": "https://external-preview.redd.it/FK2aKV_Zmm73XT_WBi7YAIU3ZvECCLmfmj_94ivkAPE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d925b3c797747156f3e1761c2dbd2b7a9b901f21", "width": 640, "height": 279}], "variants": {}, "id": "4WI2vDdx00P6JZuRulbSxSPr0L92GguFn-TKiPFusZE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ih4dp", "is_robot_indexable": true, "report_reasons": null, "author": "-segmentationfault-", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ih4dp/the_database_inside_out_with_event_streams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@hugo.oliveira.rocha/the-database-inside-out-with-event-streams-86d4a54192eb", "subreddit_subscribers": 98130, "created_utc": 1681213375.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4x4vn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL vs. CDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_12j5trj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wQP27dL5al-8kR8TGDDvGcOuxm80AItyulx7Sz7QXV4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681264273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/3h92v1g80dta1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/3h92v1g80dta1.png?auto=webp&amp;v=enabled&amp;s=63d5203fc7bed308e71e87576615b581a717f3f8", "width": 576, "height": 655}, "resolutions": [{"url": "https://preview.redd.it/3h92v1g80dta1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae4fb966e6e83d4f4766c41961ea2f3be96b347c", "width": 108, "height": 122}, {"url": "https://preview.redd.it/3h92v1g80dta1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e53dbd047868088800104715e94fb3323a1dd2e8", "width": 216, "height": 245}, {"url": "https://preview.redd.it/3h92v1g80dta1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e9e0336e481ca076e181aed1b5df0a724f135931", "width": 320, "height": 363}], "variants": {}, "id": "cfhLkak740-EJdE72eMW9hDZulVs6r-Ms4__F26lVbY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12j5trj", "is_robot_indexable": true, "report_reasons": null, "author": "xilanthro", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12j5trj/etl_vs_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/3h92v1g80dta1.png", "subreddit_subscribers": 98130, "created_utc": 1681264273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\n\nSo I'm looking at this funnel report from Mixpanel right now, and I just wanted to see if I'm understanding the conversion rates, as they aren't quite making sense to me.\n\n[https://imgur.com/a/xMAtMKc](https://imgur.com/a/xMAtMKc)\n\nIn the 3rd step, \"Message Delivered\", it says before the step that the conversion rate is 65.8%, but when I click on it, it says 2811 unique users and says 39.5% instead.\n\nBoth numbers seem pretty far off from each other.\n\nDoes 65.8% represent the percentage of users who completed both the \"View Home Page\" + \"Process Payment\" and moved on to the \"Message Delivered\" step (but didn't complete it)? OR is it the percentage of users who completed only the \"Process Payment\" step and moved on to the \"Message Delivered\" step (but didn't complete it).\n\nDoes 39.5% represent conversion rate of all users who entered the funnel and then completed the third step, including those who did not complete the second step?\n\nThanks guys in advance :)", "author_fullname": "t2_h2t9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to understand the difference in conversion rates in a Mixpanel funnel report", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12j5dth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681263257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m looking at this funnel report from Mixpanel right now, and I just wanted to see if I&amp;#39;m understanding the conversion rates, as they aren&amp;#39;t quite making sense to me.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/xMAtMKc\"&gt;https://imgur.com/a/xMAtMKc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In the 3rd step, &amp;quot;Message Delivered&amp;quot;, it says before the step that the conversion rate is 65.8%, but when I click on it, it says 2811 unique users and says 39.5% instead.&lt;/p&gt;\n\n&lt;p&gt;Both numbers seem pretty far off from each other.&lt;/p&gt;\n\n&lt;p&gt;Does 65.8% represent the percentage of users who completed both the &amp;quot;View Home Page&amp;quot; + &amp;quot;Process Payment&amp;quot; and moved on to the &amp;quot;Message Delivered&amp;quot; step (but didn&amp;#39;t complete it)? OR is it the percentage of users who completed only the &amp;quot;Process Payment&amp;quot; step and moved on to the &amp;quot;Message Delivered&amp;quot; step (but didn&amp;#39;t complete it).&lt;/p&gt;\n\n&lt;p&gt;Does 39.5% represent conversion rate of all users who entered the funnel and then completed the third step, including those who did not complete the second step?&lt;/p&gt;\n\n&lt;p&gt;Thanks guys in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OULZ1npAtngvUkL2KVJO8ApvoWrMjh_Z-5oagT2URt8.jpg?auto=webp&amp;v=enabled&amp;s=383c27b808bbf5356685bf927bc4053aca328e2d", "width": 2618, "height": 1072}, "resolutions": [{"url": "https://external-preview.redd.it/OULZ1npAtngvUkL2KVJO8ApvoWrMjh_Z-5oagT2URt8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e60d9880eeb1537737976ef8f53f848cce34f6c7", "width": 108, "height": 44}, {"url": "https://external-preview.redd.it/OULZ1npAtngvUkL2KVJO8ApvoWrMjh_Z-5oagT2URt8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5fac2900ccfeeba9afb98be9e6f1ab7c1dd4d5f", "width": 216, "height": 88}, {"url": "https://external-preview.redd.it/OULZ1npAtngvUkL2KVJO8ApvoWrMjh_Z-5oagT2URt8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e8860767af2202c74d6499a22e50722f9d7557e", "width": 320, "height": 131}, {"url": "https://external-preview.redd.it/OULZ1npAtngvUkL2KVJO8ApvoWrMjh_Z-5oagT2URt8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c9f9f0c43f8cab2f1bee8ddf59204d66da862a2", "width": 640, "height": 262}, {"url": "https://external-preview.redd.it/OULZ1npAtngvUkL2KVJO8ApvoWrMjh_Z-5oagT2URt8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e0156c628fa5ec04ab7d4b1e7a1e24bea166338", "width": 960, "height": 393}, {"url": "https://external-preview.redd.it/OULZ1npAtngvUkL2KVJO8ApvoWrMjh_Z-5oagT2URt8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1dda0a61a9a9884ddc0077cab98247019b5555f7", "width": 1080, "height": 442}], "variants": {}, "id": "N6DJuO_dHXjZyF9l_HWTSAObF0Tf6ptrlwhO-Q7AVlk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12j5dth", "is_robot_indexable": true, "report_reasons": null, "author": "jackielarson", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12j5dth/trying_to_understand_the_difference_in_conversion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12j5dth/trying_to_understand_the_difference_in_conversion/", "subreddit_subscribers": 98130, "created_utc": 1681263257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm kind of new to this but my understanding is that non-structured data can but uploaded to a data warehouse and them transformed. ELT so to speak.\n\nNow when I think of non-structured data I'm thinking of something like video or pictures or things of that nature. I've never worked with either of those things. However, I seem to be working a lot with poorly formatted text files from a legacy financial system.\n\nThe files look something like the below example...\n\nDon't focus on the made up data, focus on the structure.\n\nThis is the kind of garbage I've been working with.  I've been using AWK and SED and regular expressions to parse through all this crap to turn it into a Column/Row dataset.\n\nIt's not easy, sometimes I'll have to take header data like 'OP: A031' and 'USER:XSYS1414 ' and create columns for those as well because that will change throughout the entire file.\n\nI guess my questions are:\n\n1. What do you call doing this type of shit of turning dog-shit into diamonds? ETL Development?\n2. Is there an easier way of doing this?\n\n&amp;#x200B;\n\n\\--1             ------- REPORT.THIS.IS-THE.REPORT\\_NAME\\_5411 -------                                1/2111 USER:XSYS1414         \n\n4/11/2023 2340  OP: A031                                                                                                                     X\\_RTXN\\_JNL\n\nLOC# ACCT\\_NBR  POLICY  OP# DATETIME TXN\\_CODE     RATE    AMT\\_DUE    NEXT\\_DUE  LC         BALANCE \n\n   12A 1234567     G18         H152  0955         60                   4.5        1,000.16      5/11/2023  0.00   -9,999.16 \n\n   99   84124581 T800         T1000 0955          50                   2.5         500.16       5/11/2023  0.00   1,500.16 \n\n   99   645665485T800         T1000 0955         60                   4.5        1,000.16      5/12/2023  39.00-9,999.16 \n\n\\--2             ------- REPORT.THIS.IS-THE.REPORT\\_NAME\\_5411 -------                                2/2111 USER:XSYS1414         \n\n4/11/2023 2340  OP: A031                                                                                                                     X\\_RTXN\\_JNL\n\n   12A 1234567     G18         H152  0955         60                   4.5        1,000.16      5/11/2023  0.00   -9,999.16 \n\n   99   84124581 T800         T1000 0955          50                   2.5         500.16       5/11/2023  0.00   1,500.16 \n\n   99   645665485T800         T1000 0955         60                   4.5        1,000.16      5/12/2023  39.00-9,999.16 \n\n\\--3             ------- REPORT.THIS.IS-THE.REPORT\\_NAME\\_5411 -------                                3/2111 USER:XSYS1414         \n\n4/11/2023 2340  OP: A031                                                                                                                     X\\_RTXN\\_JNL        \n\n   12A 1234567     G18         H152  0955         60                   4.5        1,000.16      5/11/2023  0.00   -9,999.16 \n\n   99   84124581 T800         T1000 0955          50                   2.5         500.16       5/11/2023  0.00   1,500.16 \n\n   99   645665485T800         T1000 0955         60                   4.5        1,000.16      5/12/2023  39.00-9,999.16 \n\n\\*\n\n\\*\\*\n\n\\*\\*\\*                                           LOC#                       BALANCE             AVG BALANCE\n\n\\*\\*\\*\\*                                         12A                          $1,565,568.13      $487.684.08\n\n\\*\\*\\*\\*\\*                                         99                           $5,665,138.19     $268.586.12\n\n\\*\\*\\*\\*\\*\\*                                                                    =============    =============\n\n\\*\\*\\*\\*\\*\\*\\*                                                                    $99,874,698.15    $3,555,987.01\n\n\\--4             ------- REPORT.THIS.IS-THE.REPORT\\_NAME\\_5411 -------                                3/2111 USER:XSYS1414         \n\n4/11/2023 2340  OP: A031                                                                                                                     X\\_RTXN\\_JNL        \n\n   12A 1234567     G18         H152  0955         60                   4.5        1,000.16      5/11/2023  0.00   -9,999.16 \n\n   99   84124581 T800         T1000 0955          50                   2.5         500.16       5/11/2023  0.00   1,500.16 \n\n   99   645665485T800         T1000 0955         60                   4.5        1,000.16      5/12/2023  39.00-9,999.16", "author_fullname": "t2_12fb7o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi All. Question about ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12j53i2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681262621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m kind of new to this but my understanding is that non-structured data can but uploaded to a data warehouse and them transformed. ELT so to speak.&lt;/p&gt;\n\n&lt;p&gt;Now when I think of non-structured data I&amp;#39;m thinking of something like video or pictures or things of that nature. I&amp;#39;ve never worked with either of those things. However, I seem to be working a lot with poorly formatted text files from a legacy financial system.&lt;/p&gt;\n\n&lt;p&gt;The files look something like the below example...&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t focus on the made up data, focus on the structure.&lt;/p&gt;\n\n&lt;p&gt;This is the kind of garbage I&amp;#39;ve been working with.  I&amp;#39;ve been using AWK and SED and regular expressions to parse through all this crap to turn it into a Column/Row dataset.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not easy, sometimes I&amp;#39;ll have to take header data like &amp;#39;OP: A031&amp;#39; and &amp;#39;USER:XSYS1414 &amp;#39; and create columns for those as well because that will change throughout the entire file.&lt;/p&gt;\n\n&lt;p&gt;I guess my questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What do you call doing this type of shit of turning dog-shit into diamonds? ETL Development?&lt;/li&gt;\n&lt;li&gt;Is there an easier way of doing this?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;--1             ------- REPORT.THIS.IS-THE.REPORT_NAME_5411 -------                                1/2111 USER:XSYS1414         &lt;/p&gt;\n\n&lt;p&gt;4/11/2023 2340  OP: A031                                                                                                                     X_RTXN_JNL&lt;/p&gt;\n\n&lt;p&gt;LOC# ACCT_NBR  POLICY  OP# DATETIME TXN_CODE     RATE    AMT_DUE    NEXT_DUE  LC         BALANCE &lt;/p&gt;\n\n&lt;p&gt;12A 1234567     G18         H152  0955         60                   4.5        1,000.16      5/11/2023  0.00   -9,999.16 &lt;/p&gt;\n\n&lt;p&gt;99   84124581 T800         T1000 0955          50                   2.5         500.16       5/11/2023  0.00   1,500.16 &lt;/p&gt;\n\n&lt;p&gt;99   645665485T800         T1000 0955         60                   4.5        1,000.16      5/12/2023  39.00-9,999.16 &lt;/p&gt;\n\n&lt;p&gt;--2             ------- REPORT.THIS.IS-THE.REPORT_NAME_5411 -------                                2/2111 USER:XSYS1414         &lt;/p&gt;\n\n&lt;p&gt;4/11/2023 2340  OP: A031                                                                                                                     X_RTXN_JNL&lt;/p&gt;\n\n&lt;p&gt;12A 1234567     G18         H152  0955         60                   4.5        1,000.16      5/11/2023  0.00   -9,999.16 &lt;/p&gt;\n\n&lt;p&gt;99   84124581 T800         T1000 0955          50                   2.5         500.16       5/11/2023  0.00   1,500.16 &lt;/p&gt;\n\n&lt;p&gt;99   645665485T800         T1000 0955         60                   4.5        1,000.16      5/12/2023  39.00-9,999.16 &lt;/p&gt;\n\n&lt;p&gt;--3             ------- REPORT.THIS.IS-THE.REPORT_NAME_5411 -------                                3/2111 USER:XSYS1414         &lt;/p&gt;\n\n&lt;p&gt;4/11/2023 2340  OP: A031                                                                                                                     X_RTXN_JNL        &lt;/p&gt;\n\n&lt;p&gt;12A 1234567     G18         H152  0955         60                   4.5        1,000.16      5/11/2023  0.00   -9,999.16 &lt;/p&gt;\n\n&lt;p&gt;99   84124581 T800         T1000 0955          50                   2.5         500.16       5/11/2023  0.00   1,500.16 &lt;/p&gt;\n\n&lt;p&gt;99   645665485T800         T1000 0955         60                   4.5        1,000.16      5/12/2023  39.00-9,999.16 &lt;/p&gt;\n\n&lt;p&gt;*&lt;/p&gt;\n\n&lt;p&gt;**&lt;/p&gt;\n\n&lt;p&gt;***                                           LOC#                       BALANCE             AVG BALANCE&lt;/p&gt;\n\n&lt;p&gt;****                                         12A                          $1,565,568.13      $487.684.08&lt;/p&gt;\n\n&lt;p&gt;*****                                         99                           $5,665,138.19     $268.586.12&lt;/p&gt;\n\n&lt;p&gt;******                                                                    =============    =============&lt;/p&gt;\n\n&lt;p&gt;*******                                                                    $99,874,698.15    $3,555,987.01&lt;/p&gt;\n\n&lt;p&gt;--4             ------- REPORT.THIS.IS-THE.REPORT_NAME_5411 -------                                3/2111 USER:XSYS1414         &lt;/p&gt;\n\n&lt;p&gt;4/11/2023 2340  OP: A031                                                                                                                     X_RTXN_JNL        &lt;/p&gt;\n\n&lt;p&gt;12A 1234567     G18         H152  0955         60                   4.5        1,000.16      5/11/2023  0.00   -9,999.16 &lt;/p&gt;\n\n&lt;p&gt;99   84124581 T800         T1000 0955          50                   2.5         500.16       5/11/2023  0.00   1,500.16 &lt;/p&gt;\n\n&lt;p&gt;99   645665485T800         T1000 0955         60                   4.5        1,000.16      5/12/2023  39.00-9,999.16&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12j53i2", "is_robot_indexable": true, "report_reasons": null, "author": "Mad_Finesse", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12j53i2/hi_all_question_about_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12j53i2/hi_all_question_about_etl/", "subreddit_subscribers": 98130, "created_utc": 1681262621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Data Lineage and Techniques to Implement it", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "name": "t3_12j14x0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VF6MCDNr9N1PWnqW5p6_9mYALWcVmjAFSNZkp40aBQA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681254216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/alvin-ai/what-is-data-lineage-and-techniques-to-implement-it-4f1939b11327", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-h1Hqjbj9IvWRBFPSqWrG-r7WNvMK2bjvWdJtTZTeeY.jpg?auto=webp&amp;v=enabled&amp;s=c6f1da1b8154f316a58090cc6c6c7f530585d921", "width": 1200, "height": 696}, "resolutions": [{"url": "https://external-preview.redd.it/-h1Hqjbj9IvWRBFPSqWrG-r7WNvMK2bjvWdJtTZTeeY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94e87a2fd0ca7f2fcbc92e947dcf67fa3e48135b", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/-h1Hqjbj9IvWRBFPSqWrG-r7WNvMK2bjvWdJtTZTeeY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e6eb179644db7d2e38e6d21faa63371a109f398", "width": 216, "height": 125}, {"url": "https://external-preview.redd.it/-h1Hqjbj9IvWRBFPSqWrG-r7WNvMK2bjvWdJtTZTeeY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e5af42b03665c9cfd65694ffe5570cd7f316392", "width": 320, "height": 185}, {"url": "https://external-preview.redd.it/-h1Hqjbj9IvWRBFPSqWrG-r7WNvMK2bjvWdJtTZTeeY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2a34c5e9eab1d91d9e651113f9e0c477bb865e2", "width": 640, "height": 371}, {"url": "https://external-preview.redd.it/-h1Hqjbj9IvWRBFPSqWrG-r7WNvMK2bjvWdJtTZTeeY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a1e5ac0d470a08abc7922c5c3118d20595522ff", "width": 960, "height": 556}, {"url": "https://external-preview.redd.it/-h1Hqjbj9IvWRBFPSqWrG-r7WNvMK2bjvWdJtTZTeeY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0f14ec14f4b4cc1015da7542d31312d5c228424", "width": 1080, "height": 626}], "variants": {}, "id": "VBeG5Dp67xreLwp4k-0bslAs1J16qklu6xm7xlmcq08"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12j14x0", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12j14x0/what_is_data_lineage_and_techniques_to_implement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/alvin-ai/what-is-data-lineage-and-techniques-to-implement-it-4f1939b11327", "subreddit_subscribers": 98130, "created_utc": 1681254216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is an article that I made while looking for a new job opportunity. I hope you like it and feel useful. \ud83d\ude42\n\nhttps://link.medium.com/DlqgDCJaVyb", "author_fullname": "t2_7dy3sswp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Technical Questions for Data Engineer Position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12iz03x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681249832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is an article that I made while looking for a new job opportunity. I hope you like it and feel useful. \ud83d\ude42&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://link.medium.com/DlqgDCJaVyb\"&gt;https://link.medium.com/DlqgDCJaVyb&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/stmCrhIOkvhAnChrOXgM5u_pFO4rIBPulHtME_M6PUY.jpg?auto=webp&amp;v=enabled&amp;s=82b52d66521725fb9753026764f6d42f33631a3b", "width": 654, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/stmCrhIOkvhAnChrOXgM5u_pFO4rIBPulHtME_M6PUY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ad4a26f4c64a03987d74b6b02175833d5116803", "width": 108, "height": 105}, {"url": "https://external-preview.redd.it/stmCrhIOkvhAnChrOXgM5u_pFO4rIBPulHtME_M6PUY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=185c19e79728ccd451688b0f73cff82bd9626d07", "width": 216, "height": 211}, {"url": "https://external-preview.redd.it/stmCrhIOkvhAnChrOXgM5u_pFO4rIBPulHtME_M6PUY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e102f85878f9878c62fcb4388720560e2c5acb2", "width": 320, "height": 313}, {"url": "https://external-preview.redd.it/stmCrhIOkvhAnChrOXgM5u_pFO4rIBPulHtME_M6PUY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9feca868b20dee598b0a0cca632b9a508f934403", "width": 640, "height": 626}], "variants": {}, "id": "gtOGOD6SzSH2tDhEzVVZ7S6sXOQHSGRHz6hspz_seeU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12iz03x", "is_robot_indexable": true, "report_reasons": null, "author": "Asleep-Organization7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12iz03x/technical_questions_for_data_engineer_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12iz03x/technical_questions_for_data_engineer_position/", "subreddit_subscribers": 98130, "created_utc": 1681249832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The problem: delete a small subset of keys from many large stage files\n\nCurrently the job is written using RDDs and it's processed in a sequential order. \n\nMy idea is to use a glob to read all stage files at the same time. Add input_file_name and perform a left anti join to delete the subset of keys. Obviously broadcasting the keys table. Then if the before and after counts are not the same then write to disk using the path from input_file_name. Each stage file could be around or bigger than a terabyte.\n\nMy concern is that my job will be doing the same work as before but I wanted to know if you guys think there is any performance advantage to processing the deletion of the keys all at once? \n\nOr if anyone has a better idea I'm all ears.", "author_fullname": "t2_1z6shw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on large batch delete", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12itnjn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681239313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The problem: delete a small subset of keys from many large stage files&lt;/p&gt;\n\n&lt;p&gt;Currently the job is written using RDDs and it&amp;#39;s processed in a sequential order. &lt;/p&gt;\n\n&lt;p&gt;My idea is to use a glob to read all stage files at the same time. Add input_file_name and perform a left anti join to delete the subset of keys. Obviously broadcasting the keys table. Then if the before and after counts are not the same then write to disk using the path from input_file_name. Each stage file could be around or bigger than a terabyte.&lt;/p&gt;\n\n&lt;p&gt;My concern is that my job will be doing the same work as before but I wanted to know if you guys think there is any performance advantage to processing the deletion of the keys all at once? &lt;/p&gt;\n\n&lt;p&gt;Or if anyone has a better idea I&amp;#39;m all ears.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12itnjn", "is_robot_indexable": true, "report_reasons": null, "author": "cockoala", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12itnjn/advice_on_large_batch_delete/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12itnjn/advice_on_large_batch_delete/", "subreddit_subscribers": 98130, "created_utc": 1681239313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for some experiences and tips to how to best network and play politics effectively to do the best work possible. I feel DE requires more networking than swe, but curious to hear other\u2019s opinions", "author_fullname": "t2_7jt0qboi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Politics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12isu7q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681237770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some experiences and tips to how to best network and play politics effectively to do the best work possible. I feel DE requires more networking than swe, but curious to hear other\u2019s opinions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12isu7q", "is_robot_indexable": true, "report_reasons": null, "author": "omscsdatathrow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12isu7q/data_engineering_politics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12isu7q/data_engineering_politics/", "subreddit_subscribers": 98130, "created_utc": 1681237770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What exactly is the difference between a linter (like sqlfluff) and a formatter (like sqlfmt)? Doesn\u2019t sqlfluff act like a formatter if you run \u201csqlfluff fix\u201d? Besides, are there better SQL formatters available? What are you working with?", "author_fullname": "t2_gzpboep7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL formatting (sqlfluff vs sqlfmt)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12irh9h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681235209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What exactly is the difference between a linter (like sqlfluff) and a formatter (like sqlfmt)? Doesn\u2019t sqlfluff act like a formatter if you run \u201csqlfluff fix\u201d? Besides, are there better SQL formatters available? What are you working with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12irh9h", "is_robot_indexable": true, "report_reasons": null, "author": "themouthoftruth", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12irh9h/sql_formatting_sqlfluff_vs_sqlfmt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12irh9h/sql_formatting_sqlfluff_vs_sqlfmt/", "subreddit_subscribers": 98130, "created_utc": 1681235209.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}