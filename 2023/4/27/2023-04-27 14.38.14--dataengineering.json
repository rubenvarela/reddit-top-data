{"kind": "Listing", "data": {"after": "t3_12zks5j", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My boss, who is universally liked in the company, resigned for reasons (I don\u2019t want to divulge too much\u2026 she made the right decision for herself, but she never should\u2019ve been put in that position). She was universally liked and added a ton of value to the company. Everyone on our team is upset right now and reeling. \n\nAnd on top of that, I\u2019m behind on work, feeling demotivated, and my polars code fucking broke. Thanks life.", "author_fullname": "t2_3qlqubb2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Crappy day", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zsn6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682533489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My boss, who is universally liked in the company, resigned for reasons (I don\u2019t want to divulge too much\u2026 she made the right decision for herself, but she never should\u2019ve been put in that position). She was universally liked and added a ton of value to the company. Everyone on our team is upset right now and reeling. &lt;/p&gt;\n\n&lt;p&gt;And on top of that, I\u2019m behind on work, feeling demotivated, and my polars code fucking broke. Thanks life.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12zsn6p", "is_robot_indexable": true, "report_reasons": null, "author": "EarthGoddessDude", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zsn6p/crappy_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zsn6p/crappy_day/", "subreddit_subscribers": 102841, "created_utc": 1682533489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m recently promoted to Data Engineer within my company through a series of reorganizations.   I have 15 years of experience in software development, but this role is new to me and the first Data Engineer in our company.\n\nSo my questions are fairly general, what is a data engineer typically responsible for?  We are moving to GCP, what tools should I be looking at? \n\nI\u2019ve looked into Dataflow and am intrigued by some of the capabilities, but there seem to be several similar tools in the GCP platform.\n\nI appreciate any feedback or resources to help me get moving!", "author_fullname": "t2_c4b8qxbi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1305mx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682558214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m recently promoted to Data Engineer within my company through a series of reorganizations.   I have 15 years of experience in software development, but this role is new to me and the first Data Engineer in our company.&lt;/p&gt;\n\n&lt;p&gt;So my questions are fairly general, what is a data engineer typically responsible for?  We are moving to GCP, what tools should I be looking at? &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve looked into Dataflow and am intrigued by some of the capabilities, but there seem to be several similar tools in the GCP platform.&lt;/p&gt;\n\n&lt;p&gt;I appreciate any feedback or resources to help me get moving!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1305mx8", "is_robot_indexable": true, "report_reasons": null, "author": "Last-Horror-8078", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1305mx8/where_to_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1305mx8/where_to_start/", "subreddit_subscribers": 102841, "created_utc": 1682558214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_12zvs99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "#46d160", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YodbhGxt3cgLPhGbrDxOCX0oXc7XUreKyYwUq3JBG8o.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682536637.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "uber.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.uber.com/blog/ubers-lakehouse-architecture/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?auto=webp&amp;v=enabled&amp;s=738d00df5b7206fc59d5996b519d112f33aaae95", "width": 2160, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd11b4c60ab00d914050db16fcadd300a3e8ae1d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6cfa71789557ef26d9e637177f204df09b1d5f54", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc256950d3b19f768f265d43420d4fccffbc18c3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57655c70deaf4a18f6c312b099a32a725d3ce153", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=431a5717dd6e3c334274cb67fc23083bdf5980f9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b03716501e5f6133ec4594511381a9fabb62663", "width": 1080, "height": 540}], "variants": {}, "id": "pK0l2qBH_m6UWDT556VbwOYL_vSXGcjcX7mIGAuwFO0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12zvs99", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/12zvs99/setting_ubers_transactional_data_lake_in_motion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.uber.com/blog/ubers-lakehouse-architecture/", "subreddit_subscribers": 102841, "created_utc": 1682536637.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, i started recently as a DE and wondered if the speed our ELT-Pipeline is slow?\n\nIts a pipeline from an IBM DB2 table to SQL Server table (once full, not per delta), which is used as a data warehouse. We pull the data via a SQL Command, put the data into a temporal table, then add 2 Columns (HashKey and LoadDate) and then insert it into a persisted table. The whole process took us 6 hours!\n\nRow count : 27 Million\n\nNumber of Columns of the table: 95\n\nCompression type: Column Store (just from temporal table to persisted table)\n\nData space: 2584 MB\n\nIndex space: 1561 MB\n\nETL Tool: Microsoft Integration Services\n\nTime from Source to temporal table: 6 hours\n\nTime from temporal table to persisted table: 39 min\n\nFull Process from Source to temporal table (SSIS data flow within a loop, 10.000 rows per batch):\n\n1. truncate temporal table\n2. connect to source via sql command source query (SELECT 95 attributes FROM source)\n3. do some TRIM operations and type conversions\n4. add a hash key column and loaddate column\n5. Insert the data into the temporal table\n\n&amp;#x200B;\n\nHow much time should this take and what could we do to make the process faster?", "author_fullname": "t2_jo31wrc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the Speed of our ELT Pipeline too slow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130ff1p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682597170.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682588080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i started recently as a DE and wondered if the speed our ELT-Pipeline is slow?&lt;/p&gt;\n\n&lt;p&gt;Its a pipeline from an IBM DB2 table to SQL Server table (once full, not per delta), which is used as a data warehouse. We pull the data via a SQL Command, put the data into a temporal table, then add 2 Columns (HashKey and LoadDate) and then insert it into a persisted table. The whole process took us 6 hours!&lt;/p&gt;\n\n&lt;p&gt;Row count : 27 Million&lt;/p&gt;\n\n&lt;p&gt;Number of Columns of the table: 95&lt;/p&gt;\n\n&lt;p&gt;Compression type: Column Store (just from temporal table to persisted table)&lt;/p&gt;\n\n&lt;p&gt;Data space: 2584 MB&lt;/p&gt;\n\n&lt;p&gt;Index space: 1561 MB&lt;/p&gt;\n\n&lt;p&gt;ETL Tool: Microsoft Integration Services&lt;/p&gt;\n\n&lt;p&gt;Time from Source to temporal table: 6 hours&lt;/p&gt;\n\n&lt;p&gt;Time from temporal table to persisted table: 39 min&lt;/p&gt;\n\n&lt;p&gt;Full Process from Source to temporal table (SSIS data flow within a loop, 10.000 rows per batch):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;truncate temporal table&lt;/li&gt;\n&lt;li&gt;connect to source via sql command source query (SELECT 95 attributes FROM source)&lt;/li&gt;\n&lt;li&gt;do some TRIM operations and type conversions&lt;/li&gt;\n&lt;li&gt;add a hash key column and loaddate column&lt;/li&gt;\n&lt;li&gt;Insert the data into the temporal table&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How much time should this take and what could we do to make the process faster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130ff1p", "is_robot_indexable": true, "report_reasons": null, "author": "Elegant_Good6212", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130ff1p/is_the_speed_of_our_elt_pipeline_too_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130ff1p/is_the_speed_of_our_elt_pipeline_too_slow/", "subreddit_subscribers": 102841, "created_utc": 1682588080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "*This is a re-post cause it seems the original one for some reason was entirely missing the content. Apologies for the inconvenience.*\n\n\\---\n\nHey folks,\n\na few months ago I started following the [Data Engineering zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp), and today I finally finished my capstone project.\n\nYou can find the repo here: [https://github.com/wtfzambo/subreddit-analytics](https://github.com/wtfzambo/subreddit-analytics)  \nAnd play with [the dashboard here](http://34.110.171.173/superset/dashboard/1/?preselect_filters=%7B%7D&amp;standalone=true&amp;native_filters_key=KFfFl8-EEzG9SrKryLvTeoCOnaq-WTXh-4v-ocGqmKqcqp_gDdRoYj-tG8p57MX6)\n\n\\---\n\nThe project can be summarized with the following question: \"What do people talk about in this subreddit?\".\n\nSo I pulled as much data as I could from here, using Pushshift API and PRAW,  performed some NLP shenanigans and eventually created a dashboard that  allows the user to find the most relevant submissions/posts for a given topic.\n\nThis was quite a challenge, considering I did it in parallel with my full time job, but that allowed to touch some tech that I only knew superficially, as well as brush up some knowledge from my data scientist past.\n\nI'm quite proud that the final result also makes some sense, considering that the NLP techniques I applied is basically TF-IDF plus some custom logic I made up on the spot.\n\nIf you want give me any feedback or suggestion, that's more than welcome. Cheers!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Subreddit analytics - Data engineering project. Feedback welcome! [re-post cuz original was broken]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zzkpk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682542965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;This is a re-post cause it seems the original one for some reason was entirely missing the content. Apologies for the inconvenience.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;a few months ago I started following the &lt;a href=\"https://github.com/DataTalksClub/data-engineering-zoomcamp\"&gt;Data Engineering zoomcamp&lt;/a&gt;, and today I finally finished my capstone project.&lt;/p&gt;\n\n&lt;p&gt;You can find the repo here: &lt;a href=\"https://github.com/wtfzambo/subreddit-analytics\"&gt;https://github.com/wtfzambo/subreddit-analytics&lt;/a&gt;&lt;br/&gt;\nAnd play with &lt;a href=\"http://34.110.171.173/superset/dashboard/1/?preselect_filters=%7B%7D&amp;amp;standalone=true&amp;amp;native_filters_key=KFfFl8-EEzG9SrKryLvTeoCOnaq-WTXh-4v-ocGqmKqcqp_gDdRoYj-tG8p57MX6\"&gt;the dashboard here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;The project can be summarized with the following question: &amp;quot;What do people talk about in this subreddit?&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;So I pulled as much data as I could from here, using Pushshift API and PRAW,  performed some NLP shenanigans and eventually created a dashboard that  allows the user to find the most relevant submissions/posts for a given topic.&lt;/p&gt;\n\n&lt;p&gt;This was quite a challenge, considering I did it in parallel with my full time job, but that allowed to touch some tech that I only knew superficially, as well as brush up some knowledge from my data scientist past.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite proud that the final result also makes some sense, considering that the NLP techniques I applied is basically TF-IDF plus some custom logic I made up on the spot.&lt;/p&gt;\n\n&lt;p&gt;If you want give me any feedback or suggestion, that&amp;#39;s more than welcome. Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?auto=webp&amp;v=enabled&amp;s=ce2ce0d14c1bc3d8a6b1dc756564b42cfc7b0c17", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf679fa8234f0535aab678d0b4c3f9c4769a5194", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26e01a1bac77f3ad510830f3f50ff0f856d0bfe7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64a2f1ba85e2bd0375504ace972dd0ea45c2e103", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58b565658fd357d04b5239a6b0f0a9669d7548ba", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4515f15f9352121af7756108fd214adc79c81e0c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef9770d46d339450c3920a652cf063bfe9796aa3", "width": 1080, "height": 540}], "variants": {}, "id": "idjUifa7pHCepB7-AFAM6OQvRx1z_Yb2aIOdvdTW7Kk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12zzkpk", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zzkpk/subreddit_analytics_data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zzkpk/subreddit_analytics_data_engineering_project/", "subreddit_subscribers": 102841, "created_utc": 1682542965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm in a company that uses pip.\nSince my senior colleague went away 1.5 years ago, my manager instead of hiring another person, he made me became an one man band by doing data engineer / analyst / PMO / SA with tight deadlines.\n\nWhen there's something more complex that he doesn't know, he doesn't want to use it or to make the client pay to use the right tools.\nHe insist that these things can be done in one day.\n\nDon't know what to say to him.", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to vent - manager that doesn't like complex things", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130he0d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682592163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in a company that uses pip.\nSince my senior colleague went away 1.5 years ago, my manager instead of hiring another person, he made me became an one man band by doing data engineer / analyst / PMO / SA with tight deadlines.&lt;/p&gt;\n\n&lt;p&gt;When there&amp;#39;s something more complex that he doesn&amp;#39;t know, he doesn&amp;#39;t want to use it or to make the client pay to use the right tools.\nHe insist that these things can be done in one day.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t know what to say to him.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130he0d", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130he0d/need_to_vent_manager_that_doesnt_like_complex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130he0d/need_to_vent_manager_that_doesnt_like_complex/", "subreddit_subscribers": 102841, "created_utc": 1682592163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is transitioning into medallion architecture making it the first learning for alot of us. \n\nFolks who have grinded their ass off with this architecture, how has your experience been?", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How has your experience working with the Databrick medallion architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_130lbzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682600232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is transitioning into medallion architecture making it the first learning for alot of us. &lt;/p&gt;\n\n&lt;p&gt;Folks who have grinded their ass off with this architecture, how has your experience been?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130lbzy", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130lbzy/how_has_your_experience_working_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130lbzy/how_has_your_experience_working_with_the/", "subreddit_subscribers": 102841, "created_utc": 1682600232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company has a well-structured data set which is about 10TB in size. Each file in this dataset represents a \"parameter\" which has two columns (x and y).\n\nManagement has requested me to research about making a data visualization software that can be accessed through the intranet by multiple departments within the company. \n\nSkillwise, I would rate myself about 6/10 in JS and Python. I do most of my work in C/C++. I am given about 3 months to do an initial draft of the software (the time frame was given based on my request as I have to do a lot of learning).\n\nI did some research and came up with Django for backend and React for front end with Chart.js for handling the bulk of data visualization. MySQL may be used for data storage. The VIZ is mostly simple line charts (max 50 parameters, each with 2000 data points), scatter plots and bar plots. Bonus points if user can interact in many ways with the charts.\n\nAm I on the right track? Can anyone give me some suggestions regarding the tech stack I am planning to use and suggest additional components for say, optimizing data retrieval?", "author_fullname": "t2_2teg11zt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestion regarding making a data visualization tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1307v04", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682564248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has a well-structured data set which is about 10TB in size. Each file in this dataset represents a &amp;quot;parameter&amp;quot; which has two columns (x and y).&lt;/p&gt;\n\n&lt;p&gt;Management has requested me to research about making a data visualization software that can be accessed through the intranet by multiple departments within the company. &lt;/p&gt;\n\n&lt;p&gt;Skillwise, I would rate myself about 6/10 in JS and Python. I do most of my work in C/C++. I am given about 3 months to do an initial draft of the software (the time frame was given based on my request as I have to do a lot of learning).&lt;/p&gt;\n\n&lt;p&gt;I did some research and came up with Django for backend and React for front end with Chart.js for handling the bulk of data visualization. MySQL may be used for data storage. The VIZ is mostly simple line charts (max 50 parameters, each with 2000 data points), scatter plots and bar plots. Bonus points if user can interact in many ways with the charts.&lt;/p&gt;\n\n&lt;p&gt;Am I on the right track? Can anyone give me some suggestions regarding the tech stack I am planning to use and suggest additional components for say, optimizing data retrieval?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1307v04", "is_robot_indexable": true, "report_reasons": null, "author": "ciado63", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1307v04/need_suggestion_regarding_making_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1307v04/need_suggestion_regarding_making_a_data/", "subreddit_subscribers": 102841, "created_utc": 1682564248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today at work, I was trying to reduce the size of a container image and I wanted to check what's in some conda environments, especially what packages are taking up all that sweet storage (looking at you Tensorflow). \n\nI thought, surely `conda` has a way to show me the packages and their sizes in general and not just when I install them? But it seems that it doesn't, so I hacked together something silly with the help of the *coreutils* and awk.\n\nSee below my `conda-size-report.sh`,\n\n    env=$1\n    \n    # Select the conda environment path matching the name passed as argument\n    path=$(conda env list | awk -v env=\"$env\" '$1==env {print $2}')\n    \n    if [[ -d $path ]]\n    then\n        # Do not follow symlinks, indicated by 'l' at the beginning of the line\n        pythons=$(ls -ld $path/lib/python* | grep -v ^l | awk '{print $NF}')\n        for p in $pythons\n        do\n    \techo $p | awk -F/ '{printf \"Python version: %s\\n\",$NF}'\n    \t# cd to python version and print sizes -&gt; sort by human-readable size -&gt; print size and pkg name and add total size\n    \t(cd $p &amp;&amp; du -ch --max-depth=1 $p/site-packages | sort -hk1 --unique | awk -F/ '{print $1, $NF}' | sed -e 's/site-packages/+Total+/')\n        done\n    else\n        echo \"No conda environment: $env\"\n    fi\n\nSo right now, with an environment called  `my_env` for example, I can simply do:\n\n    sh conda-size-report.sh my_env\n\nand a simple, two columns output like this:\n\n    Python version: python3.8\n    770K    gunicorn\n    .\n    .\n    34M     numpy\n    71M     pandas\n    535M    tensorflow\n    1203M   +Total+\n\nAnd that's it, you can enjoy your time again, one hacky script at a time.", "author_fullname": "t2_7gd9z37n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A tiny script to explore package sizes in conda environments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zyo0a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682540782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today at work, I was trying to reduce the size of a container image and I wanted to check what&amp;#39;s in some conda environments, especially what packages are taking up all that sweet storage (looking at you Tensorflow). &lt;/p&gt;\n\n&lt;p&gt;I thought, surely &lt;code&gt;conda&lt;/code&gt; has a way to show me the packages and their sizes in general and not just when I install them? But it seems that it doesn&amp;#39;t, so I hacked together something silly with the help of the &lt;em&gt;coreutils&lt;/em&gt; and awk.&lt;/p&gt;\n\n&lt;p&gt;See below my &lt;code&gt;conda-size-report.sh&lt;/code&gt;,&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;env=$1\n\n# Select the conda environment path matching the name passed as argument\npath=$(conda env list | awk -v env=&amp;quot;$env&amp;quot; &amp;#39;$1==env {print $2}&amp;#39;)\n\nif [[ -d $path ]]\nthen\n    # Do not follow symlinks, indicated by &amp;#39;l&amp;#39; at the beginning of the line\n    pythons=$(ls -ld $path/lib/python* | grep -v ^l | awk &amp;#39;{print $NF}&amp;#39;)\n    for p in $pythons\n    do\n    echo $p | awk -F/ &amp;#39;{printf &amp;quot;Python version: %s\\n&amp;quot;,$NF}&amp;#39;\n    # cd to python version and print sizes -&amp;gt; sort by human-readable size -&amp;gt; print size and pkg name and add total size\n    (cd $p &amp;amp;&amp;amp; du -ch --max-depth=1 $p/site-packages | sort -hk1 --unique | awk -F/ &amp;#39;{print $1, $NF}&amp;#39; | sed -e &amp;#39;s/site-packages/+Total+/&amp;#39;)\n    done\nelse\n    echo &amp;quot;No conda environment: $env&amp;quot;\nfi\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So right now, with an environment called  &lt;code&gt;my_env&lt;/code&gt; for example, I can simply do:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sh conda-size-report.sh my_env\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and a simple, two columns output like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Python version: python3.8\n770K    gunicorn\n.\n.\n34M     numpy\n71M     pandas\n535M    tensorflow\n1203M   +Total+\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And that&amp;#39;s it, you can enjoy your time again, one hacky script at a time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12zyo0a", "is_robot_indexable": true, "report_reasons": null, "author": "NoisyFrequency", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zyo0a/a_tiny_script_to_explore_package_sizes_in_conda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zyo0a/a_tiny_script_to_explore_package_sizes_in_conda/", "subreddit_subscribers": 102841, "created_utc": 1682540782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wish I could find some references or use-cases of architectures about data engineering for manufactoring IoT, these are my goals:\n\n1. Some sensors are 1Hz (LF), others run at 20Hz (HF)\n2. Some sensors need some ingestion pipelines (thinking about map/reduce/filter/apply functions on the data stream) or processing pipelines\n3. How to handle the stream of data/mini-batch/batches of data with queues or topics and similar tools\n4. How to handle different industrial protocols like MQTT, MODBUS, OPCUA etc.\n5. What  should be done edge/on-premise vs on the cloud\n\nI read about Apache Kafka, ActiveMQ, RabbitMQ, then about Apache Spark for the processing but I don't want to dig into those frameworks without knowing what are the right tools since I don't have that much time to dedicate to my education, sadly.\n\nAlso I wish I could understand the best way to think about where, in the pipelines, the smart features should be provided, e.g. close to the machine assets or close to the module that sends the data to the datalake, if I should think about hot processing (processing stream of data) vs cooler processing (processing mini-batches of data). Do I need to handle messages and stream in a pub/sub architecture or do I need to handle dataframes of data? Do I need read or write performances? (I read about avro and parquet recently). I'm a bit lost where to start.", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools about data engineering architecture for IoT (manufactoring): stream processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130bncu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682575469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wish I could find some references or use-cases of architectures about data engineering for manufactoring IoT, these are my goals:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Some sensors are 1Hz (LF), others run at 20Hz (HF)&lt;/li&gt;\n&lt;li&gt;Some sensors need some ingestion pipelines (thinking about map/reduce/filter/apply functions on the data stream) or processing pipelines&lt;/li&gt;\n&lt;li&gt;How to handle the stream of data/mini-batch/batches of data with queues or topics and similar tools&lt;/li&gt;\n&lt;li&gt;How to handle different industrial protocols like MQTT, MODBUS, OPCUA etc.&lt;/li&gt;\n&lt;li&gt;What  should be done edge/on-premise vs on the cloud&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I read about Apache Kafka, ActiveMQ, RabbitMQ, then about Apache Spark for the processing but I don&amp;#39;t want to dig into those frameworks without knowing what are the right tools since I don&amp;#39;t have that much time to dedicate to my education, sadly.&lt;/p&gt;\n\n&lt;p&gt;Also I wish I could understand the best way to think about where, in the pipelines, the smart features should be provided, e.g. close to the machine assets or close to the module that sends the data to the datalake, if I should think about hot processing (processing stream of data) vs cooler processing (processing mini-batches of data). Do I need to handle messages and stream in a pub/sub architecture or do I need to handle dataframes of data? Do I need read or write performances? (I read about avro and parquet recently). I&amp;#39;m a bit lost where to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130bncu", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130bncu/best_tools_about_data_engineering_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130bncu/best_tools_about_data_engineering_architecture/", "subreddit_subscribers": 102841, "created_utc": 1682575469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI recently had the recruiter call with Databricks for the Delivery Solution Architect role. I am looking for some details into the next round which is a meeting with the hiring manager. \n\nI would like to hear from anyone who has been through the process recently and what should I look out for in terms of preparing for the interview. Areas for technical questions or experience to highlight specifically for this role.\n\nThank you in advance for the help.", "author_fullname": "t2_ihu7yjvj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DSA role at Databricks: Interview prep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zupib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682535524.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I recently had the recruiter call with Databricks for the Delivery Solution Architect role. I am looking for some details into the next round which is a meeting with the hiring manager. &lt;/p&gt;\n\n&lt;p&gt;I would like to hear from anyone who has been through the process recently and what should I look out for in terms of preparing for the interview. Areas for technical questions or experience to highlight specifically for this role.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for the help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12zupib", "is_robot_indexable": true, "report_reasons": null, "author": "AM72128", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zupib/dsa_role_at_databricks_interview_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zupib/dsa_role_at_databricks_interview_prep/", "subreddit_subscribers": 102841, "created_utc": 1682535524.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company in the process of migrating to an AWS service that dumps JSON log data to S3 buckets.\n\nI need to get the data in these logs, transform and store on an on premise database to use in analytical processes.  At some point in the future this will be output to a cloud database.\n\nThe steer from our Technical Architects is that if we want access to these logs we need to do it via AWS Athena as they will not allow direct access to the S3 buckets that store the log data.\n\nWe are allowed to define the Athena tables ourselves so we could pull all the log contents.\n\nIn my reasearch it seems like AWS is primarily a UI for querying data in S3, similar to Hue, and not really designed to serve data to downstream processes.  It looks like it can be done, either by retrieving the query results that are stored in S3 or by setting up an API using other AWS services.\n\nI already have developed a small PySpark pipeline to process the JSON data and would prefer to access the data direct in the S3 buckets.\n\nTLDR: i need a pipepline to transform JSON data stored in S3 and stored in an on premise database.  What would be the best approach?  Is AWS Athena a good fit in this pipeline?", "author_fullname": "t2_h5e9i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automating retrieval of query results from AWS Athean", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130cuof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682579401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company in the process of migrating to an AWS service that dumps JSON log data to S3 buckets.&lt;/p&gt;\n\n&lt;p&gt;I need to get the data in these logs, transform and store on an on premise database to use in analytical processes.  At some point in the future this will be output to a cloud database.&lt;/p&gt;\n\n&lt;p&gt;The steer from our Technical Architects is that if we want access to these logs we need to do it via AWS Athena as they will not allow direct access to the S3 buckets that store the log data.&lt;/p&gt;\n\n&lt;p&gt;We are allowed to define the Athena tables ourselves so we could pull all the log contents.&lt;/p&gt;\n\n&lt;p&gt;In my reasearch it seems like AWS is primarily a UI for querying data in S3, similar to Hue, and not really designed to serve data to downstream processes.  It looks like it can be done, either by retrieving the query results that are stored in S3 or by setting up an API using other AWS services.&lt;/p&gt;\n\n&lt;p&gt;I already have developed a small PySpark pipeline to process the JSON data and would prefer to access the data direct in the S3 buckets.&lt;/p&gt;\n\n&lt;p&gt;TLDR: i need a pipepline to transform JSON data stored in S3 and stored in an on premise database.  What would be the best approach?  Is AWS Athena a good fit in this pipeline?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130cuof", "is_robot_indexable": true, "report_reasons": null, "author": "the_glover", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130cuof/automating_retrieval_of_query_results_from_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130cuof/automating_retrieval_of_query_results_from_aws/", "subreddit_subscribers": 102841, "created_utc": 1682579401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a crosspost from StackOverflow, where I did not receive a useful answer, hoping to find one in this community. ([https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb](https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb))\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI am trying to use DuckDB to show the user-created schema that I have  written into a Parquet file.  I can demonstrate in Python (using the  code example at [Get schema of parquet file in Python](https://stackoverflow.com/questions/41567081/get-schema-of-parquet-file-in-python)) that the schema is as I desire, but cannot seem to find a way in DuckDB to get this information.\n\nNeither of the following queries reports the user-created schema\n\nselect \\* from parquet\\_schema('FileWithMetadata.parquet')\n\nselect \\* from parquet\\_metadata('FileWithMetadata.parquet')\n\nupdate:\n\nHere is the code snippet that creates the metadata in the Parquet  file where a is a Pandas dataframe of daily basin flows for a number of  different simulations:\n\ntable = pa.Table.from\\_pandas(a) \n\nmy\\_schema = pa.schema(\\[pa.field(\"Flow\", \"float\", True, metadata={\"data\":\"flow in mm per day\"}),    \n\npa.field(\"DayIndex\", \"int64\", False, metadata={\"data\":\"index of days\"}),     pa.field(\"BasinIndex\", \"string\", True, metadata={\"data\":\"flow in mm per day\"}),     pa.field(\"Simulation\", \"int64\", True, metadata={\"data\":\"simulation number\"})     \\],      metadata={\"info\":\"long format basin flows\"})  t2 = table.cast(my\\_schema) \n\npq.write\\_table(t2, 'SHALongWithMetadata1.parquet')\n\nand the code to read it back is:\n\nimport pyarrow.parquet as pq \n\npfile = pq.read\\_table(\"SHALongWithMetadata1.parquet\") \n\nprint(\"Column names: {}\".format(pfile.column\\_names)) \n\nprint(\"Schema: {}\".format(pfile.schema))  \n\nand this yields as output :  \n\nColumn names: \\['Flow', 'DayIndex', 'BasinIndex', 'Simulation'\\] \n\nSchema: Flow: float   -- field metadata --   data: 'flow in mm per day' \n\nDayIndex: int64 not null   -- field metadata --   data: 'index of days' BasinIndex: string   -- field metadata --   data: 'flow in mm per day' \n\nSimulation: int64   -- field metadata --   data: 'simulation number' -- schema metadata -- info: 'long format basin flows' \n\n&amp;#x200B;\n\nThe suggested answer on StackOverflow was as follows, it does not work to report the user-defined metadata only the standard information about the columns\n\n DESCRIBE SELECT \\* FROM 'FileWithMetadata.parquet';  DESCRIBE TABLE 'FileWithMetadata.parquet'; \n\nTable function parquet\\_metadata  repeats the same information for each row group in the Parquet file, so use SELECT DISTINCT to report only one set of unique column names and types:\n\n    SELECT DISTINCT   path_in_schema,   type  FROM parquet_metadata('FileWithMetadata.parquet');", "author_fullname": "t2_2cs9jenu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to show user schema in Parquet with DuckDb", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1303z09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682553733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a crosspost from StackOverflow, where I did not receive a useful answer, hoping to find one in this community. (&lt;a href=\"https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb\"&gt;https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am trying to use DuckDB to show the user-created schema that I have  written into a Parquet file.  I can demonstrate in Python (using the  code example at &lt;a href=\"https://stackoverflow.com/questions/41567081/get-schema-of-parquet-file-in-python\"&gt;Get schema of parquet file in Python&lt;/a&gt;) that the schema is as I desire, but cannot seem to find a way in DuckDB to get this information.&lt;/p&gt;\n\n&lt;p&gt;Neither of the following queries reports the user-created schema&lt;/p&gt;\n\n&lt;p&gt;select * from parquet_schema(&amp;#39;FileWithMetadata.parquet&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;select * from parquet_metadata(&amp;#39;FileWithMetadata.parquet&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;update:&lt;/p&gt;\n\n&lt;p&gt;Here is the code snippet that creates the metadata in the Parquet  file where a is a Pandas dataframe of daily basin flows for a number of  different simulations:&lt;/p&gt;\n\n&lt;p&gt;table = pa.Table.from_pandas(a) &lt;/p&gt;\n\n&lt;p&gt;my_schema = pa.schema([pa.field(&amp;quot;Flow&amp;quot;, &amp;quot;float&amp;quot;, True, metadata={&amp;quot;data&amp;quot;:&amp;quot;flow in mm per day&amp;quot;}),    &lt;/p&gt;\n\n&lt;p&gt;pa.field(&amp;quot;DayIndex&amp;quot;, &amp;quot;int64&amp;quot;, False, metadata={&amp;quot;data&amp;quot;:&amp;quot;index of days&amp;quot;}),     pa.field(&amp;quot;BasinIndex&amp;quot;, &amp;quot;string&amp;quot;, True, metadata={&amp;quot;data&amp;quot;:&amp;quot;flow in mm per day&amp;quot;}),     pa.field(&amp;quot;Simulation&amp;quot;, &amp;quot;int64&amp;quot;, True, metadata={&amp;quot;data&amp;quot;:&amp;quot;simulation number&amp;quot;})     ],      metadata={&amp;quot;info&amp;quot;:&amp;quot;long format basin flows&amp;quot;})  t2 = table.cast(my_schema) &lt;/p&gt;\n\n&lt;p&gt;pq.write_table(t2, &amp;#39;SHALongWithMetadata1.parquet&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;and the code to read it back is:&lt;/p&gt;\n\n&lt;p&gt;import pyarrow.parquet as pq &lt;/p&gt;\n\n&lt;p&gt;pfile = pq.read_table(&amp;quot;SHALongWithMetadata1.parquet&amp;quot;) &lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;Column names: {}&amp;quot;.format(pfile.column_names)) &lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;Schema: {}&amp;quot;.format(pfile.schema))  &lt;/p&gt;\n\n&lt;p&gt;and this yields as output :  &lt;/p&gt;\n\n&lt;p&gt;Column names: [&amp;#39;Flow&amp;#39;, &amp;#39;DayIndex&amp;#39;, &amp;#39;BasinIndex&amp;#39;, &amp;#39;Simulation&amp;#39;] &lt;/p&gt;\n\n&lt;p&gt;Schema: Flow: float   -- field metadata --   data: &amp;#39;flow in mm per day&amp;#39; &lt;/p&gt;\n\n&lt;p&gt;DayIndex: int64 not null   -- field metadata --   data: &amp;#39;index of days&amp;#39; BasinIndex: string   -- field metadata --   data: &amp;#39;flow in mm per day&amp;#39; &lt;/p&gt;\n\n&lt;p&gt;Simulation: int64   -- field metadata --   data: &amp;#39;simulation number&amp;#39; -- schema metadata -- info: &amp;#39;long format basin flows&amp;#39; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The suggested answer on StackOverflow was as follows, it does not work to report the user-defined metadata only the standard information about the columns&lt;/p&gt;\n\n&lt;p&gt;DESCRIBE SELECT * FROM &amp;#39;FileWithMetadata.parquet&amp;#39;;  DESCRIBE TABLE &amp;#39;FileWithMetadata.parquet&amp;#39;; &lt;/p&gt;\n\n&lt;p&gt;Table function parquet_metadata  repeats the same information for each row group in the Parquet file, so use SELECT DISTINCT to report only one set of unique column names and types:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT DISTINCT   path_in_schema,   type  FROM parquet_metadata(&amp;#39;FileWithMetadata.parquet&amp;#39;);\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1303z09", "is_robot_indexable": true, "report_reasons": null, "author": "rmales", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1303z09/how_to_show_user_schema_in_parquet_with_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1303z09/how_to_show_user_schema_in_parquet_with_duckdb/", "subreddit_subscribers": 102841, "created_utc": 1682553733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI've got a fairly wide Dimension Table and I'd Like to utilize a slowly changing dimension 2 approach. My Idea was to use an MD5 Hash with all the columns. When the Data was updated, i would Check The md5 hashes for The Same primary keys and add a new Line when the md5 changed. The Problem is that there are very many columns, making the md5 function very big.\n\nHow would you approach The Problem to Check If one column value changed with The new Data?\n\nI am using redshift.", "author_fullname": "t2_as1vw8gf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Check for column Changes in scd 2 table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zwn5e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682537515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a fairly wide Dimension Table and I&amp;#39;d Like to utilize a slowly changing dimension 2 approach. My Idea was to use an MD5 Hash with all the columns. When the Data was updated, i would Check The md5 hashes for The Same primary keys and add a new Line when the md5 changed. The Problem is that there are very many columns, making the md5 function very big.&lt;/p&gt;\n\n&lt;p&gt;How would you approach The Problem to Check If one column value changed with The new Data?&lt;/p&gt;\n\n&lt;p&gt;I am using redshift.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zwn5e", "is_robot_indexable": true, "report_reasons": null, "author": "One_Indication_6921", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zwn5e/check_for_column_changes_in_scd_2_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zwn5e/check_for_column_changes_in_scd_2_table/", "subreddit_subscribers": 102841, "created_utc": 1682537515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team has been struggling for months with finding a good replication and CDC solution. We are trying to replicate data from an app database and a few third party databases to BigQuery (or blob storage then eventually BQ). When I joined we had just signed a license with Fivetran which has worked seamlessly for the third party DBs but is extremely costly and prone to failure when using it for our application database. \n\nDoes anyone have a good solution for replication and CDC from Postgres to GCP? Our team is currently evaluating Confluent + Kafka. Ideally we want a solution that can handle a wide variety of types, handle schema drift, and won\u2019t break the bank. \n\nAddl context:\nI am a senior data engineer at a ~300 person startup. DE is relatively new at my company having been around for a year. We use GCP, Postgres, Airflow, and are a heavy Python shop.", "author_fullname": "t2_36yd2hgi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your company handle replication and CDC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_130ls2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682601191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team has been struggling for months with finding a good replication and CDC solution. We are trying to replicate data from an app database and a few third party databases to BigQuery (or blob storage then eventually BQ). When I joined we had just signed a license with Fivetran which has worked seamlessly for the third party DBs but is extremely costly and prone to failure when using it for our application database. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a good solution for replication and CDC from Postgres to GCP? Our team is currently evaluating Confluent + Kafka. Ideally we want a solution that can handle a wide variety of types, handle schema drift, and won\u2019t break the bank. &lt;/p&gt;\n\n&lt;p&gt;Addl context:\nI am a senior data engineer at a ~300 person startup. DE is relatively new at my company having been around for a year. We use GCP, Postgres, Airflow, and are a heavy Python shop.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130ls2m", "is_robot_indexable": true, "report_reasons": null, "author": "PROTECTyaNECK44", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130ls2m/how_does_your_company_handle_replication_and_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130ls2m/how_does_your_company_handle_replication_and_cdc/", "subreddit_subscribers": 102841, "created_utc": 1682601191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In general which cloud vendor has best data engineering services?", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud vendor with best data engineering/data science tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_130link", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682600625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In general which cloud vendor has best data engineering services?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130link", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130link/cloud_vendor_with_best_data_engineeringdata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130link/cloud_vendor_with_best_data_engineeringdata/", "subreddit_subscribers": 102841, "created_utc": 1682600625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When you run a delete with no where clause", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_130l3vi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-3k2JSFFISmQdRGy93d4ImyzwCV2GkrJG7EI9JGyr7g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682599742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/xwirx9nwsgwa1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/xwirx9nwsgwa1.jpg?auto=webp&amp;v=enabled&amp;s=32bea8c813c0b458d74a9eb356a964ebe4d24fd3", "width": 1284, "height": 1637}, "resolutions": [{"url": "https://preview.redd.it/xwirx9nwsgwa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65964f00c0801d6263fb7cd76d72c951714dbd6b", "width": 108, "height": 137}, {"url": "https://preview.redd.it/xwirx9nwsgwa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed83dcfe1e114441f9412d426085a663a56425fb", "width": 216, "height": 275}, {"url": "https://preview.redd.it/xwirx9nwsgwa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d89bb669d0eb3b6aee2442a554f2fd01cd5b5de0", "width": 320, "height": 407}, {"url": "https://preview.redd.it/xwirx9nwsgwa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a830de7d9bf19c7f600c7e829377df8797229de", "width": 640, "height": 815}, {"url": "https://preview.redd.it/xwirx9nwsgwa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bada6464a5a6f572f3564d8c1e68d86c8cd5c9ea", "width": 960, "height": 1223}, {"url": "https://preview.redd.it/xwirx9nwsgwa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da85cfc257eed35412fedf8268cbeea1b1b6c951", "width": 1080, "height": 1376}], "variants": {}, "id": "WZxzpR_UcaZJ3izEkm76OFUIfRMWhqxh7-qCBaQN4g4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "130l3vi", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130l3vi/when_you_run_a_delete_with_no_where_clause/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/xwirx9nwsgwa1.jpg", "subreddit_subscribers": 102841, "created_utc": 1682599742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a startup and we need to deploy a presto cluster that can support at least 100QPS. The presto cluster will act as a query engine on top of Pinot. What are the things to consider for the presto cluster deployment on an EC2 instance. The reason for using presto is to have a complete MYSQL_ANSI compatibility which Pinot doesn\u2019t provide right now. Any help is appreciated.", "author_fullname": "t2_icq6ey6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with deploying Presto in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130h06j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682591465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a startup and we need to deploy a presto cluster that can support at least 100QPS. The presto cluster will act as a query engine on top of Pinot. What are the things to consider for the presto cluster deployment on an EC2 instance. The reason for using presto is to have a complete MYSQL_ANSI compatibility which Pinot doesn\u2019t provide right now. Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130h06j", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Wrongdoer-939", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130h06j/need_help_with_deploying_presto_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130h06j/need_help_with_deploying_presto_in_production/", "subreddit_subscribers": 102841, "created_utc": 1682591465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "seems like a killer combo. i went ahead and spun up a poc but things have gone horribly wrong somewhere. \n\n&amp;#x200B;\n\nwhat ive done\n\n\\- created a poc table using spark &amp; iceberg in s3 as parquet files\n\n\\- queried that table in athena\n\n\\- installed trino, setup a connection with iceberg configured to use glue as the backend catalog\n\n\\- the information schema queries work fine\n\n\\- the count(\\*) queries work fine\n\n\\- select queries return NULL for all columns :(\n\n\\- at first i thought this was a compression issue, changing the compression to NONE (correct) didnt change the column values\n\n&amp;#x200B;\n\nwhat should i be looking at? im stoked at the potential but bummed by my initial attempts.", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does anyone have experience with trino on top of an iceberg s3 parquet table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1302wh0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682550899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;seems like a killer combo. i went ahead and spun up a poc but things have gone horribly wrong somewhere. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what ive done&lt;/p&gt;\n\n&lt;p&gt;- created a poc table using spark &amp;amp; iceberg in s3 as parquet files&lt;/p&gt;\n\n&lt;p&gt;- queried that table in athena&lt;/p&gt;\n\n&lt;p&gt;- installed trino, setup a connection with iceberg configured to use glue as the backend catalog&lt;/p&gt;\n\n&lt;p&gt;- the information schema queries work fine&lt;/p&gt;\n\n&lt;p&gt;- the count(*) queries work fine&lt;/p&gt;\n\n&lt;p&gt;- select queries return NULL for all columns :(&lt;/p&gt;\n\n&lt;p&gt;- at first i thought this was a compression issue, changing the compression to NONE (correct) didnt change the column values&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what should i be looking at? im stoked at the potential but bummed by my initial attempts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1302wh0", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1302wh0/does_anyone_have_experience_with_trino_on_top_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1302wh0/does_anyone_have_experience_with_trino_on_top_of/", "subreddit_subscribers": 102841, "created_utc": 1682550899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lost at SQL - SQL learning game", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12zs30c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "#46d160", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/LRVs0c8DpGHiQz9idvgQA_f-3_48VzOXlOaYBLf49v4.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682532227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "lost-at-sql.therobinlord.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://lost-at-sql.therobinlord.com/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?auto=webp&amp;v=enabled&amp;s=78f23f10374dc092ae710b8193fc737c8bb7647b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a637b8771235f62dd2c30408bf79f32ae120e73b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87d87f99de66cd4a79c8acbcb11633ec11c3d5ac", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=915f8bd055dbcc9ec832c1acb8fa84587124b033", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4bba8f842f2f1335a673d5cbcdc0d0b8cd543e1", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88d363207cffaee4bfee8ab7fa2a07a0c76c0ccf", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aab3d50e24d200661cedb96dc905f11b15c2eabe", "width": 1080, "height": 567}], "variants": {}, "id": "_eh15lhwgjZCGtCXd2oKUjEJ_FRZQBEKMEp0X9IoD-g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12zs30c", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/12zs30c/lost_at_sql_sql_learning_game/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://lost-at-sql.therobinlord.com/", "subreddit_subscribers": 102841, "created_utc": 1682532227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am creating an API wrapper for an API which requires OAuth2 authorization code flow in python.  \nI have created the wrapper such that the user can login and then copy and paste the redirect url, where the authorization code is extracted and exchanged for access and refresh tokens.\n\nHowever, I would like this process to be automated as much as possible and run the ETL pipeline without any human interaction.  \nDoes anyone have experience working with OAuth2 in python when creating ETL pipelines?", "author_fullname": "t2_226yoed5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OAuth2 ETL pipeline in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zrycy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682531953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am creating an API wrapper for an API which requires OAuth2 authorization code flow in python.&lt;br/&gt;\nI have created the wrapper such that the user can login and then copy and paste the redirect url, where the authorization code is extracted and exchanged for access and refresh tokens.&lt;/p&gt;\n\n&lt;p&gt;However, I would like this process to be automated as much as possible and run the ETL pipeline without any human interaction.&lt;br/&gt;\nDoes anyone have experience working with OAuth2 in python when creating ETL pipelines?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zrycy", "is_robot_indexable": true, "report_reasons": null, "author": "C_Ronsholt", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zrycy/oauth2_etl_pipeline_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zrycy/oauth2_etl_pipeline_in_python/", "subreddit_subscribers": 102841, "created_utc": 1682531953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Please check out Blog on [How Apache Spark Works Internally](https://www.sparkcodehub.com/spark-how-it-works) . It is one of the most detailed blog about the Internal working of Apache Spark", "author_fullname": "t2_vgniwlxx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Apache Spark Works", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zrw9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.47, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682531818.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please check out Blog on &lt;a href=\"https://www.sparkcodehub.com/spark-how-it-works\"&gt;How Apache Spark Works Internally&lt;/a&gt; . It is one of the most detailed blog about the Internal working of Apache Spark&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12zrw9p", "is_robot_indexable": true, "report_reasons": null, "author": "Sparkcodehub", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zrw9p/how_apache_spark_works/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zrw9p/how_apache_spark_works/", "subreddit_subscribers": 102841, "created_utc": 1682531818.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,  \n\n\nWe have a classic azure flow and we are now scaling out our team from 5 to just over 25. We have a variety of data sources classic to business \\[salesforce, finance systems etc\\].  \n\n\nThe business has brutal expectations of the team and we want to communicate clearly around:  \n\n\ncapacity  \nexpectations\n\nwhile  also trying to resolve how we distribute work and scale out a scrum based team to a team of over 25.  \n\n\nSo, to summarise the questions:\n\n1. How do you communicate capacity of the team back to the senior stakeholders?\n2. How can we challenge expectations around timelines and unreasonable pace?\n3. How can we scale our ways of working so that we can distribute work better?", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Estimating capacity and scaling a team [Azure flow]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zrlef", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682531135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,  &lt;/p&gt;\n\n&lt;p&gt;We have a classic azure flow and we are now scaling out our team from 5 to just over 25. We have a variety of data sources classic to business [salesforce, finance systems etc].  &lt;/p&gt;\n\n&lt;p&gt;The business has brutal expectations of the team and we want to communicate clearly around:  &lt;/p&gt;\n\n&lt;p&gt;capacity&lt;br/&gt;\nexpectations&lt;/p&gt;\n\n&lt;p&gt;while  also trying to resolve how we distribute work and scale out a scrum based team to a team of over 25.  &lt;/p&gt;\n\n&lt;p&gt;So, to summarise the questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How do you communicate capacity of the team back to the senior stakeholders?&lt;/li&gt;\n&lt;li&gt;How can we challenge expectations around timelines and unreasonable pace?&lt;/li&gt;\n&lt;li&gt;How can we scale our ways of working so that we can distribute work better?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12zrlef", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zrlef/estimating_capacity_and_scaling_a_team_azure_flow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zrlef/estimating_capacity_and_scaling_a_team_azure_flow/", "subreddit_subscribers": 102841, "created_utc": 1682531135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm moving from Reporting to ETL within Data Factory and will be asking so silly beginner question for a while haha.\n\nAnyway I had an over of the ETL created by someone at work, but its not their main role, they were just showing it can be done with our data and clients. \n\nMy question is, is there somewhere like in SQL Server where I can see something like an Execution Plan? There is one step in the ETL that takes some time to complete and all it does is create x number of empty tables, which to me should be quick.\n\nI'm wondering if after each table that has been created it is reconnecting to the source to get the next table and thats whats making so long. \n\nOur system clients can configure their system at any point so we have to rebuild the data warehouse each night so the new columns are there the next day.\n\nThanks for any pointers.", "author_fullname": "t2_j52ax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Factory - Is there an Execution Plan or log", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zqjhv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682528841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m moving from Reporting to ETL within Data Factory and will be asking so silly beginner question for a while haha.&lt;/p&gt;\n\n&lt;p&gt;Anyway I had an over of the ETL created by someone at work, but its not their main role, they were just showing it can be done with our data and clients. &lt;/p&gt;\n\n&lt;p&gt;My question is, is there somewhere like in SQL Server where I can see something like an Execution Plan? There is one step in the ETL that takes some time to complete and all it does is create x number of empty tables, which to me should be quick.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if after each table that has been created it is reconnecting to the source to get the next table and thats whats making so long. &lt;/p&gt;\n\n&lt;p&gt;Our system clients can configure their system at any point so we have to rebuild the data warehouse each night so the new columns are there the next day.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any pointers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zqjhv", "is_robot_indexable": true, "report_reasons": null, "author": "lez_s", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zqjhv/data_factory_is_there_an_execution_plan_or_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zqjhv/data_factory_is_there_an_execution_plan_or_log/", "subreddit_subscribers": 102841, "created_utc": 1682528841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a personal project that I've shared a couple times before on other subreddits (working title is SoundMap). It's essentially a visual representation of the relationships between musical artists and the various projects they're in. This started as a small fun side thing with friends and grew into an obsession, like many of my other hobbies. However, it's grown to the point where the browser simply can't handle all the data and it's near impossible to work on.\n\n&amp;#x200B;\n\nI started this project in 2016-2017, in an online-based data visualization platform called Kumu - at the time it checked all the boxes for what I wanted to do and let me quickly add connections between elements, add basic color coordination, etc. As I mentioned before, this network of connections has grown to over 10,000 elements and has taken on somewhat of a... life of its own. I am trying to export the data out of here and turn it into something more user-friendly and responsive, but I am running into some roadblocks. Mainly the export options are pretty limited:\n\n1. I can export the entire project as a JSON file, but my knowledge and experience with these isn't great. This seems like the best way to go, but that's where I'm stuck.\n2. I can export the data into an Excel workbook, which includes two sheets: one listing elements, and one listing connections. I can work with this, but I just know by going this route I'm going to be putting in hours of extra work. \n\nAny thoughts or ideas?", "author_fullname": "t2_6a0rat1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone here good with Kumu/JSON files? Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zks5j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682522653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a personal project that I&amp;#39;ve shared a couple times before on other subreddits (working title is SoundMap). It&amp;#39;s essentially a visual representation of the relationships between musical artists and the various projects they&amp;#39;re in. This started as a small fun side thing with friends and grew into an obsession, like many of my other hobbies. However, it&amp;#39;s grown to the point where the browser simply can&amp;#39;t handle all the data and it&amp;#39;s near impossible to work on.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I started this project in 2016-2017, in an online-based data visualization platform called Kumu - at the time it checked all the boxes for what I wanted to do and let me quickly add connections between elements, add basic color coordination, etc. As I mentioned before, this network of connections has grown to over 10,000 elements and has taken on somewhat of a... life of its own. I am trying to export the data out of here and turn it into something more user-friendly and responsive, but I am running into some roadblocks. Mainly the export options are pretty limited:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I can export the entire project as a JSON file, but my knowledge and experience with these isn&amp;#39;t great. This seems like the best way to go, but that&amp;#39;s where I&amp;#39;m stuck.&lt;/li&gt;\n&lt;li&gt;I can export the data into an Excel workbook, which includes two sheets: one listing elements, and one listing connections. I can work with this, but I just know by going this route I&amp;#39;m going to be putting in hours of extra work. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any thoughts or ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zks5j", "is_robot_indexable": true, "report_reasons": null, "author": "st_nebula", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zks5j/anyone_here_good_with_kumujson_files_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zks5j/anyone_here_good_with_kumujson_files_help/", "subreddit_subscribers": 102841, "created_utc": 1682522653.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}