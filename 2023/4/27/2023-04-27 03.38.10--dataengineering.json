{"kind": "Listing", "data": {"after": "t3_12zdezw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9v8cn0yzu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: Learn Vendor Agnostic Technologies!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_12zhvtk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 666, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 666, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lqr4PfL8LjDhaihBKoDMu1ZaMztPOpeumQsrGReasNY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682516342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/bdjqa87ue8wa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/bdjqa87ue8wa1.png?auto=webp&amp;v=enabled&amp;s=ff5b8a5cef6602a33220fb057c9d62360a8021b4", "width": 978, "height": 1302}, "resolutions": [{"url": "https://preview.redd.it/bdjqa87ue8wa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2cd80b7e0905d4a76359afba109b7dd0d660cb3f", "width": 108, "height": 143}, {"url": "https://preview.redd.it/bdjqa87ue8wa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb945495b099eead8965755b5c0597297ba48675", "width": 216, "height": 287}, {"url": "https://preview.redd.it/bdjqa87ue8wa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c54dd00ee87da2cb99afe6c74d0ec2c7f1ff70af", "width": 320, "height": 426}, {"url": "https://preview.redd.it/bdjqa87ue8wa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bac0d1ba5f0f6b5f0e881aa3c2af6bb0c5883e8a", "width": 640, "height": 852}, {"url": "https://preview.redd.it/bdjqa87ue8wa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d02565ff969943cc6a17a7e46f5eef620fa89706", "width": 960, "height": 1278}], "variants": {}, "id": "Xiff3duE8muyQSlxKxBCka9qSyXJfKYwhy3LN1-LgDc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12zhvtk", "is_robot_indexable": true, "report_reasons": null, "author": "smashmaps", "discussion_type": null, "num_comments": 75, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zhvtk/psa_learn_vendor_agnostic_technologies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/bdjqa87ue8wa1.png", "subreddit_subscribers": 102770, "created_utc": 1682516342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My boss, who is universally liked in the company, resigned for reasons (I don\u2019t want to divulge too much\u2026 she made the right decision for herself, but she never should\u2019ve been put in that position). She was universally liked and added a ton of value to the company. Everyone on our team is upset right now and reeling. \n\nAnd on top of that, I\u2019m behind on work, feeling demotivated, and my polars code fucking broke. Thanks life.", "author_fullname": "t2_3qlqubb2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Crappy day", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zsn6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682533489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My boss, who is universally liked in the company, resigned for reasons (I don\u2019t want to divulge too much\u2026 she made the right decision for herself, but she never should\u2019ve been put in that position). She was universally liked and added a ton of value to the company. Everyone on our team is upset right now and reeling. &lt;/p&gt;\n\n&lt;p&gt;And on top of that, I\u2019m behind on work, feeling demotivated, and my polars code fucking broke. Thanks life.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12zsn6p", "is_robot_indexable": true, "report_reasons": null, "author": "EarthGoddessDude", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zsn6p/crappy_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zsn6p/crappy_day/", "subreddit_subscribers": 102770, "created_utc": 1682533489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI'm the sole 'data' person at a small company where my role spans a bucket load of titles, I know there are many in a similar position out there... As such any chance I can get to offload any work/mental capacity to a managed/streamlined platform I'm usually all for it.\n\nThe company has a bunch of legacy/niche industry software that require custom pipelines to be put together. Now I'm sure something like airflow would fill the shoes and there some, but I just can't justify the management nor the costs of a managed service for this.\n\nSurely there has to be something between, like scheduled lambdas with monitoring/alerting/validation rolled in?\n\nI've so far in my spare time cobbled together a series of scripts and a rudimentary frontend to streamline and achieve the above, but was hoping for something pre-existing I could drop in...\n\n\nReally to sum it up, my wishlist is basically bring code, aka serverless function style but have the monitoring ease of something like airflow or prefect.\n\nThanks for any comments or thoughts", "author_fullname": "t2_kjuqv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What exists on the spectrum between a cron job and airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zfj4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682510943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m the sole &amp;#39;data&amp;#39; person at a small company where my role spans a bucket load of titles, I know there are many in a similar position out there... As such any chance I can get to offload any work/mental capacity to a managed/streamlined platform I&amp;#39;m usually all for it.&lt;/p&gt;\n\n&lt;p&gt;The company has a bunch of legacy/niche industry software that require custom pipelines to be put together. Now I&amp;#39;m sure something like airflow would fill the shoes and there some, but I just can&amp;#39;t justify the management nor the costs of a managed service for this.&lt;/p&gt;\n\n&lt;p&gt;Surely there has to be something between, like scheduled lambdas with monitoring/alerting/validation rolled in?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve so far in my spare time cobbled together a series of scripts and a rudimentary frontend to streamline and achieve the above, but was hoping for something pre-existing I could drop in...&lt;/p&gt;\n\n&lt;p&gt;Really to sum it up, my wishlist is basically bring code, aka serverless function style but have the monitoring ease of something like airflow or prefect.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any comments or thoughts&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12zfj4u", "is_robot_indexable": true, "report_reasons": null, "author": "kmsherrin", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zfj4u/what_exists_on_the_spectrum_between_a_cron_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zfj4u/what_exists_on_the_spectrum_between_a_cron_job/", "subreddit_subscribers": 102770, "created_utc": 1682510943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have all seen the layoff announcements from various MDS vendors.  This week, I found out my main customer success contact at one vendor is no longer there.   I know vendors lurk here on reddit.  Can anybody share the inside scoop and what we should anticipate next?  What's the vibe?", "author_fullname": "t2_7yk2o6hxn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inside scoop from Data Engineering vendors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12z7jn7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682486823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have all seen the layoff announcements from various MDS vendors.  This week, I found out my main customer success contact at one vendor is no longer there.   I know vendors lurk here on reddit.  Can anybody share the inside scoop and what we should anticipate next?  What&amp;#39;s the vibe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12z7jn7", "is_robot_indexable": true, "report_reasons": null, "author": "grahamdietz", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12z7jn7/inside_scoop_from_data_engineering_vendors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12z7jn7/inside_scoop_from_data_engineering_vendors/", "subreddit_subscribers": 102770, "created_utc": 1682486823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The latest [Thoughtworks Tech Radar](https://www.thoughtworks.com/radar) is out. Some highlights in the data space:\n\n## \ud83d\udfe2 Adopt\n\n* [DVC](HTTPs://dvc.org)\n\n## \ud83d\udc4d\ud83c\udffb Trial\n\n* [Apache Hudi](https://hudi.apache.org/)\n* [DuckDB](https://duckdb.org/)\n* [Soda Core](https://www.soda.io/core)\n* [dbt-unit-testing](https://github.com/EqualExperts/dbt-unit-testing)\n* [Lakehouse Architecture](https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf)\n\n## \ud83d\udc40 Assess\n\n* [Neon](https://neon.tech/)\n* [dbt-expectations](https://github.com/calogica/dbt-expectations/tree/0.8.2/)\n\n## \ud83d\udfe0 Hold\n\n* [Denodo](https://www.denodo.com/en)\n\n---\n\nDid anything else catch your eye from the radar? Any thoughts on the assessments of the above tools? \n\nFor me, it was seeing Denodo listed. I've not used it, but it's fairly unusual for tools &amp; platforms to move to Hold (often the entries, or 'blips' as they're called, just don't get listed next time), so seems they had a pretty bad time with Denodo, even saying: \n\n&gt; we recommend that you do not use Denodo as a primary data transformation tool and use tools like Spark or SQL (with dbt) for your data transformations instead.", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data-eng related highlights from the latest Thoughtworks Tech Radar", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ze7uy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682507544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The latest &lt;a href=\"https://www.thoughtworks.com/radar\"&gt;Thoughtworks Tech Radar&lt;/a&gt; is out. Some highlights in the data space:&lt;/p&gt;\n\n&lt;h2&gt;\ud83d\udfe2 Adopt&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"HTTPs://dvc.org\"&gt;DVC&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;\ud83d\udc4d\ud83c\udffb Trial&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://hudi.apache.org/\"&gt;Apache Hudi&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://duckdb.org/\"&gt;DuckDB&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.soda.io/core\"&gt;Soda Core&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/EqualExperts/dbt-unit-testing\"&gt;dbt-unit-testing&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf\"&gt;Lakehouse Architecture&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;\ud83d\udc40 Assess&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://neon.tech/\"&gt;Neon&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/calogica/dbt-expectations/tree/0.8.2/\"&gt;dbt-expectations&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;\ud83d\udfe0 Hold&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.denodo.com/en\"&gt;Denodo&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Did anything else catch your eye from the radar? Any thoughts on the assessments of the above tools? &lt;/p&gt;\n\n&lt;p&gt;For me, it was seeing Denodo listed. I&amp;#39;ve not used it, but it&amp;#39;s fairly unusual for tools &amp;amp; platforms to move to Hold (often the entries, or &amp;#39;blips&amp;#39; as they&amp;#39;re called, just don&amp;#39;t get listed next time), so seems they had a pretty bad time with Denodo, even saying: &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;we recommend that you do not use Denodo as a primary data transformation tool and use tools like Spark or SQL (with dbt) for your data transformations instead.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/a4ecY2hwS5qxufqQSZnrTLtKQZRHDTcLN-CJfOWGyco.jpg?auto=webp&amp;v=enabled&amp;s=811182d29905a1e1e9403c451cf55bb6e8f6f368", "width": 1600, "height": 837}, "resolutions": [{"url": "https://external-preview.redd.it/a4ecY2hwS5qxufqQSZnrTLtKQZRHDTcLN-CJfOWGyco.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64ff67d5c54e2223029ebe80e5afbea47bedd2d7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/a4ecY2hwS5qxufqQSZnrTLtKQZRHDTcLN-CJfOWGyco.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f125b62c2d28fe65a11ab525349a719a7a9e0d06", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/a4ecY2hwS5qxufqQSZnrTLtKQZRHDTcLN-CJfOWGyco.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=492e504d582234f81d47fd498965c4311a0947ad", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/a4ecY2hwS5qxufqQSZnrTLtKQZRHDTcLN-CJfOWGyco.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54ad7c054ee7bdf6bb0dea7fad5a7821a18da98e", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/a4ecY2hwS5qxufqQSZnrTLtKQZRHDTcLN-CJfOWGyco.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1b9965c5698e7f4220c164dcfe0fa5ea058a274", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/a4ecY2hwS5qxufqQSZnrTLtKQZRHDTcLN-CJfOWGyco.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=689783f11d2d3dfda79b30353949a6c5d0c30306", "width": 1080, "height": 564}], "variants": {}, "id": "qKpvdmTr2prhwnfo9Lt4Bu8EUjs0RuFpA_BAZYOqGlQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ze7uy", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ze7uy/dataeng_related_highlights_from_the_latest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ze7uy/dataeng_related_highlights_from_the_latest/", "subreddit_subscribers": 102770, "created_utc": 1682507544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "*This is a re-post cause it seems the original one for some reason was entirely missing the content. Apologies for the inconvenience.*\n\n\\---\n\nHey folks,\n\na few months ago I started following the [Data Engineering zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp), and today I finally finished my capstone project.\n\nYou can find the repo here: [https://github.com/wtfzambo/subreddit-analytics](https://github.com/wtfzambo/subreddit-analytics)  \nAnd play with [the dashboard here](http://34.110.171.173/superset/dashboard/1/?preselect_filters=%7B%7D&amp;standalone=true&amp;native_filters_key=KFfFl8-EEzG9SrKryLvTeoCOnaq-WTXh-4v-ocGqmKqcqp_gDdRoYj-tG8p57MX6)\n\n\\---\n\nThe project can be summarized with the following question: \"What do people talk about in this subreddit?\".\n\nSo I pulled as much data as I could from here, using Pushshift API and PRAW,  performed some NLP shenanigans and eventually created a dashboard that  allows the user to find the most relevant submissions/posts for a given topic.\n\nThis was quite a challenge, considering I did it in parallel with my full time job, but that allowed to touch some tech that I only knew superficially, as well as brush up some knowledge from my data scientist past.\n\nI'm quite proud that the final result also makes some sense, considering that the NLP techniques I applied is basically TF-IDF plus some custom logic I made up on the spot.\n\nIf you want give me any feedback or suggestion, that's more than welcome. Cheers!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Subreddit analytics - Data engineering project. Feedback welcome! [re-post cuz original was broken]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zzkpk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682542965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;This is a re-post cause it seems the original one for some reason was entirely missing the content. Apologies for the inconvenience.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;a few months ago I started following the &lt;a href=\"https://github.com/DataTalksClub/data-engineering-zoomcamp\"&gt;Data Engineering zoomcamp&lt;/a&gt;, and today I finally finished my capstone project.&lt;/p&gt;\n\n&lt;p&gt;You can find the repo here: &lt;a href=\"https://github.com/wtfzambo/subreddit-analytics\"&gt;https://github.com/wtfzambo/subreddit-analytics&lt;/a&gt;&lt;br/&gt;\nAnd play with &lt;a href=\"http://34.110.171.173/superset/dashboard/1/?preselect_filters=%7B%7D&amp;amp;standalone=true&amp;amp;native_filters_key=KFfFl8-EEzG9SrKryLvTeoCOnaq-WTXh-4v-ocGqmKqcqp_gDdRoYj-tG8p57MX6\"&gt;the dashboard here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;The project can be summarized with the following question: &amp;quot;What do people talk about in this subreddit?&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;So I pulled as much data as I could from here, using Pushshift API and PRAW,  performed some NLP shenanigans and eventually created a dashboard that  allows the user to find the most relevant submissions/posts for a given topic.&lt;/p&gt;\n\n&lt;p&gt;This was quite a challenge, considering I did it in parallel with my full time job, but that allowed to touch some tech that I only knew superficially, as well as brush up some knowledge from my data scientist past.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite proud that the final result also makes some sense, considering that the NLP techniques I applied is basically TF-IDF plus some custom logic I made up on the spot.&lt;/p&gt;\n\n&lt;p&gt;If you want give me any feedback or suggestion, that&amp;#39;s more than welcome. Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?auto=webp&amp;v=enabled&amp;s=ce2ce0d14c1bc3d8a6b1dc756564b42cfc7b0c17", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf679fa8234f0535aab678d0b4c3f9c4769a5194", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26e01a1bac77f3ad510830f3f50ff0f856d0bfe7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64a2f1ba85e2bd0375504ace972dd0ea45c2e103", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58b565658fd357d04b5239a6b0f0a9669d7548ba", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4515f15f9352121af7756108fd214adc79c81e0c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4Q0Ol6t47BPjCA_k10A5o06Xo4xkfO3LsYyM6uGD1TE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef9770d46d339450c3920a652cf063bfe9796aa3", "width": 1080, "height": 540}], "variants": {}, "id": "idjUifa7pHCepB7-AFAM6OQvRx1z_Yb2aIOdvdTW7Kk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12zzkpk", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zzkpk/subreddit_analytics_data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zzkpk/subreddit_analytics_data_engineering_project/", "subreddit_subscribers": 102770, "created_utc": 1682542965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got offered to take a second job by a colleague and I'll be doing it on my own free time. However the job is more of a BI role (doing analysis and data visualization). I also have access to tons of learning materials in my current company and may take some certifications when I want. \n\nSo basically I am torn on what to do between the two on my free time. Looking for advice from wiser people here. Thank you.\n\nTake second job = more money\n\nPrioritize learning = more DE knowledge", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Take a second job or study more", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12z7u43", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682487727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got offered to take a second job by a colleague and I&amp;#39;ll be doing it on my own free time. However the job is more of a BI role (doing analysis and data visualization). I also have access to tons of learning materials in my current company and may take some certifications when I want. &lt;/p&gt;\n\n&lt;p&gt;So basically I am torn on what to do between the two on my free time. Looking for advice from wiser people here. Thank you.&lt;/p&gt;\n\n&lt;p&gt;Take second job = more money&lt;/p&gt;\n\n&lt;p&gt;Prioritize learning = more DE knowledge&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12z7u43", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12z7u43/take_a_second_job_or_study_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12z7u43/take_a_second_job_or_study_more/", "subreddit_subscribers": 102770, "created_utc": 1682487727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to share the Terraform tutorial **(Infrastructure As Code for Cloud)**, cheat sheet, and usage scenarios that I created as a notebook for myself. This repo covers Terraform with **(How-To) HANDS-On LABs and AWS SAMPLEs** (comprehensive, but simple):\n\n* Resources, Data Sources, Variables, Meta Arguments, Provisioners, Dynamic Blocks, Modules, Workspaces, Templates, Remote State.\n* Provisioning AWS Components (EC2, Lambda, ECS, EKS, API Gateway, ELB, CodePipeline, CodeBuild, etc.), use cases, and details. Possible usage scenarios are aimed to update over time.\n\n**Tutorial Link:** [**https://github.com/omerbsezer/Fast-Terraform**](https://github.com/omerbsezer/Fast-Terraform)\n\n**Extra Kubernetes-Tutorial Link:** [**https://github.com/omerbsezer/Fast-Kubernetes**](https://github.com/omerbsezer/Fast-Kubernetes)\n\n**Quick Look (How-To): Terraform Hands-on LABs**\n\nThese LABs focus on Terraform features, and help to learn Terraform:\n\n* [LAB-00: Installing Terraform, AWS Configuration with Terraform](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB00-Terraform-Install-AWS-Configuration.md)\n* [LAB-01: Terraform Docker =&gt; Pull Docker Image, Create Docker Container on Local Machine](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB01-Terraform-Docker-Without-Cloud.md)\n* [LAB-02: Resources =&gt; Provision Basic EC2 (Ubuntu 22.04)](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB02-Resources-Basic-EC2.md)\n* [LAB-03: Variables, Locals, Output =&gt; Provision EC2s](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB03-Variables-Locals-Output-EC2.md)\n* [LAB-04: Meta Arguments (Count, For\\_Each, Map) =&gt; Provision IAM Users, Groups, Policies, Attachment Policy-User](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB04-Meta-Arguments-IAM-User-Group-Policy.md)\n* [LAB-05: Dynamic Blocks =&gt; Provision Security Groups, EC2, VPC](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB05-Dynamic-Blocks-Security-Groups-EC2.md)\n* [LAB-06: Data Sources with Depends\\_on =&gt; Provision EC2](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB06-Data-Sources-EC2.md)\n* [LAB-07: Provisioners (file, remote-exec), Null Resources (local-exec) =&gt; Provision Key-Pair, SSH Connection](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB07-Provisioners-Null-Resources.md)\n* [LAB-08: Modules =&gt; Provision EC2](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB08-Modules-EC2.md)\n* [LAB-09: Workspaces =&gt; Provision EC2 with Different tfvars Files](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB09-Workspaces-EC2.md)\n* [LAB-10: Templates =&gt; Provision IAM User, User Access Key, Policy](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB10-Templates-User-Policy.md)\n* [LAB-11: Backend - Remote States =&gt; Provision EC2 and Save State File on S3](https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB11-Backend-Remote-State.md)\n* [Terraform Cheatsheet](https://github.com/omerbsezer/Fast-Terraform/blob/main/Terraform-Cheatsheet.md)\n\n**Quick Look (How-To): AWS Terraform Hands-on Samples**\n\nThese samples focus on how to create and use AWS components (EC2, EBS, EFS, IAM Roles, IAM Policies, Key-Pairs, VPC with Network Components, Lambda, ECR, ECS with Fargate, EKS with Managed Nodes, ASG, ELB, API Gateway, S3, CloudFront, CodeCommit, CodePipeline, CodeBuild, CodeDeploy) with Terraform:\n\n* [SAMPLE-01: Provisioning EC2s (Windows 2019 Server, Ubuntu 20.04) on VPC (Subnet), Creating Key-Pair, Connecting Ubuntu using SSH, and Connecting Windows Using RDP](https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE01-EC2-VPC-Ubuntu-Win-SSH-RDP.md)\n* [SAMPLE-02: Provisioning Lambda Function, API Gateway and Reaching HTML Page in Python Code From Browser](https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE02-Lambda-API-Gateway-Python.md)\n* [SAMPLE-03: EBS (Elastic Block Storage: HDD, SDD) and EFS (Elastic File System: NFS) Configuration with EC2s (Ubuntu and Windows Instances)](https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE03-EC2-EBS-EFS.md)\n* [SAMPLE-04: Provisioning ECR (Elastic Container Repository), Pushing Image to ECR, Provisioning ECS (Elastic Container Service), VPC (Virtual Private Cloud), ELB (Elastic Load Balancer), ECS Tasks and Service on Fargate Cluster](https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE04-ECR-ECS-ELB-VPC-ECS-Service.md)\n* [SAMPLE-05: Provisioning ECR, Lambda Function and API Gateway to run Flask App Container on Lambda](https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE05-Lambda-Container-ApiGateway-FlaskApp.md)\n* [SAMPLE-06: Provisioning EKS (Elastic Kubernetes Service) with Managed Nodes using Blueprint and Modules](https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE06-EKS-ManagedNodes-Blueprint.md)\n* [SAMPLE-07: CI/CD on AWS =&gt; Provisioning CodeCommit and CodePipeline, Triggering CodeBuild and CodeDeploy, Running on Lambda Container](https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE07-CodeCommit-Pipeline-Build-Deploy-Lambda.md)\n* [SAMPLE-08: Provisioning S3 and CloudFront to serve Static Web Site](https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE08-S3-CloudFront-Static-WebSite.md)\n\n**Table of Contents**\n\n* [Motivation](https://github.com/omerbsezer/Fast-Terraform#motivation)\n* [What is Terraform?](https://github.com/omerbsezer/Fast-Terraform#what_is_terraform)\n* [How Terraform Works?](https://github.com/omerbsezer/Fast-Terraform#how_terrafom_works)\n* [Terraform File Components](https://github.com/omerbsezer/Fast-Terraform#terrafom_file_components)\n   * [Providers](https://github.com/omerbsezer/Fast-Terraform#providers)\n   * [Resources](https://github.com/omerbsezer/Fast-Terraform#resources)\n   * [Variables (tfvar)](https://github.com/omerbsezer/Fast-Terraform#variables)\n   * [Values (Locals, Outputs)](https://github.com/omerbsezer/Fast-Terraform#values)\n   * [Meta Arguments](https://github.com/omerbsezer/Fast-Terraform#meta_arguments)\n   * [Dynamic Blocks](https://github.com/omerbsezer/Fast-Terraform#dynamic_blocks)\n   * [Data Sources](https://github.com/omerbsezer/Fast-Terraform#datasources)\n   * [Provisioners (file, remote\\_exec, local\\_exec), Null Resource](https://github.com/omerbsezer/Fast-Terraform#provisioners)\n   * [Modules](https://github.com/omerbsezer/Fast-Terraform#modules)\n   * [Workspaces](https://github.com/omerbsezer/Fast-Terraform#workspaces)\n   * [Templates](https://github.com/omerbsezer/Fast-Terraform#templates)\n   * [Backends and Remote States](https://github.com/omerbsezer/Fast-Terraform#backends_remote_states)\n* [Terraform Best Practices](https://github.com/omerbsezer/Fast-Terraform#best_practice)\n* [AWS Terraform Hands-on Samples](https://github.com/omerbsezer/Fast-Terraform#samples)\n   * [SAMPLE-01: EC2s (Windows 2019 Server, Ubuntu 20.04), VPC, Key-Pairs for SSH, RDP connections](https://github.com/omerbsezer/Fast-Terraform#ec2_vpc_key_pair_ssh_rdp)\n   * [SAMPLE-02: Provisioning Lambda Function, API Gateway and Reaching HTML Page in Python Code From Browsers](https://github.com/omerbsezer/Fast-Terraform#lambda_apigateway_python)\n   * [SAMPLE-03: EBS (Elastic Block Storage: HDD, SDD) and EFS (Elastic File System: NFS) Configuration with EC2s (Ubuntu and Windows Instances)](https://github.com/omerbsezer/Fast-Terraform#ebs_efs_ec2)\n   * [SAMPLE-04: Provisioning ECR (Elastic Container Repository), Pushing Image to ECR, Provisioning ECS (Elastic Container Service), VPC (Virtual Private Cloud), ELB (Elastic Load Balancer), ECS Tasks and Service on Fargate Cluster](https://github.com/omerbsezer/Fast-Terraform#ecr_ecs_elb_vpc_ecs_service_fargate)\n   * [SAMPLE-05: Provisioning ECR, Lambda Function and API Gateway to run Flask App Container on Lambda](https://github.com/omerbsezer/Fast-Terraform#ecr_lambda_apigateway_container)\n   * [SAMPLE-06: Provisioning EKS (Elastic Kubernetes Service) with Managed Nodes using Blueprint and Modules](https://github.com/omerbsezer/Fast-Terraform#eks_managednodes_blueprint)\n   * [SAMPLE-07: CI/CD on AWS =&gt; Provisioning CodeCommit and CodePipeline, Triggering CodeBuild and CodeDeploy, Running on Lambda Container](https://github.com/omerbsezer/Fast-Terraform#ci_cd)\n   * [SAMPLE-08: Provisioning S3 and CloudFront to serve Static Web Site](https://github.com/omerbsezer/Fast-Terraform#s3_cloudfront)\n* [Details](https://github.com/omerbsezer/Fast-Terraform#details)\n* [Terraform Cheatsheet](https://github.com/omerbsezer/Fast-Terraform#cheatsheet)\n* [Other Useful Resources Related Terraform](https://github.com/omerbsezer/Fast-Terraform#resource)\n* [References](https://github.com/omerbsezer/Fast-Terraform#references)", "author_fullname": "t2_muiope", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fast-Terraform: Terraform Tutorial, How-To: Hands-on LABs, and AWS Hands-on Sample Usage Scenarios (Infrastructure As Code)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zfwq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682511860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to share the Terraform tutorial &lt;strong&gt;(Infrastructure As Code for Cloud)&lt;/strong&gt;, cheat sheet, and usage scenarios that I created as a notebook for myself. This repo covers Terraform with &lt;strong&gt;(How-To) HANDS-On LABs and AWS SAMPLEs&lt;/strong&gt; (comprehensive, but simple):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Resources, Data Sources, Variables, Meta Arguments, Provisioners, Dynamic Blocks, Modules, Workspaces, Templates, Remote State.&lt;/li&gt;\n&lt;li&gt;Provisioning AWS Components (EC2, Lambda, ECS, EKS, API Gateway, ELB, CodePipeline, CodeBuild, etc.), use cases, and details. Possible usage scenarios are aimed to update over time.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Tutorial Link:&lt;/strong&gt; &lt;a href=\"https://github.com/omerbsezer/Fast-Terraform\"&gt;&lt;strong&gt;https://github.com/omerbsezer/Fast-Terraform&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Extra Kubernetes-Tutorial Link:&lt;/strong&gt; &lt;a href=\"https://github.com/omerbsezer/Fast-Kubernetes\"&gt;&lt;strong&gt;https://github.com/omerbsezer/Fast-Kubernetes&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quick Look (How-To): Terraform Hands-on LABs&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;These LABs focus on Terraform features, and help to learn Terraform:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB00-Terraform-Install-AWS-Configuration.md\"&gt;LAB-00: Installing Terraform, AWS Configuration with Terraform&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB01-Terraform-Docker-Without-Cloud.md\"&gt;LAB-01: Terraform Docker =&amp;gt; Pull Docker Image, Create Docker Container on Local Machine&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB02-Resources-Basic-EC2.md\"&gt;LAB-02: Resources =&amp;gt; Provision Basic EC2 (Ubuntu 22.04)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB03-Variables-Locals-Output-EC2.md\"&gt;LAB-03: Variables, Locals, Output =&amp;gt; Provision EC2s&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB04-Meta-Arguments-IAM-User-Group-Policy.md\"&gt;LAB-04: Meta Arguments (Count, For_Each, Map) =&amp;gt; Provision IAM Users, Groups, Policies, Attachment Policy-User&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB05-Dynamic-Blocks-Security-Groups-EC2.md\"&gt;LAB-05: Dynamic Blocks =&amp;gt; Provision Security Groups, EC2, VPC&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB06-Data-Sources-EC2.md\"&gt;LAB-06: Data Sources with Depends_on =&amp;gt; Provision EC2&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB07-Provisioners-Null-Resources.md\"&gt;LAB-07: Provisioners (file, remote-exec), Null Resources (local-exec) =&amp;gt; Provision Key-Pair, SSH Connection&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB08-Modules-EC2.md\"&gt;LAB-08: Modules =&amp;gt; Provision EC2&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB09-Workspaces-EC2.md\"&gt;LAB-09: Workspaces =&amp;gt; Provision EC2 with Different tfvars Files&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB10-Templates-User-Policy.md\"&gt;LAB-10: Templates =&amp;gt; Provision IAM User, User Access Key, Policy&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/LAB11-Backend-Remote-State.md\"&gt;LAB-11: Backend - Remote States =&amp;gt; Provision EC2 and Save State File on S3&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/Terraform-Cheatsheet.md\"&gt;Terraform Cheatsheet&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Quick Look (How-To): AWS Terraform Hands-on Samples&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;These samples focus on how to create and use AWS components (EC2, EBS, EFS, IAM Roles, IAM Policies, Key-Pairs, VPC with Network Components, Lambda, ECR, ECS with Fargate, EKS with Managed Nodes, ASG, ELB, API Gateway, S3, CloudFront, CodeCommit, CodePipeline, CodeBuild, CodeDeploy) with Terraform:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE01-EC2-VPC-Ubuntu-Win-SSH-RDP.md\"&gt;SAMPLE-01: Provisioning EC2s (Windows 2019 Server, Ubuntu 20.04) on VPC (Subnet), Creating Key-Pair, Connecting Ubuntu using SSH, and Connecting Windows Using RDP&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE02-Lambda-API-Gateway-Python.md\"&gt;SAMPLE-02: Provisioning Lambda Function, API Gateway and Reaching HTML Page in Python Code From Browser&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE03-EC2-EBS-EFS.md\"&gt;SAMPLE-03: EBS (Elastic Block Storage: HDD, SDD) and EFS (Elastic File System: NFS) Configuration with EC2s (Ubuntu and Windows Instances)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE04-ECR-ECS-ELB-VPC-ECS-Service.md\"&gt;SAMPLE-04: Provisioning ECR (Elastic Container Repository), Pushing Image to ECR, Provisioning ECS (Elastic Container Service), VPC (Virtual Private Cloud), ELB (Elastic Load Balancer), ECS Tasks and Service on Fargate Cluster&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE05-Lambda-Container-ApiGateway-FlaskApp.md\"&gt;SAMPLE-05: Provisioning ECR, Lambda Function and API Gateway to run Flask App Container on Lambda&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE06-EKS-ManagedNodes-Blueprint.md\"&gt;SAMPLE-06: Provisioning EKS (Elastic Kubernetes Service) with Managed Nodes using Blueprint and Modules&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE07-CodeCommit-Pipeline-Build-Deploy-Lambda.md\"&gt;SAMPLE-07: CI/CD on AWS =&amp;gt; Provisioning CodeCommit and CodePipeline, Triggering CodeBuild and CodeDeploy, Running on Lambda Container&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform/blob/main/SAMPLE08-S3-CloudFront-Static-WebSite.md\"&gt;SAMPLE-08: Provisioning S3 and CloudFront to serve Static Web Site&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#motivation\"&gt;Motivation&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#what_is_terraform\"&gt;What is Terraform?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#how_terrafom_works\"&gt;How Terraform Works?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#terrafom_file_components\"&gt;Terraform File Components&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#providers\"&gt;Providers&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#resources\"&gt;Resources&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#variables\"&gt;Variables (tfvar)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#values\"&gt;Values (Locals, Outputs)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#meta_arguments\"&gt;Meta Arguments&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#dynamic_blocks\"&gt;Dynamic Blocks&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#datasources\"&gt;Data Sources&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#provisioners\"&gt;Provisioners (file, remote_exec, local_exec), Null Resource&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#modules\"&gt;Modules&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#workspaces\"&gt;Workspaces&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#templates\"&gt;Templates&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#backends_remote_states\"&gt;Backends and Remote States&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#best_practice\"&gt;Terraform Best Practices&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#samples\"&gt;AWS Terraform Hands-on Samples&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#ec2_vpc_key_pair_ssh_rdp\"&gt;SAMPLE-01: EC2s (Windows 2019 Server, Ubuntu 20.04), VPC, Key-Pairs for SSH, RDP connections&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#lambda_apigateway_python\"&gt;SAMPLE-02: Provisioning Lambda Function, API Gateway and Reaching HTML Page in Python Code From Browsers&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#ebs_efs_ec2\"&gt;SAMPLE-03: EBS (Elastic Block Storage: HDD, SDD) and EFS (Elastic File System: NFS) Configuration with EC2s (Ubuntu and Windows Instances)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#ecr_ecs_elb_vpc_ecs_service_fargate\"&gt;SAMPLE-04: Provisioning ECR (Elastic Container Repository), Pushing Image to ECR, Provisioning ECS (Elastic Container Service), VPC (Virtual Private Cloud), ELB (Elastic Load Balancer), ECS Tasks and Service on Fargate Cluster&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#ecr_lambda_apigateway_container\"&gt;SAMPLE-05: Provisioning ECR, Lambda Function and API Gateway to run Flask App Container on Lambda&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#eks_managednodes_blueprint\"&gt;SAMPLE-06: Provisioning EKS (Elastic Kubernetes Service) with Managed Nodes using Blueprint and Modules&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#ci_cd\"&gt;SAMPLE-07: CI/CD on AWS =&amp;gt; Provisioning CodeCommit and CodePipeline, Triggering CodeBuild and CodeDeploy, Running on Lambda Container&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#s3_cloudfront\"&gt;SAMPLE-08: Provisioning S3 and CloudFront to serve Static Web Site&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#details\"&gt;Details&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#cheatsheet\"&gt;Terraform Cheatsheet&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#resource\"&gt;Other Useful Resources Related Terraform&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omerbsezer/Fast-Terraform#references\"&gt;References&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AOccRH0R74eV4U6EfVBZFJoiCZGuxwPgLgjDZQgUiyo.jpg?auto=webp&amp;v=enabled&amp;s=aff26802866c8cce610984e0b23824759b5e96d9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/AOccRH0R74eV4U6EfVBZFJoiCZGuxwPgLgjDZQgUiyo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=155564a5e52eb1b21bd0c1f37bada689f46741a1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/AOccRH0R74eV4U6EfVBZFJoiCZGuxwPgLgjDZQgUiyo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e786a876c9e7662b513d04d0ccf664e8f6da45d8", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/AOccRH0R74eV4U6EfVBZFJoiCZGuxwPgLgjDZQgUiyo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f4ddd263c3406b748c18985efa7fab7a9c4f154", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/AOccRH0R74eV4U6EfVBZFJoiCZGuxwPgLgjDZQgUiyo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=978ae3396e5dabfe24da9c5417a8c7809e727957", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/AOccRH0R74eV4U6EfVBZFJoiCZGuxwPgLgjDZQgUiyo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76ff4eaee9dee400851b777417f062428646a90c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/AOccRH0R74eV4U6EfVBZFJoiCZGuxwPgLgjDZQgUiyo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea5a448c788a5c53455f942fc541d7f026aba6d9", "width": 1080, "height": 540}], "variants": {}, "id": "_glqqYG7gBRy19Ov4gqUzroMtXxsOzvirCtVjW58RnI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "12zfwq0", "is_robot_indexable": true, "report_reasons": null, "author": "obsezer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zfwq0/fastterraform_terraform_tutorial_howto_handson/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zfwq0/fastterraform_terraform_tutorial_howto_handson/", "subreddit_subscribers": 102770, "created_utc": 1682511860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_12zvs99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "#46d160", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YodbhGxt3cgLPhGbrDxOCX0oXc7XUreKyYwUq3JBG8o.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682536637.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "uber.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.uber.com/blog/ubers-lakehouse-architecture/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?auto=webp&amp;v=enabled&amp;s=738d00df5b7206fc59d5996b519d112f33aaae95", "width": 2160, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd11b4c60ab00d914050db16fcadd300a3e8ae1d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6cfa71789557ef26d9e637177f204df09b1d5f54", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc256950d3b19f768f265d43420d4fccffbc18c3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57655c70deaf4a18f6c312b099a32a725d3ce153", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=431a5717dd6e3c334274cb67fc23083bdf5980f9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/gPIJ3s3kPjuuP1vFapZ6_-iLxIwWPPUMJonSrxJDTAI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b03716501e5f6133ec4594511381a9fabb62663", "width": 1080, "height": 540}], "variants": {}, "id": "pK0l2qBH_m6UWDT556VbwOYL_vSXGcjcX7mIGAuwFO0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12zvs99", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/12zvs99/setting_ubers_transactional_data_lake_in_motion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.uber.com/blog/ubers-lakehouse-architecture/", "subreddit_subscribers": 102770, "created_utc": 1682536637.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a company that has used sql server as the majority. We have made the move to cloud in the recent years and also have onboarded a data library with Starburst which is backed by Trino. I recently learned about DBT and thought it was an amazing tool to allow for documentation and more clarity on where our data is coming from with the built in DAG. However, every time i introduced it to an analyst or engineer they are all clutching onto their tsql and stored procedures. I know change is difficult but has anyone else made this transition at their company and how did you win them over?", "author_fullname": "t2_eww42", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to convince analysts/de to use DBT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zh3pv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682514641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a company that has used sql server as the majority. We have made the move to cloud in the recent years and also have onboarded a data library with Starburst which is backed by Trino. I recently learned about DBT and thought it was an amazing tool to allow for documentation and more clarity on where our data is coming from with the built in DAG. However, every time i introduced it to an analyst or engineer they are all clutching onto their tsql and stored procedures. I know change is difficult but has anyone else made this transition at their company and how did you win them over?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zh3pv", "is_robot_indexable": true, "report_reasons": null, "author": "Annaphasia", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zh3pv/how_to_convince_analystsde_to_use_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zh3pv/how_to_convince_analystsde_to_use_dbt/", "subreddit_subscribers": 102770, "created_utc": 1682514641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today at work, I was trying to reduce the size of a container image and I wanted to check what's in some conda environments, especially what packages are taking up all that sweet storage (looking at you Tensorflow). \n\nI thought, surely `conda` has a way to show me the packages and their sizes in general and not just when I install them? But it seems that it doesn't, so I hacked together something silly with the help of the *coreutils* and awk.\n\nSee below my `conda-size-report.sh`,\n\n    env=$1\n    \n    # Select the conda environment path matching the name passed as argument\n    path=$(conda env list | awk -v env=\"$env\" '$1==env {print $2}')\n    \n    if [[ -d $path ]]\n    then\n        # Do not follow symlinks, indicated by 'l' at the beginning of the line\n        pythons=$(ls -ld $path/lib/python* | grep -v ^l | awk '{print $NF}')\n        for p in $pythons\n        do\n    \techo $p | awk -F/ '{printf \"Python version: %s\\n\",$NF}'\n    \t# cd to python version and print sizes -&gt; sort by human-readable size -&gt; print size and pkg name and add total size\n    \t(cd $p &amp;&amp; du -ch --max-depth=1 $p/site-packages | sort -hk1 --unique | awk -F/ '{print $1, $NF}' | sed -e 's/site-packages/+Total+/')\n        done\n    else\n        echo \"No conda environment: $env\"\n    fi\n\nSo right now, with an environment called  `my_env` for example, I can simply do:\n\n    sh conda-size-report.sh my_env\n\nand a simple, two columns output like this:\n\n    Python version: python3.8\n    770K    gunicorn\n    .\n    .\n    34M     numpy\n    71M     pandas\n    535M    tensorflow\n    1203M   +Total+\n\nAnd that's it, you can enjoy your time again, one hacky script at a time.", "author_fullname": "t2_7gd9z37n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A tiny script to explore package sizes in conda environments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zyo0a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682540782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today at work, I was trying to reduce the size of a container image and I wanted to check what&amp;#39;s in some conda environments, especially what packages are taking up all that sweet storage (looking at you Tensorflow). &lt;/p&gt;\n\n&lt;p&gt;I thought, surely &lt;code&gt;conda&lt;/code&gt; has a way to show me the packages and their sizes in general and not just when I install them? But it seems that it doesn&amp;#39;t, so I hacked together something silly with the help of the &lt;em&gt;coreutils&lt;/em&gt; and awk.&lt;/p&gt;\n\n&lt;p&gt;See below my &lt;code&gt;conda-size-report.sh&lt;/code&gt;,&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;env=$1\n\n# Select the conda environment path matching the name passed as argument\npath=$(conda env list | awk -v env=&amp;quot;$env&amp;quot; &amp;#39;$1==env {print $2}&amp;#39;)\n\nif [[ -d $path ]]\nthen\n    # Do not follow symlinks, indicated by &amp;#39;l&amp;#39; at the beginning of the line\n    pythons=$(ls -ld $path/lib/python* | grep -v ^l | awk &amp;#39;{print $NF}&amp;#39;)\n    for p in $pythons\n    do\n    echo $p | awk -F/ &amp;#39;{printf &amp;quot;Python version: %s\\n&amp;quot;,$NF}&amp;#39;\n    # cd to python version and print sizes -&amp;gt; sort by human-readable size -&amp;gt; print size and pkg name and add total size\n    (cd $p &amp;amp;&amp;amp; du -ch --max-depth=1 $p/site-packages | sort -hk1 --unique | awk -F/ &amp;#39;{print $1, $NF}&amp;#39; | sed -e &amp;#39;s/site-packages/+Total+/&amp;#39;)\n    done\nelse\n    echo &amp;quot;No conda environment: $env&amp;quot;\nfi\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So right now, with an environment called  &lt;code&gt;my_env&lt;/code&gt; for example, I can simply do:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sh conda-size-report.sh my_env\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and a simple, two columns output like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Python version: python3.8\n770K    gunicorn\n.\n.\n34M     numpy\n71M     pandas\n535M    tensorflow\n1203M   +Total+\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And that&amp;#39;s it, you can enjoy your time again, one hacky script at a time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12zyo0a", "is_robot_indexable": true, "report_reasons": null, "author": "NoisyFrequency", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zyo0a/a_tiny_script_to_explore_package_sizes_in_conda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zyo0a/a_tiny_script_to_explore_package_sizes_in_conda/", "subreddit_subscribers": 102770, "created_utc": 1682540782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI don\u2019t *think* this is possible, and have asked on the dbt Slack but got no response so wanted to check here.\n\ndbt can connect to Databricks and can connect to DuckDB.\n\nDuckDB can connect to DeltaLake (via Arrow).\n\nIs it possible to leverage dbt for my transformation pipelines with my data stored in S3 using delta-rs in Python, with DuckDB as my compute engine?\n\nIf anyone has done it, is there a guide I can follow, or some basic pointers to get up and running?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt with Delta Lake &amp; DuckDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zf0bu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682509635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t &lt;em&gt;think&lt;/em&gt; this is possible, and have asked on the dbt Slack but got no response so wanted to check here.&lt;/p&gt;\n\n&lt;p&gt;dbt can connect to Databricks and can connect to DuckDB.&lt;/p&gt;\n\n&lt;p&gt;DuckDB can connect to DeltaLake (via Arrow).&lt;/p&gt;\n\n&lt;p&gt;Is it possible to leverage dbt for my transformation pipelines with my data stored in S3 using delta-rs in Python, with DuckDB as my compute engine?&lt;/p&gt;\n\n&lt;p&gt;If anyone has done it, is there a guide I can follow, or some basic pointers to get up and running?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zf0bu", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zf0bu/dbt_with_delta_lake_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zf0bu/dbt_with_delta_lake_duckdb/", "subreddit_subscribers": 102770, "created_utc": 1682509635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI've got a fairly wide Dimension Table and I'd Like to utilize a slowly changing dimension 2 approach. My Idea was to use an MD5 Hash with all the columns. When the Data was updated, i would Check The md5 hashes for The Same primary keys and add a new Line when the md5 changed. The Problem is that there are very many columns, making the md5 function very big.\n\nHow would you approach The Problem to Check If one column value changed with The new Data?\n\nI am using redshift.", "author_fullname": "t2_as1vw8gf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Check for column Changes in scd 2 table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zwn5e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682537515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a fairly wide Dimension Table and I&amp;#39;d Like to utilize a slowly changing dimension 2 approach. My Idea was to use an MD5 Hash with all the columns. When the Data was updated, i would Check The md5 hashes for The Same primary keys and add a new Line when the md5 changed. The Problem is that there are very many columns, making the md5 function very big.&lt;/p&gt;\n\n&lt;p&gt;How would you approach The Problem to Check If one column value changed with The new Data?&lt;/p&gt;\n\n&lt;p&gt;I am using redshift.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zwn5e", "is_robot_indexable": true, "report_reasons": null, "author": "One_Indication_6921", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zwn5e/check_for_column_changes_in_scd_2_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zwn5e/check_for_column_changes_in_scd_2_table/", "subreddit_subscribers": 102770, "created_utc": 1682537515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI recently had the recruiter call with Databricks for the Delivery Solution Architect role. I am looking for some details into the next round which is a meeting with the hiring manager. \n\nI would like to hear from anyone who has been through the process recently and what should I look out for in terms of preparing for the interview. Areas for technical questions or experience to highlight specifically for this role.\n\nThank you in advance for the help.", "author_fullname": "t2_ihu7yjvj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DSA role at Databricks: Interview prep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zupib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682535524.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I recently had the recruiter call with Databricks for the Delivery Solution Architect role. I am looking for some details into the next round which is a meeting with the hiring manager. &lt;/p&gt;\n\n&lt;p&gt;I would like to hear from anyone who has been through the process recently and what should I look out for in terms of preparing for the interview. Areas for technical questions or experience to highlight specifically for this role.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for the help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12zupib", "is_robot_indexable": true, "report_reasons": null, "author": "AM72128", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zupib/dsa_role_at_databricks_interview_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zupib/dsa_role_at_databricks_interview_prep/", "subreddit_subscribers": 102770, "created_utc": 1682535524.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lost at SQL - SQL learning game", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12zs30c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "#46d160", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/LRVs0c8DpGHiQz9idvgQA_f-3_48VzOXlOaYBLf49v4.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682532227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "lost-at-sql.therobinlord.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://lost-at-sql.therobinlord.com/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?auto=webp&amp;v=enabled&amp;s=78f23f10374dc092ae710b8193fc737c8bb7647b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a637b8771235f62dd2c30408bf79f32ae120e73b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87d87f99de66cd4a79c8acbcb11633ec11c3d5ac", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=915f8bd055dbcc9ec832c1acb8fa84587124b033", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4bba8f842f2f1335a673d5cbcdc0d0b8cd543e1", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88d363207cffaee4bfee8ab7fa2a07a0c76c0ccf", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/m6dUja2ttubl7eeSd7vsv1CtJrDXxbvMuVhtF_30GCQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aab3d50e24d200661cedb96dc905f11b15c2eabe", "width": 1080, "height": 567}], "variants": {}, "id": "_eh15lhwgjZCGtCXd2oKUjEJ_FRZQBEKMEp0X9IoD-g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12zs30c", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/12zs30c/lost_at_sql_sql_learning_game/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://lost-at-sql.therobinlord.com/", "subreddit_subscribers": 102770, "created_utc": 1682532227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DEs of Reddit,\n\nI\u2019m about to start a new project which requires me to download daily a large file from a public website (~30Gb) and land this file into AWS S3. I\u2019m trying to brainstorm the most efficient solution and I was wondering if anybody had an experience downloading large files like this? \n\nAnything in particular I should be aware of/consider ? \n\nThank you!", "author_fullname": "t2_a3m6qw38", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading large files from public websites - Brainstorm", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ze0m5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682507000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DEs of Reddit,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m about to start a new project which requires me to download daily a large file from a public website (~30Gb) and land this file into AWS S3. I\u2019m trying to brainstorm the most efficient solution and I was wondering if anybody had an experience downloading large files like this? &lt;/p&gt;\n\n&lt;p&gt;Anything in particular I should be aware of/consider ? &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ze0m5", "is_robot_indexable": true, "report_reasons": null, "author": "sk808mafia", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ze0m5/downloading_large_files_from_public_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ze0m5/downloading_large_files_from_public_websites/", "subreddit_subscribers": 102770, "created_utc": 1682507000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company has a well-structured data set which is about 10TB in size. Each file in this dataset represents a \"parameter\" which has two columns (x and y).\n\nManagement has requested me to research about making a data visualization software that can be accessed through the intranet by multiple departments within the company. \n\nSkillwise, I would rate myself about 6/10 in JS and Python. I do most of my work in C/C++. I am given about 3 months to do an initial draft of the software (the time frame was given based on my request as I have to do a lot of learning).\n\nI did some research and came up with Django for backend and React for front end with Chart.js for handling the bulk of data visualization. MySQL may be used for data storage. The VIZ is mostly simple line charts (max 50 parameters, each with 2000 data points), scatter plots and bar plots. Bonus points if user can interact in many ways with the charts.\n\nAm I on the right track? Can anyone give me some suggestions regarding the tech stack I am planning to use and suggest additional components for say, optimizing data retrieval?", "author_fullname": "t2_2teg11zt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestion regarding making a data visualization tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1307v04", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682564248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has a well-structured data set which is about 10TB in size. Each file in this dataset represents a &amp;quot;parameter&amp;quot; which has two columns (x and y).&lt;/p&gt;\n\n&lt;p&gt;Management has requested me to research about making a data visualization software that can be accessed through the intranet by multiple departments within the company. &lt;/p&gt;\n\n&lt;p&gt;Skillwise, I would rate myself about 6/10 in JS and Python. I do most of my work in C/C++. I am given about 3 months to do an initial draft of the software (the time frame was given based on my request as I have to do a lot of learning).&lt;/p&gt;\n\n&lt;p&gt;I did some research and came up with Django for backend and React for front end with Chart.js for handling the bulk of data visualization. MySQL may be used for data storage. The VIZ is mostly simple line charts (max 50 parameters, each with 2000 data points), scatter plots and bar plots. Bonus points if user can interact in many ways with the charts.&lt;/p&gt;\n\n&lt;p&gt;Am I on the right track? Can anyone give me some suggestions regarding the tech stack I am planning to use and suggest additional components for say, optimizing data retrieval?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1307v04", "is_robot_indexable": true, "report_reasons": null, "author": "ciado63", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1307v04/need_suggestion_regarding_making_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1307v04/need_suggestion_regarding_making_a_data/", "subreddit_subscribers": 102770, "created_utc": 1682564248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m recently promoted to Data Engineer within my company through a series of reorganizations.   I have 15 years of experience in software development, but this role is new to me and the first Data Engineer in our company.\n\nSo my questions are fairly general, what is a data engineer typically responsible for?  We are moving to GCP, what tools should I be looking at? \n\nI\u2019ve looked into Dataflow and am intrigued by some of the capabilities, but there seem to be several similar tools in the GCP platform.\n\nI appreciate any feedback or resources to help me get moving!", "author_fullname": "t2_c4b8qxbi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1305mx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682558214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m recently promoted to Data Engineer within my company through a series of reorganizations.   I have 15 years of experience in software development, but this role is new to me and the first Data Engineer in our company.&lt;/p&gt;\n\n&lt;p&gt;So my questions are fairly general, what is a data engineer typically responsible for?  We are moving to GCP, what tools should I be looking at? &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve looked into Dataflow and am intrigued by some of the capabilities, but there seem to be several similar tools in the GCP platform.&lt;/p&gt;\n\n&lt;p&gt;I appreciate any feedback or resources to help me get moving!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1305mx8", "is_robot_indexable": true, "report_reasons": null, "author": "Last-Horror-8078", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1305mx8/where_to_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1305mx8/where_to_start/", "subreddit_subscribers": 102770, "created_utc": 1682558214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a crosspost from StackOverflow, where I did not receive a useful answer, hoping to find one in this community. ([https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb](https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb))\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI am trying to use DuckDB to show the user-created schema that I have  written into a Parquet file.  I can demonstrate in Python (using the  code example at [Get schema of parquet file in Python](https://stackoverflow.com/questions/41567081/get-schema-of-parquet-file-in-python)) that the schema is as I desire, but cannot seem to find a way in DuckDB to get this information.\n\nNeither of the following queries reports the user-created schema\n\nselect \\* from parquet\\_schema('FileWithMetadata.parquet')\n\nselect \\* from parquet\\_metadata('FileWithMetadata.parquet')\n\nupdate:\n\nHere is the code snippet that creates the metadata in the Parquet  file where a is a Pandas dataframe of daily basin flows for a number of  different simulations:\n\ntable = pa.Table.from\\_pandas(a) \n\nmy\\_schema = pa.schema(\\[pa.field(\"Flow\", \"float\", True, metadata={\"data\":\"flow in mm per day\"}),    \n\npa.field(\"DayIndex\", \"int64\", False, metadata={\"data\":\"index of days\"}),     pa.field(\"BasinIndex\", \"string\", True, metadata={\"data\":\"flow in mm per day\"}),     pa.field(\"Simulation\", \"int64\", True, metadata={\"data\":\"simulation number\"})     \\],      metadata={\"info\":\"long format basin flows\"})  t2 = table.cast(my\\_schema) \n\npq.write\\_table(t2, 'SHALongWithMetadata1.parquet')\n\nand the code to read it back is:\n\nimport pyarrow.parquet as pq \n\npfile = pq.read\\_table(\"SHALongWithMetadata1.parquet\") \n\nprint(\"Column names: {}\".format(pfile.column\\_names)) \n\nprint(\"Schema: {}\".format(pfile.schema))  \n\nand this yields as output :  \n\nColumn names: \\['Flow', 'DayIndex', 'BasinIndex', 'Simulation'\\] \n\nSchema: Flow: float   -- field metadata --   data: 'flow in mm per day' \n\nDayIndex: int64 not null   -- field metadata --   data: 'index of days' BasinIndex: string   -- field metadata --   data: 'flow in mm per day' \n\nSimulation: int64   -- field metadata --   data: 'simulation number' -- schema metadata -- info: 'long format basin flows' \n\n&amp;#x200B;\n\nThe suggested answer on StackOverflow was as follows, it does not work to report the user-defined metadata only the standard information about the columns\n\n DESCRIBE SELECT \\* FROM 'FileWithMetadata.parquet';  DESCRIBE TABLE 'FileWithMetadata.parquet'; \n\nTable function parquet\\_metadata  repeats the same information for each row group in the Parquet file, so use SELECT DISTINCT to report only one set of unique column names and types:\n\n    SELECT DISTINCT   path_in_schema,   type  FROM parquet_metadata('FileWithMetadata.parquet');", "author_fullname": "t2_2cs9jenu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to show user schema in Parquet with DuckDb", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1303z09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682553733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a crosspost from StackOverflow, where I did not receive a useful answer, hoping to find one in this community. (&lt;a href=\"https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb\"&gt;https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am trying to use DuckDB to show the user-created schema that I have  written into a Parquet file.  I can demonstrate in Python (using the  code example at &lt;a href=\"https://stackoverflow.com/questions/41567081/get-schema-of-parquet-file-in-python\"&gt;Get schema of parquet file in Python&lt;/a&gt;) that the schema is as I desire, but cannot seem to find a way in DuckDB to get this information.&lt;/p&gt;\n\n&lt;p&gt;Neither of the following queries reports the user-created schema&lt;/p&gt;\n\n&lt;p&gt;select * from parquet_schema(&amp;#39;FileWithMetadata.parquet&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;select * from parquet_metadata(&amp;#39;FileWithMetadata.parquet&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;update:&lt;/p&gt;\n\n&lt;p&gt;Here is the code snippet that creates the metadata in the Parquet  file where a is a Pandas dataframe of daily basin flows for a number of  different simulations:&lt;/p&gt;\n\n&lt;p&gt;table = pa.Table.from_pandas(a) &lt;/p&gt;\n\n&lt;p&gt;my_schema = pa.schema([pa.field(&amp;quot;Flow&amp;quot;, &amp;quot;float&amp;quot;, True, metadata={&amp;quot;data&amp;quot;:&amp;quot;flow in mm per day&amp;quot;}),    &lt;/p&gt;\n\n&lt;p&gt;pa.field(&amp;quot;DayIndex&amp;quot;, &amp;quot;int64&amp;quot;, False, metadata={&amp;quot;data&amp;quot;:&amp;quot;index of days&amp;quot;}),     pa.field(&amp;quot;BasinIndex&amp;quot;, &amp;quot;string&amp;quot;, True, metadata={&amp;quot;data&amp;quot;:&amp;quot;flow in mm per day&amp;quot;}),     pa.field(&amp;quot;Simulation&amp;quot;, &amp;quot;int64&amp;quot;, True, metadata={&amp;quot;data&amp;quot;:&amp;quot;simulation number&amp;quot;})     ],      metadata={&amp;quot;info&amp;quot;:&amp;quot;long format basin flows&amp;quot;})  t2 = table.cast(my_schema) &lt;/p&gt;\n\n&lt;p&gt;pq.write_table(t2, &amp;#39;SHALongWithMetadata1.parquet&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;and the code to read it back is:&lt;/p&gt;\n\n&lt;p&gt;import pyarrow.parquet as pq &lt;/p&gt;\n\n&lt;p&gt;pfile = pq.read_table(&amp;quot;SHALongWithMetadata1.parquet&amp;quot;) &lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;Column names: {}&amp;quot;.format(pfile.column_names)) &lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;Schema: {}&amp;quot;.format(pfile.schema))  &lt;/p&gt;\n\n&lt;p&gt;and this yields as output :  &lt;/p&gt;\n\n&lt;p&gt;Column names: [&amp;#39;Flow&amp;#39;, &amp;#39;DayIndex&amp;#39;, &amp;#39;BasinIndex&amp;#39;, &amp;#39;Simulation&amp;#39;] &lt;/p&gt;\n\n&lt;p&gt;Schema: Flow: float   -- field metadata --   data: &amp;#39;flow in mm per day&amp;#39; &lt;/p&gt;\n\n&lt;p&gt;DayIndex: int64 not null   -- field metadata --   data: &amp;#39;index of days&amp;#39; BasinIndex: string   -- field metadata --   data: &amp;#39;flow in mm per day&amp;#39; &lt;/p&gt;\n\n&lt;p&gt;Simulation: int64   -- field metadata --   data: &amp;#39;simulation number&amp;#39; -- schema metadata -- info: &amp;#39;long format basin flows&amp;#39; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The suggested answer on StackOverflow was as follows, it does not work to report the user-defined metadata only the standard information about the columns&lt;/p&gt;\n\n&lt;p&gt;DESCRIBE SELECT * FROM &amp;#39;FileWithMetadata.parquet&amp;#39;;  DESCRIBE TABLE &amp;#39;FileWithMetadata.parquet&amp;#39;; &lt;/p&gt;\n\n&lt;p&gt;Table function parquet_metadata  repeats the same information for each row group in the Parquet file, so use SELECT DISTINCT to report only one set of unique column names and types:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT DISTINCT   path_in_schema,   type  FROM parquet_metadata(&amp;#39;FileWithMetadata.parquet&amp;#39;);\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1303z09", "is_robot_indexable": true, "report_reasons": null, "author": "rmales", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1303z09/how_to_show_user_schema_in_parquet_with_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1303z09/how_to_show_user_schema_in_parquet_with_duckdb/", "subreddit_subscribers": 102770, "created_utc": 1682553733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "seems like a killer combo. i went ahead and spun up a poc but things have gone horribly wrong somewhere. \n\n&amp;#x200B;\n\nwhat ive done\n\n\\- created a poc table using spark &amp; iceberg in s3 as parquet files\n\n\\- queried that table in athena\n\n\\- installed trino, setup a connection with iceberg configured to use glue as the backend catalog\n\n\\- the information schema queries work fine\n\n\\- the count(\\*) queries work fine\n\n\\- select queries return NULL for all columns :(\n\n\\- at first i thought this was a compression issue, changing the compression to NONE (correct) didnt change the column values\n\n&amp;#x200B;\n\nwhat should i be looking at? im stoked at the potential but bummed by my initial attempts.", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does anyone have experience with trino on top of an iceberg s3 parquet table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1302wh0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682550899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;seems like a killer combo. i went ahead and spun up a poc but things have gone horribly wrong somewhere. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what ive done&lt;/p&gt;\n\n&lt;p&gt;- created a poc table using spark &amp;amp; iceberg in s3 as parquet files&lt;/p&gt;\n\n&lt;p&gt;- queried that table in athena&lt;/p&gt;\n\n&lt;p&gt;- installed trino, setup a connection with iceberg configured to use glue as the backend catalog&lt;/p&gt;\n\n&lt;p&gt;- the information schema queries work fine&lt;/p&gt;\n\n&lt;p&gt;- the count(*) queries work fine&lt;/p&gt;\n\n&lt;p&gt;- select queries return NULL for all columns :(&lt;/p&gt;\n\n&lt;p&gt;- at first i thought this was a compression issue, changing the compression to NONE (correct) didnt change the column values&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what should i be looking at? im stoked at the potential but bummed by my initial attempts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1302wh0", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1302wh0/does_anyone_have_experience_with_trino_on_top_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1302wh0/does_anyone_have_experience_with_trino_on_top_of/", "subreddit_subscribers": 102770, "created_utc": 1682550899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am creating an API wrapper for an API which requires OAuth2 authorization code flow in python.  \nI have created the wrapper such that the user can login and then copy and paste the redirect url, where the authorization code is extracted and exchanged for access and refresh tokens.\n\nHowever, I would like this process to be automated as much as possible and run the ETL pipeline without any human interaction.  \nDoes anyone have experience working with OAuth2 in python when creating ETL pipelines?", "author_fullname": "t2_226yoed5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OAuth2 ETL pipeline in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zrycy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682531953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am creating an API wrapper for an API which requires OAuth2 authorization code flow in python.&lt;br/&gt;\nI have created the wrapper such that the user can login and then copy and paste the redirect url, where the authorization code is extracted and exchanged for access and refresh tokens.&lt;/p&gt;\n\n&lt;p&gt;However, I would like this process to be automated as much as possible and run the ETL pipeline without any human interaction.&lt;br/&gt;\nDoes anyone have experience working with OAuth2 in python when creating ETL pipelines?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zrycy", "is_robot_indexable": true, "report_reasons": null, "author": "C_Ronsholt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zrycy/oauth2_etl_pipeline_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zrycy/oauth2_etl_pipeline_in_python/", "subreddit_subscribers": 102770, "created_utc": 1682531953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,  \n\n\nWe have a classic azure flow and we are now scaling out our team from 5 to just over 25. We have a variety of data sources classic to business \\[salesforce, finance systems etc\\].  \n\n\nThe business has brutal expectations of the team and we want to communicate clearly around:  \n\n\ncapacity  \nexpectations\n\nwhile  also trying to resolve how we distribute work and scale out a scrum based team to a team of over 25.  \n\n\nSo, to summarise the questions:\n\n1. How do you communicate capacity of the team back to the senior stakeholders?\n2. How can we challenge expectations around timelines and unreasonable pace?\n3. How can we scale our ways of working so that we can distribute work better?", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Estimating capacity and scaling a team [Azure flow]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zrlef", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682531135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,  &lt;/p&gt;\n\n&lt;p&gt;We have a classic azure flow and we are now scaling out our team from 5 to just over 25. We have a variety of data sources classic to business [salesforce, finance systems etc].  &lt;/p&gt;\n\n&lt;p&gt;The business has brutal expectations of the team and we want to communicate clearly around:  &lt;/p&gt;\n\n&lt;p&gt;capacity&lt;br/&gt;\nexpectations&lt;/p&gt;\n\n&lt;p&gt;while  also trying to resolve how we distribute work and scale out a scrum based team to a team of over 25.  &lt;/p&gt;\n\n&lt;p&gt;So, to summarise the questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How do you communicate capacity of the team back to the senior stakeholders?&lt;/li&gt;\n&lt;li&gt;How can we challenge expectations around timelines and unreasonable pace?&lt;/li&gt;\n&lt;li&gt;How can we scale our ways of working so that we can distribute work better?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12zrlef", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zrlef/estimating_capacity_and_scaling_a_team_azure_flow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zrlef/estimating_capacity_and_scaling_a_team_azure_flow/", "subreddit_subscribers": 102770, "created_utc": 1682531135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm moving from Reporting to ETL within Data Factory and will be asking so silly beginner question for a while haha.\n\nAnyway I had an over of the ETL created by someone at work, but its not their main role, they were just showing it can be done with our data and clients. \n\nMy question is, is there somewhere like in SQL Server where I can see something like an Execution Plan? There is one step in the ETL that takes some time to complete and all it does is create x number of empty tables, which to me should be quick.\n\nI'm wondering if after each table that has been created it is reconnecting to the source to get the next table and thats whats making so long. \n\nOur system clients can configure their system at any point so we have to rebuild the data warehouse each night so the new columns are there the next day.\n\nThanks for any pointers.", "author_fullname": "t2_j52ax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Factory - Is there an Execution Plan or log", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zqjhv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682528841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m moving from Reporting to ETL within Data Factory and will be asking so silly beginner question for a while haha.&lt;/p&gt;\n\n&lt;p&gt;Anyway I had an over of the ETL created by someone at work, but its not their main role, they were just showing it can be done with our data and clients. &lt;/p&gt;\n\n&lt;p&gt;My question is, is there somewhere like in SQL Server where I can see something like an Execution Plan? There is one step in the ETL that takes some time to complete and all it does is create x number of empty tables, which to me should be quick.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if after each table that has been created it is reconnecting to the source to get the next table and thats whats making so long. &lt;/p&gt;\n\n&lt;p&gt;Our system clients can configure their system at any point so we have to rebuild the data warehouse each night so the new columns are there the next day.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any pointers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zqjhv", "is_robot_indexable": true, "report_reasons": null, "author": "lez_s", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zqjhv/data_factory_is_there_an_execution_plan_or_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zqjhv/data_factory_is_there_an_execution_plan_or_log/", "subreddit_subscribers": 102770, "created_utc": 1682528841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a personal project that I've shared a couple times before on other subreddits (working title is SoundMap). It's essentially a visual representation of the relationships between musical artists and the various projects they're in. This started as a small fun side thing with friends and grew into an obsession, like many of my other hobbies. However, it's grown to the point where the browser simply can't handle all the data and it's near impossible to work on.\n\n&amp;#x200B;\n\nI started this project in 2016-2017, in an online-based data visualization platform called Kumu - at the time it checked all the boxes for what I wanted to do and let me quickly add connections between elements, add basic color coordination, etc. As I mentioned before, this network of connections has grown to over 10,000 elements and has taken on somewhat of a... life of its own. I am trying to export the data out of here and turn it into something more user-friendly and responsive, but I am running into some roadblocks. Mainly the export options are pretty limited:\n\n1. I can export the entire project as a JSON file, but my knowledge and experience with these isn't great. This seems like the best way to go, but that's where I'm stuck.\n2. I can export the data into an Excel workbook, which includes two sheets: one listing elements, and one listing connections. I can work with this, but I just know by going this route I'm going to be putting in hours of extra work. \n\nAny thoughts or ideas?", "author_fullname": "t2_6a0rat1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone here good with Kumu/JSON files? Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zks5j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682522653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a personal project that I&amp;#39;ve shared a couple times before on other subreddits (working title is SoundMap). It&amp;#39;s essentially a visual representation of the relationships between musical artists and the various projects they&amp;#39;re in. This started as a small fun side thing with friends and grew into an obsession, like many of my other hobbies. However, it&amp;#39;s grown to the point where the browser simply can&amp;#39;t handle all the data and it&amp;#39;s near impossible to work on.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I started this project in 2016-2017, in an online-based data visualization platform called Kumu - at the time it checked all the boxes for what I wanted to do and let me quickly add connections between elements, add basic color coordination, etc. As I mentioned before, this network of connections has grown to over 10,000 elements and has taken on somewhat of a... life of its own. I am trying to export the data out of here and turn it into something more user-friendly and responsive, but I am running into some roadblocks. Mainly the export options are pretty limited:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I can export the entire project as a JSON file, but my knowledge and experience with these isn&amp;#39;t great. This seems like the best way to go, but that&amp;#39;s where I&amp;#39;m stuck.&lt;/li&gt;\n&lt;li&gt;I can export the data into an Excel workbook, which includes two sheets: one listing elements, and one listing connections. I can work with this, but I just know by going this route I&amp;#39;m going to be putting in hours of extra work. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any thoughts or ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zks5j", "is_robot_indexable": true, "report_reasons": null, "author": "st_nebula", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zks5j/anyone_here_good_with_kumujson_files_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zks5j/anyone_here_good_with_kumujson_files_help/", "subreddit_subscribers": 102770, "created_utc": 1682522653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is working on modernizing our stack, but we still rely on a great number of legacy systems and processes. I'm in the initial stages of putting together a POC for an orchestrater to manage the mess of DBMS and ETL platforms we've accumulated, and everything I've read about Dagster sounds great. Unfortunately, it doesn't seem to natively support Oracle/SQL Server, or some of our other legacy systems. \n\nI know that with Airflow, you can write your own hooks and operarors to interface with APIs or use existing python packages or bash commands. Is this sort of thing possible with dagster as well? I haven't seen anything talking about that kind of customization. The docs give a broad overview of how to write code for assets, but I'm still not clear on whether I could create an asset class to access Oracle using SQLAlchemy, for example. \n\nI see a lot of examples that interface with airbyte and dbt, which I think could be used as Middleware for this purpose, but I'd prefer to keep things as simple and self-contained as possible.\n\nAny thoughts or experience would be greatly appreciated.", "author_fullname": "t2_3boovm3w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Custom assets in dagster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12zdezw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682505307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is working on modernizing our stack, but we still rely on a great number of legacy systems and processes. I&amp;#39;m in the initial stages of putting together a POC for an orchestrater to manage the mess of DBMS and ETL platforms we&amp;#39;ve accumulated, and everything I&amp;#39;ve read about Dagster sounds great. Unfortunately, it doesn&amp;#39;t seem to natively support Oracle/SQL Server, or some of our other legacy systems. &lt;/p&gt;\n\n&lt;p&gt;I know that with Airflow, you can write your own hooks and operarors to interface with APIs or use existing python packages or bash commands. Is this sort of thing possible with dagster as well? I haven&amp;#39;t seen anything talking about that kind of customization. The docs give a broad overview of how to write code for assets, but I&amp;#39;m still not clear on whether I could create an asset class to access Oracle using SQLAlchemy, for example. &lt;/p&gt;\n\n&lt;p&gt;I see a lot of examples that interface with airbyte and dbt, which I think could be used as Middleware for this purpose, but I&amp;#39;d prefer to keep things as simple and self-contained as possible.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts or experience would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12zdezw", "is_robot_indexable": true, "report_reasons": null, "author": "myrstica", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zdezw/custom_assets_in_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12zdezw/custom_assets_in_dagster/", "subreddit_subscribers": 102770, "created_utc": 1682505307.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}