{"kind": "Listing", "data": {"after": "t3_12zztbq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My personal favorite... A man's health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.", "author_fullname": "t2_3xnau4cx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your favorite data quality horror story?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130rfc2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682609164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My personal favorite... A man&amp;#39;s health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130rfc2", "is_robot_indexable": true, "report_reasons": null, "author": "superconductiveKyle", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/", "subreddit_subscribers": 102910, "created_utc": 1682609164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm in a company that uses pip.\nSince my senior colleague went away 1.5 years ago, my manager instead of hiring another person, he made me became an one man band by doing data engineer / analyst / PMO / SA with tight deadlines.\n\nWhen there's something more complex that he doesn't know, he doesn't want to use it or to make the client pay to use the right tools.\nHe insist that these things can be done in one day.\n\nDon't know what to say to him.", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to vent - manager that doesn't like complex things", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130he0d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682592163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in a company that uses pip.\nSince my senior colleague went away 1.5 years ago, my manager instead of hiring another person, he made me became an one man band by doing data engineer / analyst / PMO / SA with tight deadlines.&lt;/p&gt;\n\n&lt;p&gt;When there&amp;#39;s something more complex that he doesn&amp;#39;t know, he doesn&amp;#39;t want to use it or to make the client pay to use the right tools.\nHe insist that these things can be done in one day.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t know what to say to him.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130he0d", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130he0d/need_to_vent_manager_that_doesnt_like_complex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130he0d/need_to_vent_manager_that_doesnt_like_complex/", "subreddit_subscribers": 102910, "created_utc": 1682592163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is transitioning into medallion architecture making it the first learning for alot of us. \n\nFolks who have grinded their ass off with this architecture, how has your experience been?", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How has your experience working with the Databrick medallion architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130lbzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682600232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is transitioning into medallion architecture making it the first learning for alot of us. &lt;/p&gt;\n\n&lt;p&gt;Folks who have grinded their ass off with this architecture, how has your experience been?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130lbzy", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130lbzy/how_has_your_experience_working_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130lbzy/how_has_your_experience_working_with_the/", "subreddit_subscribers": 102910, "created_utc": 1682600232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, i started recently as a DE and wondered if the speed our ELT-Pipeline is slow?\n\nIts a pipeline from an IBM DB2 table to SQL Server table (once full, not per delta), which is used as a data warehouse. We pull the data via a SQL Command, put the data into a temporal table, then add 2 Columns (HashKey and LoadDate) and then insert it into a persisted table. The whole process took us 6 hours!\n\nRow count : 27 Million\n\nNumber of Columns of the table: 95\n\nCompression type: Column Store (just from temporal table to persisted table)\n\nData space: 2584 MB\n\nIndex space: 1561 MB\n\nETL Tool: Microsoft Integration Services\n\nTime from Source to temporal table: 6 hours\n\nTime from temporal table to persisted table: 39 min\n\nFull Process from Source to temporal table (SSIS data flow within a loop, 10.000 rows per batch):\n\n1. truncate temporal table\n2. connect to source via sql command source query (SELECT 95 attributes FROM source)\n3. do some TRIM operations and type conversions\n4. add a hash key column and loaddate column\n5. Insert the data into the temporal table\n\n&amp;#x200B;\n\nHow much time should this take and what could we do to make the process faster?", "author_fullname": "t2_jo31wrc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the Speed of our ELT Pipeline too slow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130ff1p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682597170.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682588080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i started recently as a DE and wondered if the speed our ELT-Pipeline is slow?&lt;/p&gt;\n\n&lt;p&gt;Its a pipeline from an IBM DB2 table to SQL Server table (once full, not per delta), which is used as a data warehouse. We pull the data via a SQL Command, put the data into a temporal table, then add 2 Columns (HashKey and LoadDate) and then insert it into a persisted table. The whole process took us 6 hours!&lt;/p&gt;\n\n&lt;p&gt;Row count : 27 Million&lt;/p&gt;\n\n&lt;p&gt;Number of Columns of the table: 95&lt;/p&gt;\n\n&lt;p&gt;Compression type: Column Store (just from temporal table to persisted table)&lt;/p&gt;\n\n&lt;p&gt;Data space: 2584 MB&lt;/p&gt;\n\n&lt;p&gt;Index space: 1561 MB&lt;/p&gt;\n\n&lt;p&gt;ETL Tool: Microsoft Integration Services&lt;/p&gt;\n\n&lt;p&gt;Time from Source to temporal table: 6 hours&lt;/p&gt;\n\n&lt;p&gt;Time from temporal table to persisted table: 39 min&lt;/p&gt;\n\n&lt;p&gt;Full Process from Source to temporal table (SSIS data flow within a loop, 10.000 rows per batch):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;truncate temporal table&lt;/li&gt;\n&lt;li&gt;connect to source via sql command source query (SELECT 95 attributes FROM source)&lt;/li&gt;\n&lt;li&gt;do some TRIM operations and type conversions&lt;/li&gt;\n&lt;li&gt;add a hash key column and loaddate column&lt;/li&gt;\n&lt;li&gt;Insert the data into the temporal table&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How much time should this take and what could we do to make the process faster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130ff1p", "is_robot_indexable": true, "report_reasons": null, "author": "Elegant_Good6212", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130ff1p/is_the_speed_of_our_elt_pipeline_too_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130ff1p/is_the_speed_of_our_elt_pipeline_too_slow/", "subreddit_subscribers": 102910, "created_utc": 1682588080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m recently promoted to Data Engineer within my company through a series of reorganizations.   I have 15 years of experience in software development, but this role is new to me and the first Data Engineer in our company.\n\nSo my questions are fairly general, what is a data engineer typically responsible for?  We are moving to GCP, what tools should I be looking at? \n\nI\u2019ve looked into Dataflow and am intrigued by some of the capabilities, but there seem to be several similar tools in the GCP platform.\n\nI appreciate any feedback or resources to help me get moving!", "author_fullname": "t2_c4b8qxbi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1305mx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682558214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m recently promoted to Data Engineer within my company through a series of reorganizations.   I have 15 years of experience in software development, but this role is new to me and the first Data Engineer in our company.&lt;/p&gt;\n\n&lt;p&gt;So my questions are fairly general, what is a data engineer typically responsible for?  We are moving to GCP, what tools should I be looking at? &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve looked into Dataflow and am intrigued by some of the capabilities, but there seem to be several similar tools in the GCP platform.&lt;/p&gt;\n\n&lt;p&gt;I appreciate any feedback or resources to help me get moving!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1305mx8", "is_robot_indexable": true, "report_reasons": null, "author": "Last-Horror-8078", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1305mx8/where_to_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1305mx8/where_to_start/", "subreddit_subscribers": 102910, "created_utc": 1682558214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work in data professionally with a team who only knows SQL whereas I know less SQL than they do and a lot more Python. We're using Spark for data transformations and are currently trying to upsert to Delta Lake files from existing parquet EDIT using notebooks in Synapse. \n\nAs my team only knows SQL (as does the lead), the Lead thinks it'd be best if we only use SQL to minimise the amount of time required for everybody to get up and running.\n\nI'm currently reading in the parquet file using the syntax:\n\n`CREATE TABLE IF NOT EXISTS SourceTable USING Parquet LOCATION &lt;location&gt;`\n\nand add in a column\n\n`ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))`\n\nI now want to populate this column with values with `UPDATE SourceTable SET NewColumn = 'test'` just to check it works and I'm getting the error:\n\n`Error: UPDATE destination only supports Delta sources.`\n\nI get this and it's music to my ears because data manipulation in PySpark is infinitely easier for me (this is going to be dynamic so ease of string interpolation and variable injection is what I want).\n\nThis leads me to have a few questions:\n\n* Is it really impossible to `UPDATE` a table object created from a Parquet file using Spark SQL?\n\n* Under the hood, I get that in PySpark the parquet file is converted into a DataFrame which is an object in memory. As you can't this using Spark SQL, does this mean the parquet file doesn't get converted into memory and you are, in fact, hitting the file directly?\n\n* I'm trying to build a strong case where we use PySpark 90%+ of the time for these kinds of transformations and, of course, being unable to view changes in memory is a pretty big shitter.  Are there any other massive limitations of Spark SQL I should know about?\n\nThank you!\n\nEDIT\n\nMore detail.\n\nWe currently use the Data Lakehouse paradigm in Azure Synapse. We take parquet files (bronze) and then essentially upsert them into Delta files (silver) based off a key. This key is a hash of a column/set of columns.\n\nThis was previously achieved using Azure Data Flows although we're trying to move away from using these because they're a massive pain in the tits to maintain and shift into notebooks/code. The Data flow has the following pattern for the row hash:\n\n* Reads in the parquet file\n\n* Creates a new column with an expression which is essentially hashing a column/set of columns\n\n* Does an upsert against the delta file, checking if the row hash exists as the condition.\n\nThe issue:\n\n* In note books using Spark SQL, I can't recreate this pattern because when I add a new column, I can't populate it with values as I'm, effectively, trying to \"edit\" the parquet file.\n\n* This is confusing for me because one of the questions I have at the top is \"Why can I do this in a data frame, but not in a table created using Spark SQL syntax? Is it tables created from Parquets using Spark SQL aren't the same as dataframes?\".\n\n* I can do this all in Pyspark, but ideally, I'd like to use Spark SQL because my team can't do it in Python.\n\nEDIT 2:\n\nSample code comparison:\n\n    from pyspark.sql.functions import sha2\n    \n    source_df = spark.read.load(\"path/to/parquet/file.parquet\", format=\"parquet\")\n    source_df = source_df.withColumn(\"NewColumn\", sha2(source_df.ColumnToHash, 256))\n\nI can then do the upsert to the Delta file.\n\nMy Spark SQL code:\n\n    CREATE TABLE IF NOT EXISTS SourceTable USING Parquet\n    LOCATION \"path/to/parquet/file.parquet\"\n\n    ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))\n\nBut then I can't do anything to `NewColumn`.  Worth adding a minor correction - it's not a row hash, I'm trying to hash a specific column/set of columns.", "author_fullname": "t2_bqhp2bh8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Limitations of Spark SQL vs. PySpark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130nxgp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682611347.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682605793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work in data professionally with a team who only knows SQL whereas I know less SQL than they do and a lot more Python. We&amp;#39;re using Spark for data transformations and are currently trying to upsert to Delta Lake files from existing parquet EDIT using notebooks in Synapse. &lt;/p&gt;\n\n&lt;p&gt;As my team only knows SQL (as does the lead), the Lead thinks it&amp;#39;d be best if we only use SQL to minimise the amount of time required for everybody to get up and running.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently reading in the parquet file using the syntax:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;CREATE TABLE IF NOT EXISTS SourceTable USING Parquet LOCATION &amp;lt;location&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;and add in a column&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I now want to populate this column with values with &lt;code&gt;UPDATE SourceTable SET NewColumn = &amp;#39;test&amp;#39;&lt;/code&gt; just to check it works and I&amp;#39;m getting the error:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Error: UPDATE destination only supports Delta sources.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I get this and it&amp;#39;s music to my ears because data manipulation in PySpark is infinitely easier for me (this is going to be dynamic so ease of string interpolation and variable injection is what I want).&lt;/p&gt;\n\n&lt;p&gt;This leads me to have a few questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Is it really impossible to &lt;code&gt;UPDATE&lt;/code&gt; a table object created from a Parquet file using Spark SQL?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Under the hood, I get that in PySpark the parquet file is converted into a DataFrame which is an object in memory. As you can&amp;#39;t this using Spark SQL, does this mean the parquet file doesn&amp;#39;t get converted into memory and you are, in fact, hitting the file directly?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I&amp;#39;m trying to build a strong case where we use PySpark 90%+ of the time for these kinds of transformations and, of course, being unable to view changes in memory is a pretty big shitter.  Are there any other massive limitations of Spark SQL I should know about?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;EDIT&lt;/p&gt;\n\n&lt;p&gt;More detail.&lt;/p&gt;\n\n&lt;p&gt;We currently use the Data Lakehouse paradigm in Azure Synapse. We take parquet files (bronze) and then essentially upsert them into Delta files (silver) based off a key. This key is a hash of a column/set of columns.&lt;/p&gt;\n\n&lt;p&gt;This was previously achieved using Azure Data Flows although we&amp;#39;re trying to move away from using these because they&amp;#39;re a massive pain in the tits to maintain and shift into notebooks/code. The Data flow has the following pattern for the row hash:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Reads in the parquet file&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Creates a new column with an expression which is essentially hashing a column/set of columns&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Does an upsert against the delta file, checking if the row hash exists as the condition.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The issue:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;In note books using Spark SQL, I can&amp;#39;t recreate this pattern because when I add a new column, I can&amp;#39;t populate it with values as I&amp;#39;m, effectively, trying to &amp;quot;edit&amp;quot; the parquet file.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This is confusing for me because one of the questions I have at the top is &amp;quot;Why can I do this in a data frame, but not in a table created using Spark SQL syntax? Is it tables created from Parquets using Spark SQL aren&amp;#39;t the same as dataframes?&amp;quot;.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I can do this all in Pyspark, but ideally, I&amp;#39;d like to use Spark SQL because my team can&amp;#39;t do it in Python.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;EDIT 2:&lt;/p&gt;\n\n&lt;p&gt;Sample code comparison:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from pyspark.sql.functions import sha2\n\nsource_df = spark.read.load(&amp;quot;path/to/parquet/file.parquet&amp;quot;, format=&amp;quot;parquet&amp;quot;)\nsource_df = source_df.withColumn(&amp;quot;NewColumn&amp;quot;, sha2(source_df.ColumnToHash, 256))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I can then do the upsert to the Delta file.&lt;/p&gt;\n\n&lt;p&gt;My Spark SQL code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE IF NOT EXISTS SourceTable USING Parquet\nLOCATION &amp;quot;path/to/parquet/file.parquet&amp;quot;\n\nALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But then I can&amp;#39;t do anything to &lt;code&gt;NewColumn&lt;/code&gt;.  Worth adding a minor correction - it&amp;#39;s not a row hash, I&amp;#39;m trying to hash a specific column/set of columns.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130nxgp", "is_robot_indexable": true, "report_reasons": null, "author": "standard_throw", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130nxgp/limitations_of_spark_sql_vs_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130nxgp/limitations_of_spark_sql_vs_pyspark/", "subreddit_subscribers": 102910, "created_utc": 1682605793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company has a well-structured data set which is about 10TB in size. Each file in this dataset represents a \"parameter\" which has two columns (x and y).\n\nManagement has requested me to research about making a data visualization software that can be accessed through the intranet by multiple departments within the company. \n\nSkillwise, I would rate myself about 6/10 in JS and Python. I do most of my work in C/C++. I am given about 3 months to do an initial draft of the software (the time frame was given based on my request as I have to do a lot of learning).\n\nI did some research and came up with Django for backend and React for front end with Chart.js for handling the bulk of data visualization. MySQL may be used for data storage. The VIZ is mostly simple line charts (max 50 parameters, each with 2000 data points), scatter plots and bar plots. Bonus points if user can interact in many ways with the charts.\n\nAm I on the right track? Can anyone give me some suggestions regarding the tech stack I am planning to use and suggest additional components for say, optimizing data retrieval?", "author_fullname": "t2_2teg11zt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestion regarding making a data visualization tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1307v04", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682564248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has a well-structured data set which is about 10TB in size. Each file in this dataset represents a &amp;quot;parameter&amp;quot; which has two columns (x and y).&lt;/p&gt;\n\n&lt;p&gt;Management has requested me to research about making a data visualization software that can be accessed through the intranet by multiple departments within the company. &lt;/p&gt;\n\n&lt;p&gt;Skillwise, I would rate myself about 6/10 in JS and Python. I do most of my work in C/C++. I am given about 3 months to do an initial draft of the software (the time frame was given based on my request as I have to do a lot of learning).&lt;/p&gt;\n\n&lt;p&gt;I did some research and came up with Django for backend and React for front end with Chart.js for handling the bulk of data visualization. MySQL may be used for data storage. The VIZ is mostly simple line charts (max 50 parameters, each with 2000 data points), scatter plots and bar plots. Bonus points if user can interact in many ways with the charts.&lt;/p&gt;\n\n&lt;p&gt;Am I on the right track? Can anyone give me some suggestions regarding the tech stack I am planning to use and suggest additional components for say, optimizing data retrieval?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1307v04", "is_robot_indexable": true, "report_reasons": null, "author": "ciado63", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1307v04/need_suggestion_regarding_making_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1307v04/need_suggestion_regarding_making_a_data/", "subreddit_subscribers": 102910, "created_utc": 1682564248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been a data engineer for a good amount of time, but have not had to deal with CQRS until now. I have been tasked with creating some kind of data pipeline and reporting datastore for one of our applications. This application has a CQRS event store that I am considering getting data from. The application team would write a projection that basically publishes all events to something (probably Kinesis, but doesn\u2019t really matter for this conversation), and I would pickup data from there and do whatever I need with it. \n\nOne of the things that seems awesome about an event store is that it would contain all events going back to the beginning of time. This simplifies reporting because we could incrementally pull/process whatever data we need at any point in time and not have to go with a \u201ccollect all the data, worry about use case later\u201d approach that datalakes push towards.\n\nIn practice, the application team has mentioned the idea of snapshotting the event store for performance purposes, which means that something would have to happen in order to retain history. They\u2019ve also mentioned the idea of having a second event store that would not be truncated, but that doesn\u2019t seem like a great idea to me. They also have issues where events sometimes get hung or stuck, so I question the resilience of this in general.\n\nWith that said, I\u2019m curious what other data teams are doing to get data out of CQRS event stores. Is a projection that basically blasts all the data out to some streaming technology a reasonable option, or would it make more sense to write a projection for say each dimension table/fact table/report? What has worked or not worked for you when it comes to reporting on data that exists in a CQRS event store?", "author_fullname": "t2_14aeem", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reporting for application using a CQRS event store", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1313uw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682624515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been a data engineer for a good amount of time, but have not had to deal with CQRS until now. I have been tasked with creating some kind of data pipeline and reporting datastore for one of our applications. This application has a CQRS event store that I am considering getting data from. The application team would write a projection that basically publishes all events to something (probably Kinesis, but doesn\u2019t really matter for this conversation), and I would pickup data from there and do whatever I need with it. &lt;/p&gt;\n\n&lt;p&gt;One of the things that seems awesome about an event store is that it would contain all events going back to the beginning of time. This simplifies reporting because we could incrementally pull/process whatever data we need at any point in time and not have to go with a \u201ccollect all the data, worry about use case later\u201d approach that datalakes push towards.&lt;/p&gt;\n\n&lt;p&gt;In practice, the application team has mentioned the idea of snapshotting the event store for performance purposes, which means that something would have to happen in order to retain history. They\u2019ve also mentioned the idea of having a second event store that would not be truncated, but that doesn\u2019t seem like a great idea to me. They also have issues where events sometimes get hung or stuck, so I question the resilience of this in general.&lt;/p&gt;\n\n&lt;p&gt;With that said, I\u2019m curious what other data teams are doing to get data out of CQRS event stores. Is a projection that basically blasts all the data out to some streaming technology a reasonable option, or would it make more sense to write a projection for say each dimension table/fact table/report? What has worked or not worked for you when it comes to reporting on data that exists in a CQRS event store?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1313uw5", "is_robot_indexable": true, "report_reasons": null, "author": "raginjason", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1313uw5/reporting_for_application_using_a_cqrs_event_store/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1313uw5/reporting_for_application_using_a_cqrs_event_store/", "subreddit_subscribers": 102910, "created_utc": 1682624515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently interviewed with a company and their ETL/data warehouse stack is Informatica + IBM DB2.  I've heard bad things about Informatica, but I didn't even know IBM had a data warehouse offering? Does anyone have any insights about it? How does it compare to other modern options like Snowflake or BQ?", "author_fullname": "t2_4rifsjav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IBM DB2 Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130tg7s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682611175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently interviewed with a company and their ETL/data warehouse stack is Informatica + IBM DB2.  I&amp;#39;ve heard bad things about Informatica, but I didn&amp;#39;t even know IBM had a data warehouse offering? Does anyone have any insights about it? How does it compare to other modern options like Snowflake or BQ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130tg7s", "is_robot_indexable": true, "report_reasons": null, "author": "Techthrowaway2222888", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130tg7s/ibm_db2_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130tg7s/ibm_db2_warehouse/", "subreddit_subscribers": 102910, "created_utc": 1682611175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team has been struggling for months with finding a good replication and CDC solution. We are trying to replicate data from an app database and a few third party databases to BigQuery (or blob storage then eventually BQ). When I joined we had just signed a license with Fivetran which has worked seamlessly for the third party DBs but is extremely costly and prone to failure when using it for our application database. \n\nDoes anyone have a good solution for replication and CDC from Postgres to GCP? Our team is currently evaluating Confluent + Kafka. Ideally we want a solution that can handle a wide variety of types, handle schema drift, and won\u2019t break the bank. \n\nAddl context:\nI am a senior data engineer at a ~300 person startup. DE is relatively new at my company having been around for a year. We use GCP, Postgres, Airflow, and are a heavy Python shop.", "author_fullname": "t2_36yd2hgi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your company handle replication and CDC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130ls2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682601191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team has been struggling for months with finding a good replication and CDC solution. We are trying to replicate data from an app database and a few third party databases to BigQuery (or blob storage then eventually BQ). When I joined we had just signed a license with Fivetran which has worked seamlessly for the third party DBs but is extremely costly and prone to failure when using it for our application database. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a good solution for replication and CDC from Postgres to GCP? Our team is currently evaluating Confluent + Kafka. Ideally we want a solution that can handle a wide variety of types, handle schema drift, and won\u2019t break the bank. &lt;/p&gt;\n\n&lt;p&gt;Addl context:\nI am a senior data engineer at a ~300 person startup. DE is relatively new at my company having been around for a year. We use GCP, Postgres, Airflow, and are a heavy Python shop.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130ls2m", "is_robot_indexable": true, "report_reasons": null, "author": "PROTECTyaNECK44", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130ls2m/how_does_your_company_handle_replication_and_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130ls2m/how_does_your_company_handle_replication_and_cdc/", "subreddit_subscribers": 102910, "created_utc": 1682601191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wish I could find some references or use-cases of architectures about data engineering for manufactoring IoT, these are my goals:\n\n1. Some sensors are 1Hz (LF), others run at 20Hz (HF)\n2. Some sensors need some ingestion pipelines (thinking about map/reduce/filter/apply functions on the data stream) or processing pipelines\n3. How to handle the stream of data/mini-batch/batches of data with queues or topics and similar tools\n4. How to handle different industrial protocols like MQTT, MODBUS, OPCUA etc.\n5. What  should be done edge/on-premise vs on the cloud\n\nI read about Apache Kafka, ActiveMQ, RabbitMQ, then about Apache Spark for the processing but I don't want to dig into those frameworks without knowing what are the right tools since I don't have that much time to dedicate to my education, sadly.\n\nAlso I wish I could understand the best way to think about where, in the pipelines, the smart features should be provided, e.g. close to the machine assets or close to the module that sends the data to the datalake, if I should think about hot processing (processing stream of data) vs cooler processing (processing mini-batches of data). Do I need to handle messages and stream in a pub/sub architecture or do I need to handle dataframes of data? Do I need read or write performances? (I read about avro and parquet recently). I'm a bit lost where to start.", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools about data engineering architecture for IoT (manufactoring): stream processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130bncu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682575469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wish I could find some references or use-cases of architectures about data engineering for manufactoring IoT, these are my goals:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Some sensors are 1Hz (LF), others run at 20Hz (HF)&lt;/li&gt;\n&lt;li&gt;Some sensors need some ingestion pipelines (thinking about map/reduce/filter/apply functions on the data stream) or processing pipelines&lt;/li&gt;\n&lt;li&gt;How to handle the stream of data/mini-batch/batches of data with queues or topics and similar tools&lt;/li&gt;\n&lt;li&gt;How to handle different industrial protocols like MQTT, MODBUS, OPCUA etc.&lt;/li&gt;\n&lt;li&gt;What  should be done edge/on-premise vs on the cloud&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I read about Apache Kafka, ActiveMQ, RabbitMQ, then about Apache Spark for the processing but I don&amp;#39;t want to dig into those frameworks without knowing what are the right tools since I don&amp;#39;t have that much time to dedicate to my education, sadly.&lt;/p&gt;\n\n&lt;p&gt;Also I wish I could understand the best way to think about where, in the pipelines, the smart features should be provided, e.g. close to the machine assets or close to the module that sends the data to the datalake, if I should think about hot processing (processing stream of data) vs cooler processing (processing mini-batches of data). Do I need to handle messages and stream in a pub/sub architecture or do I need to handle dataframes of data? Do I need read or write performances? (I read about avro and parquet recently). I&amp;#39;m a bit lost where to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130bncu", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130bncu/best_tools_about_data_engineering_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130bncu/best_tools_about_data_engineering_architecture/", "subreddit_subscribers": 102910, "created_utc": 1682575469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically detecting breaking changes in SQL queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1310k7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682618763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tobikodata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tobikodata.com/automatically-detecting-breaking-changes-in-sql-queries.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1310k7b", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1310k7b/automatically_detecting_breaking_changes_in_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tobikodata.com/automatically-detecting-breaking-changes-in-sql-queries.html", "subreddit_subscribers": 102910, "created_utc": 1682618763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Im a super junior data engineer and my boss wanted this specificly:\n\nOur pipeline currently fetches a JSON from an API. Normaly, its between 1-5 mb.  \nHowever, sometimes, because of some error externally, we get an empty JSON file. This file is around 10 byte in size.\n\nOur dataflow, which is part of the pipeline, cannot accept empty JSON files as input, it fails because of malformed SCHEMA.\n\nBoss wants the pipeline to succeed, even though the dataflow should fail.\n\nCan anyone share some insight into which would be the best way to handle this?", "author_fullname": "t2_xcbzsw9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best/easiest way to handle failed dataflows in a ADF pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130yccc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682616467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a super junior data engineer and my boss wanted this specificly:&lt;/p&gt;\n\n&lt;p&gt;Our pipeline currently fetches a JSON from an API. Normaly, its between 1-5 mb.&lt;br/&gt;\nHowever, sometimes, because of some error externally, we get an empty JSON file. This file is around 10 byte in size.&lt;/p&gt;\n\n&lt;p&gt;Our dataflow, which is part of the pipeline, cannot accept empty JSON files as input, it fails because of malformed SCHEMA.&lt;/p&gt;\n\n&lt;p&gt;Boss wants the pipeline to succeed, even though the dataflow should fail.&lt;/p&gt;\n\n&lt;p&gt;Can anyone share some insight into which would be the best way to handle this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130yccc", "is_robot_indexable": true, "report_reasons": null, "author": "useyourname89", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130yccc/what_is_the_besteasiest_way_to_handle_failed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130yccc/what_is_the_besteasiest_way_to_handle_failed/", "subreddit_subscribers": 102910, "created_utc": 1682616467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm starting to get more into the habit of breaking up larger DAGs/pipelines into smaller ones to accomplish more specific tasks which makes it easier to debug and troubleshoot. However, I'm a little stuck on how exactly to do this. \n\nFor example, is it a best practice to create a separate DAG for every data source? Every table? If I have a bunch of SharePoint lists scattered around on a bunch of different sites, should I set up one pipeline that loops through everything at once, or break it out by site, or some other way? Should the logic for which data to extract be embedded in the code itself, or a config file along with the code, or with the orchestrator itself completely separate from the code?\n\nHopefully this isn't too many questions all at once. I guess I'm just overwhelmed with the sheer number of options.", "author_fullname": "t2_thw4nqfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you organize your extract/load pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130qgti", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682608241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting to get more into the habit of breaking up larger DAGs/pipelines into smaller ones to accomplish more specific tasks which makes it easier to debug and troubleshoot. However, I&amp;#39;m a little stuck on how exactly to do this. &lt;/p&gt;\n\n&lt;p&gt;For example, is it a best practice to create a separate DAG for every data source? Every table? If I have a bunch of SharePoint lists scattered around on a bunch of different sites, should I set up one pipeline that loops through everything at once, or break it out by site, or some other way? Should the logic for which data to extract be embedded in the code itself, or a config file along with the code, or with the orchestrator itself completely separate from the code?&lt;/p&gt;\n\n&lt;p&gt;Hopefully this isn&amp;#39;t too many questions all at once. I guess I&amp;#39;m just overwhelmed with the sheer number of options.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130qgti", "is_robot_indexable": true, "report_reasons": null, "author": "BoofThatShit720", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130qgti/how_do_you_organize_your_extractload_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130qgti/how_do_you_organize_your_extractload_pipelines/", "subreddit_subscribers": 102910, "created_utc": 1682608241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company in the process of migrating to an AWS service that dumps JSON log data to S3 buckets.\n\nI need to get the data in these logs, transform and store on an on premise database to use in analytical processes.  At some point in the future this will be output to a cloud database.\n\nThe steer from our Technical Architects is that if we want access to these logs we need to do it via AWS Athena as they will not allow direct access to the S3 buckets that store the log data.\n\nWe are allowed to define the Athena tables ourselves so we could pull all the log contents.\n\nIn my reasearch it seems like AWS is primarily a UI for querying data in S3, similar to Hue, and not really designed to serve data to downstream processes.  It looks like it can be done, either by retrieving the query results that are stored in S3 or by setting up an API using other AWS services.\n\nI already have developed a small PySpark pipeline to process the JSON data and would prefer to access the data direct in the S3 buckets.\n\nTLDR: i need a pipepline to transform JSON data stored in S3 and stored in an on premise database.  What would be the best approach?  Is AWS Athena a good fit in this pipeline?", "author_fullname": "t2_h5e9i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automating retrieval of query results from AWS Athean", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130cuof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682579401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company in the process of migrating to an AWS service that dumps JSON log data to S3 buckets.&lt;/p&gt;\n\n&lt;p&gt;I need to get the data in these logs, transform and store on an on premise database to use in analytical processes.  At some point in the future this will be output to a cloud database.&lt;/p&gt;\n\n&lt;p&gt;The steer from our Technical Architects is that if we want access to these logs we need to do it via AWS Athena as they will not allow direct access to the S3 buckets that store the log data.&lt;/p&gt;\n\n&lt;p&gt;We are allowed to define the Athena tables ourselves so we could pull all the log contents.&lt;/p&gt;\n\n&lt;p&gt;In my reasearch it seems like AWS is primarily a UI for querying data in S3, similar to Hue, and not really designed to serve data to downstream processes.  It looks like it can be done, either by retrieving the query results that are stored in S3 or by setting up an API using other AWS services.&lt;/p&gt;\n\n&lt;p&gt;I already have developed a small PySpark pipeline to process the JSON data and would prefer to access the data direct in the S3 buckets.&lt;/p&gt;\n\n&lt;p&gt;TLDR: i need a pipepline to transform JSON data stored in S3 and stored in an on premise database.  What would be the best approach?  Is AWS Athena a good fit in this pipeline?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130cuof", "is_robot_indexable": true, "report_reasons": null, "author": "the_glover", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130cuof/automating_retrieval_of_query_results_from_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130cuof/automating_retrieval_of_query_results_from_aws/", "subreddit_subscribers": 102910, "created_utc": 1682579401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a crosspost from StackOverflow, where I did not receive a useful answer, hoping to find one in this community. ([https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb](https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb))\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI am trying to use DuckDB to show the user-created schema that I have  written into a Parquet file.  I can demonstrate in Python (using the  code example at [Get schema of parquet file in Python](https://stackoverflow.com/questions/41567081/get-schema-of-parquet-file-in-python)) that the schema is as I desire, but cannot seem to find a way in DuckDB to get this information.\n\nNeither of the following queries reports the user-created schema\n\nselect \\* from parquet\\_schema('FileWithMetadata.parquet')\n\nselect \\* from parquet\\_metadata('FileWithMetadata.parquet')\n\nupdate:\n\nHere is the code snippet that creates the metadata in the Parquet  file where a is a Pandas dataframe of daily basin flows for a number of  different simulations:\n\ntable = pa.Table.from\\_pandas(a) \n\nmy\\_schema = pa.schema(\\[pa.field(\"Flow\", \"float\", True, metadata={\"data\":\"flow in mm per day\"}),    \n\npa.field(\"DayIndex\", \"int64\", False, metadata={\"data\":\"index of days\"}),     pa.field(\"BasinIndex\", \"string\", True, metadata={\"data\":\"flow in mm per day\"}),     pa.field(\"Simulation\", \"int64\", True, metadata={\"data\":\"simulation number\"})     \\],      metadata={\"info\":\"long format basin flows\"})  t2 = table.cast(my\\_schema) \n\npq.write\\_table(t2, 'SHALongWithMetadata1.parquet')\n\nand the code to read it back is:\n\nimport pyarrow.parquet as pq \n\npfile = pq.read\\_table(\"SHALongWithMetadata1.parquet\") \n\nprint(\"Column names: {}\".format(pfile.column\\_names)) \n\nprint(\"Schema: {}\".format(pfile.schema))  \n\nand this yields as output :  \n\nColumn names: \\['Flow', 'DayIndex', 'BasinIndex', 'Simulation'\\] \n\nSchema: Flow: float   -- field metadata --   data: 'flow in mm per day' \n\nDayIndex: int64 not null   -- field metadata --   data: 'index of days' BasinIndex: string   -- field metadata --   data: 'flow in mm per day' \n\nSimulation: int64   -- field metadata --   data: 'simulation number' -- schema metadata -- info: 'long format basin flows' \n\n&amp;#x200B;\n\nThe suggested answer on StackOverflow was as follows, it does not work to report the user-defined metadata only the standard information about the columns\n\n DESCRIBE SELECT \\* FROM 'FileWithMetadata.parquet';  DESCRIBE TABLE 'FileWithMetadata.parquet'; \n\nTable function parquet\\_metadata  repeats the same information for each row group in the Parquet file, so use SELECT DISTINCT to report only one set of unique column names and types:\n\n    SELECT DISTINCT   path_in_schema,   type  FROM parquet_metadata('FileWithMetadata.parquet');", "author_fullname": "t2_2cs9jenu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to show user schema in Parquet with DuckDb", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1303z09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682553733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a crosspost from StackOverflow, where I did not receive a useful answer, hoping to find one in this community. (&lt;a href=\"https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb\"&gt;https://stackoverflow.com/questions/76066470/how-to-show-user-schema-in-a-parquet-file-using-duckdb&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am trying to use DuckDB to show the user-created schema that I have  written into a Parquet file.  I can demonstrate in Python (using the  code example at &lt;a href=\"https://stackoverflow.com/questions/41567081/get-schema-of-parquet-file-in-python\"&gt;Get schema of parquet file in Python&lt;/a&gt;) that the schema is as I desire, but cannot seem to find a way in DuckDB to get this information.&lt;/p&gt;\n\n&lt;p&gt;Neither of the following queries reports the user-created schema&lt;/p&gt;\n\n&lt;p&gt;select * from parquet_schema(&amp;#39;FileWithMetadata.parquet&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;select * from parquet_metadata(&amp;#39;FileWithMetadata.parquet&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;update:&lt;/p&gt;\n\n&lt;p&gt;Here is the code snippet that creates the metadata in the Parquet  file where a is a Pandas dataframe of daily basin flows for a number of  different simulations:&lt;/p&gt;\n\n&lt;p&gt;table = pa.Table.from_pandas(a) &lt;/p&gt;\n\n&lt;p&gt;my_schema = pa.schema([pa.field(&amp;quot;Flow&amp;quot;, &amp;quot;float&amp;quot;, True, metadata={&amp;quot;data&amp;quot;:&amp;quot;flow in mm per day&amp;quot;}),    &lt;/p&gt;\n\n&lt;p&gt;pa.field(&amp;quot;DayIndex&amp;quot;, &amp;quot;int64&amp;quot;, False, metadata={&amp;quot;data&amp;quot;:&amp;quot;index of days&amp;quot;}),     pa.field(&amp;quot;BasinIndex&amp;quot;, &amp;quot;string&amp;quot;, True, metadata={&amp;quot;data&amp;quot;:&amp;quot;flow in mm per day&amp;quot;}),     pa.field(&amp;quot;Simulation&amp;quot;, &amp;quot;int64&amp;quot;, True, metadata={&amp;quot;data&amp;quot;:&amp;quot;simulation number&amp;quot;})     ],      metadata={&amp;quot;info&amp;quot;:&amp;quot;long format basin flows&amp;quot;})  t2 = table.cast(my_schema) &lt;/p&gt;\n\n&lt;p&gt;pq.write_table(t2, &amp;#39;SHALongWithMetadata1.parquet&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;and the code to read it back is:&lt;/p&gt;\n\n&lt;p&gt;import pyarrow.parquet as pq &lt;/p&gt;\n\n&lt;p&gt;pfile = pq.read_table(&amp;quot;SHALongWithMetadata1.parquet&amp;quot;) &lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;Column names: {}&amp;quot;.format(pfile.column_names)) &lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;Schema: {}&amp;quot;.format(pfile.schema))  &lt;/p&gt;\n\n&lt;p&gt;and this yields as output :  &lt;/p&gt;\n\n&lt;p&gt;Column names: [&amp;#39;Flow&amp;#39;, &amp;#39;DayIndex&amp;#39;, &amp;#39;BasinIndex&amp;#39;, &amp;#39;Simulation&amp;#39;] &lt;/p&gt;\n\n&lt;p&gt;Schema: Flow: float   -- field metadata --   data: &amp;#39;flow in mm per day&amp;#39; &lt;/p&gt;\n\n&lt;p&gt;DayIndex: int64 not null   -- field metadata --   data: &amp;#39;index of days&amp;#39; BasinIndex: string   -- field metadata --   data: &amp;#39;flow in mm per day&amp;#39; &lt;/p&gt;\n\n&lt;p&gt;Simulation: int64   -- field metadata --   data: &amp;#39;simulation number&amp;#39; -- schema metadata -- info: &amp;#39;long format basin flows&amp;#39; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The suggested answer on StackOverflow was as follows, it does not work to report the user-defined metadata only the standard information about the columns&lt;/p&gt;\n\n&lt;p&gt;DESCRIBE SELECT * FROM &amp;#39;FileWithMetadata.parquet&amp;#39;;  DESCRIBE TABLE &amp;#39;FileWithMetadata.parquet&amp;#39;; &lt;/p&gt;\n\n&lt;p&gt;Table function parquet_metadata  repeats the same information for each row group in the Parquet file, so use SELECT DISTINCT to report only one set of unique column names and types:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT DISTINCT   path_in_schema,   type  FROM parquet_metadata(&amp;#39;FileWithMetadata.parquet&amp;#39;);\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1303z09", "is_robot_indexable": true, "report_reasons": null, "author": "rmales", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1303z09/how_to_show_user_schema_in_parquet_with_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1303z09/how_to_show_user_schema_in_parquet_with_duckdb/", "subreddit_subscribers": 102910, "created_utc": 1682553733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't think snowflake is right place to put auditing information and should be in some OLTP database. Want to hear from you.", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do you guys store your audit logs and orchestration metadata in snowflake environment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1313y1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682624668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think snowflake is right place to put auditing information and should be in some OLTP database. Want to hear from you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1313y1r", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1313y1r/where_do_you_guys_store_your_audit_logs_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1313y1r/where_do_you_guys_store_your_audit_logs_and/", "subreddit_subscribers": 102910, "created_utc": 1682624668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to get the group\u2019s experiences on what they\u2019ve done and recommend.\n\n My company is undergoing the implementation of a data warehouse. We\u2019re probably middle of the road sized data sets but have decided on using snowflake as our storage. Almost all our transformations will be done in snowflake stored procs as well.\n\nWe\u2019ve brought in some consultants for evaluations and each have come back with different options from only using views out of a data lake to more standard approaches.\n\nWe\u2019re considering two of their recommendations, 1 is a kimball star schema and the other is Data vault 2.0.\nI\u2019m fairly familiar with the star schema but one of the consultants is labeling this as a legacy approach and data vault is more modern. I don\u2019t quite understand it because data vault isn\u2019t new. Also, the end state of a vault presents the data in a star schema. \n\nWhat\u2019s models are you guys moving forward with? Asa side note, I\u2019d also like to hear some specifics , pros cons, you\u2019ve seen with data vault.\n\n[View Poll](https://www.reddit.com/poll/1312r30)", "author_fullname": "t2_6nrc61lj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DW Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312r30", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to get the group\u2019s experiences on what they\u2019ve done and recommend.&lt;/p&gt;\n\n&lt;p&gt;My company is undergoing the implementation of a data warehouse. We\u2019re probably middle of the road sized data sets but have decided on using snowflake as our storage. Almost all our transformations will be done in snowflake stored procs as well.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve brought in some consultants for evaluations and each have come back with different options from only using views out of a data lake to more standard approaches.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re considering two of their recommendations, 1 is a kimball star schema and the other is Data vault 2.0.\nI\u2019m fairly familiar with the star schema but one of the consultants is labeling this as a legacy approach and data vault is more modern. I don\u2019t quite understand it because data vault isn\u2019t new. Also, the end state of a vault presents the data in a star schema. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s models are you guys moving forward with? Asa side note, I\u2019d also like to hear some specifics , pros cons, you\u2019ve seen with data vault.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1312r30\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1312r30", "is_robot_indexable": true, "report_reasons": null, "author": "paulypavilion", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1682881813052, "options": [{"text": "Star schema", "id": "22770462"}, {"text": "Snowflake Schema", "id": "22770463"}, {"text": "Data Lake only", "id": "22770464"}, {"text": "Data lake +views", "id": "22770465"}, {"text": "Other", "id": "22770466"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 38, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312r30/dw_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1312r30/dw_architecture/", "subreddit_subscribers": 102910, "created_utc": 1682622613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was given a test task that definitely wants to test my skills in data processing optimization. I have some `.csv` data and need to run several transformations and count statistics on it. \n\nIf I just do stuff with plain pandas on `.csv` files it will be ineffective. Any good tips? I have few ideas like converting `.csv` to parquet or `avro`. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   \n\n\nThe data itself is small and in `.csv` but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.", "author_fullname": "t2_lyf92k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on processing 10M+ records locally with python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312gxj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was given a test task that definitely wants to test my skills in data processing optimization. I have some &lt;code&gt;.csv&lt;/code&gt; data and need to run several transformations and count statistics on it. &lt;/p&gt;\n\n&lt;p&gt;If I just do stuff with plain pandas on &lt;code&gt;.csv&lt;/code&gt; files it will be ineffective. Any good tips? I have few ideas like converting &lt;code&gt;.csv&lt;/code&gt; to parquet or &lt;code&gt;avro&lt;/code&gt;. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   &lt;/p&gt;\n\n&lt;p&gt;The data itself is small and in &lt;code&gt;.csv&lt;/code&gt; but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1312gxj", "is_robot_indexable": true, "report_reasons": null, "author": "chelicerae-aureus", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "subreddit_subscribers": 102910, "created_utc": 1682622124.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "i'm looking for advice to make my development more seamless.\n\nmy idea is, have my IDE tied to wsl (not via ssh -- vm option is out because of vpn), create a project folder, write whatever python scripts and other stuff into there...\n\nthen have it run in a docker container.\n\n&amp;#x200B;\n\nso, the project folder would include a dockerfile, which i would have to build to run the scripts.\n\nAre you guys updating dockerfiles, or are you just creating images out of your containers?\n\nAlso, i noticed it's not really possible to use pipenv when sshing into the container for an interpreter.  So, i'm just looking for detailed examples of how you all do it.", "author_fullname": "t2_5qteskd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what's your dev workflow like", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130tmqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682611503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;m looking for advice to make my development more seamless.&lt;/p&gt;\n\n&lt;p&gt;my idea is, have my IDE tied to wsl (not via ssh -- vm option is out because of vpn), create a project folder, write whatever python scripts and other stuff into there...&lt;/p&gt;\n\n&lt;p&gt;then have it run in a docker container.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;so, the project folder would include a dockerfile, which i would have to build to run the scripts.&lt;/p&gt;\n\n&lt;p&gt;Are you guys updating dockerfiles, or are you just creating images out of your containers?&lt;/p&gt;\n\n&lt;p&gt;Also, i noticed it&amp;#39;s not really possible to use pipenv when sshing into the container for an interpreter.  So, i&amp;#39;m just looking for detailed examples of how you all do it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130tmqy", "is_robot_indexable": true, "report_reasons": null, "author": "iseestupid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130tmqy/whats_your_dev_workflow_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130tmqy/whats_your_dev_workflow_like/", "subreddit_subscribers": 102910, "created_utc": 1682611503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In general which cloud vendor has best data engineering services?\n\nA little bit context about where this question is coming from. So I used MWAA and it has a lot of stability issues. So just wanted to know how is the product experience with different cloud vendors when it comes to data engineering ones", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud vendor with best data engineering/data science tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130link", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682608946.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682600625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In general which cloud vendor has best data engineering services?&lt;/p&gt;\n\n&lt;p&gt;A little bit context about where this question is coming from. So I used MWAA and it has a lot of stability issues. So just wanted to know how is the product experience with different cloud vendors when it comes to data engineering ones&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130link", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130link/cloud_vendor_with_best_data_engineeringdata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130link/cloud_vendor_with_best_data_engineeringdata/", "subreddit_subscribers": 102910, "created_utc": 1682600625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a startup and we need to deploy a presto cluster that can support at least 100QPS. The presto cluster will act as a query engine on top of Pinot. What are the things to consider for the presto cluster deployment on an EC2 instance. The reason for using presto is to have a complete MYSQL_ANSI compatibility which Pinot doesn\u2019t provide right now. Any help is appreciated.", "author_fullname": "t2_icq6ey6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with deploying Presto in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130h06j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682591465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a startup and we need to deploy a presto cluster that can support at least 100QPS. The presto cluster will act as a query engine on top of Pinot. What are the things to consider for the presto cluster deployment on an EC2 instance. The reason for using presto is to have a complete MYSQL_ANSI compatibility which Pinot doesn\u2019t provide right now. Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130h06j", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Wrongdoer-939", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130h06j/need_help_with_deploying_presto_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130h06j/need_help_with_deploying_presto_in_production/", "subreddit_subscribers": 102910, "created_utc": 1682591465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "seems like a killer combo. i went ahead and spun up a poc but things have gone horribly wrong somewhere. \n\n&amp;#x200B;\n\nwhat ive done\n\n\\- created a poc table using spark &amp; iceberg in s3 as parquet files\n\n\\- queried that table in athena\n\n\\- installed trino, setup a connection with iceberg configured to use glue as the backend catalog\n\n\\- the information schema queries work fine\n\n\\- the count(\\*) queries work fine\n\n\\- select queries return NULL for all columns :(\n\n\\- at first i thought this was a compression issue, changing the compression to NONE (correct) didnt change the column values\n\n&amp;#x200B;\n\nwhat should i be looking at? im stoked at the potential but bummed by my initial attempts.", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does anyone have experience with trino on top of an iceberg s3 parquet table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1302wh0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682550899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;seems like a killer combo. i went ahead and spun up a poc but things have gone horribly wrong somewhere. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what ive done&lt;/p&gt;\n\n&lt;p&gt;- created a poc table using spark &amp;amp; iceberg in s3 as parquet files&lt;/p&gt;\n\n&lt;p&gt;- queried that table in athena&lt;/p&gt;\n\n&lt;p&gt;- installed trino, setup a connection with iceberg configured to use glue as the backend catalog&lt;/p&gt;\n\n&lt;p&gt;- the information schema queries work fine&lt;/p&gt;\n\n&lt;p&gt;- the count(*) queries work fine&lt;/p&gt;\n\n&lt;p&gt;- select queries return NULL for all columns :(&lt;/p&gt;\n\n&lt;p&gt;- at first i thought this was a compression issue, changing the compression to NONE (correct) didnt change the column values&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what should i be looking at? im stoked at the potential but bummed by my initial attempts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1302wh0", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1302wh0/does_anyone_have_experience_with_trino_on_top_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1302wh0/does_anyone_have_experience_with_trino_on_top_of/", "subreddit_subscribers": 102910, "created_utc": 1682550899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is Z-Order?\n\nZ-Order is a method of sorting data to cluster data based on multiple fields equally. So instead of sorting by X and then Y, you sort by X and Y equally. This can be great when data is often searched based on X and Y.\n\nHow does this work?\n\nEssentially imagine all the data is sorted into four quadrants, something like this array:\n\n[\n  [], [],\n  [], []\n]\n\nWe sort the data as following:\n\n- If X is 1-50 and Y is 1-50 it will be sorted into the upper left quadrant (first sub-array)\n\n- If X is 51-100 and Y is 1-50 it will be sorted into the upper right quadrant (second sub-array)\n\n- If X is 1-50 and Y is 51-100 it will be sorted into the lower left quadrant (third sub-array)\n\n- If X is 51-100 and Y is 51-100 it will be sorted into the upper left quadrant (fourth sub-array)\n\nBy clustering the data in this manner if I search for data where X is 32 and Y is 58, I can search only the relevant subdivision of clustered data eliminating the other 3 subdivisions from my search saving me lots of time.", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Z-Order?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1310uyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/B9VzKMkx7d-pVMPXFPE2ZJwSzXAU_ty9fK_moYsRXSA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682619292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is Z-Order?&lt;/p&gt;\n\n&lt;p&gt;Z-Order is a method of sorting data to cluster data based on multiple fields equally. So instead of sorting by X and then Y, you sort by X and Y equally. This can be great when data is often searched based on X and Y.&lt;/p&gt;\n\n&lt;p&gt;How does this work?&lt;/p&gt;\n\n&lt;p&gt;Essentially imagine all the data is sorted into four quadrants, something like this array:&lt;/p&gt;\n\n&lt;p&gt;[\n  [], [],\n  [], []\n]&lt;/p&gt;\n\n&lt;p&gt;We sort the data as following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;If X is 1-50 and Y is 1-50 it will be sorted into the upper left quadrant (first sub-array)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If X is 51-100 and Y is 1-50 it will be sorted into the upper right quadrant (second sub-array)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If X is 1-50 and Y is 51-100 it will be sorted into the lower left quadrant (third sub-array)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If X is 51-100 and Y is 51-100 it will be sorted into the upper left quadrant (fourth sub-array)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;By clustering the data in this manner if I search for data where X is 32 and Y is 58, I can search only the relevant subdivision of clustered data eliminating the other 3 subdivisions from my search saving me lots of time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/r73mubi1fiwa1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?auto=webp&amp;v=enabled&amp;s=36c4f2a4f1f8367d856ebfde2aa45c24c184a672", "width": 1280, "height": 720}, "resolutions": [{"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4022b6cca5be0aa17d58f0a99bf48b4e3c1f999a", "width": 108, "height": 60}, {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b4e3b42cccb2bb995150c3b1fe6a787733ac6f0", "width": 216, "height": 121}, {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49b1e0781008c474b79ec58c7ac8baa0bfa3ac6f", "width": 320, "height": 180}, {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af092c6d3abbae936f02f4b9a9da99b607f0bb37", "width": 640, "height": 360}, {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cce0f5fa538fbc81b2f05ac83ab5d44008d1fcff", "width": 960, "height": 540}, {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e705fabdded159b201ad80f7b2b504109eb0755", "width": 1080, "height": 607}], "variants": {}, "id": "uoNoQIEnawZLpQEzA5CKW_ItMzpDn8Ksz7iYEFE0VbI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1310uyl", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1310uyl/what_is_zorder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/r73mubi1fiwa1.jpg", "subreddit_subscribers": 102910, "created_utc": 1682619292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_c7c8zdg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Palantir Foundry \u2014 The Data Operating System That is Not Talked About Enough", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_12zztbq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5Ih5dFe5x2BCLc91lDxIGGG47FF-Ml96k-f9BSRHW9c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682543522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "betterprogramming.pub", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://betterprogramming.pub/palantir-foundry-the-data-operating-system-that-is-not-talked-about-enough-9fb1c98a6b3d", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4hSYoc7TNWQUA9Xao8Bdyyn-KGoSzonDoaYAKY7C3Tg.jpg?auto=webp&amp;v=enabled&amp;s=15ad0a5e1da61398242f4c548a4dcc22979ab957", "width": 1055, "height": 578}, "resolutions": [{"url": "https://external-preview.redd.it/4hSYoc7TNWQUA9Xao8Bdyyn-KGoSzonDoaYAKY7C3Tg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=27c8800bc8700d5161886e7c03d1505af59281e7", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/4hSYoc7TNWQUA9Xao8Bdyyn-KGoSzonDoaYAKY7C3Tg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b0165e5ae81d3643931193d158de7dedea374ea", "width": 216, "height": 118}, {"url": "https://external-preview.redd.it/4hSYoc7TNWQUA9Xao8Bdyyn-KGoSzonDoaYAKY7C3Tg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c392555d2d7ca68ae8a768ad50c256d3ebac6cf", "width": 320, "height": 175}, {"url": "https://external-preview.redd.it/4hSYoc7TNWQUA9Xao8Bdyyn-KGoSzonDoaYAKY7C3Tg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=418471f9d1ea9531a78db6941825d4434fedf806", "width": 640, "height": 350}, {"url": "https://external-preview.redd.it/4hSYoc7TNWQUA9Xao8Bdyyn-KGoSzonDoaYAKY7C3Tg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a023e76cd9e589b043536732996a99f7c23d47e", "width": 960, "height": 525}], "variants": {}, "id": "iBTvcZmfxGEoSgv1R4FPj84mgaweZuK84dosMNhOxD4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12zztbq", "is_robot_indexable": true, "report_reasons": null, "author": "heyimsinged", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12zztbq/palantir_foundry_the_data_operating_system_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://betterprogramming.pub/palantir-foundry-the-data-operating-system-that-is-not-talked-about-enough-9fb1c98a6b3d", "subreddit_subscribers": 102910, "created_utc": 1682543522.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}