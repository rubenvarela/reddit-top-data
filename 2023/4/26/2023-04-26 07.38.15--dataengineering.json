{"kind": "Listing", "data": {"after": "t3_12yy0tq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work with a colleague who I swear to fucking god has the worst practices ever.  The stuff they do should be considered war crimes (in the data world, of course).\n\nThe person in question has never done CI/CD before and has been copying and pasting blocks of code into the next environment when they want to \"promote\" a change.  Naturally, this has left environments being massively out of sync.  \n\nSince then, we've implemented CI/CD to move database meta data and schemas between environments.  It's consistent, it works, it's better than copying and pasting.  The person in question, however, always rejects the idea of CI/CD with the following talking point:\n\nThem: \"We can't guarantee consistency between environments\"\n\nMe: \"We can because that's sorted through CI/CD\"\n\nThem: \"Yeah, but when I want to make emergency changes to production, it won't align\"\n\nMe: \"But why are we making changes in production directly? They'll get erased next time we release because they don't exist in the previous environments.  For us to be consistent, we have to be in the mindset of working through environments\"\n\nThem: \"I don't want to have to go through every single environment to make a single change\"\n\nThey will then continue to argue forever.  For some strange reason, this happens a lot and I feel like I'm going insane because I feel like it shouldn't be up for debate.  I feel like the person I'm arguing with wants to patch everything, I want to do everything correctly.  They want to apply ad hoc fixes as and when they feel like it, I want to be consistent so when something breaks we can test it at a lower level before releasing it. \n\nI'm 100% ready to be wrong.  Do people regularly makes production only changes?  Am I misunderstanding the whole point of CI/CD? How do you guys facilitate emergency changes with CI/CD?\n\nThank you.", "author_fullname": "t2_anttcncw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should production changes be handled? Rant/debate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yj6uv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682428694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work with a colleague who I swear to fucking god has the worst practices ever.  The stuff they do should be considered war crimes (in the data world, of course).&lt;/p&gt;\n\n&lt;p&gt;The person in question has never done CI/CD before and has been copying and pasting blocks of code into the next environment when they want to &amp;quot;promote&amp;quot; a change.  Naturally, this has left environments being massively out of sync.  &lt;/p&gt;\n\n&lt;p&gt;Since then, we&amp;#39;ve implemented CI/CD to move database meta data and schemas between environments.  It&amp;#39;s consistent, it works, it&amp;#39;s better than copying and pasting.  The person in question, however, always rejects the idea of CI/CD with the following talking point:&lt;/p&gt;\n\n&lt;p&gt;Them: &amp;quot;We can&amp;#39;t guarantee consistency between environments&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Me: &amp;quot;We can because that&amp;#39;s sorted through CI/CD&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Them: &amp;quot;Yeah, but when I want to make emergency changes to production, it won&amp;#39;t align&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Me: &amp;quot;But why are we making changes in production directly? They&amp;#39;ll get erased next time we release because they don&amp;#39;t exist in the previous environments.  For us to be consistent, we have to be in the mindset of working through environments&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Them: &amp;quot;I don&amp;#39;t want to have to go through every single environment to make a single change&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;They will then continue to argue forever.  For some strange reason, this happens a lot and I feel like I&amp;#39;m going insane because I feel like it shouldn&amp;#39;t be up for debate.  I feel like the person I&amp;#39;m arguing with wants to patch everything, I want to do everything correctly.  They want to apply ad hoc fixes as and when they feel like it, I want to be consistent so when something breaks we can test it at a lower level before releasing it. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m 100% ready to be wrong.  Do people regularly makes production only changes?  Am I misunderstanding the whole point of CI/CD? How do you guys facilitate emergency changes with CI/CD?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yj6uv", "is_robot_indexable": true, "report_reasons": null, "author": "average_ukpf_user", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yj6uv/how_should_production_changes_be_handled/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yj6uv/how_should_production_changes_be_handled/", "subreddit_subscribers": 102566, "created_utc": 1682428694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4adbh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Testing PySpark Applications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 34, "top_awarded_type": null, "hide_score": false, "name": "t3_12ye4l1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QjLXhadh-xvOrYNnWYyVW2RRmD1MWktV_rke3is5-iU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682415084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/task-group/how-to-trust-your-data-pipelines-best-practices-for-testing-pyspark-applications-5ba05a36aa00", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?auto=webp&amp;v=enabled&amp;s=27424ecddb2e238018a2cc5020714c27f8f8b363", "width": 1170, "height": 288}, "resolutions": [{"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0513419deb358ee72bd0acb12e168240aa6a57c", "width": 108, "height": 26}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fac812cf850bae4f91a32d31f9c04efb5d70991e", "width": 216, "height": 53}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8eef8d3fa75ea5e0575d649ab41b004d7f54498c", "width": 320, "height": 78}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6396cd57992c0cd585f0ed05a217363ac0568ec2", "width": 640, "height": 157}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0654666e06f321826ebc19488fb6a9eee6cb7d3", "width": 960, "height": 236}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90fb38384969478b4d5aa3eb78e9deb1adfa1b5f", "width": 1080, "height": 265}], "variants": {}, "id": "wor6e_3wpAKnOt_wKnLAlSwcO58lUWH1czL9DxiFiX8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ye4l1", "is_robot_indexable": true, "report_reasons": null, "author": "alfet", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ye4l1/best_practices_for_testing_pyspark_applications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/task-group/how-to-trust-your-data-pipelines-best-practices-for-testing-pyspark-applications-5ba05a36aa00", "subreddit_subscribers": 102566, "created_utc": 1682415084.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_20tfe7ur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering and DataOps: A Beginner's Guide to Building Data Solutions and Solving Real-World Challenges", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_12ybcqo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JspNOjLI_4EFW2mFS5IQe8mgrct34voOY_55DCLLYrg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682406275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "chaosgenius.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.chaosgenius.io/blog/data-engineering-and-dataops-beginners-guide-to-building-data-solutions-and-solving-real-world-challenges/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?auto=webp&amp;v=enabled&amp;s=0c672d9f9def968617df5e594108289933769bcc", "width": 4000, "height": 2400}, "resolutions": [{"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94de9e67438d561dac53f6aa67f74ed81686c74a", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b39fc1e891c97928b67b768cfb2a44364d9581b", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1409b7ec75b01fb8e58450c13bb2c52986bf9b7e", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c9ccd575d261751cc267eea527f39ba69aeff8c", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa23b5cc27d8fca03ea360638f9a1ac90eecd9ee", "width": 960, "height": 576}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98e38fea795867bfeae0e7c41c32c6ab31afa893", "width": 1080, "height": 648}], "variants": {}, "id": "eN31HKOXDX_DK4Vg5fj3aDAKHFmxW72mKecuYjio5Wo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ybcqo", "is_robot_indexable": true, "report_reasons": null, "author": "pramit_marattha", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ybcqo/data_engineering_and_dataops_a_beginners_guide_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.chaosgenius.io/blog/data-engineering-and-dataops-beginners-guide-to-building-data-solutions-and-solving-real-world-challenges/", "subreddit_subscribers": 102566, "created_utc": 1682406275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're looking at stuff like Synapse/trino/presto for the engine but for now we're just loading raw json + converting to parquet.\n\nIt's time series data so it partitions nicely along an hourly cadence. This creates files that are about 100Mb. We are about to expand by quite a lot (acquisition) so we think this will ~5x soon enough. \n\nFrom those raw json (500Mb) files we can convert to parquet for the query layer but unsure of how big we ought to go here. We can leave as is and rely on parquets/compression efficiency or compact several hours into a single file. \n\nDon't have a lot of know-how for this so we appreciate any tips. Also, we know a simple set up with a DB might do for this volume of data but the chosen paradigm for the company is data lake + query engine, and our department is just the first to build it out. Thanks.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How big are your data lake raw files? Parquet files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yknp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682432040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re looking at stuff like Synapse/trino/presto for the engine but for now we&amp;#39;re just loading raw json + converting to parquet.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s time series data so it partitions nicely along an hourly cadence. This creates files that are about 100Mb. We are about to expand by quite a lot (acquisition) so we think this will ~5x soon enough. &lt;/p&gt;\n\n&lt;p&gt;From those raw json (500Mb) files we can convert to parquet for the query layer but unsure of how big we ought to go here. We can leave as is and rely on parquets/compression efficiency or compact several hours into a single file. &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t have a lot of know-how for this so we appreciate any tips. Also, we know a simple set up with a DB might do for this volume of data but the chosen paradigm for the company is data lake + query engine, and our department is just the first to build it out. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yknp3", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yknp3/how_big_are_your_data_lake_raw_files_parquet_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yknp3/how_big_are_your_data_lake_raw_files_parquet_files/", "subreddit_subscribers": 102566, "created_utc": 1682432040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "From PLC/sensor level  all the way up to reporting and visuals, what tools do you guys use?\n\nMy company uses the following:\n\n* Kepware\n* SQL\n* Power BI\n\nThat's it. Nothing else. I'm being tasked with helping on all kinds of Industry 4.0 initiatives but it seems like everything is already hitting its cap and we've barely gotten started.\n\nKepware VM is at max capacity which should hopefully be as easy as migrating to a more powerful VM. SQL gateways for Power BI, Power Apps, Flows, etc. seems to be failing. No one can give me a root cause and I don't have access myself. Power BI refreshes get slower and slower by the day.\n\nLuckily, I think we will be getting OSI PI soon which should do wonders. \n\nHas anyone else experienced this and if so what did your manufacturing stack look like. How did it eventually improve....please tell me it eventually improved.", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone work in the Industry 4.0 space? More specifically in manufacturing? What does your data stack look like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yw3x2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682456852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From PLC/sensor level  all the way up to reporting and visuals, what tools do you guys use?&lt;/p&gt;\n\n&lt;p&gt;My company uses the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Kepware&lt;/li&gt;\n&lt;li&gt;SQL&lt;/li&gt;\n&lt;li&gt;Power BI&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That&amp;#39;s it. Nothing else. I&amp;#39;m being tasked with helping on all kinds of Industry 4.0 initiatives but it seems like everything is already hitting its cap and we&amp;#39;ve barely gotten started.&lt;/p&gt;\n\n&lt;p&gt;Kepware VM is at max capacity which should hopefully be as easy as migrating to a more powerful VM. SQL gateways for Power BI, Power Apps, Flows, etc. seems to be failing. No one can give me a root cause and I don&amp;#39;t have access myself. Power BI refreshes get slower and slower by the day.&lt;/p&gt;\n\n&lt;p&gt;Luckily, I think we will be getting OSI PI soon which should do wonders. &lt;/p&gt;\n\n&lt;p&gt;Has anyone else experienced this and if so what did your manufacturing stack look like. How did it eventually improve....please tell me it eventually improved.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yw3x2", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yw3x2/anyone_work_in_the_industry_40_space_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yw3x2/anyone_work_in_the_industry_40_space_more/", "subreddit_subscribers": 102566, "created_utc": 1682456852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, my organization has been using O365 with the power platform for a minute now. We've got some real MVPs on our team who are treating SharePoint like a database for their power apps (\"front-end\") and power bi reports with power automate work around. But, here's the kicker - the company doesn't want to invest in a proper database like SQL server or full Datavers. \n\nPersonally, I think this is a recipe for disaster. I mean, SharePoint is great and all, but using it as a \"database\" just doesn't seem sustainable. What are your thoughts? Should we continue to use SharePoint or should we explore other options? Let me know in the comments below.", "author_fullname": "t2_8bvav827h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hey guys, so I work for an organization and we're thinking about using SharePoint as a \"database.\" Do you guys have any tips or recommendations on how to make this work? We're kind of new to SharePoint so any advice would be greatly appreciated! Thanks in advance.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yw1ul", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682456731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, my organization has been using O365 with the power platform for a minute now. We&amp;#39;ve got some real MVPs on our team who are treating SharePoint like a database for their power apps (&amp;quot;front-end&amp;quot;) and power bi reports with power automate work around. But, here&amp;#39;s the kicker - the company doesn&amp;#39;t want to invest in a proper database like SQL server or full Datavers. &lt;/p&gt;\n\n&lt;p&gt;Personally, I think this is a recipe for disaster. I mean, SharePoint is great and all, but using it as a &amp;quot;database&amp;quot; just doesn&amp;#39;t seem sustainable. What are your thoughts? Should we continue to use SharePoint or should we explore other options? Let me know in the comments below.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yw1ul", "is_robot_indexable": true, "report_reasons": null, "author": "CassiusBlackwood", "discussion_type": null, "num_comments": 58, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yw1ul/hey_guys_so_i_work_for_an_organization_and_were/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yw1ul/hey_guys_so_i_work_for_an_organization_and_were/", "subreddit_subscribers": 102566, "created_utc": 1682456731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Essentially title. Is it left up to the data architects? Principal/staff engineers?", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who sets the pipeline/warehouse strategy and roadmap in your organization?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12z1s36", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682470743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Essentially title. Is it left up to the data architects? Principal/staff engineers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12z1s36", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12z1s36/who_sets_the_pipelinewarehouse_strategy_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12z1s36/who_sets_the_pipelinewarehouse_strategy_and/", "subreddit_subscribers": 102566, "created_utc": 1682470743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What skills would you consider soft skills as a DE?\nThere are some obvious ones like;\n- Communication\n- Stakeholder management\n- Writing (mails, documentation, guidelines)\n- Listening to client requests\n- Peer reviews \n\nBut what about some less obvious, would you consider them to be closer to soft skills or hard skills ones like;\n- Making a proper code review (so that others can review easier)\n- Giving proper code review feedback\n- Improving productivity\n\nOr would you say there is an extra list of skills between soft and hard skills?\n\nWhat are your struggles with these types of skills?", "author_fullname": "t2_1jkhpl2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Soft skills aimed at DE's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yw8c8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682457116.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What skills would you consider soft skills as a DE?\nThere are some obvious ones like;\n- Communication\n- Stakeholder management\n- Writing (mails, documentation, guidelines)\n- Listening to client requests\n- Peer reviews &lt;/p&gt;\n\n&lt;p&gt;But what about some less obvious, would you consider them to be closer to soft skills or hard skills ones like;\n- Making a proper code review (so that others can review easier)\n- Giving proper code review feedback\n- Improving productivity&lt;/p&gt;\n\n&lt;p&gt;Or would you say there is an extra list of skills between soft and hard skills?&lt;/p&gt;\n\n&lt;p&gt;What are your struggles with these types of skills?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yw8c8", "is_robot_indexable": true, "report_reasons": null, "author": "Luxi36", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yw8c8/soft_skills_aimed_at_des/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yw8c8/soft_skills_aimed_at_des/", "subreddit_subscribers": 102566, "created_utc": 1682457116.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are thinking of adopting snowpipe to ingest data from Kafka to Snowflake (currently using Fivetran). Would be happy for some homes reviews.", "author_fullname": "t2_19dlf55q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are you using Snowpipe? What\u2019s its pros and cons?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ytogu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682451730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are thinking of adopting snowpipe to ingest data from Kafka to Snowflake (currently using Fivetran). Would be happy for some homes reviews.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ytogu", "is_robot_indexable": true, "report_reasons": null, "author": "themo_legrange", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ytogu/are_you_using_snowpipe_whats_its_pros_and_cons/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ytogu/are_you_using_snowpipe_whats_its_pros_and_cons/", "subreddit_subscribers": 102566, "created_utc": 1682451730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Question:\n\nWhen you\u2019re building the databricks architecture utilizing the medallion model,\nWhere are your bronze, silver, gold tier delta files sitting?\n\nCurrently we have bronze tier data in AZDL Gen 2 , loading those files into a databricks DF, doing transformations, then creating delta files into a silver layer in AZDL gen 2.  \n\nWould it make more sense to not have the delta files in AZDL but in the databricks file system? \n\nOR would it make sense to load the silver files into the databricks file system then copy them into the AZDL. \n\nDatabricks is relatively new to me but we are starting to shift away from synapse", "author_fullname": "t2_uf4ne7uq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yw8co", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682457116.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question:&lt;/p&gt;\n\n&lt;p&gt;When you\u2019re building the databricks architecture utilizing the medallion model,\nWhere are your bronze, silver, gold tier delta files sitting?&lt;/p&gt;\n\n&lt;p&gt;Currently we have bronze tier data in AZDL Gen 2 , loading those files into a databricks DF, doing transformations, then creating delta files into a silver layer in AZDL gen 2.  &lt;/p&gt;\n\n&lt;p&gt;Would it make more sense to not have the delta files in AZDL but in the databricks file system? &lt;/p&gt;\n\n&lt;p&gt;OR would it make sense to load the silver files into the databricks file system then copy them into the AZDL. &lt;/p&gt;\n\n&lt;p&gt;Databricks is relatively new to me but we are starting to shift away from synapse&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yw8co", "is_robot_indexable": true, "report_reasons": null, "author": "NipsAhoy2", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yw8co/databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yw8co/databricks/", "subreddit_subscribers": 102566, "created_utc": 1682457116.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello.  Soon I will start my first project as a sort of de (I dont want to  overestimate my duties, will no code I guess, use only Azure?) with  components of Azure Data Factory, Azure Synapse, Data Lake, Blob Storage  and Databricks.\n\nWe will be taking  data from on prem I guess or they will already sort of deliver it in  any format, we will be moving these into external database, I hope its  going to be Azure SQL, forget the graphical representation.\n\nI  would like to ask you for all of the tips of what to look after / for,  possible traps, obvious rookie mistakes and any protips how to handle  the project overall. Will have no help in that part (ingestion and  transformation). Currently running through documentation and some  youtube tutorials\n\nThank you kindly in advance!", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First project - tips from more experienced folks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yd8a8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682412236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.  Soon I will start my first project as a sort of de (I dont want to  overestimate my duties, will no code I guess, use only Azure?) with  components of Azure Data Factory, Azure Synapse, Data Lake, Blob Storage  and Databricks.&lt;/p&gt;\n\n&lt;p&gt;We will be taking  data from on prem I guess or they will already sort of deliver it in  any format, we will be moving these into external database, I hope its  going to be Azure SQL, forget the graphical representation.&lt;/p&gt;\n\n&lt;p&gt;I  would like to ask you for all of the tips of what to look after / for,  possible traps, obvious rookie mistakes and any protips how to handle  the project overall. Will have no help in that part (ingestion and  transformation). Currently running through documentation and some  youtube tutorials&lt;/p&gt;\n\n&lt;p&gt;Thank you kindly in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yd8a8", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yd8a8/first_project_tips_from_more_experienced_folks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yd8a8/first_project_tips_from_more_experienced_folks/", "subreddit_subscribers": 102566, "created_utc": 1682412236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently interviewed for a role and the hiring manager mentioned there would be system-design interview questions. I have traditionally been more of an analytically focused DE (close to an analytics engineer role) and have little experience with system design questions. Does anyone have advice on where I should start for interview prep? Is going through Grokking the System Design Interview enough? Does anyone have any experience ramping up on this topic within 3 weeks? For context, I am still early in my career, so this would likely be a junior-level role since I have less than 3 YOE.", "author_fullname": "t2_6wqif47u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep Advice - System Design - Where to start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yk9r6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682431175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently interviewed for a role and the hiring manager mentioned there would be system-design interview questions. I have traditionally been more of an analytically focused DE (close to an analytics engineer role) and have little experience with system design questions. Does anyone have advice on where I should start for interview prep? Is going through Grokking the System Design Interview enough? Does anyone have any experience ramping up on this topic within 3 weeks? For context, I am still early in my career, so this would likely be a junior-level role since I have less than 3 YOE.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12yk9r6", "is_robot_indexable": true, "report_reasons": null, "author": "tagavor_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yk9r6/interview_prep_advice_system_design_where_to_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yk9r6/interview_prep_advice_system_design_where_to_start/", "subreddit_subscribers": 102566, "created_utc": 1682431175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have all seen the layoff announcements from various MDS vendors.  This week, I found out my main customer success contact at one vendor is no longer there.   I know vendors lurk here on reddit.  Can anybody share the inside scoop and what we should anticipate next?  What's the vibe?", "author_fullname": "t2_7yk2o6hxn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inside scoop from Data Engineering vendors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12z7jn7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682486823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have all seen the layoff announcements from various MDS vendors.  This week, I found out my main customer success contact at one vendor is no longer there.   I know vendors lurk here on reddit.  Can anybody share the inside scoop and what we should anticipate next?  What&amp;#39;s the vibe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12z7jn7", "is_robot_indexable": true, "report_reasons": null, "author": "grahamdietz", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12z7jn7/inside_scoop_from_data_engineering_vendors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12z7jn7/inside_scoop_from_data_engineering_vendors/", "subreddit_subscribers": 102566, "created_utc": 1682486823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Everyone,\n\nIn one of my projects I\u2019m using python to connect to a SFTP server and use SSH keys to authenticate myself. All of a sudden the client SFTP server started showing errors and wasn\u2019t accepting SSH Keys. \u201cPAM Authentication\u201d shows up. \n\nThe SFTP is managed by another vendor and they identified that this is due to their SSH being backdated. While they\u2019re fixing that, they gave me password for PAM Authentication. I can log in using CLI. \n\nI just don\u2019t understand how to use this password with python/paramiko. I feel this password is definitely different than the username and password combo for the server. \n\nIs there anyone who wrote some python scripts for connection using PAM password?\n\nThanks!", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Ingestion from SFTP Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yxwtg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682460874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt;\n\n&lt;p&gt;In one of my projects I\u2019m using python to connect to a SFTP server and use SSH keys to authenticate myself. All of a sudden the client SFTP server started showing errors and wasn\u2019t accepting SSH Keys. \u201cPAM Authentication\u201d shows up. &lt;/p&gt;\n\n&lt;p&gt;The SFTP is managed by another vendor and they identified that this is due to their SSH being backdated. While they\u2019re fixing that, they gave me password for PAM Authentication. I can log in using CLI. &lt;/p&gt;\n\n&lt;p&gt;I just don\u2019t understand how to use this password with python/paramiko. I feel this password is definitely different than the username and password combo for the server. &lt;/p&gt;\n\n&lt;p&gt;Is there anyone who wrote some python scripts for connection using PAM password?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yxwtg", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yxwtg/data_ingestion_from_sftp_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yxwtg/data_ingestion_from_sftp_server/", "subreddit_subscribers": 102566, "created_utc": 1682460874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently in a position where I do a lot of the data modeling for report purposes. It's at a new company for me but in an industry I know well so I'm pretty familiar with the data I'm working with. My experience is mostly in analytics though with only the last year or so being more engineer focused so I'm still learning a lot.  \n\n\nI feel like I spend an inordinate amount of time just trying to unravel all our data models so that I can trace something from source system through to business report. I do this because we have a lot of data integrity questions that come up and to help me better understand the process. We have layers and layers of built in logic even for the most straightforward measures. \n\nI think what I'm seeing, on top of the data integrity concerns, as well as consistent integration breakdowns, speaks to a poorly designed warehouse but I don't feel like I have enough, except my experience, to go off of. I know it's not like anything I've dealt with in my past though. What questions do I need to be asking, or what else can I be looking at to help me better understand where our issues are? Any ideas or thoughts that can help point me in the right direction are appreciated!", "author_fullname": "t2_2q171de9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm spending a lot of time unraveling data models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yt7e4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682450727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently in a position where I do a lot of the data modeling for report purposes. It&amp;#39;s at a new company for me but in an industry I know well so I&amp;#39;m pretty familiar with the data I&amp;#39;m working with. My experience is mostly in analytics though with only the last year or so being more engineer focused so I&amp;#39;m still learning a lot.  &lt;/p&gt;\n\n&lt;p&gt;I feel like I spend an inordinate amount of time just trying to unravel all our data models so that I can trace something from source system through to business report. I do this because we have a lot of data integrity questions that come up and to help me better understand the process. We have layers and layers of built in logic even for the most straightforward measures. &lt;/p&gt;\n\n&lt;p&gt;I think what I&amp;#39;m seeing, on top of the data integrity concerns, as well as consistent integration breakdowns, speaks to a poorly designed warehouse but I don&amp;#39;t feel like I have enough, except my experience, to go off of. I know it&amp;#39;s not like anything I&amp;#39;ve dealt with in my past though. What questions do I need to be asking, or what else can I be looking at to help me better understand where our issues are? Any ideas or thoughts that can help point me in the right direction are appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12yt7e4", "is_robot_indexable": true, "report_reasons": null, "author": "lahma_mama", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yt7e4/im_spending_a_lot_of_time_unraveling_data_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yt7e4/im_spending_a_lot_of_time_unraveling_data_models/", "subreddit_subscribers": 102566, "created_utc": 1682450727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, \nI have started a new role recently where I am at a large legacy manufacturing company that is trying to get into analytics (lol). I was hired to do data engineering but it seems like most of what i will be doing to start off is working on getting their data policies in order. They have been using SAP for 30 years and are trying to modernize somewhat.  Currently at their plants they have analytics on site that are running based on production data and from my view those are all fine. However in their administration building is where they need a shitload of work. Basically they have always had web devs or SAP devs and never really had anyone in admin who was a DBA or trained in that regard. From just my first month i have a few things that I know need to change and was wondering if anyone had advice.\n\n1. Every project was made as its own database. Is there a way to quickly go back and put all of these projects into one database but under different schemas? To put it in perspective one of their sql servers has 55 databases.\n\n2. How do you get people to buy into data governance as a policy when it really hasnt been enforced before? Right now everyone has access to everything and its kind of a wild west. \n\n3. They currently have a few people who are very good at getting data out of SAP BW for queries. However this creates problems because the data analytics people arent really SAP trained and the data gets lost in translation over time, not to mention the bottleneck of always having to ask one of the couple people who understand SAP BW. Does anyone have experience in creating an analytics database in a third party program off of an SAP ERP system? There is really no getting rid of SAP in any capacity when its been used this long.", "author_fullname": "t2_6ra3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for new role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yl02m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682432808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, \nI have started a new role recently where I am at a large legacy manufacturing company that is trying to get into analytics (lol). I was hired to do data engineering but it seems like most of what i will be doing to start off is working on getting their data policies in order. They have been using SAP for 30 years and are trying to modernize somewhat.  Currently at their plants they have analytics on site that are running based on production data and from my view those are all fine. However in their administration building is where they need a shitload of work. Basically they have always had web devs or SAP devs and never really had anyone in admin who was a DBA or trained in that regard. From just my first month i have a few things that I know need to change and was wondering if anyone had advice.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Every project was made as its own database. Is there a way to quickly go back and put all of these projects into one database but under different schemas? To put it in perspective one of their sql servers has 55 databases.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How do you get people to buy into data governance as a policy when it really hasnt been enforced before? Right now everyone has access to everything and its kind of a wild west. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;They currently have a few people who are very good at getting data out of SAP BW for queries. However this creates problems because the data analytics people arent really SAP trained and the data gets lost in translation over time, not to mention the bottleneck of always having to ask one of the couple people who understand SAP BW. Does anyone have experience in creating an analytics database in a third party program off of an SAP ERP system? There is really no getting rid of SAP in any capacity when its been used this long.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yl02m", "is_robot_indexable": true, "report_reasons": null, "author": "deemerritt", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yl02m/advice_for_new_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yl02m/advice_for_new_role/", "subreddit_subscribers": 102566, "created_utc": 1682432808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bunch of sites across the country collecting data.\n\nEach piece of equipment has a battery and a .txt file recording the battery level every minute, appending the record to the site .txt file. So, a file that is growing one record every minute.\n\nThe equipment is managed by an external company, so i don't have control to change the way the .txt add records.\n\nMy team want to write this data to a database (BigQuery) and i can't think of an efficient way to do this other than searching and deleting all the records for the site and re-writing them to the database. As each file may contains 200k records and doing a query to write only new records will, i guess, be slow.\n\nAny suggestions or am patterns i could use? And suggestions would be awesome. Thanks :-)", "author_fullname": "t2_qd6ssd6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's an efficient write to DB a file that's continuously being appended to?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yjk8d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682429548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of sites across the country collecting data.&lt;/p&gt;\n\n&lt;p&gt;Each piece of equipment has a battery and a .txt file recording the battery level every minute, appending the record to the site .txt file. So, a file that is growing one record every minute.&lt;/p&gt;\n\n&lt;p&gt;The equipment is managed by an external company, so i don&amp;#39;t have control to change the way the .txt add records.&lt;/p&gt;\n\n&lt;p&gt;My team want to write this data to a database (BigQuery) and i can&amp;#39;t think of an efficient way to do this other than searching and deleting all the records for the site and re-writing them to the database. As each file may contains 200k records and doing a query to write only new records will, i guess, be slow.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or am patterns i could use? And suggestions would be awesome. Thanks :-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12yjk8d", "is_robot_indexable": true, "report_reasons": null, "author": "Careful-Doughnut-59", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yjk8d/whats_an_efficient_write_to_db_a_file_thats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yjk8d/whats_an_efficient_write_to_db_a_file_thats/", "subreddit_subscribers": 102566, "created_utc": 1682429548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am learning Aws glue but it's very expensive, so  I need a way that I can test it on local first.", "author_fullname": "t2_8szkmsobv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Can I test AWS glue jobs on local?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12z8375", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682488509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning Aws glue but it&amp;#39;s very expensive, so  I need a way that I can test it on local first.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12z8375", "is_robot_indexable": true, "report_reasons": null, "author": "neutronajs", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12z8375/how_can_i_test_aws_glue_jobs_on_local/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12z8375/how_can_i_test_aws_glue_jobs_on_local/", "subreddit_subscribers": 102566, "created_utc": 1682488509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got offered to take a second job by a colleague and I'll be doing it on my own free time. However the job is more of a BI role (doing analysis and data visualization). I also have access to tons of learning materials in my current company and may take some certifications when I want. \n\nSo basically I am torn on what to do between the two on my free time. Looking for advice from wiser people here. Thank you.\n\nTake second job = more money\n\nPrioritize learning = more DE knowledge", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Take a second job or study more", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12z7u43", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682487727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got offered to take a second job by a colleague and I&amp;#39;ll be doing it on my own free time. However the job is more of a BI role (doing analysis and data visualization). I also have access to tons of learning materials in my current company and may take some certifications when I want. &lt;/p&gt;\n\n&lt;p&gt;So basically I am torn on what to do between the two on my free time. Looking for advice from wiser people here. Thank you.&lt;/p&gt;\n\n&lt;p&gt;Take second job = more money&lt;/p&gt;\n\n&lt;p&gt;Prioritize learning = more DE knowledge&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12z7u43", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12z7u43/take_a_second_job_or_study_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12z7u43/take_a_second_job_or_study_more/", "subreddit_subscribers": 102566, "created_utc": 1682487727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I was tasked to build a CICD pipeline for Synapse. I want to be able to deploy from Dev to different environment. I found the Sqlpackage, where it extract the schema of a workspace and publish it to a destination workspace. I'm not sure but I think it is not the right approach since it is not parametrizable and throwing error when deploying external views. So far it worked only with tables.\nAny idea how can I build it? Thank you in advance", "author_fullname": "t2_pp0zirow", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD for Synapse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ysvyg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682450021.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I was tasked to build a CICD pipeline for Synapse. I want to be able to deploy from Dev to different environment. I found the Sqlpackage, where it extract the schema of a workspace and publish it to a destination workspace. I&amp;#39;m not sure but I think it is not the right approach since it is not parametrizable and throwing error when deploying external views. So far it worked only with tables.\nAny idea how can I build it? Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ysvyg", "is_robot_indexable": true, "report_reasons": null, "author": "These_Rip_9327", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ysvyg/cicd_for_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ysvyg/cicd_for_synapse/", "subreddit_subscribers": 102566, "created_utc": 1682450021.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got assigned to a new project which uses AWS stack and noticed that they are only using Glue to call stored procedures or run queries in external database. From what I know, Glue is just spark on the backend so by not using spark they are not using Glue to its full capabilities. So my question is, is this an acceptable way of using Glue?", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Glue not fully utilized?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yrhwz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682446925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got assigned to a new project which uses AWS stack and noticed that they are only using Glue to call stored procedures or run queries in external database. From what I know, Glue is just spark on the backend so by not using spark they are not using Glue to its full capabilities. So my question is, is this an acceptable way of using Glue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12yrhwz", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yrhwz/aws_glue_not_fully_utilized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yrhwz/aws_glue_not_fully_utilized/", "subreddit_subscribers": 102566, "created_utc": 1682446925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Business Value of Metadata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_12you5a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EHES8eKB3vtBgsdAesjBcGWniy51TpSivgMmve21LL8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682441175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/alvin-ai/the-business-value-of-metadata-efa46f78a6da", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gsDwIaxoZmwWR2kIqWLqJ87JJ3rA5zy-WAoF82Jz-wU.jpg?auto=webp&amp;v=enabled&amp;s=52a7f0a1a29f6fcbed6044190f4419d8c7db6ea9", "width": 1200, "height": 692}, "resolutions": [{"url": "https://external-preview.redd.it/gsDwIaxoZmwWR2kIqWLqJ87JJ3rA5zy-WAoF82Jz-wU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1fae95481cfa84bed915d1d20480d4fb030c472", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/gsDwIaxoZmwWR2kIqWLqJ87JJ3rA5zy-WAoF82Jz-wU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50b80bbe9a84be94b24656c94868a1bd76babac1", "width": 216, "height": 124}, {"url": "https://external-preview.redd.it/gsDwIaxoZmwWR2kIqWLqJ87JJ3rA5zy-WAoF82Jz-wU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02375c4beeb71f2bc69588ec37b949cf2937ea4e", "width": 320, "height": 184}, {"url": "https://external-preview.redd.it/gsDwIaxoZmwWR2kIqWLqJ87JJ3rA5zy-WAoF82Jz-wU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=355acf7fd8c020eee63b5a36e58fc66588eb49b4", "width": 640, "height": 369}, {"url": "https://external-preview.redd.it/gsDwIaxoZmwWR2kIqWLqJ87JJ3rA5zy-WAoF82Jz-wU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=884a6f268b31cb73bf1f290b1da30f8d16c416e5", "width": 960, "height": 553}, {"url": "https://external-preview.redd.it/gsDwIaxoZmwWR2kIqWLqJ87JJ3rA5zy-WAoF82Jz-wU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d1f8f55607a0a4e8a56baf818ded87cc76159e5", "width": 1080, "height": 622}], "variants": {}, "id": "Mhev0XFTT0iV1DglMqIEg298usdpo1bM03JWxEvDc9A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12you5a", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12you5a/the_business_value_of_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/alvin-ai/the-business-value-of-metadata-efa46f78a6da", "subreddit_subscribers": 102566, "created_utc": 1682441175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Morning guys,\n\nI\u2019m not very knowledgeable on CS and needed all of your opinions/expertise:\n\nI have a grad course or two that requires a windows OS. I only have a MacBook (M2 max) and a windows PC (AMD/GTX3070).\n\nI don\u2019t want to purchase a windows laptop just for this course. Is remoting into my home desktop a terrible idea? What are the setbacks? \n\n(I\u2019ve heard internet speed could be a bottle neck. But honestly, what speed would I want for this? I\u2019m looking for specifics so I can be the most prepared for this)", "author_fullname": "t2_3pqlnc5y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RDP MacBook to Windows PC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yhkpc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682424757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Morning guys,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not very knowledgeable on CS and needed all of your opinions/expertise:&lt;/p&gt;\n\n&lt;p&gt;I have a grad course or two that requires a windows OS. I only have a MacBook (M2 max) and a windows PC (AMD/GTX3070).&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t want to purchase a windows laptop just for this course. Is remoting into my home desktop a terrible idea? What are the setbacks? &lt;/p&gt;\n\n&lt;p&gt;(I\u2019ve heard internet speed could be a bottle neck. But honestly, what speed would I want for this? I\u2019m looking for specifics so I can be the most prepared for this)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12yhkpc", "is_robot_indexable": true, "report_reasons": null, "author": "RealTrashyC", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yhkpc/rdp_macbook_to_windows_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yhkpc/rdp_macbook_to_windows_pc/", "subreddit_subscribers": 102566, "created_utc": 1682424757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I just changed teams internally in my company, and want to know which is the best practice to follow here. In my previous team, we use local development for our DEV env, then we followed usual steps of CICD, raising a PR will run pipeline on STG env and finally merging to master would run pipelines on PRD. \n\nNow in my new team, for dev we use dev server which is basically a clone of STG and PRD, just pointing to a SANDBOX database. \n\nI honestly don't mind either, using a dev server seems more \"clean\" but it's basically the same. The only problem here is that my DE team plans to open this repo to analysts and data scientists to create dbt models. In this case, the dev server env seems a bit too complicated for them to set up (exporting a bunch of credentials, setting up venv) every time, specially also because they all have windows computers. I was thinking on just using vscode devcontainers to set up an easy local dev experience for analysts and data scientists but my DE peers don't seem happy about this for some reason. They want the analysts and everyone else to do what they do to set up their dev server which honestly, it's a bit too much even for me, I could only imagine analysts reactions. \n\nAnyone has any ideas on best practices here and also best way to help analysts development of dbt models easier.", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dev server vs local development", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yd3fw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682411804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just changed teams internally in my company, and want to know which is the best practice to follow here. In my previous team, we use local development for our DEV env, then we followed usual steps of CICD, raising a PR will run pipeline on STG env and finally merging to master would run pipelines on PRD. &lt;/p&gt;\n\n&lt;p&gt;Now in my new team, for dev we use dev server which is basically a clone of STG and PRD, just pointing to a SANDBOX database. &lt;/p&gt;\n\n&lt;p&gt;I honestly don&amp;#39;t mind either, using a dev server seems more &amp;quot;clean&amp;quot; but it&amp;#39;s basically the same. The only problem here is that my DE team plans to open this repo to analysts and data scientists to create dbt models. In this case, the dev server env seems a bit too complicated for them to set up (exporting a bunch of credentials, setting up venv) every time, specially also because they all have windows computers. I was thinking on just using vscode devcontainers to set up an easy local dev experience for analysts and data scientists but my DE peers don&amp;#39;t seem happy about this for some reason. They want the analysts and everyone else to do what they do to set up their dev server which honestly, it&amp;#39;s a bit too much even for me, I could only imagine analysts reactions. &lt;/p&gt;\n\n&lt;p&gt;Anyone has any ideas on best practices here and also best way to help analysts development of dbt models easier.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yd3fw", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yd3fw/dev_server_vs_local_development/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yd3fw/dev_server_vs_local_development/", "subreddit_subscribers": 102566, "created_utc": 1682411804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all!\n\nI'm new to iceberg but a pretty decent DE.  Right now my company is paying a fortune for a splunk implementation and I'm thinking I can replace it pretty quickly and just do a cost per query in Athena with proper partitioning and bucketing.\n\nHowever since the logs are currently just simple zipped json files, I am going to be doing some large transformations to get everything in an optimized fashion and want to 'measure twice cut once' so to speak.\n\nI know the perks of Avro and Iceberg are schema evolution, but I honestly can't see that happening at most once every couple of years so I'm not seeing a benefit. My naivete with Iceberg makes me think the main power of that would be for the time travel, but again, logs are immutable so not much of a perk.\n\nIf I do proper partitioning, will parquet take me to where I want it or should I be looking at something more modern?\n\nThanks!", "author_fullname": "t2_bu6ed", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iceberg on Cloudtrail Logs with Athena", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yy0tq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682461148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to iceberg but a pretty decent DE.  Right now my company is paying a fortune for a splunk implementation and I&amp;#39;m thinking I can replace it pretty quickly and just do a cost per query in Athena with proper partitioning and bucketing.&lt;/p&gt;\n\n&lt;p&gt;However since the logs are currently just simple zipped json files, I am going to be doing some large transformations to get everything in an optimized fashion and want to &amp;#39;measure twice cut once&amp;#39; so to speak.&lt;/p&gt;\n\n&lt;p&gt;I know the perks of Avro and Iceberg are schema evolution, but I honestly can&amp;#39;t see that happening at most once every couple of years so I&amp;#39;m not seeing a benefit. My naivete with Iceberg makes me think the main power of that would be for the time travel, but again, logs are immutable so not much of a perk.&lt;/p&gt;\n\n&lt;p&gt;If I do proper partitioning, will parquet take me to where I want it or should I be looking at something more modern?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yy0tq", "is_robot_indexable": true, "report_reasons": null, "author": "pankswork", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yy0tq/iceberg_on_cloudtrail_logs_with_athena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yy0tq/iceberg_on_cloudtrail_logs_with_athena/", "subreddit_subscribers": 102566, "created_utc": 1682461148.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}