{"kind": "Listing", "data": {"after": "t3_12ctgd3", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "With Pandas 2.0, no existing code should break and everything will work as is. However, the primary update that is subtle is the use of Apache Arrow API vs. Numpy for managing and ingesting data (using methods like read\\_csv, read\\_sql, read\\_parquet, etc). This new integration is hope to increase efficiency in terms of memory use and improving the usage of data types such string,  datatime, and categories.  \n\n\n&gt;Python data structures (lists, dictionaries, tuples, etc) are very slow and can't be used. So the data representation is not Python and is not standard, and an implementation needs to happen via Python extensions, usually implemented in C (also in C++, Rust and others). For many years, the main extension to represent arrays and perform operations on them in a fast way has been NumPy. And this is what pandas was initially built on.  \n&gt;  \n&gt;While NumPy has been good enough to make pandas the popular library it is, it was never built as a backend for dataframe libraries, and it has some important limitations.\n\n**Summary of improvements include:**\n\n* **Managing missing values:** By using Arrow, pandas is able to deal with missing values without having to implement its own version for each data type. Instead, the Apache Arrow in-memory data representation includes an equivalent representation as part of its specification\n* **Speed:** Given an example of a dataframe with 2.5 million rows running in the author's laptop, running the `endswith` function is 31.6x fasters using Apache Arrow vs. Numpy (14.9ms vs. 471ms, respectively)\n* **Interoperability:** Ingesting a data in one format and outputting it in a different format should not be challenging. For example, moving from SAS data to Latex, using Pandas &lt;2.0 would require:\n   * Load the data from SAS into a pandas dataframe\n   * Export the dataframe to a parquet file\n   * Load the parquet file from Polars\n   * Make the transformations in Polars\n   * Export the Polars dataframe into a second parquet file\n   * Load the Parquet into pandas\n   * Export the data to the final LATEX file  \nHowever, with PyArrow, the operation can be as simple as such (after Polars bug fixes and using Pandas 2.0):\n\n&amp;#8203;\n\n    loaded_pandas_data = pandas.read_sas(fname) \n    \n    polars_data = polars.from_pandas(loaded_pandas_data) \n    # perform operations with pandas polars \n    \n    to_export_pandas_data = polars.to_pandas(use_pyarrow_extension_array=True) to_export_pandas_data.to_latex()\n\n* **Expanding Data Type Support:**\n\n&gt;Arrow types are broader and better when used outside of a numerical tool like NumPy. It has better support for dates and time, including types for date-only or time-only data, different precision (e.g. seconds, milliseconds, etc.), different sizes (32 bits, 63 bits, etc.). The boolean type in Arrow uses a single bit per value, consuming one eighth of memory. It also supports other types, like decimals, or binary data, as well as complex types (for example a column where each value is a list). There is [a table](https://pandas.pydata.org/docs/dev/reference/arrays.html?highlight=arrowdtype#pyarrow) in the pandas documentation mapping Arrow to NumPy types.\n\n[https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i)", "author_fullname": "t2_48648", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pandas 2.0 is going live, and Apache Arrow will replace Numpy, and that's a great thing!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dbhsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 341, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 341, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680763026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With Pandas 2.0, no existing code should break and everything will work as is. However, the primary update that is subtle is the use of Apache Arrow API vs. Numpy for managing and ingesting data (using methods like read_csv, read_sql, read_parquet, etc). This new integration is hope to increase efficiency in terms of memory use and improving the usage of data types such string,  datatime, and categories.  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Python data structures (lists, dictionaries, tuples, etc) are very slow and can&amp;#39;t be used. So the data representation is not Python and is not standard, and an implementation needs to happen via Python extensions, usually implemented in C (also in C++, Rust and others). For many years, the main extension to represent arrays and perform operations on them in a fast way has been NumPy. And this is what pandas was initially built on.  &lt;/p&gt;\n\n&lt;p&gt;While NumPy has been good enough to make pandas the popular library it is, it was never built as a backend for dataframe libraries, and it has some important limitations.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Summary of improvements include:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Managing missing values:&lt;/strong&gt; By using Arrow, pandas is able to deal with missing values without having to implement its own version for each data type. Instead, the Apache Arrow in-memory data representation includes an equivalent representation as part of its specification&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Given an example of a dataframe with 2.5 million rows running in the author&amp;#39;s laptop, running the &lt;code&gt;endswith&lt;/code&gt; function is 31.6x fasters using Apache Arrow vs. Numpy (14.9ms vs. 471ms, respectively)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Interoperability:&lt;/strong&gt; Ingesting a data in one format and outputting it in a different format should not be challenging. For example, moving from SAS data to Latex, using Pandas &amp;lt;2.0 would require:\n\n&lt;ul&gt;\n&lt;li&gt;Load the data from SAS into a pandas dataframe&lt;/li&gt;\n&lt;li&gt;Export the dataframe to a parquet file&lt;/li&gt;\n&lt;li&gt;Load the parquet file from Polars&lt;/li&gt;\n&lt;li&gt;Make the transformations in Polars&lt;/li&gt;\n&lt;li&gt;Export the Polars dataframe into a second parquet file&lt;/li&gt;\n&lt;li&gt;Load the Parquet into pandas&lt;/li&gt;\n&lt;li&gt;Export the data to the final LATEX file&lt;br/&gt;\nHowever, with PyArrow, the operation can be as simple as such (after Polars bug fixes and using Pandas 2.0):&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;loaded_pandas_data = pandas.read_sas(fname) \n\npolars_data = polars.from_pandas(loaded_pandas_data) \n# perform operations with pandas polars \n\nto_export_pandas_data = polars.to_pandas(use_pyarrow_extension_array=True) to_export_pandas_data.to_latex()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Expanding Data Type Support:&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Arrow types are broader and better when used outside of a numerical tool like NumPy. It has better support for dates and time, including types for date-only or time-only data, different precision (e.g. seconds, milliseconds, etc.), different sizes (32 bits, 63 bits, etc.). The boolean type in Arrow uses a single bit per value, consuming one eighth of memory. It also supports other types, like decimals, or binary data, as well as complex types (for example a column where each value is a list). There is &lt;a href=\"https://pandas.pydata.org/docs/dev/reference/arrays.html?highlight=arrowdtype#pyarrow\"&gt;a table&lt;/a&gt; in the pandas documentation mapping Arrow to NumPy types.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i\"&gt;https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dbhsg", "is_robot_indexable": true, "report_reasons": null, "author": "forbiscuit", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dbhsg/pandas_20_is_going_live_and_apache_arrow_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dbhsg/pandas_20_is_going_live_and_apache_arrow_will/", "subreddit_subscribers": 868508, "created_utc": 1680763026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Whenever I get stuck on something, feel like I've exhausted every option and resource I have, and finally decide to ask for help by posting on Teams or emailing a colleague, 8/10 I usually figure out the issue within 5 minutes of asking for help.  \n\nThen, I'm forced to posted an update that I figured it out because I wasn't on VPN, didn't update libraries or some other silly thing.  Does this happen to anyone else?", "author_fullname": "t2_4k8au9km", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I usually figure out the problem 5 minutes after I create the ticket, send email", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12coplb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 172, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 172, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680709961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whenever I get stuck on something, feel like I&amp;#39;ve exhausted every option and resource I have, and finally decide to ask for help by posting on Teams or emailing a colleague, 8/10 I usually figure out the issue within 5 minutes of asking for help.  &lt;/p&gt;\n\n&lt;p&gt;Then, I&amp;#39;m forced to posted an update that I figured it out because I wasn&amp;#39;t on VPN, didn&amp;#39;t update libraries or some other silly thing.  Does this happen to anyone else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12coplb", "is_robot_indexable": true, "report_reasons": null, "author": "Contango_4eva", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12coplb/i_usually_figure_out_the_problem_5_minutes_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12coplb/i_usually_figure_out_the_problem_5_minutes_after/", "subreddit_subscribers": 868508, "created_utc": 1680709961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was sitting there yesterday on a video call interviewing for a senior role. She was telling me about how excited everyone is for the company mission. Telling me about all their backers and partners including Amazon, MSFT, governments etc.\n\nAnd I'm sitting there thinking....the mission of what, exactly? To receive a wage in exchange for helping to extract more wealth from the general population and push it toward the top few %? \n\nIsn't that what nearly all models and algorithms are doing? More efficiently transferring wealth to the top few % of people and we get a relatively tiny cut of that in return? At some point, as housing, education and healthcare costs takes up a higher and higher % of everyone's paycheck (from 20% to 50%, eventually 85%) there will be so little wealth left to extract that our \"relatively\" tiny cut of 100-200k per year will become an absolutely tiny cut as well.\n\nIsn't that what your real mission is? Even in healthcare, \"We are improving patient lives!\" you mean by lowering everyone's salaries because premiums and healthcare prices have to go up to help pay for this extremely expensive \"high tech\" proprietary medical thing that a few people benefit from? But you were able to rub elbows with (essentially bribe) enough \"key opinion leaders\" who got this thing to be covered by insurance and taxpayers?", "author_fullname": "t2_w71g3v97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ever disassociate during job interviews because you feel like everything the company, and what you'll be doing, is just quickening the return to the feudal age?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dhmus", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680781957.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680781526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was sitting there yesterday on a video call interviewing for a senior role. She was telling me about how excited everyone is for the company mission. Telling me about all their backers and partners including Amazon, MSFT, governments etc.&lt;/p&gt;\n\n&lt;p&gt;And I&amp;#39;m sitting there thinking....the mission of what, exactly? To receive a wage in exchange for helping to extract more wealth from the general population and push it toward the top few %? &lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t that what nearly all models and algorithms are doing? More efficiently transferring wealth to the top few % of people and we get a relatively tiny cut of that in return? At some point, as housing, education and healthcare costs takes up a higher and higher % of everyone&amp;#39;s paycheck (from 20% to 50%, eventually 85%) there will be so little wealth left to extract that our &amp;quot;relatively&amp;quot; tiny cut of 100-200k per year will become an absolutely tiny cut as well.&lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t that what your real mission is? Even in healthcare, &amp;quot;We are improving patient lives!&amp;quot; you mean by lowering everyone&amp;#39;s salaries because premiums and healthcare prices have to go up to help pay for this extremely expensive &amp;quot;high tech&amp;quot; proprietary medical thing that a few people benefit from? But you were able to rub elbows with (essentially bribe) enough &amp;quot;key opinion leaders&amp;quot; who got this thing to be covered by insurance and taxpayers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dhmus", "is_robot_indexable": true, "report_reasons": null, "author": "SnowceanDiving", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dhmus/ever_disassociate_during_job_interviews_because/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dhmus/ever_disassociate_during_job_interviews_because/", "subreddit_subscribers": 868508, "created_utc": 1680781526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m going to be handed a complex codebase developed by consultants that is poorly commented. It is being used to predict supply chain logistics and is littered with obscure company specific information. Checking some of the Python files, some are 1 to 3 thousand lines of code, and I see about 30 folders in the directory. I\u2019ve never seen such a massive code base for any data science project in my career. How do I own this without getting fired?", "author_fullname": "t2_3vrcu2fcm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you do if you\u2019re expected to own a project with 10s of thousands of lines of code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12czyh7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680733195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m going to be handed a complex codebase developed by consultants that is poorly commented. It is being used to predict supply chain logistics and is littered with obscure company specific information. Checking some of the Python files, some are 1 to 3 thousand lines of code, and I see about 30 folders in the directory. I\u2019ve never seen such a massive code base for any data science project in my career. How do I own this without getting fired?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12czyh7", "is_robot_indexable": true, "report_reasons": null, "author": "Open_Peace_1745", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12czyh7/what_would_you_do_if_youre_expected_to_own_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12czyh7/what_would_you_do_if_youre_expected_to_own_a/", "subreddit_subscribers": 868508, "created_utc": 1680733195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nI've been really focusing on my presentation skills lately, but I feel like my slides\n\n1. dont always convey my information as best as possible\n2. isnt eye catching\n\nDoes anyone have any suggestions in material that could help me out?", "author_fullname": "t2_bva05apx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improving my Presentation Slides", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cy2s0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680729041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been really focusing on my presentation skills lately, but I feel like my slides&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;dont always convey my information as best as possible&lt;/li&gt;\n&lt;li&gt;isnt eye catching&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Does anyone have any suggestions in material that could help me out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cy2s0", "is_robot_indexable": true, "report_reasons": null, "author": "GuillerminaCharity", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cy2s0/improving_my_presentation_slides/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cy2s0/improving_my_presentation_slides/", "subreddit_subscribers": 868508, "created_utc": 1680729041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nPosting this as it might be of interest to data scientists, both practitioners and professionals, who use Bayesian methods for parameter estimation and model evaluation.\n\n**tl;dr:** My research group just released a new open-source Python package (PyVBMC) for *sample-efficient* Bayesian inference, i.e. inference with a small number of likelihood evaluations: [PyVBMC](https://acerbilab.github.io/pyvbmc/)\n\nMore info:\n\n* `pip install pyvbmc` (or install on Anaconda via conda-forge)\n* The method runs out of the box, and we included extensive documentations and tutorials for easy accessibility: [Examples \u2014 PyVBMC](https://acerbilab.github.io/pyvbmc/examples.html)\n* A few more technical details on a [Twitter](https://twitter.com/AcerbiLuigi/status/1643549233587318784) or [Mastodon](https://mastodon.social/@AcerbiLuigi/110147657708113411) thread\n* We also have a tl;dr preprint on arXiv: [PyVBMC: Efficient Bayesian inference in Python](https://arxiv.org/abs/2303.09519)\n* Relevant papers were published at the *NeurIPS* machine learning conference in [2018](https://arxiv.org/abs/1810.05558) and [2020](https://arxiv.org/abs/2006.08655)\n\nPlease get in touch in this thread or on Twitter/Mastodon if you have any questions or comments. Thanks again for your time, and I hope this is of actual interest. Feedback is welcome!", "author_fullname": "t2_bi2haf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Python open-source software for sample-efficient Bayesian inference", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cvne6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680723989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Posting this as it might be of interest to data scientists, both practitioners and professionals, who use Bayesian methods for parameter estimation and model evaluation.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; My research group just released a new open-source Python package (PyVBMC) for &lt;em&gt;sample-efficient&lt;/em&gt; Bayesian inference, i.e. inference with a small number of likelihood evaluations: &lt;a href=\"https://acerbilab.github.io/pyvbmc/\"&gt;PyVBMC&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More info:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;pip install pyvbmc&lt;/code&gt; (or install on Anaconda via conda-forge)&lt;/li&gt;\n&lt;li&gt;The method runs out of the box, and we included extensive documentations and tutorials for easy accessibility: &lt;a href=\"https://acerbilab.github.io/pyvbmc/examples.html\"&gt;Examples \u2014 PyVBMC&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;A few more technical details on a &lt;a href=\"https://twitter.com/AcerbiLuigi/status/1643549233587318784\"&gt;Twitter&lt;/a&gt; or &lt;a href=\"https://mastodon.social/@AcerbiLuigi/110147657708113411\"&gt;Mastodon&lt;/a&gt; thread&lt;/li&gt;\n&lt;li&gt;We also have a tl;dr preprint on arXiv: &lt;a href=\"https://arxiv.org/abs/2303.09519\"&gt;PyVBMC: Efficient Bayesian inference in Python&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Relevant papers were published at the &lt;em&gt;NeurIPS&lt;/em&gt; machine learning conference in &lt;a href=\"https://arxiv.org/abs/1810.05558\"&gt;2018&lt;/a&gt; and &lt;a href=\"https://arxiv.org/abs/2006.08655\"&gt;2020&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Please get in touch in this thread or on Twitter/Mastodon if you have any questions or comments. Thanks again for your time, and I hope this is of actual interest. Feedback is welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cvne6", "is_robot_indexable": true, "report_reasons": null, "author": "emiurgo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cvne6/new_python_opensource_software_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cvne6/new_python_opensource_software_for/", "subreddit_subscribers": 868508, "created_utc": 1680723989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Due to GDPR I unfortunately cannot disclose that information.", "author_fullname": "t2_lba2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many European data scientists does it take to screw in a light bulb?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctk7w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680719638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Due to GDPR I unfortunately cannot disclose that information.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ctk7w", "is_robot_indexable": true, "report_reasons": null, "author": "tty-tourist", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ctk7w/how_many_european_data_scientists_does_it_take_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ctk7w/how_many_european_data_scientists_does_it_take_to/", "subreddit_subscribers": 868508, "created_utc": 1680719638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi All, \n\nI am looking into the easiest way to set up machine learning infrastructure for a team. It seems like Sagemaker does exactly that. However, almost nobody uses it when I ask other DS teams. Curious why that is. None of the people I talked to had a strong reason not to use it. \n\nIs anybody here using it that wants to share the pros and cons?", "author_fullname": "t2_13mxw7nv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why not Sagemaker?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cupkq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680721999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, &lt;/p&gt;\n\n&lt;p&gt;I am looking into the easiest way to set up machine learning infrastructure for a team. It seems like Sagemaker does exactly that. However, almost nobody uses it when I ask other DS teams. Curious why that is. None of the people I talked to had a strong reason not to use it. &lt;/p&gt;\n\n&lt;p&gt;Is anybody here using it that wants to share the pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cupkq", "is_robot_indexable": true, "report_reasons": null, "author": "fokke2508", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cupkq/why_not_sagemaker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cupkq/why_not_sagemaker/", "subreddit_subscribers": 868508, "created_utc": 1680721999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have some time series data where I've noticed that the error/accuracy of my predictions from a large ensemble of models has a moderate autocorrelation to past predictions. If a model was accurate yesterday then it has a decent probability of being accurate today. I was wondering if anyone has any ideas on how to capitalize on this.\n\nMy current strategy has been to simply weight each model in the ensemble based on a rolling 3 day correlation to the target live variable. Models with a high 3 day average would have a higher weight and vice versa. I've had decent success with this strategy so far, but I'm trying to brainstorm ways to take this to the next level.\n\nOther ideas I had were to train a LSTM model on the historical performance of each model in attempt to predict the current days performance.\n\nCurios to hear if anyone has any ideas on how improve on this.\n\n[Autocorrelation of model predictions](https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3e5e89e0e35a7380bea579449cedf93d7f8bb577)", "author_fullname": "t2_d43cut1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are ways to capitalize on autocorrelation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5fae6kxce6sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55809e12059b4d9a55bcf068b56fc33329a56d65"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84c3fa6c7341b66e9fa108232da6b434c8d83738"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ee9d9bf6e03381bbca907adf7e5a547b868b77c"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95022d6d89919b09f8ec27d3d4f512e97f82aa0f"}], "s": {"y": 480, "x": 640, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3e5e89e0e35a7380bea579449cedf93d7f8bb577"}, "id": "5fae6kxce6sa1"}}, "name": "t3_12d79g9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/URbWOLYXKtJSLomT80r1NC-7LDMcnD_rgTfKZQ3EilE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680750540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some time series data where I&amp;#39;ve noticed that the error/accuracy of my predictions from a large ensemble of models has a moderate autocorrelation to past predictions. If a model was accurate yesterday then it has a decent probability of being accurate today. I was wondering if anyone has any ideas on how to capitalize on this.&lt;/p&gt;\n\n&lt;p&gt;My current strategy has been to simply weight each model in the ensemble based on a rolling 3 day correlation to the target live variable. Models with a high 3 day average would have a higher weight and vice versa. I&amp;#39;ve had decent success with this strategy so far, but I&amp;#39;m trying to brainstorm ways to take this to the next level.&lt;/p&gt;\n\n&lt;p&gt;Other ideas I had were to train a LSTM model on the historical performance of each model in attempt to predict the current days performance.&lt;/p&gt;\n\n&lt;p&gt;Curios to hear if anyone has any ideas on how improve on this.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3e5e89e0e35a7380bea579449cedf93d7f8bb577\"&gt;Autocorrelation of model predictions&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12d79g9", "is_robot_indexable": true, "report_reasons": null, "author": "_McFuggin_", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12d79g9/what_are_ways_to_capitalize_on_autocorrelation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12d79g9/what_are_ways_to_capitalize_on_autocorrelation/", "subreddit_subscribers": 868508, "created_utc": 1680750540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I understand that SQL is a language that allows us to access the data base,  but REST API's also allow us to do that with their CRUD methods.   So my question is, are they both similar in what they do? If I am a developer at a given company would I most likely use one over the other or would I use them both?   I am currently self studying a lot and trying to comprehend how they are different.", "author_fullname": "t2_4325x4ta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forgive my ignorance, but is SQL Queries a substitute for REST APIs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cqv8u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680714155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand that SQL is a language that allows us to access the data base,  but REST API&amp;#39;s also allow us to do that with their CRUD methods.   So my question is, are they both similar in what they do? If I am a developer at a given company would I most likely use one over the other or would I use them both?   I am currently self studying a lot and trying to comprehend how they are different.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cqv8u", "is_robot_indexable": true, "report_reasons": null, "author": "ala4akbar", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cqv8u/forgive_my_ignorance_but_is_sql_queries_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cqv8u/forgive_my_ignorance_but_is_sql_queries_a/", "subreddit_subscribers": 868508, "created_utc": 1680714155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently building some forecasting models using a lightgbm regressor. \n\nIt is in an auto platform that I'm evaluating for work, and unfortunately it just auto creates the train / test split.  This is done by randomly splitting and shuffling the data. So the training set may include the very first day and the very last day. It also auto-chooses the model so I have no choice in that either.\n\nIt auto creates features from the date column, e.g. day of year, week of year etc. And uses those as predictors.\n\nI've read comments that this training split is a bad idea, and a walk forward method should be used. \n\nI agree that walk forward makes more sense, and i use that for when I'm doing time series in python. \n\nBUT... i haven't seen an explanation as to why randomly shuffling and splitting the data is not recommended. \n\nCan somebody explain?", "author_fullname": "t2_vtpzvynk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using tree based methods for forecasting. Why is it a bad idea to shuffle the data for training and testing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dcoi5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680767062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently building some forecasting models using a lightgbm regressor. &lt;/p&gt;\n\n&lt;p&gt;It is in an auto platform that I&amp;#39;m evaluating for work, and unfortunately it just auto creates the train / test split.  This is done by randomly splitting and shuffling the data. So the training set may include the very first day and the very last day. It also auto-chooses the model so I have no choice in that either.&lt;/p&gt;\n\n&lt;p&gt;It auto creates features from the date column, e.g. day of year, week of year etc. And uses those as predictors.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read comments that this training split is a bad idea, and a walk forward method should be used. &lt;/p&gt;\n\n&lt;p&gt;I agree that walk forward makes more sense, and i use that for when I&amp;#39;m doing time series in python. &lt;/p&gt;\n\n&lt;p&gt;BUT... i haven&amp;#39;t seen an explanation as to why randomly shuffling and splitting the data is not recommended. &lt;/p&gt;\n\n&lt;p&gt;Can somebody explain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dcoi5", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Lemon-402", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dcoi5/using_tree_based_methods_for_forecasting_why_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dcoi5/using_tree_based_methods_for_forecasting_why_is/", "subreddit_subscribers": 868508, "created_utc": 1680767062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone. Unless anything unexpected happens, I will be finally switching careers into data science in about 4 weeks. For me it\u2019s the realisation of a dream I started 3 years ago, and I\u2019m just so excited about this job I can\u2019t wait to start and to learn more. \nI talked to my future boss and asked him about topics I could start preparing/getting familiar with and he mentioned PySpark and Databricks. I want to have at least some knowledge about them before I start so I don\u2019t get totally overwhelmed on the first day. Do you guys know any good free resources (videos, articles, YouTube channels, etc\u2026) that I can take a look at?", "author_fullname": "t2_6xiu1hs9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks &amp; PySpark free learning resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cuueo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680722283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. Unless anything unexpected happens, I will be finally switching careers into data science in about 4 weeks. For me it\u2019s the realisation of a dream I started 3 years ago, and I\u2019m just so excited about this job I can\u2019t wait to start and to learn more. \nI talked to my future boss and asked him about topics I could start preparing/getting familiar with and he mentioned PySpark and Databricks. I want to have at least some knowledge about them before I start so I don\u2019t get totally overwhelmed on the first day. Do you guys know any good free resources (videos, articles, YouTube channels, etc\u2026) that I can take a look at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cuueo", "is_robot_indexable": true, "report_reasons": null, "author": "Davidat0r", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cuueo/databricks_pyspark_free_learning_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cuueo/databricks_pyspark_free_learning_resources/", "subreddit_subscribers": 868508, "created_utc": 1680722283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "P.s- Will share additional information as we discuss as I am also exploring the same.\n\nEdit 1-  I've got high level customer info about age, investor type, province and their type of account.\n\nI have also got customer transaction information and how long they've stayed invested also the redemption date.", "author_fullname": "t2_du1e9k98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working on a customer redemption classification problem with a very limited amount of data. With the aim to predict redemptions in the next 3 months. I've got customer info and transaction info. What are your ideas to go about the same.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12da863", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680760982.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680758948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;P.s- Will share additional information as we discuss as I am also exploring the same.&lt;/p&gt;\n\n&lt;p&gt;Edit 1-  I&amp;#39;ve got high level customer info about age, investor type, province and their type of account.&lt;/p&gt;\n\n&lt;p&gt;I have also got customer transaction information and how long they&amp;#39;ve stayed invested also the redemption date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12da863", "is_robot_indexable": true, "report_reasons": null, "author": "Lazzy_Engineer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12da863/working_on_a_customer_redemption_classification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12da863/working_on_a_customer_redemption_classification/", "subreddit_subscribers": 868508, "created_utc": 1680758948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi there :) \n\nI am currently evaluating to educate myself in the field of AI and Data Analysis. \n\nWhat I am curious now, is how realistic it is to find jobs as a freelancer, if I want to a) work with F#, and b) work on my own codebases.\n\nUsually in software jobs, you work on the existing codebases of previous colleagues, in this field it kinda seems this could be different.\n\nAs far as I understand, do I get simply get data from a database, that I have to access, evaluate and present.\n\nSo I thought if that's the case, I could choose the stack on my own anyway, and I would be not so bound to Python, Excel and the common SQL DSL. \n\nHow independent can I choose my technology as a freelancer?", "author_fullname": "t2_78je0kwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "F# Freelancing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d94qr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680761228.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680755704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there :) &lt;/p&gt;\n\n&lt;p&gt;I am currently evaluating to educate myself in the field of AI and Data Analysis. &lt;/p&gt;\n\n&lt;p&gt;What I am curious now, is how realistic it is to find jobs as a freelancer, if I want to a) work with F#, and b) work on my own codebases.&lt;/p&gt;\n\n&lt;p&gt;Usually in software jobs, you work on the existing codebases of previous colleagues, in this field it kinda seems this could be different.&lt;/p&gt;\n\n&lt;p&gt;As far as I understand, do I get simply get data from a database, that I have to access, evaluate and present.&lt;/p&gt;\n\n&lt;p&gt;So I thought if that&amp;#39;s the case, I could choose the stack on my own anyway, and I would be not so bound to Python, Excel and the common SQL DSL. &lt;/p&gt;\n\n&lt;p&gt;How independent can I choose my technology as a freelancer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12d94qr", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Stomach_8", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12d94qr/f_freelancing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12d94qr/f_freelancing/", "subreddit_subscribers": 868508, "created_utc": 1680755704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ay1q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Arrow String improvements in Pandas/Dask DataFrames", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_12dk7v7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6uwjML35z-FxsE_pRboam-GDwq291V8TrPtVWQof1WA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680787351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/coiled-hq/pyarrow-strings-in-dask-dataframes-55a0c4871586", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?auto=webp&amp;v=enabled&amp;s=6859023178db536f94b55bb63382a7a09109032b", "width": 792, "height": 490}, "resolutions": [{"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9618b07d38c42c9b44e92644148b22822523cd9e", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e340c81948a4fba6552f26b728af8711788bc35", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ac1c66e5bf7be22194afbe2e088a490ce2f46dd", "width": 320, "height": 197}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4cd149bae3ae18eb7da5790f10a0c5f43a190ed", "width": 640, "height": 395}], "variants": {}, "id": "V5FocKkZS3WgTL0zjz6iujrafykJdPf8Om_OTahpOug"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dk7v7", "is_robot_indexable": true, "report_reasons": null, "author": "mrocklin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dk7v7/arrow_string_improvements_in_pandasdask_dataframes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/coiled-hq/pyarrow-strings-in-dask-dataframes-55a0c4871586", "subreddit_subscribers": 868508, "created_utc": 1680787351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi there,\nI'm an undergrad right now and i still have a year or two to graduate.Im actually from India and trying to get abroad job opportunities.So to qualify needs and to build a better profile I need some good certifications and resources which are worthy right now.", "author_fullname": "t2_79w6a7es", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need free certification and resources (which are worth right now)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dk0ky", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680786916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,\nI&amp;#39;m an undergrad right now and i still have a year or two to graduate.Im actually from India and trying to get abroad job opportunities.So to qualify needs and to build a better profile I need some good certifications and resources which are worthy right now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dk0ky", "is_robot_indexable": true, "report_reasons": null, "author": "Impressive-Minimum65", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dk0ky/need_free_certification_and_resources_which_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dk0ky/need_free_certification_and_resources_which_are/", "subreddit_subscribers": 868508, "created_utc": 1680786916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm looking to further my education in data science and looking to see if any current data scientists are able to answer some industry questions 1  on 1. This would be a prequisite for acceptance into the cohort.  Any current data professionals be able to help?", "author_fullname": "t2_70ffsxmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Informational Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12djc5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680785420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to further my education in data science and looking to see if any current data scientists are able to answer some industry questions 1  on 1. This would be a prequisite for acceptance into the cohort.  Any current data professionals be able to help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12djc5e", "is_robot_indexable": true, "report_reasons": null, "author": "Sea-Complex2759", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12djc5e/informational_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12djc5e/informational_interview/", "subreddit_subscribers": 868508, "created_utc": 1680785420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_4aj5kedr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copilot for Data Analysts and Scientists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "name": "t3_12dj3p1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/S4NvIz2Yu4cw-LbC6opvLpZmOuxYNBzIAuO2mwv5PVs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680784876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "probeai.app", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "http://probeai.app", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?auto=webp&amp;v=enabled&amp;s=39c96244d01c619f072638ed1b135771ffdc1f9a", "width": 1512, "height": 902}, "resolutions": [{"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7ea3edcfc856d16386fdc3023b6e1e968622acc", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2eb779a93aa7c0f078292864a2070b5e53b9d5a", "width": 216, "height": 128}, {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdc2434ca938edc67a4307496eb2357296c079f8", "width": 320, "height": 190}, {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72f8af6a5256d2e7718adb0976776b37cfe36921", "width": 640, "height": 381}, {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30ce965e82973f4408e202bcdb741236d619b520", "width": 960, "height": 572}, {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d4b63fc799e44bc3cac33118f5bcbc65319f4e4", "width": 1080, "height": 644}], "variants": {}, "id": "X96x_BIF_c3FBKbM5vL0UruDpZtfeCA_t9_iFhoPIKY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dj3p1", "is_robot_indexable": true, "report_reasons": null, "author": "atharvakharbade", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dj3p1/copilot_for_data_analysts_and_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "http://probeai.app", "subreddit_subscribers": 868508, "created_utc": 1680784876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a college student and early into my Data Science major. Besides class/coursework experience, I don\u2019t have anything to add to my resume (I have worked many jobs in the past, but these were all relating to sales and basic high school jobs).\n\nWondering what types of positions/job titles/experiences accept someone who is super early into the industry and doesn\u2019t have a lot on their resume (obviously not expecting to get a data science internship right now, but hopefully something to guide me there). I\u2019ve taken classes in python, Java, c++, Unix/Linux, statistics, and business analytics.\n\nAnything helps, thanks!", "author_fullname": "t2_tfkfcmgl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for starting career path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d59wi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680747787.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680745524.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a college student and early into my Data Science major. Besides class/coursework experience, I don\u2019t have anything to add to my resume (I have worked many jobs in the past, but these were all relating to sales and basic high school jobs).&lt;/p&gt;\n\n&lt;p&gt;Wondering what types of positions/job titles/experiences accept someone who is super early into the industry and doesn\u2019t have a lot on their resume (obviously not expecting to get a data science internship right now, but hopefully something to guide me there). I\u2019ve taken classes in python, Java, c++, Unix/Linux, statistics, and business analytics.&lt;/p&gt;\n\n&lt;p&gt;Anything helps, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12d59wi", "is_robot_indexable": true, "report_reasons": null, "author": "t_c9595", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12d59wi/advice_for_starting_career_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12d59wi/advice_for_starting_career_path/", "subreddit_subscribers": 868508, "created_utc": 1680745524.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \\~25x smaller than GPT-3, challenging the notion that is big always better?\n\nFrom my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back!\n\nWould love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?\n\nP.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset", "author_fullname": "t2_sgam369p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do we really need 100B+ parameters in a large language model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12coioi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680709574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DataBricks&amp;#39;s open-source LLM, &lt;a href=\"https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html\"&gt;Dolly&lt;/a&gt; performs reasonably well on many instruction-based tasks while being ~25x smaller than GPT-3, challenging the notion that is big always better?&lt;/p&gt;\n\n&lt;p&gt;From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back!&lt;/p&gt;\n\n&lt;p&gt;Would love to hear everyone&amp;#39;s opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?&lt;/p&gt;\n\n&lt;p&gt;P.S. I am kinda betting on the latter and building &lt;a href=\"https://github.com/uptrain-ai/uptrain\"&gt;UpTrain&lt;/a&gt;, an open-source project which helps you collect that high quality fine-tuning dataset&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?auto=webp&amp;v=enabled&amp;s=2f30ce106babdfb3bcc0b7e090898bc69fb7a6af", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=566ac34f9234070978fc998c54c5f2c932326feb", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd78dfa4c9b3d6aacd8f0a90c0f6912041084620", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32815a77a86e403532277c345110535827d20f0b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e90c5e2d00b68ac8cfaa0fc724dd6fd969cc3a61", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bf50761221e07260b99111867fc93c4894c5ac3", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11e63fa8b3ad7ebe392e5460dd6e51df9ffc3cee", "width": 1080, "height": 607}], "variants": {}, "id": "PdGLRg9dbAxaekj3ldlsrDK4hEJmGpkDGSxOX3CaJoA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12coioi", "is_robot_indexable": true, "report_reasons": null, "author": "Vegetable-Skill-9700", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12coioi/do_we_really_need_100b_parameters_in_a_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12coioi/do_we_really_need_100b_parameters_in_a_large/", "subreddit_subscribers": 868508, "created_utc": 1680709574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nSorry in advance if this post is too long.I'd like to ask you to comment on whether you think the plan I recently came up with makes sense or not. (I've read [wiki](https://www.reddit.com/r/learnmachinelearning/wiki/index/) but need maybe more personal advice)\n\n**BACKGROUND**\n\nI have a bachelor's degree in neurobiology, around the end of my studies really got into ML and stuff so I took few online courses like \\`Python for Data Science\\` etc. A man's gotta eat though, and knowing that I had no chance at landing a DS/ML related job with no math background whatsoever, I enrolled in Python Dev bootcamp.I sent my CV to several companies and luckily I got a job interview at one place, a software house specializing in software dev and blockchain (let's call it **XYZ)**. I did ok I suppose, I passed technical part, but they wanted me to also have a talk with CTO of the **XYZ**. He was asking me about the DS/ML courses I had taken (I had those completion certificates in my CV) and later said that they had a NLP project I might be able to help them with.The project itself was a research project started by some startup which then went bankrupt, but since  the project was funded by a grant from The National Centre for Research and Development, it was handed over to **XYZ** to complete it.Later I found that as **XYZ** specializes in blockchain and not ML, they couldn't find anyone for the job. So that's that, I've got a job, and I got to work alongside senior data scientist/MLEng (it was just us two working on it), so I've definitely learnt alot and got some hands-on experience.Still, I try to keep learning - I did Andrew Ng's ML Specialization, tried my luck with some Kaggle competitions (no luck) and I've been learning math from Khan Academy.Nevertheless I feel like I need some proper university education or I'll be struggling sooner or later. I can't go to uni full-time (because of work), so I though maybe doing online master is an option.So here's the plan:\n\n**THE PLAN**\n\n1. Complete [OSSU CS](https://github.com/ossu/computer-science) in order to catch up on things I didn't have in college that are (whether I want to or not) necessary\n2. Enroll to [Georgia Tech online Master in CS](https://omscs.gatech.edu/), complete it\n3. Get rid of impostor syndrome and have strong background in CS and ML\n\nNow, what do you think of it? Is it good idea or there's no point in doing all of this?\n\nThanks in advance", "author_fullname": "t2_5zf39oiq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on my DS/ML learning plan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12djvrn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680786613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Sorry in advance if this post is too long.I&amp;#39;d like to ask you to comment on whether you think the plan I recently came up with makes sense or not. (I&amp;#39;ve read &lt;a href=\"https://www.reddit.com/r/learnmachinelearning/wiki/index/\"&gt;wiki&lt;/a&gt; but need maybe more personal advice)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;BACKGROUND&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have a bachelor&amp;#39;s degree in neurobiology, around the end of my studies really got into ML and stuff so I took few online courses like `Python for Data Science` etc. A man&amp;#39;s gotta eat though, and knowing that I had no chance at landing a DS/ML related job with no math background whatsoever, I enrolled in Python Dev bootcamp.I sent my CV to several companies and luckily I got a job interview at one place, a software house specializing in software dev and blockchain (let&amp;#39;s call it &lt;strong&gt;XYZ)&lt;/strong&gt;. I did ok I suppose, I passed technical part, but they wanted me to also have a talk with CTO of the &lt;strong&gt;XYZ&lt;/strong&gt;. He was asking me about the DS/ML courses I had taken (I had those completion certificates in my CV) and later said that they had a NLP project I might be able to help them with.The project itself was a research project started by some startup which then went bankrupt, but since  the project was funded by a grant from The National Centre for Research and Development, it was handed over to &lt;strong&gt;XYZ&lt;/strong&gt; to complete it.Later I found that as &lt;strong&gt;XYZ&lt;/strong&gt; specializes in blockchain and not ML, they couldn&amp;#39;t find anyone for the job. So that&amp;#39;s that, I&amp;#39;ve got a job, and I got to work alongside senior data scientist/MLEng (it was just us two working on it), so I&amp;#39;ve definitely learnt alot and got some hands-on experience.Still, I try to keep learning - I did Andrew Ng&amp;#39;s ML Specialization, tried my luck with some Kaggle competitions (no luck) and I&amp;#39;ve been learning math from Khan Academy.Nevertheless I feel like I need some proper university education or I&amp;#39;ll be struggling sooner or later. I can&amp;#39;t go to uni full-time (because of work), so I though maybe doing online master is an option.So here&amp;#39;s the plan:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;THE PLAN&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Complete &lt;a href=\"https://github.com/ossu/computer-science\"&gt;OSSU CS&lt;/a&gt; in order to catch up on things I didn&amp;#39;t have in college that are (whether I want to or not) necessary&lt;/li&gt;\n&lt;li&gt;Enroll to &lt;a href=\"https://omscs.gatech.edu/\"&gt;Georgia Tech online Master in CS&lt;/a&gt;, complete it&lt;/li&gt;\n&lt;li&gt;Get rid of impostor syndrome and have strong background in CS and ML&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, what do you think of it? Is it good idea or there&amp;#39;s no point in doing all of this?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oI06fG0rYjGPjZ5NxyDIkRX_WZ9R45MEfpQTOTmcrQo.jpg?auto=webp&amp;v=enabled&amp;s=9e4e2943d5c235b0eb7eab7045361919605f9955", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/oI06fG0rYjGPjZ5NxyDIkRX_WZ9R45MEfpQTOTmcrQo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a306b2aca1ebe09eaeab5f48ddbfbe3a1bf5aee", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/oI06fG0rYjGPjZ5NxyDIkRX_WZ9R45MEfpQTOTmcrQo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ef7b33f9bb58119769523233b6fa0c00f618890", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/oI06fG0rYjGPjZ5NxyDIkRX_WZ9R45MEfpQTOTmcrQo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8221a6408f95b2917f361ab64342e293904eaefd", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/oI06fG0rYjGPjZ5NxyDIkRX_WZ9R45MEfpQTOTmcrQo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4de6b05323db6caa6f901c3482507d42d0938b8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/oI06fG0rYjGPjZ5NxyDIkRX_WZ9R45MEfpQTOTmcrQo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=679da39c64bf0afc41a37977cf6dbbbd2e134267", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/oI06fG0rYjGPjZ5NxyDIkRX_WZ9R45MEfpQTOTmcrQo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a46bfd7319c142d512a871ffa03ce9c67cc9038", "width": 1080, "height": 540}], "variants": {}, "id": "9l7wHi42LeUX0ypxb7WXFO3AbbtlPrb_RQffRRGH82k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12djvrn", "is_robot_indexable": true, "report_reasons": null, "author": "brgsk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12djvrn/advice_on_my_dsml_learning_plan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12djvrn/advice_on_my_dsml_learning_plan/", "subreddit_subscribers": 868508, "created_utc": 1680786613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My new project with Arduino: Ultra-Tiny Solution of Daily Activities \u200b\u200bRecognition. Can you repeat? Please, leave your comments.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "name": "t3_12dge9v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_8sds8", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sYUBvdJXNIEroAyPdmrxhzxZG_3FeN7XJhUNm2cF35U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "arduino", "selftext": "Prototyping: a Hand Tracker with Nicla Sense ME and Bosch sensors\n\n## Things used in this project:\n\n**Hardware components**\n\n[Arduino Nicla Sense ME](https://referral.element14.com/OrderCodeView?fsku=3874817&amp;nsku=99AJ4432&amp;COM=e14c-noscript&amp;CMP=e14c-noscript&amp;osetc=e14-noscript-tracking-loss) \u00d71  \n[Bosch BHI260 sensor](https://www.bosch-sensortec.com/news/new-generation-of-smart-sensor-hubs-bhi260-and-bha260.html)\n\n## Software apps and online services\n\n[Neuton Tiny ML](https://neuton.ai/)  \n[Arduino IDE](https://www.arduino.cc/en/software)\n\nIn my project, I want to show an example of how you can significantly increase the complexity of intelligence functionality, and determine what a wearable device can recognize with a very small and accurate neural network. Also, you will find a detailed guide on how to implement Multiple Daily Activities recognition using the Nicla Sense ME. You also can\u00a0[**participate in my practical webinar**](https://us06web.zoom.us/webinar/register/3416800160594/WN_X0wH_gaqSiylUGehTFXlSQ)\u00a0on\u00a0**April 11 at 5 pm CET**. Where you can watch how to create this solution from leaders from Neuton.AI, Arduino and Bosch. Don't miss this opportunity to learn from experts directly.\n\n[https://youtu.be/3nkoJZD\\_Lh8](https://youtu.be/3nkoJZD_Lh8)\n\nIf you have Nicla Sense ME, you can try my pre-trained model in your experiment/project/classes.\n\nOur test results show that the model perfectly recognizes the movements of 30 participants whose data was not used for training the model.\n\nFollowing this tutorial, you also can easily make your own experiment and train your neural network on your data to recognize the same or different movements.\n\nWe believe that the ability to recognize an extremely complex and similar action using small neural networks opens up a new era for always-on devices.\n\nBelow, you will find a full tutorial on how to reproduce this experiment, including data collection, model training, model embedding and inference. In addition, we have created a public repository with a pre-trained model that has demonstrated good generalization on new users. It is a precompiled model archive that you may just download and embed into your Nicla Sense ME with a few clicks of a mouse and start testing.\n\n[**https://github.com/NeutonTinyML/hand-activity-recognition**](https://github.com/NeutonTinyML/hand-activity-recognition)\n\n## Data Collection with Nicla Sense ME\n\nFor this case, I\u2019ve selected 5 types of activities (for your experiment, you may choose other activities):\n\n* Washing hands\n* Brushing teeth\n* Clapping\n* Brushing hair\n* Random activity (negative class)\n\nFor each activity type, I collected sensor data with Nicla Sense ME for 10 minutes non-stop. Data was collected from 7 different people from 7 different locations. It was quite a challenging, yet fun process because, for proper data collection, it was undesirable to interrupt. Here\u2019s some advice for those who plan to re-conduct my experiment: before collecting data for another activity, get some rest! However, if 10 minutes of constant hair brushing is too much, you can do two 5-minute data collection streams and then concatenate the two files.\n\nFor the negative class, we collected 60 minutes of data. There\u2019s no need to do anything special this time; just get on with your everyday activities. Type on the keyboard, operate a mouse, answer calls, drink tea, or do whatever you want. Just be sure you don\u2019t sit completely idle with your hands still during negative class data collection. The negative class is a subset of everything else excluding the 4 classes outlined above.\n\n## Steps:\n\nNow let me provide you with detailed guidelines on how to tune Arduino\u2019s board, Nicla Sense ME to collect data.\n\n**1.**\u00a0Visit\u00a0[**https://www.arduino.cc/en/software**](https://www.arduino.cc/en/software)[,](https://www.arduino.cc/en/software)\u00a0download Arduino IDE 2.x.x for your OS, and install a package.\n\n**2.**\u00a0Open Arduino IDE, click on the\u00a0**Boards Manager**\u00a0icon, type \u201cnicla sense me\u201d and install the\u00a0**Arduino Mbed OS Nicla Boards**\u00a0package.\n\nClick on the\u00a0**Library Manager icon**, type \u201cnicla sense me\u201d in the search box and install two libraries:\u00a0**Arduino\\_BHY2**\u00a0and\u00a0**ArduinoBLE**.\n\nType \u201cprintf\u201d in the search box and install\u00a0**LibPrintf**.\n\n**3.**\u00a0In the main menu, select\u00a0**File-&gt;Examples-&gt;Arduino\\_BHY2&gt;BHYFirmwareUpdate**.\n\n**4.**\u00a0Connect your\u00a0**Nicla Sense ME**\u00a0board to the USB and select the port in the dropdown menu.\n\n**5.**\u00a0Click on the Upload button and wait until the uploading process is finished.\n\n**6.**\u00a0Click on the\u00a0**Serial Monitor**\u00a0icon and select\u00a0**115200 baud**, then click the RESET button on your Nicla board.\n\n**7.**\u00a0After 5-10 seconds you\u2019ll see that the BHY firmware is uploaded.\n\n**8.** Download the Arduino precompiled library for sensor data collection using a Nicla Sense ME from [**the Neuton repository.**](https://github.com/NeutonTinyML/hand-activity-recognition)\n\nmodel -&gt; Arduino\\_Neuton.zip (do not uncompress the archive)\n\n**9.** Install the \u2018Arduino\\_Neuton.zip\u2019 model into the Arduino IDE:\n\nSketch -&gt; Include Library -&gt; Add .ZIP Library\u2026 (point to the downloaded \u2018Arduino\\_Neuton.zip\u2019 archive)\n\nhttps://preview.redd.it/dt1grc4jg8sa1.jpg?width=1840&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1e386acb758209202ac65e6407f3611f045e4d27\n\n**10.** Quit &amp; restart the Arduino IDE (it will take up to a minute for the Arduino IDE to index all of it\u2019s examples, including the freshly installed \u2018Arduino\\_Neuton\u2019)\n\n**11.** Open the installed example: File -&gt; Examples -&gt; Arduino\\_Neuton -&gt; Inertial\\_Sensor\\_Data\\_Collection (a new Arduino IDE window will pop up, close the previous Arduino IDE window)\n\nhttps://preview.redd.it/4b39v85og8sa1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=97e3e32ce2dbfe134d3fb3531f8e195d5f9883df\n\n**12.** Connect your Nicla Sense ME to your computer\u2019s USB port.\n\n**13.** In the IDE select your MCU and upload the firmware.\n\n**14.**\u00a0After the firmware is uploaded (1-2 minutes), open the Serial Monitor - you should see the sensors readings (In case you will see the connection error \u2013 restart the Arduino IDE and open the Serial Monitor once again)\n\nNow you can unplug the board from the USB port.\n\n**15.**\u00a0Attach the board firmly to the watch or bracelet. The Micro-USB port should be in the bottom-right corner.\n\nhttps://preview.redd.it/u3q15z2er3ra1.png?width=612&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e7734fafef3cfa398d6a23d99c8f7ad5a7f20a8a\n\n**16.**\u00a0Put a watch or a bracelet with a circuit board on your right hand, and connect the cable to it. Make sure that the cable near the connector is not taut so as not to damage the board.\n\n**17.**\u00a0Depending on your OS, open the serial port and try to log data. A serial port can be seen in the hint.\n\nhttps://preview.redd.it/504en08gr3ra1.png?width=1306&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=03994ba652b0ea9c154c699a2aa6fe4af274d5d2\n\nUse this command in the terminal if you\u2019re on macOS:\n\n**stty -f /dev/cu.usbmodem85EB3A0F2 115200 | cat /dev/cu.usbmodem85EB3A0F2 | tee 4\\_brushing\\_hair.csv**\n\nThis will set the port speed to 115200 baud. Print out data from the serial port as text and save it to a CSV file.\n\nhttps://preview.redd.it/e5oi0ljir3ra1.png?width=1596&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2c686bee67e11d127b9d7e4cbcfb3370271a95d8\n\nPress Control-C to stop recording. There will be a CSV file with data from sensors (accelerometer, linear accelerometer, and gyroscope) and timestamp.\n\nhttps://preview.redd.it/bks9n7hjr3ra1.png?width=1592&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f452d539b15cfbfadeb7296bd1e62fa99fe4a5bd\n\n**18.**\u00a0Start recording the movement. Press the Reset button on the Nicla board and restart logging to the file.\n\nFor example, clap for 10 minutes (use the stopwatch application in your smartphone). During this process, change the position of your hands to make the data more diverse. Do not stop moving while the recording is in progress.\n\nThe recording will start in \\~7 seconds after pushing the Reset button.\n\nOnce 10 minutes are finished, continue the movement for 10-20 seconds and stop recording.\n\n**19.**\u00a0Repeat step 18 for each activity type (use different file names):\n\nhttps://preview.redd.it/ccm1r5eor3ra1.png?width=288&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0a3b0606ae26446d326daf001fa936823bf06a05\n\nReset the board before recording each movement.\n\n**20.**\u00a0Repeat step 18 for the negative class and start recording for 1 hour.\n\n[**Here\u2019s the video**](https://drive.google.com/file/d/106bVkRNZPmXwlctTsF9wtmeEbS9FMAgg/view?usp=share_link) on how the data should be collected (make sure to do the actual activity: i.e. brushing hair should be done with an actual hairbrush on your head, not in the air).\n\n## Model training\n\nAfter you have collected data on all activities and combined them into one dataset, proceed to the\u00a0[**Neuton.ai**](https://login.neuton.ai/?utm_source=reddit&amp;utm_medium=project&amp;utm_campaign=Ultra-Tiny+Solution+of+Daily+Activities+%E2%80%8B%E2%80%8BRecognition)\u00a0platform to train your model.\n\n**1.**\u00a0Create a new solution\n\nhttps://preview.redd.it/gr3pw96tr3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=77506bbc9f1fa56bc07a6883c27af09dcb9b747c\n\n**2.**\u00a0Select the data type and upload the data\n\nhttps://preview.redd.it/3ththa1ur3ra1.jpg?width=892&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=58ce6f80240f2c8cbb66cf9c4f13e8382ba77eaa\n\nhttps://preview.redd.it/tvqrx1nur3ra1.jpg?width=1426&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=504dcaa6ce7ed6622c3b7cc57de045e218cd3c70\n\n**3.**\u00a0In the left field remove variables lacc\\_X, lacc\\_Y, lacc\\_Z (linear accelerometer is not needed for this model); in the right field select the target variable and click next.\n\nhttps://preview.redd.it/18m11zryr3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1b25cc45b4de00892e38514166178285320db1b2\n\n**4.**\u00a0Select the task type.\n\nhttps://preview.redd.it/kw9gxc60s3ra1.jpg?width=1306&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=95fd5bb4735430323398988b3298f16c5a38a550\n\n**5.**\u00a0Select input data type as INT16.\n\nhttps://preview.redd.it/k306wz61s3ra1.jpg?width=1260&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a86d3d099a80a5f5d32a7039f5ff20eec9341ff0\n\n**6.**\u00a0Enable Digital Signal Preprocessing, select Window size 200 and Sliding shift 5.\n\nhttps://preview.redd.it/mxac6t72s3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=86cee3a36680436d9344c67a6b555da490eb5ae1\n\n**7.**\u00a0For each of the features (acc\\_X, acc\\_Y, acc\\_Z, gyro\\_X, gyro\\_Y, gyro\\_Z) repeat the following steps:\n\na. Click \u2018Edit\u2019\n\nhttps://preview.redd.it/txmla2e3s3ra1.jpg?width=1824&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0d231efc02565ec396ddffa797531d38d69df85a\n\nb. Select \u2018Remove all\u2019\n\nhttps://preview.redd.it/2214pk94s3ra1.jpg?width=1788&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b446cf5ad18da0ac95ef4af9ff110ea77466a7b6\n\nc. Select \u2018Statistical\u2019 features and check the following features: Mean, Root mean square, Mean Absolute Deviation, Standard Deviation, Mean-crossing Rate, Zero-crossing Rate\n\nhttps://preview.redd.it/ai8e1825s3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6ea60af3a3f2f7bc12ee8ef63260a95f46bdfd42\n\n**8.**\u00a0Select the microcontroller bit depth: 8 bit and start training.\n\nhttps://preview.redd.it/51gk2076s3ra1.jpg?width=1324&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=891901581f7ba2d1656e4c43bc1be28575e1f588\n\nAfter the model is trained, proceed to the Prediction tab, check out model quality metrics and download the archive with the model for embedding.\n\nhttps://preview.redd.it/u35z3987s3ra1.jpg?width=1022&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=541bcb13e7c5f9e3b4db0e3294c351b4b0a1e0d7\n\nThe downloaded model source code will look like this:\n\nhttps://preview.redd.it/5yvqpe99s3ra1.jpg?width=1460&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a541f9a703d38b30cb34d642453a359af80c5b2b\n\nThe archive contains everything required for embedding the model into any MCU. Use the archive contents to compile an Arduino Sketch, flush your Nicla Sense ME with this sketch and start inference.\n\nIn case you would like to inference our pretrained model, just download the precompiled sketch for Arduino Nicla Sense ME in our repository:\u00a0[**https://github.com/NeutonTinyML/hand-activity-recognition**](https://github.com/NeutonTinyML/hand-activity-recognition)**.**", "author_fullname": "t2_8sds8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My new project with Arduino: Ultra-Tiny Solution of Daily Activities \u200b\u200bRecognition. Can you repeat? Please, leave your comments.", "link_flair_richtext": [{"e": "text", "t": "Look what I made!"}], "subreddit_name_prefixed": "r/arduino", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"tvqrx1nur3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 61, "x": 108, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbeff0c98296eb76f4069e0990a17604905e6c45"}, {"y": 122, "x": 216, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e91bc53f8e0135fb2ceefc2a54365089d4bb37f3"}, {"y": 182, "x": 320, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c24845b32d2b3c6d33855375122f768e9587994c"}, {"y": 364, "x": 640, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c0965a77b839926c684460b7b419790e62065f9"}, {"y": 546, "x": 960, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c0c865b2744a92e67269034aac4313447138ad8"}, {"y": 614, "x": 1080, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a83cf717ad602ab63dabcdc6757dd0905ad70fc"}], "s": {"y": 812, "x": 1426, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=1426&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=504dcaa6ce7ed6622c3b7cc57de045e218cd3c70"}, "id": "tvqrx1nur3ra1"}, "2214pk94s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=107f42b08257ebbf00982bc916298d6e4859f8d4"}, {"y": 129, "x": 216, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a9e0e078627214072998f2b16c108c2f7161518"}, {"y": 192, "x": 320, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50a777899aafcf66a031fb4f7a84387528d1c167"}, {"y": 384, "x": 640, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cd671c46c3ee176fb39e95c48b3cd8f6dd09000"}, {"y": 576, "x": 960, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a55c9ef2702ac12c13927b15bc7c646b1a7e8fc1"}, {"y": 648, "x": 1080, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50519ea09214d6b9027699522b138c34b65b7875"}], "s": {"y": 1074, "x": 1788, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=1788&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b446cf5ad18da0ac95ef4af9ff110ea77466a7b6"}, "id": "2214pk94s3ra1"}, "4b39v85og8sa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 95, "x": 108, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3112f5152d63b434ec88fa3b09e5d7d25648b46"}, {"y": 190, "x": 216, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b62e6a47bdaae868b25c082991e9016146c02f11"}, {"y": 282, "x": 320, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6894c0afaa09e45965154725e4307ba0279ee3e"}, {"y": 564, "x": 640, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=939566b13b16f63d9b12068192ec44d654f23792"}, {"y": 847, "x": 960, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15227ec06cacc3980c2436fd0fca04e586825e5f"}, {"y": 952, "x": 1080, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed349ee91fdc25ef31a2f91b0eab90efebb17e77"}], "s": {"y": 1807, "x": 2048, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=97e3e32ce2dbfe134d3fb3531f8e195d5f9883df"}, "id": "4b39v85og8sa1"}, "3ththa1ur3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 129, "x": 108, "u": "https://preview.redd.it/3ththa1ur3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e2bee0e8b8ab1eeaf114e338c3a48899327548f"}, {"y": 259, "x": 216, "u": "https://preview.redd.it/3ththa1ur3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f224deb366044988c9e067b4cce1678f4af214ba"}, {"y": 384, "x": 320, "u": "https://preview.redd.it/3ththa1ur3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f9e17bb9f83785c77bf91b48cfaef15444f3081"}, {"y": 769, "x": 640, "u": "https://preview.redd.it/3ththa1ur3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0cdbf9f4c2b46b348cc45d2f1963436157673b72"}], "s": {"y": 1072, "x": 892, "u": "https://preview.redd.it/3ththa1ur3ra1.jpg?width=892&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=58ce6f80240f2c8cbb66cf9c4f13e8382ba77eaa"}, "id": "3ththa1ur3ra1"}, "txmla2e3s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57b7eb2306adb71eb044cfe40d9bb42a873c2256"}, {"y": 100, "x": 216, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=343aa46729c29bc5cf39a0e0f577a05f8d6fa801"}, {"y": 148, "x": 320, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39436f3621d0080ce382d995c755ec89bf2cccf0"}, {"y": 296, "x": 640, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb579e716247a4a45ea5b8dde97dac640b94ab3c"}, {"y": 445, "x": 960, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af250f3de945e1a718e1ffe1150b936953307fde"}, {"y": 500, "x": 1080, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eae228c76a24b022ce8a582a2ac03c5c20647ec4"}], "s": {"y": 846, "x": 1824, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=1824&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0d231efc02565ec396ddffa797531d38d69df85a"}, "id": "txmla2e3s3ra1"}, "18m11zryr3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7614754e11e2f9f116deb0fbb4c6b5496e01fb8f"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02442a576ce14af510bc4bbc34f93a41817a3214"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cb0d67ef86eb4babc67b755f7f95997152f5ad6"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b06b04c18cf3ff5098dda415d8caf9c71996d356"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77e795f4e104f2fae0edec23c451847bab91c999"}, {"y": 676, "x": 1080, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6fbe62bbcaaae20e5aa91916c695e421e5dfb6d"}], "s": {"y": 1282, "x": 2048, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1b25cc45b4de00892e38514166178285320db1b2"}, "id": "18m11zryr3ra1"}, "504en08gr3ra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 49, "x": 108, "u": "https://preview.redd.it/504en08gr3ra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abbd91cc6f99845493f07d6e638174386eae18be"}, {"y": 99, "x": 216, "u": "https://preview.redd.it/504en08gr3ra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f55dd9d212ab7a1066bb50e462d4540264ab4f3e"}, {"y": 147, "x": 320, "u": "https://preview.redd.it/504en08gr3ra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ac2a6d92a1d389e49504256cb2f18ade7f9f306"}, {"y": 295, "x": 640, "u": "https://preview.redd.it/504en08gr3ra1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b6bea0e977faf9df3af9e980945bbd406fe3ba5"}, {"y": 443, "x": 960, "u": "https://preview.redd.it/504en08gr3ra1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58571b4d567799da42e5a0d482c27db5425d77a5"}, {"y": 499, "x": 1080, "u": "https://preview.redd.it/504en08gr3ra1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=777c561347d1c64b738537b3464b6030553736cb"}], "s": {"y": 604, "x": 1306, "u": "https://preview.redd.it/504en08gr3ra1.png?width=1306&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=03994ba652b0ea9c154c699a2aa6fe4af274d5d2"}, "id": "504en08gr3ra1"}, "bks9n7hjr3ra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 72, "x": 108, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=181ec25aabf3c130b80f2b444d82e497771393c0"}, {"y": 144, "x": 216, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8d9cd4b98f63ecdbac1329c43cbf5610c9d5bb8"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b18d6c0e50f6409652bc0ceefce10621a7399ee"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c271b262c4e6337cd84a2002b25fa6e63c84299"}, {"y": 640, "x": 960, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2582f30b09d73aaaf5a4ef93caa5eecfdc6d34d0"}, {"y": 720, "x": 1080, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70560ba59935b0c2a4835541b30a1328e5c1c0ff"}], "s": {"y": 1062, "x": 1592, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=1592&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f452d539b15cfbfadeb7296bd1e62fa99fe4a5bd"}, "id": "bks9n7hjr3ra1"}, "u3q15z2er3ra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 105, "x": 108, "u": "https://preview.redd.it/u3q15z2er3ra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf11b6464ef4b8639e26a94b3d50c8ae6ab386d9"}, {"y": 210, "x": 216, "u": "https://preview.redd.it/u3q15z2er3ra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf8a0f55c0b826869f2723e6481bda1a94a2fc99"}, {"y": 311, "x": 320, "u": "https://preview.redd.it/u3q15z2er3ra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16ab60ddfb27f78f0f66b12ea99d2fdd5e81e804"}], "s": {"y": 596, "x": 612, "u": "https://preview.redd.it/u3q15z2er3ra1.png?width=612&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e7734fafef3cfa398d6a23d99c8f7ad5a7f20a8a"}, "id": "u3q15z2er3ra1"}, "51gk2076s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 33, "x": 108, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0a1e48c78cb0a420f44d4c61b57de133a523c8d"}, {"y": 67, "x": 216, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca42b4e16aa3b4e31eca75965fbcf00dd65b1267"}, {"y": 100, "x": 320, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4da05cd09ef87bdb28cfc46cbb4895afc607d4c1"}, {"y": 200, "x": 640, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d77b715049e62f9cfb0b98ea7ae796fcc0f45b1"}, {"y": 300, "x": 960, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f7bcf0f9b71ad0764f112066787a7d6ee2ad821"}, {"y": 337, "x": 1080, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16362df5384fcf79b261bb28b7c24d19d3ac2016"}], "s": {"y": 414, "x": 1324, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=1324&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=891901581f7ba2d1656e4c43bc1be28575e1f588"}, "id": "51gk2076s3ra1"}, "ai8e1825s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cfd8ddc942169637496c8808f4c460fea445d18"}, {"y": 107, "x": 216, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c42b320894871760efa443abe0d34e785841820d"}, {"y": 159, "x": 320, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c17e944c231fdf22c439a37efda051a4c13214ff"}, {"y": 318, "x": 640, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f059b1afd3fa5c55ac1c77fef211068d036dc3ec"}, {"y": 477, "x": 960, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50cb6ce02486783c907069021c5938f9d1ca39b4"}, {"y": 536, "x": 1080, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe0bcd79bd56dd022242a98bed43a15dcc311d12"}], "s": {"y": 1018, "x": 2048, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6ea60af3a3f2f7bc12ee8ef63260a95f46bdfd42"}, "id": "ai8e1825s3ra1"}, "ccm1r5eor3ra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/ccm1r5eor3ra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be8bc2d3bcf9c2557e62fa7f2360695bf4d07928"}, {"y": 109, "x": 216, "u": "https://preview.redd.it/ccm1r5eor3ra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33dcc2e827a96d7b2729223a08c783b5e1607e36"}], "s": {"y": 146, "x": 288, "u": "https://preview.redd.it/ccm1r5eor3ra1.png?width=288&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0a3b0606ae26446d326daf001fa936823bf06a05"}, "id": "ccm1r5eor3ra1"}, "mxac6t72s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 83, "x": 108, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee46a1fed439d94736e0ac0deb0dcfe039ae35e8"}, {"y": 167, "x": 216, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a3d50cd3f563b618f404c4fa86fb9b6db8fa21e"}, {"y": 248, "x": 320, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce2af96bbc76ca199d3ed793393a7980380bbdca"}, {"y": 496, "x": 640, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f8d487c3b3d9bf4887c25785442f7b287173c2b"}, {"y": 744, "x": 960, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7597cd11429ed528724dfc2bb8fd036b9b5809f2"}, {"y": 837, "x": 1080, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44a0848d590b5df0643b37d25f4e8d5ce2fabe4f"}], "s": {"y": 1588, "x": 2048, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=86cee3a36680436d9344c67a6b555da490eb5ae1"}, "id": "mxac6t72s3ra1"}, "kw9gxc60s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7083b14bb0aa141306b79d21b3b690d3a3dcdb6"}, {"y": 134, "x": 216, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d65399333fe90133857be130050dabf5b3a3729"}, {"y": 199, "x": 320, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0572baa66ec2433a82ea2dd9ee24fff38c8a6ea7"}, {"y": 398, "x": 640, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d80d74d6e40f6d2ebcfb63b6039c87ab09d4fcc"}, {"y": 598, "x": 960, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=868d4290d98ceb8df1256aed44e3dae62a482bbb"}, {"y": 673, "x": 1080, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5349f8fd61e97711e4e04fc067e992c4d2ddbb5e"}], "s": {"y": 814, "x": 1306, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=1306&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=95fd5bb4735430323398988b3298f16c5a38a550"}, "id": "kw9gxc60s3ra1"}, "k306wz61s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 32, "x": 108, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b915a2cb218c64464c76f49f30136b576a9b6385"}, {"y": 65, "x": 216, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05a619469cc2e69f220412506cb041d20c7dc086"}, {"y": 97, "x": 320, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b122618a47025005851a41d2327a1b4b38ee0a09"}, {"y": 195, "x": 640, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22bca2d47bedf3801c71bbabb029020a1d808d1a"}, {"y": 292, "x": 960, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd8cc819463a3c50c08d58f0398c42a2042b503b"}, {"y": 329, "x": 1080, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=081d91f4946b32002c904142dad842e8e56f9cc0"}], "s": {"y": 384, "x": 1260, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=1260&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a86d3d099a80a5f5d32a7039f5ff20eec9341ff0"}, "id": "k306wz61s3ra1"}, "e5oi0ljir3ra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 70, "x": 108, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c8276234e46ed277d986a108737bf0c35c207f2"}, {"y": 141, "x": 216, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1a847585509cbc61cc9296ee16230f83faadb3e"}, {"y": 210, "x": 320, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acebe45286c3556db6b6319ede5419586efd6464"}, {"y": 420, "x": 640, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a925566995c91605bafce879972aecebbe0d3938"}, {"y": 630, "x": 960, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=146f5b39517e3ac77cf92028583bd3dc2155f135"}, {"y": 709, "x": 1080, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c8e0f2aaf0571eba204262e2921b90c17a1fe35"}], "s": {"y": 1048, "x": 1596, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=1596&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2c686bee67e11d127b9d7e4cbcfb3370271a95d8"}, "id": "e5oi0ljir3ra1"}, "dt1grc4jg8sa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 38, "x": 108, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=076e69b1ad3aa912f5f605a7d154e4817a81834f"}, {"y": 77, "x": 216, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b8fe0cf56a4469540490cb428bf02970c0cff0b"}, {"y": 114, "x": 320, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5fe9a191e5ab5ece9a50b3145b5cc6aa0d0e830"}, {"y": 228, "x": 640, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7895360955b5177f791fb2a5f8db90533ab7db9b"}, {"y": 343, "x": 960, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d33826234bf4eba3e91fc99056a75e72e8945b2"}, {"y": 386, "x": 1080, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07f9e665bd622c1254ee8931a4a50f1d69ae8e47"}], "s": {"y": 658, "x": 1840, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=1840&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1e386acb758209202ac65e6407f3611f045e4d27"}, "id": "dt1grc4jg8sa1"}, "u35z3987s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 82, "x": 108, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85ee6b3a39e556632aa556a0906e67a84a83bbfd"}, {"y": 165, "x": 216, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=766ff34db8432742f042e6653b0da91f15165e0b"}, {"y": 245, "x": 320, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3946fa4d099a5b89de0fee8ce6d2096dbc944626"}, {"y": 491, "x": 640, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e9cb400a5249f6db85ad35e5606480f2df344c41"}, {"y": 737, "x": 960, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56fa5055ea03c641a752a2caaa1c8ae135d01b6c"}], "s": {"y": 785, "x": 1022, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=1022&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=541bcb13e7c5f9e3b4db0e3294c351b4b0a1e0d7"}, "id": "u35z3987s3ra1"}, "gr3pw96tr3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 5, "x": 108, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=396def3eb7358ced864682723e3e9b9dea9feca3"}, {"y": 10, "x": 216, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df0bfed1fef00918e8f2bb5b3ead1ece721d5da0"}, {"y": 15, "x": 320, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53d407ea84b4cf0c1d063eadcfd66ec8e0c9239f"}, {"y": 30, "x": 640, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eba30de0893bbfeffd46c25412551f88ff8d4878"}, {"y": 45, "x": 960, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a07e7263584f08616ddb887a8b41dc0b091f4db7"}, {"y": 50, "x": 1080, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa6cd16184187dc0a6314843e0797130060dd9b0"}], "s": {"y": 96, "x": 2048, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=77506bbc9f1fa56bc07a6883c27af09dcb9b747c"}, "id": "gr3pw96tr3ra1"}, "5yvqpe99s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 57, "x": 108, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a684a29396d7e1ddcf08d4fff41d450f1fb59e0c"}, {"y": 114, "x": 216, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=180aeb2962885d3308cd563962cf752070ade8d7"}, {"y": 170, "x": 320, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e11f9f54b8e0d3b0af74b6423c6012875661d43"}, {"y": 340, "x": 640, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d447701dc9ea05ff5253c24afb7c48130adeb5d"}, {"y": 510, "x": 960, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a16c121d7cb04649a79692e213c8fb923d2cf1a9"}, {"y": 574, "x": 1080, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a78d5bf52b1bfb60bb5b763663fcbd480b8a54b1"}], "s": {"y": 776, "x": 1460, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=1460&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a541f9a703d38b30cb34d642453a359af80c5b2b"}, "id": "5yvqpe99s3ra1"}}, "name": "t3_127rccj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Look what I made!", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sYUBvdJXNIEroAyPdmrxhzxZG_3FeN7XJhUNm2cF35U.jpg", "edited": 1680773684.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680281088.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.arduino", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Prototyping: a Hand Tracker with Nicla Sense ME and Bosch sensors&lt;/p&gt;\n\n&lt;h2&gt;Things used in this project:&lt;/h2&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware components&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://referral.element14.com/OrderCodeView?fsku=3874817&amp;amp;nsku=99AJ4432&amp;amp;COM=e14c-noscript&amp;amp;CMP=e14c-noscript&amp;amp;osetc=e14-noscript-tracking-loss\"&gt;Arduino Nicla Sense ME&lt;/a&gt; \u00d71&lt;br/&gt;\n&lt;a href=\"https://www.bosch-sensortec.com/news/new-generation-of-smart-sensor-hubs-bhi260-and-bha260.html\"&gt;Bosch BHI260 sensor&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Software apps and online services&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://neuton.ai/\"&gt;Neuton Tiny ML&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.arduino.cc/en/software\"&gt;Arduino IDE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In my project, I want to show an example of how you can significantly increase the complexity of intelligence functionality, and determine what a wearable device can recognize with a very small and accurate neural network. Also, you will find a detailed guide on how to implement Multiple Daily Activities recognition using the Nicla Sense ME. You also can\u00a0&lt;a href=\"https://us06web.zoom.us/webinar/register/3416800160594/WN_X0wH_gaqSiylUGehTFXlSQ\"&gt;&lt;strong&gt;participate in my practical webinar&lt;/strong&gt;&lt;/a&gt;\u00a0on\u00a0&lt;strong&gt;April 11 at 5 pm CET&lt;/strong&gt;. Where you can watch how to create this solution from leaders from Neuton.AI, Arduino and Bosch. Don&amp;#39;t miss this opportunity to learn from experts directly.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/3nkoJZD_Lh8\"&gt;https://youtu.be/3nkoJZD_Lh8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you have Nicla Sense ME, you can try my pre-trained model in your experiment/project/classes.&lt;/p&gt;\n\n&lt;p&gt;Our test results show that the model perfectly recognizes the movements of 30 participants whose data was not used for training the model.&lt;/p&gt;\n\n&lt;p&gt;Following this tutorial, you also can easily make your own experiment and train your neural network on your data to recognize the same or different movements.&lt;/p&gt;\n\n&lt;p&gt;We believe that the ability to recognize an extremely complex and similar action using small neural networks opens up a new era for always-on devices.&lt;/p&gt;\n\n&lt;p&gt;Below, you will find a full tutorial on how to reproduce this experiment, including data collection, model training, model embedding and inference. In addition, we have created a public repository with a pre-trained model that has demonstrated good generalization on new users. It is a precompiled model archive that you may just download and embed into your Nicla Sense ME with a few clicks of a mouse and start testing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/NeutonTinyML/hand-activity-recognition\"&gt;&lt;strong&gt;https://github.com/NeutonTinyML/hand-activity-recognition&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Data Collection with Nicla Sense ME&lt;/h2&gt;\n\n&lt;p&gt;For this case, I\u2019ve selected 5 types of activities (for your experiment, you may choose other activities):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Washing hands&lt;/li&gt;\n&lt;li&gt;Brushing teeth&lt;/li&gt;\n&lt;li&gt;Clapping&lt;/li&gt;\n&lt;li&gt;Brushing hair&lt;/li&gt;\n&lt;li&gt;Random activity (negative class)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For each activity type, I collected sensor data with Nicla Sense ME for 10 minutes non-stop. Data was collected from 7 different people from 7 different locations. It was quite a challenging, yet fun process because, for proper data collection, it was undesirable to interrupt. Here\u2019s some advice for those who plan to re-conduct my experiment: before collecting data for another activity, get some rest! However, if 10 minutes of constant hair brushing is too much, you can do two 5-minute data collection streams and then concatenate the two files.&lt;/p&gt;\n\n&lt;p&gt;For the negative class, we collected 60 minutes of data. There\u2019s no need to do anything special this time; just get on with your everyday activities. Type on the keyboard, operate a mouse, answer calls, drink tea, or do whatever you want. Just be sure you don\u2019t sit completely idle with your hands still during negative class data collection. The negative class is a subset of everything else excluding the 4 classes outlined above.&lt;/p&gt;\n\n&lt;h2&gt;Steps:&lt;/h2&gt;\n\n&lt;p&gt;Now let me provide you with detailed guidelines on how to tune Arduino\u2019s board, Nicla Sense ME to collect data.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt;\u00a0Visit\u00a0&lt;a href=\"https://www.arduino.cc/en/software\"&gt;&lt;strong&gt;https://www.arduino.cc/en/software&lt;/strong&gt;&lt;/a&gt;&lt;a href=\"https://www.arduino.cc/en/software\"&gt;,&lt;/a&gt;\u00a0download Arduino IDE 2.x.x for your OS, and install a package.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt;\u00a0Open Arduino IDE, click on the\u00a0&lt;strong&gt;Boards Manager&lt;/strong&gt;\u00a0icon, type \u201cnicla sense me\u201d and install the\u00a0&lt;strong&gt;Arduino Mbed OS Nicla Boards&lt;/strong&gt;\u00a0package.&lt;/p&gt;\n\n&lt;p&gt;Click on the\u00a0&lt;strong&gt;Library Manager icon&lt;/strong&gt;, type \u201cnicla sense me\u201d in the search box and install two libraries:\u00a0&lt;strong&gt;Arduino_BHY2&lt;/strong&gt;\u00a0and\u00a0&lt;strong&gt;ArduinoBLE&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Type \u201cprintf\u201d in the search box and install\u00a0&lt;strong&gt;LibPrintf&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt;\u00a0In the main menu, select\u00a0&lt;strong&gt;File-&amp;gt;Examples-&amp;gt;Arduino_BHY2&amp;gt;BHYFirmwareUpdate&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt;\u00a0Connect your\u00a0&lt;strong&gt;Nicla Sense ME&lt;/strong&gt;\u00a0board to the USB and select the port in the dropdown menu.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt;\u00a0Click on the Upload button and wait until the uploading process is finished.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt;\u00a0Click on the\u00a0&lt;strong&gt;Serial Monitor&lt;/strong&gt;\u00a0icon and select\u00a0&lt;strong&gt;115200 baud&lt;/strong&gt;, then click the RESET button on your Nicla board.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt;\u00a0After 5-10 seconds you\u2019ll see that the BHY firmware is uploaded.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;8.&lt;/strong&gt; Download the Arduino precompiled library for sensor data collection using a Nicla Sense ME from &lt;a href=\"https://github.com/NeutonTinyML/hand-activity-recognition\"&gt;&lt;strong&gt;the Neuton repository.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;model -&amp;gt; Arduino_Neuton.zip (do not uncompress the archive)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;9.&lt;/strong&gt; Install the \u2018Arduino_Neuton.zip\u2019 model into the Arduino IDE:&lt;/p&gt;\n\n&lt;p&gt;Sketch -&amp;gt; Include Library -&amp;gt; Add .ZIP Library\u2026 (point to the downloaded \u2018Arduino_Neuton.zip\u2019 archive)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dt1grc4jg8sa1.jpg?width=1840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1e386acb758209202ac65e6407f3611f045e4d27\"&gt;https://preview.redd.it/dt1grc4jg8sa1.jpg?width=1840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1e386acb758209202ac65e6407f3611f045e4d27&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;10.&lt;/strong&gt; Quit &amp;amp; restart the Arduino IDE (it will take up to a minute for the Arduino IDE to index all of it\u2019s examples, including the freshly installed \u2018Arduino_Neuton\u2019)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;11.&lt;/strong&gt; Open the installed example: File -&amp;gt; Examples -&amp;gt; Arduino_Neuton -&amp;gt; Inertial_Sensor_Data_Collection (a new Arduino IDE window will pop up, close the previous Arduino IDE window)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4b39v85og8sa1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=97e3e32ce2dbfe134d3fb3531f8e195d5f9883df\"&gt;https://preview.redd.it/4b39v85og8sa1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=97e3e32ce2dbfe134d3fb3531f8e195d5f9883df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;12.&lt;/strong&gt; Connect your Nicla Sense ME to your computer\u2019s USB port.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;13.&lt;/strong&gt; In the IDE select your MCU and upload the firmware.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;14.&lt;/strong&gt;\u00a0After the firmware is uploaded (1-2 minutes), open the Serial Monitor - you should see the sensors readings (In case you will see the connection error \u2013 restart the Arduino IDE and open the Serial Monitor once again)&lt;/p&gt;\n\n&lt;p&gt;Now you can unplug the board from the USB port.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;15.&lt;/strong&gt;\u00a0Attach the board firmly to the watch or bracelet. The Micro-USB port should be in the bottom-right corner.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u3q15z2er3ra1.png?width=612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e7734fafef3cfa398d6a23d99c8f7ad5a7f20a8a\"&gt;https://preview.redd.it/u3q15z2er3ra1.png?width=612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e7734fafef3cfa398d6a23d99c8f7ad5a7f20a8a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;16.&lt;/strong&gt;\u00a0Put a watch or a bracelet with a circuit board on your right hand, and connect the cable to it. Make sure that the cable near the connector is not taut so as not to damage the board.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;17.&lt;/strong&gt;\u00a0Depending on your OS, open the serial port and try to log data. A serial port can be seen in the hint.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/504en08gr3ra1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=03994ba652b0ea9c154c699a2aa6fe4af274d5d2\"&gt;https://preview.redd.it/504en08gr3ra1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=03994ba652b0ea9c154c699a2aa6fe4af274d5d2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Use this command in the terminal if you\u2019re on macOS:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;stty -f /dev/cu.usbmodem85EB3A0F2 115200 | cat /dev/cu.usbmodem85EB3A0F2 | tee 4_brushing_hair.csv&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This will set the port speed to 115200 baud. Print out data from the serial port as text and save it to a CSV file.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/e5oi0ljir3ra1.png?width=1596&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2c686bee67e11d127b9d7e4cbcfb3370271a95d8\"&gt;https://preview.redd.it/e5oi0ljir3ra1.png?width=1596&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2c686bee67e11d127b9d7e4cbcfb3370271a95d8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Press Control-C to stop recording. There will be a CSV file with data from sensors (accelerometer, linear accelerometer, and gyroscope) and timestamp.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bks9n7hjr3ra1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=f452d539b15cfbfadeb7296bd1e62fa99fe4a5bd\"&gt;https://preview.redd.it/bks9n7hjr3ra1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=f452d539b15cfbfadeb7296bd1e62fa99fe4a5bd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;18.&lt;/strong&gt;\u00a0Start recording the movement. Press the Reset button on the Nicla board and restart logging to the file.&lt;/p&gt;\n\n&lt;p&gt;For example, clap for 10 minutes (use the stopwatch application in your smartphone). During this process, change the position of your hands to make the data more diverse. Do not stop moving while the recording is in progress.&lt;/p&gt;\n\n&lt;p&gt;The recording will start in ~7 seconds after pushing the Reset button.&lt;/p&gt;\n\n&lt;p&gt;Once 10 minutes are finished, continue the movement for 10-20 seconds and stop recording.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;19.&lt;/strong&gt;\u00a0Repeat step 18 for each activity type (use different file names):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ccm1r5eor3ra1.png?width=288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0a3b0606ae26446d326daf001fa936823bf06a05\"&gt;https://preview.redd.it/ccm1r5eor3ra1.png?width=288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0a3b0606ae26446d326daf001fa936823bf06a05&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Reset the board before recording each movement.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;20.&lt;/strong&gt;\u00a0Repeat step 18 for the negative class and start recording for 1 hour.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://drive.google.com/file/d/106bVkRNZPmXwlctTsF9wtmeEbS9FMAgg/view?usp=share_link\"&gt;&lt;strong&gt;Here\u2019s the video&lt;/strong&gt;&lt;/a&gt; on how the data should be collected (make sure to do the actual activity: i.e. brushing hair should be done with an actual hairbrush on your head, not in the air).&lt;/p&gt;\n\n&lt;h2&gt;Model training&lt;/h2&gt;\n\n&lt;p&gt;After you have collected data on all activities and combined them into one dataset, proceed to the\u00a0&lt;a href=\"https://login.neuton.ai/?utm_source=reddit&amp;amp;utm_medium=project&amp;amp;utm_campaign=Ultra-Tiny+Solution+of+Daily+Activities+%E2%80%8B%E2%80%8BRecognition\"&gt;&lt;strong&gt;Neuton.ai&lt;/strong&gt;&lt;/a&gt;\u00a0platform to train your model.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt;\u00a0Create a new solution&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gr3pw96tr3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=77506bbc9f1fa56bc07a6883c27af09dcb9b747c\"&gt;https://preview.redd.it/gr3pw96tr3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=77506bbc9f1fa56bc07a6883c27af09dcb9b747c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt;\u00a0Select the data type and upload the data&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3ththa1ur3ra1.jpg?width=892&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=58ce6f80240f2c8cbb66cf9c4f13e8382ba77eaa\"&gt;https://preview.redd.it/3ththa1ur3ra1.jpg?width=892&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=58ce6f80240f2c8cbb66cf9c4f13e8382ba77eaa&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tvqrx1nur3ra1.jpg?width=1426&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=504dcaa6ce7ed6622c3b7cc57de045e218cd3c70\"&gt;https://preview.redd.it/tvqrx1nur3ra1.jpg?width=1426&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=504dcaa6ce7ed6622c3b7cc57de045e218cd3c70&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt;\u00a0In the left field remove variables lacc_X, lacc_Y, lacc_Z (linear accelerometer is not needed for this model); in the right field select the target variable and click next.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/18m11zryr3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1b25cc45b4de00892e38514166178285320db1b2\"&gt;https://preview.redd.it/18m11zryr3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1b25cc45b4de00892e38514166178285320db1b2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt;\u00a0Select the task type.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kw9gxc60s3ra1.jpg?width=1306&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=95fd5bb4735430323398988b3298f16c5a38a550\"&gt;https://preview.redd.it/kw9gxc60s3ra1.jpg?width=1306&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=95fd5bb4735430323398988b3298f16c5a38a550&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt;\u00a0Select input data type as INT16.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/k306wz61s3ra1.jpg?width=1260&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a86d3d099a80a5f5d32a7039f5ff20eec9341ff0\"&gt;https://preview.redd.it/k306wz61s3ra1.jpg?width=1260&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a86d3d099a80a5f5d32a7039f5ff20eec9341ff0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt;\u00a0Enable Digital Signal Preprocessing, select Window size 200 and Sliding shift 5.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mxac6t72s3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=86cee3a36680436d9344c67a6b555da490eb5ae1\"&gt;https://preview.redd.it/mxac6t72s3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=86cee3a36680436d9344c67a6b555da490eb5ae1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt;\u00a0For each of the features (acc_X, acc_Y, acc_Z, gyro_X, gyro_Y, gyro_Z) repeat the following steps:&lt;/p&gt;\n\n&lt;p&gt;a. Click \u2018Edit\u2019&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/txmla2e3s3ra1.jpg?width=1824&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0d231efc02565ec396ddffa797531d38d69df85a\"&gt;https://preview.redd.it/txmla2e3s3ra1.jpg?width=1824&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0d231efc02565ec396ddffa797531d38d69df85a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;b. Select \u2018Remove all\u2019&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2214pk94s3ra1.jpg?width=1788&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b446cf5ad18da0ac95ef4af9ff110ea77466a7b6\"&gt;https://preview.redd.it/2214pk94s3ra1.jpg?width=1788&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b446cf5ad18da0ac95ef4af9ff110ea77466a7b6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;c. Select \u2018Statistical\u2019 features and check the following features: Mean, Root mean square, Mean Absolute Deviation, Standard Deviation, Mean-crossing Rate, Zero-crossing Rate&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ai8e1825s3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6ea60af3a3f2f7bc12ee8ef63260a95f46bdfd42\"&gt;https://preview.redd.it/ai8e1825s3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6ea60af3a3f2f7bc12ee8ef63260a95f46bdfd42&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;8.&lt;/strong&gt;\u00a0Select the microcontroller bit depth: 8 bit and start training.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/51gk2076s3ra1.jpg?width=1324&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=891901581f7ba2d1656e4c43bc1be28575e1f588\"&gt;https://preview.redd.it/51gk2076s3ra1.jpg?width=1324&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=891901581f7ba2d1656e4c43bc1be28575e1f588&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After the model is trained, proceed to the Prediction tab, check out model quality metrics and download the archive with the model for embedding.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u35z3987s3ra1.jpg?width=1022&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=541bcb13e7c5f9e3b4db0e3294c351b4b0a1e0d7\"&gt;https://preview.redd.it/u35z3987s3ra1.jpg?width=1022&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=541bcb13e7c5f9e3b4db0e3294c351b4b0a1e0d7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The downloaded model source code will look like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5yvqpe99s3ra1.jpg?width=1460&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a541f9a703d38b30cb34d642453a359af80c5b2b\"&gt;https://preview.redd.it/5yvqpe99s3ra1.jpg?width=1460&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a541f9a703d38b30cb34d642453a359af80c5b2b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The archive contains everything required for embedding the model into any MCU. Use the archive contents to compile an Arduino Sketch, flush your Nicla Sense ME with this sketch and start inference.&lt;/p&gt;\n\n&lt;p&gt;In case you would like to inference our pretrained model, just download the precompiled sketch for Arduino Nicla Sense ME in our repository:\u00a0&lt;a href=\"https://github.com/NeutonTinyML/hand-activity-recognition\"&gt;&lt;strong&gt;https://github.com/NeutonTinyML/hand-activity-recognition&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8a993096-51ce-11e9-84d9-0e29268ab306", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qknj", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#94e044", "id": "127rccj", "is_robot_indexable": true, "report_reasons": null, "author": "wasteguru", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/arduino/comments/127rccj/my_new_project_with_arduino_ultratiny_solution_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/arduino/comments/127rccj/my_new_project_with_arduino_ultratiny_solution_of/", "subreddit_subscribers": 562211, "created_utc": 1680281088.0, "num_crossposts": 16, "media": null, "is_video": false}], "created": 1680778514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.arduino", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/arduino/comments/127rccj/my_new_project_with_arduino_ultratiny_solution_of/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dge9v", "is_robot_indexable": true, "report_reasons": null, "author": "wasteguru", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_127rccj", "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dge9v/my_new_project_with_arduino_ultratiny_solution_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/arduino/comments/127rccj/my_new_project_with_arduino_ultratiny_solution_of/", "subreddit_subscribers": 868508, "created_utc": 1680778514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_c8af2ty7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why AI Struggles in Generating Human Hands: A Deep Dive into the Complexities", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_12dayeu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JQZsL6lzjZ8cePTEqT18L6JjMNVzZwRJa6HiUvC2h-A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680761225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@ramaditya.missula/why-ai-struggles-in-generating-human-hands-a-deep-dive-into-the-complexities-24403e3478f3", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?auto=webp&amp;v=enabled&amp;s=a115ef4591c48733972d3794c780fc2c1e515044", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15183f486bd3fb807f52fd7fc4879f14e5af8417", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ddf2a272e1d9c3ea0c891fbe20f10e904d1e4b38", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9af96d97291044a94af8fd652279e3151264586", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5405a2f84efbb1a7a6b38d7163f08151870491ff", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78116c2541048b0585b87c55631b258e026fefbb", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc33d30a9277e10082ad9aa0cc1a5f26e5b9e411", "width": 1080, "height": 1080}], "variants": {}, "id": "FI8ZsgjZPgi9bOy6JnRCXfJyAFOkliNf0FqKVXVNmHw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dayeu", "is_robot_indexable": true, "report_reasons": null, "author": "WeakTry9804", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dayeu/why_ai_struggles_in_generating_human_hands_a_deep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@ramaditya.missula/why-ai-struggles-in-generating-human-hands-a-deep-dive-into-the-complexities-24403e3478f3", "subreddit_subscribers": 868508, "created_utc": 1680761225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I've been a data scientist for a few years now, partially internally at companies, and partially at consultancies. To cut a long story short, I'm a consultant now but am getting tired of it. I always need to focus on selling myself and my ideas, strict deadlines put in place by people who don't understand data science, and the relative simplicity of most of the projects. \n\nFor my next job I want to make sure I end up in a team where I can learn from my colleagues and get to work on more advanced long-term projects. \n\nSo the question is, how do I do this in the best way? What questions do I ask in interviews and what red flags should I look out for? \n\nThanks!", "author_fullname": "t2_4klx88oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions to Ensure a Fun Job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctnmo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680719828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve been a data scientist for a few years now, partially internally at companies, and partially at consultancies. To cut a long story short, I&amp;#39;m a consultant now but am getting tired of it. I always need to focus on selling myself and my ideas, strict deadlines put in place by people who don&amp;#39;t understand data science, and the relative simplicity of most of the projects. &lt;/p&gt;\n\n&lt;p&gt;For my next job I want to make sure I end up in a team where I can learn from my colleagues and get to work on more advanced long-term projects. &lt;/p&gt;\n\n&lt;p&gt;So the question is, how do I do this in the best way? What questions do I ask in interviews and what red flags should I look out for? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ctnmo", "is_robot_indexable": true, "report_reasons": null, "author": "FatMansKryptonite", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ctnmo/questions_to_ensure_a_fun_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ctnmo/questions_to_ensure_a_fun_job/", "subreddit_subscribers": 868508, "created_utc": 1680719828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys,\n\nI would like to share with you an analysis that I have done with the data on the webstite [https://www.wineenthusiast.com/](https://www.wineenthusiast.com/).\n\nI focused on different topics as: the price is related to the point that the taster has gave? distribution of wine prodution, are the tasters reliable? and much more!\n\nThe analysis can be found here (The code is on the github repo): [https://manuelenolli.github.io/wine-enthusiast-analysis/](https://manuelenolli.github.io/wine-enthusiast-analysis/)\n\nI hope you will like!\n\nPlease leave a star here if you liked :) [https://github.com/ManueleNolli/wine-enthusiast-analysis](https://github.com/ManueleNolli/wine-enthusiast-analysis)", "author_fullname": "t2_sx6v8dv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wine Enthusiast Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctgd3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680719426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I would like to share with you an analysis that I have done with the data on the webstite &lt;a href=\"https://www.wineenthusiast.com/\"&gt;https://www.wineenthusiast.com/&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I focused on different topics as: the price is related to the point that the taster has gave? distribution of wine prodution, are the tasters reliable? and much more!&lt;/p&gt;\n\n&lt;p&gt;The analysis can be found here (The code is on the github repo): &lt;a href=\"https://manuelenolli.github.io/wine-enthusiast-analysis/\"&gt;https://manuelenolli.github.io/wine-enthusiast-analysis/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I hope you will like!&lt;/p&gt;\n\n&lt;p&gt;Please leave a star here if you liked :) &lt;a href=\"https://github.com/ManueleNolli/wine-enthusiast-analysis\"&gt;https://github.com/ManueleNolli/wine-enthusiast-analysis&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?auto=webp&amp;v=enabled&amp;s=873ead75340bf44f9e4da2fc2afd953b3977bec7", "width": 1200, "height": 670}, "resolutions": [{"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76b5d3bb93675bd32fe6665590ffbe189c5dfe8c", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f565d6c39a92c8bb700e1b240608bac4571545de", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2a4cf0727b05f6478dec4b5abdee33cc39a9eea", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34b2075f1ff238186f54c919bc324455bf4c7007", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c36735cd01f574a917c2191e34ebbade2a89cc7f", "width": 960, "height": 536}, {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ed87bbaa627233b62170d2412bc073390f4e052", "width": 1080, "height": 603}], "variants": {}, "id": "03yv-DUPHDw4eeu3TTN4_mcXF0BscFiqhmC5oUGhqYA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ctgd3", "is_robot_indexable": true, "report_reasons": null, "author": "ManueleNolli", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ctgd3/wine_enthusiast_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ctgd3/wine_enthusiast_analysis/", "subreddit_subscribers": 868508, "created_utc": 1680719426.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}