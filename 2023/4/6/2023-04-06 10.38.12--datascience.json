{"kind": "Listing", "data": {"after": "t3_12ck8oj", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Whenever I get stuck on something, feel like I've exhausted every option and resource I have, and finally decide to ask for help by posting on Teams or emailing a colleague, 8/10 I usually figure out the issue within 5 minutes of asking for help.  \n\nThen, I'm forced to posted an update that I figured it out because I wasn't on VPN, didn't update libraries or some other silly thing.  Does this happen to anyone else?", "author_fullname": "t2_4k8au9km", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I usually figure out the problem 5 minutes after I create the ticket, send email", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12coplb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 164, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 164, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680709961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whenever I get stuck on something, feel like I&amp;#39;ve exhausted every option and resource I have, and finally decide to ask for help by posting on Teams or emailing a colleague, 8/10 I usually figure out the issue within 5 minutes of asking for help.  &lt;/p&gt;\n\n&lt;p&gt;Then, I&amp;#39;m forced to posted an update that I figured it out because I wasn&amp;#39;t on VPN, didn&amp;#39;t update libraries or some other silly thing.  Does this happen to anyone else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12coplb", "is_robot_indexable": true, "report_reasons": null, "author": "Contango_4eva", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12coplb/i_usually_figure_out_the_problem_5_minutes_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12coplb/i_usually_figure_out_the_problem_5_minutes_after/", "subreddit_subscribers": 868420, "created_utc": 1680709961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "With Pandas 2.0, no existing code should break and everything will work as is. However, the primary update that is subtle is the use of Apache Arrow API vs. Numpy for managing and ingesting data (using methods like read\\_csv, read\\_sql, read\\_parquet, etc). This new integration is hope to increase efficiency in terms of memory use and improving the usage of data types such string,  datatime, and categories.  \n\n\n&gt;Python data structures (lists, dictionaries, tuples, etc) are very slow and can't be used. So the data representation is not Python and is not standard, and an implementation needs to happen via Python extensions, usually implemented in C (also in C++, Rust and others). For many years, the main extension to represent arrays and perform operations on them in a fast way has been NumPy. And this is what pandas was initially built on.  \n&gt;  \n&gt;While NumPy has been good enough to make pandas the popular library it is, it was never built as a backend for dataframe libraries, and it has some important limitations.\n\n**Summary of improvements include:**\n\n* **Managing missing values:** By using Arrow, pandas is able to deal with missing values without having to implement its own version for each data type. Instead, the Apache Arrow in-memory data representation includes an equivalent representation as part of its specification\n* **Speed:** Given an example of a dataframe with 2.5 million rows running in the author's laptop, running the `endswith` function is 31.6x fasters using Apache Arrow vs. Numpy (14.9ms vs. 471ms, respectively)\n* **Interoperability:** Ingesting a data in one format and outputting it in a different format should not be challenging. For example, moving from SAS data to Latex, using Pandas &lt;2.0 would require:\n   * Load the data from SAS into a pandas dataframe\n   * Export the dataframe to a parquet file\n   * Load the parquet file from Polars\n   * Make the transformations in Polars\n   * Export the Polars dataframe into a second parquet file\n   * Load the Parquet into pandas\n   * Export the data to the final LATEX file  \nHowever, with PyArrow, the operation can be as simple as such (after Polars bug fixes and using Pandas 2.0):\n\n&amp;#8203;\n\n    loaded_pandas_data = pandas.read_sas(fname) \n    \n    polars_data = polars.from_pandas(loaded_pandas_data) \n    # perform operations with pandas polars \n    \n    to_export_pandas_data = polars.to_pandas(use_pyarrow_extension_array=True) to_export_pandas_data.to_latex()\n\n* **Expanding Data Type Support:**\n\n&gt;Arrow types are broader and better when used outside of a numerical tool like NumPy. It has better support for dates and time, including types for date-only or time-only data, different precision (e.g. seconds, milliseconds, etc.), different sizes (32 bits, 63 bits, etc.). The boolean type in Arrow uses a single bit per value, consuming one eighth of memory. It also supports other types, like decimals, or binary data, as well as complex types (for example a column where each value is a list). There is [a table](https://pandas.pydata.org/docs/dev/reference/arrays.html?highlight=arrowdtype#pyarrow) in the pandas documentation mapping Arrow to NumPy types.\n\n[https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i)", "author_fullname": "t2_48648", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pandas 2.0 is going live, and Apache Arrow will replace Numpy, and that's a great thing!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dbhsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 109, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 109, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680763026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With Pandas 2.0, no existing code should break and everything will work as is. However, the primary update that is subtle is the use of Apache Arrow API vs. Numpy for managing and ingesting data (using methods like read_csv, read_sql, read_parquet, etc). This new integration is hope to increase efficiency in terms of memory use and improving the usage of data types such string,  datatime, and categories.  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Python data structures (lists, dictionaries, tuples, etc) are very slow and can&amp;#39;t be used. So the data representation is not Python and is not standard, and an implementation needs to happen via Python extensions, usually implemented in C (also in C++, Rust and others). For many years, the main extension to represent arrays and perform operations on them in a fast way has been NumPy. And this is what pandas was initially built on.  &lt;/p&gt;\n\n&lt;p&gt;While NumPy has been good enough to make pandas the popular library it is, it was never built as a backend for dataframe libraries, and it has some important limitations.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Summary of improvements include:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Managing missing values:&lt;/strong&gt; By using Arrow, pandas is able to deal with missing values without having to implement its own version for each data type. Instead, the Apache Arrow in-memory data representation includes an equivalent representation as part of its specification&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Given an example of a dataframe with 2.5 million rows running in the author&amp;#39;s laptop, running the &lt;code&gt;endswith&lt;/code&gt; function is 31.6x fasters using Apache Arrow vs. Numpy (14.9ms vs. 471ms, respectively)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Interoperability:&lt;/strong&gt; Ingesting a data in one format and outputting it in a different format should not be challenging. For example, moving from SAS data to Latex, using Pandas &amp;lt;2.0 would require:\n\n&lt;ul&gt;\n&lt;li&gt;Load the data from SAS into a pandas dataframe&lt;/li&gt;\n&lt;li&gt;Export the dataframe to a parquet file&lt;/li&gt;\n&lt;li&gt;Load the parquet file from Polars&lt;/li&gt;\n&lt;li&gt;Make the transformations in Polars&lt;/li&gt;\n&lt;li&gt;Export the Polars dataframe into a second parquet file&lt;/li&gt;\n&lt;li&gt;Load the Parquet into pandas&lt;/li&gt;\n&lt;li&gt;Export the data to the final LATEX file&lt;br/&gt;\nHowever, with PyArrow, the operation can be as simple as such (after Polars bug fixes and using Pandas 2.0):&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;loaded_pandas_data = pandas.read_sas(fname) \n\npolars_data = polars.from_pandas(loaded_pandas_data) \n# perform operations with pandas polars \n\nto_export_pandas_data = polars.to_pandas(use_pyarrow_extension_array=True) to_export_pandas_data.to_latex()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Expanding Data Type Support:&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Arrow types are broader and better when used outside of a numerical tool like NumPy. It has better support for dates and time, including types for date-only or time-only data, different precision (e.g. seconds, milliseconds, etc.), different sizes (32 bits, 63 bits, etc.). The boolean type in Arrow uses a single bit per value, consuming one eighth of memory. It also supports other types, like decimals, or binary data, as well as complex types (for example a column where each value is a list). There is &lt;a href=\"https://pandas.pydata.org/docs/dev/reference/arrays.html?highlight=arrowdtype#pyarrow\"&gt;a table&lt;/a&gt; in the pandas documentation mapping Arrow to NumPy types.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i\"&gt;https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dbhsg", "is_robot_indexable": true, "report_reasons": null, "author": "forbiscuit", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dbhsg/pandas_20_is_going_live_and_apache_arrow_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dbhsg/pandas_20_is_going_live_and_apache_arrow_will/", "subreddit_subscribers": 868420, "created_utc": 1680763026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nI've been really focusing on my presentation skills lately, but I feel like my slides\n\n1. dont always convey my information as best as possible\n2. isnt eye catching\n\nDoes anyone have any suggestions in material that could help me out?", "author_fullname": "t2_bva05apx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improving my Presentation Slides", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cy2s0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680729041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been really focusing on my presentation skills lately, but I feel like my slides&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;dont always convey my information as best as possible&lt;/li&gt;\n&lt;li&gt;isnt eye catching&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Does anyone have any suggestions in material that could help me out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cy2s0", "is_robot_indexable": true, "report_reasons": null, "author": "GuillerminaCharity", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cy2s0/improving_my_presentation_slides/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cy2s0/improving_my_presentation_slides/", "subreddit_subscribers": 868420, "created_utc": 1680729041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m going to be handed a complex codebase developed by consultants that is poorly commented. It is being used to predict supply chain logistics and is littered with obscure company specific information. Checking some of the Python files, some are 1 to 3 thousand lines of code, and I see about 30 folders in the directory. I\u2019ve never seen such a massive code base for any data science project in my career. How do I own this without getting fired?", "author_fullname": "t2_3vrcu2fcm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you do if you\u2019re expected to own a project with 10s of thousands of lines of code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12czyh7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680733195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m going to be handed a complex codebase developed by consultants that is poorly commented. It is being used to predict supply chain logistics and is littered with obscure company specific information. Checking some of the Python files, some are 1 to 3 thousand lines of code, and I see about 30 folders in the directory. I\u2019ve never seen such a massive code base for any data science project in my career. How do I own this without getting fired?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12czyh7", "is_robot_indexable": true, "report_reasons": null, "author": "Open_Peace_1745", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12czyh7/what_would_you_do_if_youre_expected_to_own_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12czyh7/what_would_you_do_if_youre_expected_to_own_a/", "subreddit_subscribers": 868420, "created_utc": 1680733195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nPosting this as it might be of interest to data scientists, both practitioners and professionals, who use Bayesian methods for parameter estimation and model evaluation.\n\n**tl;dr:** My research group just released a new open-source Python package (PyVBMC) for *sample-efficient* Bayesian inference, i.e. inference with a small number of likelihood evaluations: [PyVBMC](https://acerbilab.github.io/pyvbmc/)\n\nMore info:\n\n* `pip install pyvbmc` (or install on Anaconda via conda-forge)\n* The method runs out of the box, and we included extensive documentations and tutorials for easy accessibility: [Examples \u2014 PyVBMC](https://acerbilab.github.io/pyvbmc/examples.html)\n* A few more technical details on a [Twitter](https://twitter.com/AcerbiLuigi/status/1643549233587318784) or [Mastodon](https://mastodon.social/@AcerbiLuigi/110147657708113411) thread\n* We also have a tl;dr preprint on arXiv: [PyVBMC: Efficient Bayesian inference in Python](https://arxiv.org/abs/2303.09519)\n* Relevant papers were published at the *NeurIPS* machine learning conference in [2018](https://arxiv.org/abs/1810.05558) and [2020](https://arxiv.org/abs/2006.08655)\n\nPlease get in touch in this thread or on Twitter/Mastodon if you have any questions or comments. Thanks again for your time, and I hope this is of actual interest. Feedback is welcome!", "author_fullname": "t2_bi2haf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Python open-source software for sample-efficient Bayesian inference", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cvne6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680723989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Posting this as it might be of interest to data scientists, both practitioners and professionals, who use Bayesian methods for parameter estimation and model evaluation.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; My research group just released a new open-source Python package (PyVBMC) for &lt;em&gt;sample-efficient&lt;/em&gt; Bayesian inference, i.e. inference with a small number of likelihood evaluations: &lt;a href=\"https://acerbilab.github.io/pyvbmc/\"&gt;PyVBMC&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More info:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;pip install pyvbmc&lt;/code&gt; (or install on Anaconda via conda-forge)&lt;/li&gt;\n&lt;li&gt;The method runs out of the box, and we included extensive documentations and tutorials for easy accessibility: &lt;a href=\"https://acerbilab.github.io/pyvbmc/examples.html\"&gt;Examples \u2014 PyVBMC&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;A few more technical details on a &lt;a href=\"https://twitter.com/AcerbiLuigi/status/1643549233587318784\"&gt;Twitter&lt;/a&gt; or &lt;a href=\"https://mastodon.social/@AcerbiLuigi/110147657708113411\"&gt;Mastodon&lt;/a&gt; thread&lt;/li&gt;\n&lt;li&gt;We also have a tl;dr preprint on arXiv: &lt;a href=\"https://arxiv.org/abs/2303.09519\"&gt;PyVBMC: Efficient Bayesian inference in Python&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Relevant papers were published at the &lt;em&gt;NeurIPS&lt;/em&gt; machine learning conference in &lt;a href=\"https://arxiv.org/abs/1810.05558\"&gt;2018&lt;/a&gt; and &lt;a href=\"https://arxiv.org/abs/2006.08655\"&gt;2020&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Please get in touch in this thread or on Twitter/Mastodon if you have any questions or comments. Thanks again for your time, and I hope this is of actual interest. Feedback is welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cvne6", "is_robot_indexable": true, "report_reasons": null, "author": "emiurgo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cvne6/new_python_opensource_software_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cvne6/new_python_opensource_software_for/", "subreddit_subscribers": 868420, "created_utc": 1680723989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Due to GDPR I unfortunately cannot disclose that information.", "author_fullname": "t2_lba2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many European data scientists does it take to screw in a light bulb?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctk7w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680719638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Due to GDPR I unfortunately cannot disclose that information.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ctk7w", "is_robot_indexable": true, "report_reasons": null, "author": "tty-tourist", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ctk7w/how_many_european_data_scientists_does_it_take_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ctk7w/how_many_european_data_scientists_does_it_take_to/", "subreddit_subscribers": 868420, "created_utc": 1680719638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi All, \n\nI am looking into the easiest way to set up machine learning infrastructure for a team. It seems like Sagemaker does exactly that. However, almost nobody uses it when I ask other DS teams. Curious why that is. None of the people I talked to had a strong reason not to use it. \n\nIs anybody here using it that wants to share the pros and cons?", "author_fullname": "t2_13mxw7nv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why not Sagemaker?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cupkq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680721999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, &lt;/p&gt;\n\n&lt;p&gt;I am looking into the easiest way to set up machine learning infrastructure for a team. It seems like Sagemaker does exactly that. However, almost nobody uses it when I ask other DS teams. Curious why that is. None of the people I talked to had a strong reason not to use it. &lt;/p&gt;\n\n&lt;p&gt;Is anybody here using it that wants to share the pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cupkq", "is_robot_indexable": true, "report_reasons": null, "author": "fokke2508", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cupkq/why_not_sagemaker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cupkq/why_not_sagemaker/", "subreddit_subscribers": 868420, "created_utc": 1680721999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have some time series data where I've noticed that the error/accuracy of my predictions from a large ensemble of models has a moderate autocorrelation to past predictions. If a model was accurate yesterday then it has a decent probability of being accurate today. I was wondering if anyone has any ideas on how to capitalize on this.\n\nMy current strategy has been to simply weight each model in the ensemble based on a rolling 3 day correlation to the target live variable. Models with a high 3 day average would have a higher weight and vice versa. I've had decent success with this strategy so far, but I'm trying to brainstorm ways to take this to the next level.\n\nOther ideas I had were to train a LSTM model on the historical performance of each model in attempt to predict the current days performance.\n\nCurios to hear if anyone has any ideas on how improve on this.\n\n[Autocorrelation of model predictions](https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3e5e89e0e35a7380bea579449cedf93d7f8bb577)", "author_fullname": "t2_d43cut1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are ways to capitalize on autocorrelation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5fae6kxce6sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55809e12059b4d9a55bcf068b56fc33329a56d65"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84c3fa6c7341b66e9fa108232da6b434c8d83738"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ee9d9bf6e03381bbca907adf7e5a547b868b77c"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95022d6d89919b09f8ec27d3d4f512e97f82aa0f"}], "s": {"y": 480, "x": 640, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3e5e89e0e35a7380bea579449cedf93d7f8bb577"}, "id": "5fae6kxce6sa1"}}, "name": "t3_12d79g9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/URbWOLYXKtJSLomT80r1NC-7LDMcnD_rgTfKZQ3EilE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680750540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some time series data where I&amp;#39;ve noticed that the error/accuracy of my predictions from a large ensemble of models has a moderate autocorrelation to past predictions. If a model was accurate yesterday then it has a decent probability of being accurate today. I was wondering if anyone has any ideas on how to capitalize on this.&lt;/p&gt;\n\n&lt;p&gt;My current strategy has been to simply weight each model in the ensemble based on a rolling 3 day correlation to the target live variable. Models with a high 3 day average would have a higher weight and vice versa. I&amp;#39;ve had decent success with this strategy so far, but I&amp;#39;m trying to brainstorm ways to take this to the next level.&lt;/p&gt;\n\n&lt;p&gt;Other ideas I had were to train a LSTM model on the historical performance of each model in attempt to predict the current days performance.&lt;/p&gt;\n\n&lt;p&gt;Curios to hear if anyone has any ideas on how improve on this.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3e5e89e0e35a7380bea579449cedf93d7f8bb577\"&gt;Autocorrelation of model predictions&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12d79g9", "is_robot_indexable": true, "report_reasons": null, "author": "_McFuggin_", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12d79g9/what_are_ways_to_capitalize_on_autocorrelation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12d79g9/what_are_ways_to_capitalize_on_autocorrelation/", "subreddit_subscribers": 868420, "created_utc": 1680750540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I understand that SQL is a language that allows us to access the data base,  but REST API's also allow us to do that with their CRUD methods.   So my question is, are they both similar in what they do? If I am a developer at a given company would I most likely use one over the other or would I use them both?   I am currently self studying a lot and trying to comprehend how they are different.", "author_fullname": "t2_4325x4ta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forgive my ignorance, but is SQL Queries a substitute for REST APIs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cqv8u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680714155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand that SQL is a language that allows us to access the data base,  but REST API&amp;#39;s also allow us to do that with their CRUD methods.   So my question is, are they both similar in what they do? If I am a developer at a given company would I most likely use one over the other or would I use them both?   I am currently self studying a lot and trying to comprehend how they are different.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cqv8u", "is_robot_indexable": true, "report_reasons": null, "author": "ala4akbar", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cqv8u/forgive_my_ignorance_but_is_sql_queries_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cqv8u/forgive_my_ignorance_but_is_sql_queries_a/", "subreddit_subscribers": 868420, "created_utc": 1680714155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just like the title says! I\u2019ve experimented a ton with everything ggplot and plotly has to offer, and I\u2019m familiar with a lot of other libraries, but what are some libraries that you guys have been using, whether with or without shiny, that display data pretty well?\n\nSide quest: what are the best ways to display regressions on top of existing data? I\u2019ve only ever used base r for lm visualization", "author_fullname": "t2_6c6onn7e2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your favorite data visualization packages in r?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cf9rh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680689300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just like the title says! I\u2019ve experimented a ton with everything ggplot and plotly has to offer, and I\u2019m familiar with a lot of other libraries, but what are some libraries that you guys have been using, whether with or without shiny, that display data pretty well?&lt;/p&gt;\n\n&lt;p&gt;Side quest: what are the best ways to display regressions on top of existing data? I\u2019ve only ever used base r for lm visualization&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cf9rh", "is_robot_indexable": true, "report_reasons": null, "author": "Mcipark", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cf9rh/what_are_your_favorite_data_visualization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cf9rh/what_are_your_favorite_data_visualization/", "subreddit_subscribers": 868420, "created_utc": 1680689300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "P.s- Will share additional information as we discuss as I am also exploring the same.\n\nEdit 1-  I've got high level customer info about age, investor type, province and their type of account.\n\nI have also got customer transaction information and how long they've stayed invested also the redemption date.", "author_fullname": "t2_du1e9k98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working on a customer redemption classification problem with a very limited amount of data. With the aim to predict redemptions in the next 3 months. I've got customer info and transaction info. What are your ideas to go about the same.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12da863", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680760982.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680758948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;P.s- Will share additional information as we discuss as I am also exploring the same.&lt;/p&gt;\n\n&lt;p&gt;Edit 1-  I&amp;#39;ve got high level customer info about age, investor type, province and their type of account.&lt;/p&gt;\n\n&lt;p&gt;I have also got customer transaction information and how long they&amp;#39;ve stayed invested also the redemption date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12da863", "is_robot_indexable": true, "report_reasons": null, "author": "Lazzy_Engineer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12da863/working_on_a_customer_redemption_classification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12da863/working_on_a_customer_redemption_classification/", "subreddit_subscribers": 868420, "created_utc": 1680758948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi there :) \n\nI am currently evaluating to educate myself in the field of AI and Data Analysis. \n\nWhat I am curious now, is how realistic it is to find jobs as a freelancer, if I want to a) work with F#, and b) work on my own codebases.\n\nUsually in software jobs, you work on the existing codebases of previous colleagues, in this field it kinda seems this could be different.\n\nAs far as I understand, do I get simply get data from a database, that I have to access, evaluate and present.\n\nSo I thought if that's the case, I could choose the stack on my own anyway, and I would be not so bound to Python, Excel and the common SQL DSL. \n\nHow independent can I choose my technology as a freelancer?", "author_fullname": "t2_78je0kwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "F# Freelancing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d94qr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680761228.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680755704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there :) &lt;/p&gt;\n\n&lt;p&gt;I am currently evaluating to educate myself in the field of AI and Data Analysis. &lt;/p&gt;\n\n&lt;p&gt;What I am curious now, is how realistic it is to find jobs as a freelancer, if I want to a) work with F#, and b) work on my own codebases.&lt;/p&gt;\n\n&lt;p&gt;Usually in software jobs, you work on the existing codebases of previous colleagues, in this field it kinda seems this could be different.&lt;/p&gt;\n\n&lt;p&gt;As far as I understand, do I get simply get data from a database, that I have to access, evaluate and present.&lt;/p&gt;\n\n&lt;p&gt;So I thought if that&amp;#39;s the case, I could choose the stack on my own anyway, and I would be not so bound to Python, Excel and the common SQL DSL. &lt;/p&gt;\n\n&lt;p&gt;How independent can I choose my technology as a freelancer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12d94qr", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Stomach_8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12d94qr/f_freelancing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12d94qr/f_freelancing/", "subreddit_subscribers": 868420, "created_utc": 1680755704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone. Unless anything unexpected happens, I will be finally switching careers into data science in about 4 weeks. For me it\u2019s the realisation of a dream I started 3 years ago, and I\u2019m just so excited about this job I can\u2019t wait to start and to learn more. \nI talked to my future boss and asked him about topics I could start preparing/getting familiar with and he mentioned PySpark and Databricks. I want to have at least some knowledge about them before I start so I don\u2019t get totally overwhelmed on the first day. Do you guys know any good free resources (videos, articles, YouTube channels, etc\u2026) that I can take a look at?", "author_fullname": "t2_6xiu1hs9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks &amp; PySpark free learning resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cuueo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680722283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. Unless anything unexpected happens, I will be finally switching careers into data science in about 4 weeks. For me it\u2019s the realisation of a dream I started 3 years ago, and I\u2019m just so excited about this job I can\u2019t wait to start and to learn more. \nI talked to my future boss and asked him about topics I could start preparing/getting familiar with and he mentioned PySpark and Databricks. I want to have at least some knowledge about them before I start so I don\u2019t get totally overwhelmed on the first day. Do you guys know any good free resources (videos, articles, YouTube channels, etc\u2026) that I can take a look at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cuueo", "is_robot_indexable": true, "report_reasons": null, "author": "Davidat0r", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cuueo/databricks_pyspark_free_learning_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cuueo/databricks_pyspark_free_learning_resources/", "subreddit_subscribers": 868420, "created_utc": 1680722283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently building some forecasting models using a lightgbm regressor. \n\nIt is in an auto platform that I'm evaluating for work, and unfortunately it just auto creates the train / test split.  This is done by randomly splitting and shuffling the data. So the training set may include the very first day and the very last day. It also auto-chooses the model so I have no choice in that either.\n\nIt auto creates features from the date column, e.g. day of year, week of year etc. And uses those as predictors.\n\nI've read comments that this training split is a bad idea, and a walk forward method should be used. \n\nI agree that walk forward makes more sense, and i use that for when I'm doing time series in python. \n\nBUT... i haven't seen an explanation as to why randomly shuffling and splitting the data is not recommended. \n\nCan somebody explain?", "author_fullname": "t2_vtpzvynk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using tree based methods for forecasting. Why is it a bad idea to shuffle the data for training and testing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dcoi5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680767062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently building some forecasting models using a lightgbm regressor. &lt;/p&gt;\n\n&lt;p&gt;It is in an auto platform that I&amp;#39;m evaluating for work, and unfortunately it just auto creates the train / test split.  This is done by randomly splitting and shuffling the data. So the training set may include the very first day and the very last day. It also auto-chooses the model so I have no choice in that either.&lt;/p&gt;\n\n&lt;p&gt;It auto creates features from the date column, e.g. day of year, week of year etc. And uses those as predictors.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read comments that this training split is a bad idea, and a walk forward method should be used. &lt;/p&gt;\n\n&lt;p&gt;I agree that walk forward makes more sense, and i use that for when I&amp;#39;m doing time series in python. &lt;/p&gt;\n\n&lt;p&gt;BUT... i haven&amp;#39;t seen an explanation as to why randomly shuffling and splitting the data is not recommended. &lt;/p&gt;\n\n&lt;p&gt;Can somebody explain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dcoi5", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Lemon-402", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dcoi5/using_tree_based_methods_for_forecasting_why_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dcoi5/using_tree_based_methods_for_forecasting_why_is/", "subreddit_subscribers": 868420, "created_utc": 1680767062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a college student and early into my Data Science major. Besides class/coursework experience, I don\u2019t have anything to add to my resume (I have worked many jobs in the past, but these were all relating to sales and basic high school jobs).\n\nWondering what types of positions/job titles/experiences accept someone who is super early into the industry and doesn\u2019t have a lot on their resume (obviously not expecting to get a data science internship right now, but hopefully something to guide me there). I\u2019ve taken classes in python, Java, c++, Unix/Linux, statistics, and business analytics.\n\nAnything helps, thanks!", "author_fullname": "t2_tfkfcmgl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for starting career path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d59wi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680747787.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680745524.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a college student and early into my Data Science major. Besides class/coursework experience, I don\u2019t have anything to add to my resume (I have worked many jobs in the past, but these were all relating to sales and basic high school jobs).&lt;/p&gt;\n\n&lt;p&gt;Wondering what types of positions/job titles/experiences accept someone who is super early into the industry and doesn\u2019t have a lot on their resume (obviously not expecting to get a data science internship right now, but hopefully something to guide me there). I\u2019ve taken classes in python, Java, c++, Unix/Linux, statistics, and business analytics.&lt;/p&gt;\n\n&lt;p&gt;Anything helps, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12d59wi", "is_robot_indexable": true, "report_reasons": null, "author": "t_c9595", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12d59wi/advice_for_starting_career_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12d59wi/advice_for_starting_career_path/", "subreddit_subscribers": 868420, "created_utc": 1680745524.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \\~25x smaller than GPT-3, challenging the notion that is big always better?\n\nFrom my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back!\n\nWould love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?\n\nP.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset", "author_fullname": "t2_sgam369p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do we really need 100B+ parameters in a large language model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12coioi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680709574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DataBricks&amp;#39;s open-source LLM, &lt;a href=\"https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html\"&gt;Dolly&lt;/a&gt; performs reasonably well on many instruction-based tasks while being ~25x smaller than GPT-3, challenging the notion that is big always better?&lt;/p&gt;\n\n&lt;p&gt;From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back!&lt;/p&gt;\n\n&lt;p&gt;Would love to hear everyone&amp;#39;s opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?&lt;/p&gt;\n\n&lt;p&gt;P.S. I am kinda betting on the latter and building &lt;a href=\"https://github.com/uptrain-ai/uptrain\"&gt;UpTrain&lt;/a&gt;, an open-source project which helps you collect that high quality fine-tuning dataset&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?auto=webp&amp;v=enabled&amp;s=2f30ce106babdfb3bcc0b7e090898bc69fb7a6af", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=566ac34f9234070978fc998c54c5f2c932326feb", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd78dfa4c9b3d6aacd8f0a90c0f6912041084620", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32815a77a86e403532277c345110535827d20f0b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e90c5e2d00b68ac8cfaa0fc724dd6fd969cc3a61", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bf50761221e07260b99111867fc93c4894c5ac3", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/i90OLQ7n1nnzSbavPMu3qr-KSsjsb8aNW3_-AzYn75w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11e63fa8b3ad7ebe392e5460dd6e51df9ffc3cee", "width": 1080, "height": 607}], "variants": {}, "id": "PdGLRg9dbAxaekj3ldlsrDK4hEJmGpkDGSxOX3CaJoA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12coioi", "is_robot_indexable": true, "report_reasons": null, "author": "Vegetable-Skill-9700", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12coioi/do_we_really_need_100b_parameters_in_a_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12coioi/do_we_really_need_100b_parameters_in_a_large/", "subreddit_subscribers": 868420, "created_utc": 1680709574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Dear Data Community,\n\nI am reaching out to seek advice and assistance regarding my problem with detecting and predicting outliers in a multivariate time series. I have conducted extensive research and implemented various techniques, but I have not been able to achieve the desired results.\n\nTherefore, I am curious if any of you have experience working with this topic and could offer any insights or recommendations. Any suggestions or guidance would be greatly appreciated.\n\nThank you in advance for your time and help.", "author_fullname": "t2_pgm5qpng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Outliers detection in temporal serie", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ckkgi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680701670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear Data Community,&lt;/p&gt;\n\n&lt;p&gt;I am reaching out to seek advice and assistance regarding my problem with detecting and predicting outliers in a multivariate time series. I have conducted extensive research and implemented various techniques, but I have not been able to achieve the desired results.&lt;/p&gt;\n\n&lt;p&gt;Therefore, I am curious if any of you have experience working with this topic and could offer any insights or recommendations. Any suggestions or guidance would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your time and help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ckkgi", "is_robot_indexable": true, "report_reasons": null, "author": "Alternative_Storm569", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ckkgi/outliers_detection_in_temporal_serie/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ckkgi/outliers_detection_in_temporal_serie/", "subreddit_subscribers": 868420, "created_utc": 1680701670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I've been a data scientist for a few years now, partially internally at companies, and partially at consultancies. To cut a long story short, I'm a consultant now but am getting tired of it. I always need to focus on selling myself and my ideas, strict deadlines put in place by people who don't understand data science, and the relative simplicity of most of the projects. \n\nFor my next job I want to make sure I end up in a team where I can learn from my colleagues and get to work on more advanced long-term projects. \n\nSo the question is, how do I do this in the best way? What questions do I ask in interviews and what red flags should I look out for? \n\nThanks!", "author_fullname": "t2_4klx88oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions to Ensure a Fun Job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctnmo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680719828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve been a data scientist for a few years now, partially internally at companies, and partially at consultancies. To cut a long story short, I&amp;#39;m a consultant now but am getting tired of it. I always need to focus on selling myself and my ideas, strict deadlines put in place by people who don&amp;#39;t understand data science, and the relative simplicity of most of the projects. &lt;/p&gt;\n\n&lt;p&gt;For my next job I want to make sure I end up in a team where I can learn from my colleagues and get to work on more advanced long-term projects. &lt;/p&gt;\n\n&lt;p&gt;So the question is, how do I do this in the best way? What questions do I ask in interviews and what red flags should I look out for? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ctnmo", "is_robot_indexable": true, "report_reasons": null, "author": "FatMansKryptonite", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ctnmo/questions_to_ensure_a_fun_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ctnmo/questions_to_ensure_a_fun_job/", "subreddit_subscribers": 868420, "created_utc": 1680719828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I worked as a Data Scientist for last ~2.5 years - on a plethora of different projects related to CV, NLP, RL, and some structured data as well. \n\nProblem is, since most were POCs/early stage products for different organizations (through the service company I worked for), I never really got too in depth or did actual research (though I have a paper in medical imaging, and patent in synthetic data gen). It was mostly all applied deep learning / ML wherein I just used frameworks like Deeplab, Detectron2, HF or other libraries. Didn't ever do much coding in pure pytorch/TF. Never productionized, or worked on data pipelines or any other infrastructural / engineering stuff. In fact most were just a few readily available models which I just tried to train and get to work. The fact that I was busy travelling the Himalayas while working remotely also didn't help my seriousness towards contributing majorly and/or studying and keeping up with architectures. \n\nNow, I'm going for an MS in Data Science (procrastinated and had very late applications this cycle, got admitted to only University of Rochester yet). I'm not sure what I'll need to know by the time I graduate to land high-paying roles in the US. I'm even afraid of delving into my own resume, feels like I can't explain what exactly my contributions were. I'm sure it is not imposter syndrome, I genuinely have bad memory and a lazy work ethic (as a result of untreated ADHD and depression). \n\nI'll brush up on all ML/DL concepts etc, but I'm really overwhelmed by the breadth of my own resume and the field in general - if they start digging deep into each sub-field, I'm fucked. For instance, CV can range from basic Image processing techniques to recent developments in Vision Transformers, and then all the new multi-modal stuff that'll be super-important in 2 years. God knows what openAI/deepmind will concoct up next. Probably superintelligence. The only interview-related stuff I can be good at is data structures and algorithms. Though I don't know if it's gonna be relevant at all now. \n\nThe primary concern is that having had 3 years behind me, I would want that to not go waste - and I expect roles of DS/MLE/Applied research at Levels 2 or 3, which may require far more knowledge and end-to-end deployment experience, which I believe I do not possess. I don't even have any SWE related projects or skills. I'm currently lost as to what the expectations would be in the job and the interviews for the senior positions I wish to obtain. \n\nKindly guide me regarding what aspects of my profile I'd need to refresh, what old and new concepts I'd need to know, and what other fields I'd need to develop skills in to land high-paying roles in SV/NYC preferably.", "author_fullname": "t2_2p7ddtjv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I prepare for landing a Senior DS role after graduating from MS Data Science with 3 years of workex?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ckf7d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680701369.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I worked as a Data Scientist for last ~2.5 years - on a plethora of different projects related to CV, NLP, RL, and some structured data as well. &lt;/p&gt;\n\n&lt;p&gt;Problem is, since most were POCs/early stage products for different organizations (through the service company I worked for), I never really got too in depth or did actual research (though I have a paper in medical imaging, and patent in synthetic data gen). It was mostly all applied deep learning / ML wherein I just used frameworks like Deeplab, Detectron2, HF or other libraries. Didn&amp;#39;t ever do much coding in pure pytorch/TF. Never productionized, or worked on data pipelines or any other infrastructural / engineering stuff. In fact most were just a few readily available models which I just tried to train and get to work. The fact that I was busy travelling the Himalayas while working remotely also didn&amp;#39;t help my seriousness towards contributing majorly and/or studying and keeping up with architectures. &lt;/p&gt;\n\n&lt;p&gt;Now, I&amp;#39;m going for an MS in Data Science (procrastinated and had very late applications this cycle, got admitted to only University of Rochester yet). I&amp;#39;m not sure what I&amp;#39;ll need to know by the time I graduate to land high-paying roles in the US. I&amp;#39;m even afraid of delving into my own resume, feels like I can&amp;#39;t explain what exactly my contributions were. I&amp;#39;m sure it is not imposter syndrome, I genuinely have bad memory and a lazy work ethic (as a result of untreated ADHD and depression). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll brush up on all ML/DL concepts etc, but I&amp;#39;m really overwhelmed by the breadth of my own resume and the field in general - if they start digging deep into each sub-field, I&amp;#39;m fucked. For instance, CV can range from basic Image processing techniques to recent developments in Vision Transformers, and then all the new multi-modal stuff that&amp;#39;ll be super-important in 2 years. God knows what openAI/deepmind will concoct up next. Probably superintelligence. The only interview-related stuff I can be good at is data structures and algorithms. Though I don&amp;#39;t know if it&amp;#39;s gonna be relevant at all now. &lt;/p&gt;\n\n&lt;p&gt;The primary concern is that having had 3 years behind me, I would want that to not go waste - and I expect roles of DS/MLE/Applied research at Levels 2 or 3, which may require far more knowledge and end-to-end deployment experience, which I believe I do not possess. I don&amp;#39;t even have any SWE related projects or skills. I&amp;#39;m currently lost as to what the expectations would be in the job and the interviews for the senior positions I wish to obtain. &lt;/p&gt;\n\n&lt;p&gt;Kindly guide me regarding what aspects of my profile I&amp;#39;d need to refresh, what old and new concepts I&amp;#39;d need to know, and what other fields I&amp;#39;d need to develop skills in to land high-paying roles in SV/NYC preferably.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ckf7d", "is_robot_indexable": true, "report_reasons": null, "author": "anotheraccount97", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ckf7d/how_should_i_prepare_for_landing_a_senior_ds_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ckf7d/how_should_i_prepare_for_landing_a_senior_ds_role/", "subreddit_subscribers": 868420, "created_utc": 1680701369.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a newbie, just finished a boot camp and interviewing voraciously for multiple positions. But honestly the more information I have about the companies I apply for, what data they collect and analyse and how much these companies know about us, our behaviours and habits etc., the more I want to retreat into an off the grid house with no Wifi. Does anyone else feel this way? And if so, how do you combat these feelings?", "author_fullname": "t2_3nzq9edv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does being in this field sometimes terrify you with how much of our data is being analysed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cf7lr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680689134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a newbie, just finished a boot camp and interviewing voraciously for multiple positions. But honestly the more information I have about the companies I apply for, what data they collect and analyse and how much these companies know about us, our behaviours and habits etc., the more I want to retreat into an off the grid house with no Wifi. Does anyone else feel this way? And if so, how do you combat these feelings?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cf7lr", "is_robot_indexable": true, "report_reasons": null, "author": "InspiredByLife162", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cf7lr/does_being_in_this_field_sometimes_terrify_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cf7lr/does_being_in_this_field_sometimes_terrify_you/", "subreddit_subscribers": 868420, "created_utc": 1680689134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys,\n\nI would like to share with you an analysis that I have done with the data on the webstite [https://www.wineenthusiast.com/](https://www.wineenthusiast.com/).\n\nI focused on different topics as: the price is related to the point that the taster has gave? distribution of wine prodution, are the tasters reliable? and much more!\n\nThe analysis can be found here (The code is on the github repo): [https://manuelenolli.github.io/wine-enthusiast-analysis/](https://manuelenolli.github.io/wine-enthusiast-analysis/)\n\nI hope you will like!\n\nPlease leave a star here if you liked :) [https://github.com/ManueleNolli/wine-enthusiast-analysis](https://github.com/ManueleNolli/wine-enthusiast-analysis)", "author_fullname": "t2_sx6v8dv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wine Enthusiast Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctgd3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680719426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I would like to share with you an analysis that I have done with the data on the webstite &lt;a href=\"https://www.wineenthusiast.com/\"&gt;https://www.wineenthusiast.com/&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I focused on different topics as: the price is related to the point that the taster has gave? distribution of wine prodution, are the tasters reliable? and much more!&lt;/p&gt;\n\n&lt;p&gt;The analysis can be found here (The code is on the github repo): &lt;a href=\"https://manuelenolli.github.io/wine-enthusiast-analysis/\"&gt;https://manuelenolli.github.io/wine-enthusiast-analysis/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I hope you will like!&lt;/p&gt;\n\n&lt;p&gt;Please leave a star here if you liked :) &lt;a href=\"https://github.com/ManueleNolli/wine-enthusiast-analysis\"&gt;https://github.com/ManueleNolli/wine-enthusiast-analysis&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?auto=webp&amp;v=enabled&amp;s=873ead75340bf44f9e4da2fc2afd953b3977bec7", "width": 1200, "height": 670}, "resolutions": [{"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76b5d3bb93675bd32fe6665590ffbe189c5dfe8c", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f565d6c39a92c8bb700e1b240608bac4571545de", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2a4cf0727b05f6478dec4b5abdee33cc39a9eea", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34b2075f1ff238186f54c919bc324455bf4c7007", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c36735cd01f574a917c2191e34ebbade2a89cc7f", "width": 960, "height": 536}, {"url": "https://external-preview.redd.it/5zTtec3patam0SixAKd1mSSjoSiI93YnBGfcppjx_MU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ed87bbaa627233b62170d2412bc073390f4e052", "width": 1080, "height": 603}], "variants": {}, "id": "03yv-DUPHDw4eeu3TTN4_mcXF0BscFiqhmC5oUGhqYA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ctgd3", "is_robot_indexable": true, "report_reasons": null, "author": "ManueleNolli", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ctgd3/wine_enthusiast_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ctgd3/wine_enthusiast_analysis/", "subreddit_subscribers": 868420, "created_utc": 1680719426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_c8af2ty7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why AI Struggles in Generating Human Hands: A Deep Dive into the Complexities", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_12dayeu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JQZsL6lzjZ8cePTEqT18L6JjMNVzZwRJa6HiUvC2h-A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680761225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@ramaditya.missula/why-ai-struggles-in-generating-human-hands-a-deep-dive-into-the-complexities-24403e3478f3", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?auto=webp&amp;v=enabled&amp;s=a115ef4591c48733972d3794c780fc2c1e515044", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15183f486bd3fb807f52fd7fc4879f14e5af8417", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ddf2a272e1d9c3ea0c891fbe20f10e904d1e4b38", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9af96d97291044a94af8fd652279e3151264586", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5405a2f84efbb1a7a6b38d7163f08151870491ff", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78116c2541048b0585b87c55631b258e026fefbb", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc33d30a9277e10082ad9aa0cc1a5f26e5b9e411", "width": 1080, "height": 1080}], "variants": {}, "id": "FI8ZsgjZPgi9bOy6JnRCXfJyAFOkliNf0FqKVXVNmHw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dayeu", "is_robot_indexable": true, "report_reasons": null, "author": "WeakTry9804", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dayeu/why_ai_struggles_in_generating_human_hands_a_deep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@ramaditya.missula/why-ai-struggles-in-generating-human-hands-a-deep-dive-into-the-complexities-24403e3478f3", "subreddit_subscribers": 868420, "created_utc": 1680761225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_shnekuyr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question from high schooler interested in Data Science. If I have no prior experience in programming what is the best and effective way to start learning data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cspqk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680717878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cspqk", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial_Bath_9798", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cspqk/question_from_high_schooler_interested_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cspqk/question_from_high_schooler_interested_in_data/", "subreddit_subscribers": 868420, "created_utc": 1680717878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was discussing something with a business owner the other day just small conversations. And he asked me some questions that I wanted to hear more from the community's opinion. He asked me \"If there was a place for the Data Science, AI, Machine Learning community could gather and share ideas would that be something of interest? \n\nI gave my opinions on current landscape with AI and ChatGPT for example. Some ethic debates currently in larger population. I said my 2 cents on things like transparency and education for the public to enable understanding, which is my personal opinion. I also described how most of us communicate through github, medium, reddit, etc. \n\nBut I'm curious to see what others think? Would a more centralized place for our community to share methods of learning, code, etc. be worthwhile? What if you could do it anonymously to some degree? \n\nThis is an open discussion.", "author_fullname": "t2_p2scn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Community and What we look forward in the future", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cot4w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680710162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was discussing something with a business owner the other day just small conversations. And he asked me some questions that I wanted to hear more from the community&amp;#39;s opinion. He asked me &amp;quot;If there was a place for the Data Science, AI, Machine Learning community could gather and share ideas would that be something of interest? &lt;/p&gt;\n\n&lt;p&gt;I gave my opinions on current landscape with AI and ChatGPT for example. Some ethic debates currently in larger population. I said my 2 cents on things like transparency and education for the public to enable understanding, which is my personal opinion. I also described how most of us communicate through github, medium, reddit, etc. &lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m curious to see what others think? Would a more centralized place for our community to share methods of learning, code, etc. be worthwhile? What if you could do it anonymously to some degree? &lt;/p&gt;\n\n&lt;p&gt;This is an open discussion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12cot4w", "is_robot_indexable": true, "report_reasons": null, "author": "avpan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12cot4w/the_community_and_what_we_look_forward_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12cot4w/the_community_and_what_we_look_forward_in_the/", "subreddit_subscribers": 868420, "created_utc": 1680710162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello guys, I am a recent addition to the subreddit. The title kinda says what is going on. I recently graduated from college (BS) with a biology/physics degree with minors in chemistry and math and I am about a year out of school with a mechanical engineering job. At this job that I detest, I was looking to move up by becoming a data scientist for the factory (eventually). I have been learning python, making graphs and cleaning data at work but unfortunately I don't have the CS background that many of you may have. I wanted to ask what would be the next step to move forward into this career path? Is there ant yt videos that may help me learn theory/processes. I am still pretty good at math, I was forced in undergrad to take 3 statistics courses and statistical mechanics (from thermo) so math shouldn't be a problem. But am just not sure what next step to take ie,. What to learn.", "author_fullname": "t2_4zlw0p0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting into Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ck8oj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.22, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680700995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I am a recent addition to the subreddit. The title kinda says what is going on. I recently graduated from college (BS) with a biology/physics degree with minors in chemistry and math and I am about a year out of school with a mechanical engineering job. At this job that I detest, I was looking to move up by becoming a data scientist for the factory (eventually). I have been learning python, making graphs and cleaning data at work but unfortunately I don&amp;#39;t have the CS background that many of you may have. I wanted to ask what would be the next step to move forward into this career path? Is there ant yt videos that may help me learn theory/processes. I am still pretty good at math, I was forced in undergrad to take 3 statistics courses and statistical mechanics (from thermo) so math shouldn&amp;#39;t be a problem. But am just not sure what next step to take ie,. What to learn.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ck8oj", "is_robot_indexable": true, "report_reasons": null, "author": "CarbonPhysician", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ck8oj/getting_into_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ck8oj/getting_into_data_science/", "subreddit_subscribers": 868420, "created_utc": 1680700995.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}