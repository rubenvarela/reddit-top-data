{"kind": "Listing", "data": {"after": "t3_12dopeu", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Found out a query that I wrote was causing an issue with duplication. The duplication compounded therefore causing one of the tables to grow exponentially larger and larger each time. It\u2019s also on a scheduled run every hour. Problem ended up costing almost $30k\u2026.\n\nAnyone got any stories of when they fucked up?", "author_fullname": "t2_5ukitegd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I messed up today\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12da1uw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 126, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 126, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680758394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Found out a query that I wrote was causing an issue with duplication. The duplication compounded therefore causing one of the tables to grow exponentially larger and larger each time. It\u2019s also on a scheduled run every hour. Problem ended up costing almost $30k\u2026.&lt;/p&gt;\n\n&lt;p&gt;Anyone got any stories of when they fucked up?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12da1uw", "is_robot_indexable": true, "report_reasons": null, "author": "burningburnerbern", "discussion_type": null, "num_comments": 69, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12da1uw/i_messed_up_today/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12da1uw/i_messed_up_today/", "subreddit_subscribers": 96365, "created_utc": 1680758394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering,\n\nI'm Matteo, and, over the last few months, I have been working with my co-founder and other folks from Goldman Sachs, Netflix, Palantir, and DBS Bank to simplify building data APIs. I have personally faced this problem myself multiple times, but, the inspiration to create a company out of it really came from this [Netflix article](https://netflixtechblog.com/bulldozer-batch-data-moving-from-data-warehouse-to-online-key-value-stores-41bac13863f8).\n\nYou know the story: you have tons of data locked in your data platform and RDBMS and suddenly,  a PM asks to integrate this data with your customer-facing app. Obviously, all in real-time. And the pain begins! You have to set up infrastructure to move and process the data in real-time (Kafka, Spark, Flink), provision a solid caching/serving layer, build APIs on top and, only at the end of all this, you can start integrating data with your mobile or web app! As if all this is not enough, because you are now serving data to customers, you have to put in place all the monitoring and recovery tools, just in case something goes wrong.\n\nThere must be an easier way !!!!!\n\nThat is what drove us to build Dozer. Dozer is a simple open-source Data APIs backend that allows you to source data in real-time from databases, data warehouses, files, etc., process it using SQL, store all the results in a caching layer, and automatically provide gRPC and REST APIs. Everything with just a bunch of SQL and YAML files. \n\nIn Dozer everything happens in real-time: we subscribe to CDC sources (i.e. Postgres CDC, Snowflake table streams, etc.), process all events using our Reactive SQL engine, and store the results in the cache. The advantage is that data in the serving layer is always pre-aggregated, and fresh, which helps us to guarantee constant low latency.\n\nWe are at a very early stage, but Dozer can already be downloaded from our [GitHub repo](https://github.com/getdozer/dozer). We have taken the decision to build it entirely in Rust, which gives us the ridiculous performance and the beauty of a self-contained binary.\n\nWe are now working on several features like cloud deployment, blue/green deployment of caches, data actions (aka real-time triggers in Typescript/Python), a nice UI, and many others.\n\nPlease try it out and let us know your feedback. We have set up a [samples-repository](https://github.com/getdozer/dozer-samples) for testing it out and a [Discord channel](https://discord.com/invite/3eWXBgJaEQ) in case you need help or would like to contribute ideas!\n\nThanks  \nMatteo", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Dozer: The Future of Data APIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12db1ol", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 69, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 69, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680761526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m Matteo, and, over the last few months, I have been working with my co-founder and other folks from Goldman Sachs, Netflix, Palantir, and DBS Bank to simplify building data APIs. I have personally faced this problem myself multiple times, but, the inspiration to create a company out of it really came from this &lt;a href=\"https://netflixtechblog.com/bulldozer-batch-data-moving-from-data-warehouse-to-online-key-value-stores-41bac13863f8\"&gt;Netflix article&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;You know the story: you have tons of data locked in your data platform and RDBMS and suddenly,  a PM asks to integrate this data with your customer-facing app. Obviously, all in real-time. And the pain begins! You have to set up infrastructure to move and process the data in real-time (Kafka, Spark, Flink), provision a solid caching/serving layer, build APIs on top and, only at the end of all this, you can start integrating data with your mobile or web app! As if all this is not enough, because you are now serving data to customers, you have to put in place all the monitoring and recovery tools, just in case something goes wrong.&lt;/p&gt;\n\n&lt;p&gt;There must be an easier way !!!!!&lt;/p&gt;\n\n&lt;p&gt;That is what drove us to build Dozer. Dozer is a simple open-source Data APIs backend that allows you to source data in real-time from databases, data warehouses, files, etc., process it using SQL, store all the results in a caching layer, and automatically provide gRPC and REST APIs. Everything with just a bunch of SQL and YAML files. &lt;/p&gt;\n\n&lt;p&gt;In Dozer everything happens in real-time: we subscribe to CDC sources (i.e. Postgres CDC, Snowflake table streams, etc.), process all events using our Reactive SQL engine, and store the results in the cache. The advantage is that data in the serving layer is always pre-aggregated, and fresh, which helps us to guarantee constant low latency.&lt;/p&gt;\n\n&lt;p&gt;We are at a very early stage, but Dozer can already be downloaded from our &lt;a href=\"https://github.com/getdozer/dozer\"&gt;GitHub repo&lt;/a&gt;. We have taken the decision to build it entirely in Rust, which gives us the ridiculous performance and the beauty of a self-contained binary.&lt;/p&gt;\n\n&lt;p&gt;We are now working on several features like cloud deployment, blue/green deployment of caches, data actions (aka real-time triggers in Typescript/Python), a nice UI, and many others.&lt;/p&gt;\n\n&lt;p&gt;Please try it out and let us know your feedback. We have set up a &lt;a href=\"https://github.com/getdozer/dozer-samples\"&gt;samples-repository&lt;/a&gt; for testing it out and a &lt;a href=\"https://discord.com/invite/3eWXBgJaEQ\"&gt;Discord channel&lt;/a&gt; in case you need help or would like to contribute ideas!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;br/&gt;\nMatteo&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?auto=webp&amp;v=enabled&amp;s=0f0e1e306e13d39799c2da671e752a19580ce5dd", "width": 1200, "height": 690}, "resolutions": [{"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50dae319be6f16eecb783a9893d0c94ba2ef4cc4", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e83cfa5581e0af59a47dabdff03443ffebed335", "width": 216, "height": 124}, {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87dec77a385aecc107507c8981ebe798331c6295", "width": 320, "height": 184}, {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3038e40ef300c6e7cf9d37bb3ea116bb594f3a4", "width": 640, "height": 368}, {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64f5bfd0d5d4a6ccf0a306d72beb8f2a6c9059b9", "width": 960, "height": 552}, {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d98fd0b51a7d3ed426cdd58b63d45ea3d084437d", "width": 1080, "height": 621}], "variants": {}, "id": "sXGvQu_-ieSZjP2jlHJuyO9HHoMDsgg92289NrHtXT8"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "12db1ol", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12db1ol/dozer_the_future_of_data_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12db1ol/dozer_the_future_of_data_apis/", "subreddit_subscribers": 96365, "created_utc": 1680761526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you guys use any orchestrator with your spark structured streaming workloads. Airflow seems like is made for batch workloads. Any good suggestions ?", "author_fullname": "t2_62mycgca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestrator for streaming workloads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d3og3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680741705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you guys use any orchestrator with your spark structured streaming workloads. Airflow seems like is made for batch workloads. Any good suggestions ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12d3og3", "is_robot_indexable": true, "report_reasons": null, "author": "Wonderful_Original61", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12d3og3/orchestrator_for_streaming_workloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12d3og3/orchestrator_for_streaming_workloads/", "subreddit_subscribers": 96365, "created_utc": 1680741705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently a data engineer at a unicorn tech company, and I started applying a couple of months ago to see if I could get a job offer in this economy.  I have 5 years of experience, and before my current job, I was a data engineer at a smaller tech company and a non-tech company in the fortune 500.\n\n\n\n\n\n\nI only applied to about 20-25 jobs in the past couple of months, since there really isn't that many job openings in my area.  Which is strange since I live in one of the big cities for tech jobs.  LinkedIn in my area is the same promoted jobs, so there appears to be more jobs available than there really are.\n\n\n\n\nOut of about 25 applications, I got one interview at a very small tech startup with a decent amount of funding.  I did really well in the recruiter and programming rounds, but I couldn't get a final round interview since there is a lot of competition for jobs.\n\n\n\n\nIt seems that even smaller tech companies and startups don't really care if you worked at a big tech company for several years and if you interviewed well, since there are so many engineers on the job market.", "author_fullname": "t2_3v6ob2bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineers, what has been your experience applying for jobs in this economy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dii09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680783746.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680783508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently a data engineer at a unicorn tech company, and I started applying a couple of months ago to see if I could get a job offer in this economy.  I have 5 years of experience, and before my current job, I was a data engineer at a smaller tech company and a non-tech company in the fortune 500.&lt;/p&gt;\n\n&lt;p&gt;I only applied to about 20-25 jobs in the past couple of months, since there really isn&amp;#39;t that many job openings in my area.  Which is strange since I live in one of the big cities for tech jobs.  LinkedIn in my area is the same promoted jobs, so there appears to be more jobs available than there really are.&lt;/p&gt;\n\n&lt;p&gt;Out of about 25 applications, I got one interview at a very small tech startup with a decent amount of funding.  I did really well in the recruiter and programming rounds, but I couldn&amp;#39;t get a final round interview since there is a lot of competition for jobs.&lt;/p&gt;\n\n&lt;p&gt;It seems that even smaller tech companies and startups don&amp;#39;t really care if you worked at a big tech company for several years and if you interviewed well, since there are so many engineers on the job market.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dii09", "is_robot_indexable": true, "report_reasons": null, "author": "data_preprocessing", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12dii09/data_engineers_what_has_been_your_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dii09/data_engineers_what_has_been_your_experience/", "subreddit_subscribers": 96365, "created_utc": 1680783508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to gauge the viability of running Python/Go/Dask data lake transformations (raw data in many files -&gt; cleaned data in many parquet files). \n\n- Spark might intro a lot of overhead and complexity\n- We don't want to buy into Databricks\n- We're running on K8s with a solid platform team for support\n- Right now, Python scripts + Fivetran drop raw data files into data lake\n- First goal is to transform these raw files into silver and gold parquet-based tables in the data lake\n- After gold, teams are allowed to load into a DWH, query directly with some BI tool, etc but that's not the focus right now.\n\nWe know our infra will change over the next couple of years as this data operation gets going so writing simple, low cost scripts in languages that we're handy with would be great.\n\nMy worry in this case is performance. Can these languages/frameworks work well when needing to transform ~50GBs of data spread across many (hourly partitioned probably) files and transform them into the silver and gold. \n\nAny experience with something like this? Thanks.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any teams building data lakes without Spark? And specifically with vanilla Python/Go or with something like Dask?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dps2u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680798898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to gauge the viability of running Python/Go/Dask data lake transformations (raw data in many files -&amp;gt; cleaned data in many parquet files). &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Spark might intro a lot of overhead and complexity&lt;/li&gt;\n&lt;li&gt;We don&amp;#39;t want to buy into Databricks&lt;/li&gt;\n&lt;li&gt;We&amp;#39;re running on K8s with a solid platform team for support&lt;/li&gt;\n&lt;li&gt;Right now, Python scripts + Fivetran drop raw data files into data lake&lt;/li&gt;\n&lt;li&gt;First goal is to transform these raw files into silver and gold parquet-based tables in the data lake&lt;/li&gt;\n&lt;li&gt;After gold, teams are allowed to load into a DWH, query directly with some BI tool, etc but that&amp;#39;s not the focus right now.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We know our infra will change over the next couple of years as this data operation gets going so writing simple, low cost scripts in languages that we&amp;#39;re handy with would be great.&lt;/p&gt;\n\n&lt;p&gt;My worry in this case is performance. Can these languages/frameworks work well when needing to transform ~50GBs of data spread across many (hourly partitioned probably) files and transform them into the silver and gold. &lt;/p&gt;\n\n&lt;p&gt;Any experience with something like this? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dps2u", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dps2u/any_teams_building_data_lakes_without_spark_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dps2u/any_teams_building_data_lakes_without_spark_and/", "subreddit_subscribers": 96365, "created_utc": 1680798898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nMy task is to test Azure Data Pipelines.\n\n***My question is HOW?***\n\nCan you provide some resources?\n\nFor someone with experience, how would a test case look like? (give some examples please)\n\nContext:\n\nI recently joined a new project as QA Automation. In the initial phase of the project the pipeline will just copy data from a on-premise database into Azure Data Lake. Later, transformations and business rules will be added to the pipeline\n\nI have 0 experience with data pipelines and Azure Data Lake. I have experience with API testing, microservices, mobile (android and ios), end-to-end starting with creating testdata in backend and testing it in the UI.\n\nTools I've been using so far: Java, RestAssured, Cucumber, Appium (for mobile), a little bit of Gatling for performance, and of course Git, BitBucket, Jenkins, Azure Repo and Azure DevOps Pipelines.\n\nI am a bit stressed because I have to come up with a presentation proposing some tools for testing the Azure Data Pipelines (the internet suggested dbt or Great Expectations). How can I come up with pros and cons to each tool if I haven't worked with them yet? :)\n\nThank you!", "author_fullname": "t2_12t1ny", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing Azure Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dg9oy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680778182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;My task is to test Azure Data Pipelines.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;My question is HOW?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Can you provide some resources?&lt;/p&gt;\n\n&lt;p&gt;For someone with experience, how would a test case look like? (give some examples please)&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;/p&gt;\n\n&lt;p&gt;I recently joined a new project as QA Automation. In the initial phase of the project the pipeline will just copy data from a on-premise database into Azure Data Lake. Later, transformations and business rules will be added to the pipeline&lt;/p&gt;\n\n&lt;p&gt;I have 0 experience with data pipelines and Azure Data Lake. I have experience with API testing, microservices, mobile (android and ios), end-to-end starting with creating testdata in backend and testing it in the UI.&lt;/p&gt;\n\n&lt;p&gt;Tools I&amp;#39;ve been using so far: Java, RestAssured, Cucumber, Appium (for mobile), a little bit of Gatling for performance, and of course Git, BitBucket, Jenkins, Azure Repo and Azure DevOps Pipelines.&lt;/p&gt;\n\n&lt;p&gt;I am a bit stressed because I have to come up with a presentation proposing some tools for testing the Azure Data Pipelines (the internet suggested dbt or Great Expectations). How can I come up with pros and cons to each tool if I haven&amp;#39;t worked with them yet? :)&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dg9oy", "is_robot_indexable": true, "report_reasons": null, "author": "intranca", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dg9oy/testing_azure_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dg9oy/testing_azure_data_pipelines/", "subreddit_subscribers": 96365, "created_utc": 1680778182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Won 1, finished 3rd in my friends challenge that had 59 people, and my wife finished 3rd with it in her work bracket.  Curious if anyone else has ever used methods like this to win as I've never done this well on my own.  Also, I was actually nowhere close to winning the Kaggle competition, so I'm curious if the people who won that had some crazy good predictions.", "author_fullname": "t2_1ddx9ayn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For the first time in my life I won a march madness bracket and used a ton of Feature Engineering with basic XGboost.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_12d5euk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-LEbbB2lniOUFAO2cKG8KMtt7QeWx_nuJwebwEJNTcw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680745868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Won 1, finished 3rd in my friends challenge that had 59 people, and my wife finished 3rd with it in her work bracket.  Curious if anyone else has ever used methods like this to win as I&amp;#39;ve never done this well on my own.  Also, I was actually nowhere close to winning the Kaggle competition, so I&amp;#39;m curious if the people who won that had some crazy good predictions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/snowflake/predicting-the-unpredictable-march-madness-using-snowpark-and-hex-f16dc4f57add?source=friends_link&amp;sk=4c2216e26a4e4f24f86c4df03444d919", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?auto=webp&amp;v=enabled&amp;s=c26a2651b234be99d55c669e1a283b5635276400", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=643993ed8a95fd2699d5cd5d6fd6cda94e7ad570", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d6c0057e2f750e593ccfa7bde754f41e610f7ee", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85dcadad2629873ae6efda6e4db54354d8cf8eef", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=945eaba96e9b78c2184772223fe1d0155578a63f", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0acad35b072acc0e1c7485ace7479d78ade1b523", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a859347350cd936f4ef90a48a28db83582feaf4b", "width": 1080, "height": 720}], "variants": {}, "id": "O6SJUM6dNR6FZjK52_mdT9LDRXkMJHCkY6JvyM8AD6A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12d5euk", "is_robot_indexable": true, "report_reasons": null, "author": "crom5805", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12d5euk/for_the_first_time_in_my_life_i_won_a_march/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/snowflake/predicting-the-unpredictable-march-madness-using-snowpark-and-hex-f16dc4f57add?source=friends_link&amp;sk=4c2216e26a4e4f24f86c4df03444d919", "subreddit_subscribers": 96365, "created_utc": 1680745868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake vs Redshift: a comprehensive guide on choosing your cloud data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12dkst1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": "transparent", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/E7S3Y4PGDYtRiG7Qx7vB_-NFu3PcyJ9ljzm69C6Ws3A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680788599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/snowflake-vs-redshift", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?auto=webp&amp;v=enabled&amp;s=47a86384027e35d5146c9faa79795835d7a8d550", "width": 1800, "height": 946}, "resolutions": [{"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=789e3825d5a199333c23ac750d8dd304cc73a331", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=978c798f81ef9e752dee778def8ecb74e3ce938f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6e45f6c05585fce3fb85f409d71d7e1e156c645", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e3f92c8c124d0017104970b7f52c32604059077", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2dcfe0591ca1ac1a3859ed799e9a0336c4bca90b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=026bcd859fd79edaf025c8df1554c501a2585eac", "width": 1080, "height": 567}], "variants": {}, "id": "FcKPoXUUPVO80Peja_IMgPPk998zQAeEIgojWPAIXeE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12dkst1", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12dkst1/snowflake_vs_redshift_a_comprehensive_guide_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/snowflake-vs-redshift", "subreddit_subscribers": 96365, "created_utc": 1680788599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious to hear how others test data pipelines that spread across multiple tools.\n\n\\- Do you create dev and staging environments for each layer?\n\n\\- Do you run the whole pipeline during development?\n\n\\- Do you write unit, integration, or end-to-end tests?\n\n\\- Do you review data differences across the pipeline?\n\nI wrote an [article](https://www.datafold.com/blog/testing-data-pipelines) discussing some challenges to testing each layer: storage, integration, orchestration, transformation, BI, activation\u2026\n\nOne of my takes is that data teams will keep moving most of the complex transformation logic to the transformation layer (notably dbt):\n\n\\- ELT pipelines move transformation out from the integration layer\n\n\\- the metrics layer moves transformations out of the BI layer\n\n\\- the activation layer already advocates for transforming data with dbt\n\nIf raw data enters the transformation layer and ready-to-be-consumed data exits, can you test end-to-end pipelines from the transformation layer?", "author_fullname": "t2_975og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing the modern data stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12djkq1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680786231.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680785962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious to hear how others test data pipelines that spread across multiple tools.&lt;/p&gt;\n\n&lt;p&gt;- Do you create dev and staging environments for each layer?&lt;/p&gt;\n\n&lt;p&gt;- Do you run the whole pipeline during development?&lt;/p&gt;\n\n&lt;p&gt;- Do you write unit, integration, or end-to-end tests?&lt;/p&gt;\n\n&lt;p&gt;- Do you review data differences across the pipeline?&lt;/p&gt;\n\n&lt;p&gt;I wrote an &lt;a href=\"https://www.datafold.com/blog/testing-data-pipelines\"&gt;article&lt;/a&gt; discussing some challenges to testing each layer: storage, integration, orchestration, transformation, BI, activation\u2026&lt;/p&gt;\n\n&lt;p&gt;One of my takes is that data teams will keep moving most of the complex transformation logic to the transformation layer (notably dbt):&lt;/p&gt;\n\n&lt;p&gt;- ELT pipelines move transformation out from the integration layer&lt;/p&gt;\n\n&lt;p&gt;- the metrics layer moves transformations out of the BI layer&lt;/p&gt;\n\n&lt;p&gt;- the activation layer already advocates for transforming data with dbt&lt;/p&gt;\n\n&lt;p&gt;If raw data enters the transformation layer and ready-to-be-consumed data exits, can you test end-to-end pipelines from the transformation layer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?auto=webp&amp;v=enabled&amp;s=629e981b19b1f5a09b7eca4029116bb93f9ffc34", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c1b1fa08a10ecc2430d2298bcabf0718ae941de", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9afafbfd738711eb984df3e8bd957b4413cddb98", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9cb4920d04a54371115a4c625830dde9b2bf377", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=935d9a143280253de2d05b9ac7c3a169e0bf0ed8", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44dbe71446119d6bb4ced5e7b697ac68e35f1337", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41cf4c548a336e53bb91e25656580d6cf778f934", "width": 1080, "height": 607}], "variants": {}, "id": "EpOJBL6tEGJlfDwVwF5pZQ7zInpkiLXsFmo98IemLk4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12djkq1", "is_robot_indexable": true, "report_reasons": null, "author": "arimbr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12djkq1/testing_the_modern_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12djkq1/testing_the_modern_data_stack/", "subreddit_subscribers": 96365, "created_utc": 1680785962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, i have a production airflow orchestrating 4 clusters on pre 2.0 version. Our scheduler is dying constantly and no one knows why.\n\nWe have over 500k entries in x-com table, and probably as much in other tables, so our db has probably never been cleaned. I belive that to be slowing us, among other things, but senior engineer in charge of my team said its not the case ( guy is known for not caring too much). \n\nAny expert here to confirm or deny this? We have a ton of other bad practices, but this would be the quickest to clean up for sure.", "author_fullname": "t2_4smbd7xs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow meta-db cleaning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12df018", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680774766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i have a production airflow orchestrating 4 clusters on pre 2.0 version. Our scheduler is dying constantly and no one knows why.&lt;/p&gt;\n\n&lt;p&gt;We have over 500k entries in x-com table, and probably as much in other tables, so our db has probably never been cleaned. I belive that to be slowing us, among other things, but senior engineer in charge of my team said its not the case ( guy is known for not caring too much). &lt;/p&gt;\n\n&lt;p&gt;Any expert here to confirm or deny this? We have a ton of other bad practices, but this would be the quickest to clean up for sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12df018", "is_robot_indexable": true, "report_reasons": null, "author": "Sneakyfrog112", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12df018/airflow_metadb_cleaning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12df018/airflow_metadb_cleaning/", "subreddit_subscribers": 96365, "created_utc": 1680774766.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have a doubt about how to build my data warehouse's data model. The company I work for is similar to a marketplace that connects sellers and buyers.\n\nHere's a dummy example of how the model of the company works if a buyer buys a product in 3 installments.\n\n&amp;#x200B;\n\ntransaction\\_table (example)\n\nhttps://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725\n\n&amp;#x200B;\n\n(the buyer's amount would be higher because a fee is charged to the buyer)\n\n&amp;#x200B;\n\nWhat would be the best way to model this in my data warehouse? Currently, I don't have a specific data warehouse set up, but rather views of the table in MySQL where I separate the table in the following way:\n\n&amp;#x200B;\n\ntransaction\\_buyer\\_view (example)\n\nhttps://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60\n\n&amp;#x200B;\n\ntransaction\\_seller\\_view (example)\n\nhttps://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75\n\n&amp;#x200B;\n\nSo, when I create a dashboard in Power BI, I retrieve information from either table depending on whether I need to meet a requirement for a team that needs to see information from the buyer's or seller's side. However, I feel that this may not be the most comfortable solution for working with the information. That's why I wanted to know if the best practices for designing a data warehouse would recommend this approach, or if it would be better to keep it in the same table as before, or to have the installments that correspond to the same purchase in the same row (instead of a new row, as it is now), or some other alternative.\n\nAlso, another question I have is what would be the best way to track cases where there an error in the database and the records of the buyers are missing, but the sellers' records are present.\n\n&amp;#x200B;\n\nThank you in advance!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data warehouse design: records of buyers and sellers in same or different table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zffzdz72o4sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2e0f5de5526f44455a37c609ea90b6442938910"}, {"y": 70, "x": 216, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62694771cf40fb8b29211cf3d04d26aca76c2c25"}, {"y": 105, "x": 320, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51782bd6a7b1f2de1a1be2414df68e43ad3906f0"}, {"y": 210, "x": 640, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e3c2a3620f30c59bf7130766c6fbdef1704a068"}, {"y": 315, "x": 960, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8134bb853de479b0f09e0484f85981d1e6be5eb3"}], "s": {"y": 325, "x": 990, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60"}, "id": "zffzdz72o4sa1"}, "9r9hfg1yn4sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0b0805a8ca305bbea5d323be35c74642fb0ffe5"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37d1e84fcea14857b98d112478823013f114bcf9"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b251a36cd5491052d8743ac86d65c83bd2ca834b"}, {"y": 378, "x": 640, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00640cbefe7b03bdd0bab94c7989e28579d74053"}, {"y": 567, "x": 960, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=108b0aa8813753a63ac384ec369a5d0e0886c728"}], "s": {"y": 598, "x": 1012, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725"}, "id": "9r9hfg1yn4sa1"}, "t9k8f7n4o4sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=27dab59246e45df6f8fab4334a2f6c90e96dc350"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0cc48b0b6af66c2f1cb335f2a6be1399144e700"}, {"y": 105, "x": 320, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f936fea5468277f034a58307535798fc4b7980b6"}, {"y": 210, "x": 640, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=023c3f8c1b1a2307f86b81dce273b5b0fd8be07f"}, {"y": 315, "x": 960, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=217b94a79f4b9d903ec0a80ae1aa921c04c39eff"}], "s": {"y": 327, "x": 994, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75"}, "id": "t9k8f7n4o4sa1"}}, "name": "t3_12cx79i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xvaDteTqhhImJzA-CGeD8yEAH51KPN3UhXKI5atxXMw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680727334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a doubt about how to build my data warehouse&amp;#39;s data model. The company I work for is similar to a marketplace that connects sellers and buyers.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a dummy example of how the model of the company works if a buyer buys a product in 3 installments.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;transaction_table (example)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725\"&gt;https://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(the buyer&amp;#39;s amount would be higher because a fee is charged to the buyer)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to model this in my data warehouse? Currently, I don&amp;#39;t have a specific data warehouse set up, but rather views of the table in MySQL where I separate the table in the following way:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;transaction_buyer_view (example)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60\"&gt;https://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;transaction_seller_view (example)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75\"&gt;https://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So, when I create a dashboard in Power BI, I retrieve information from either table depending on whether I need to meet a requirement for a team that needs to see information from the buyer&amp;#39;s or seller&amp;#39;s side. However, I feel that this may not be the most comfortable solution for working with the information. That&amp;#39;s why I wanted to know if the best practices for designing a data warehouse would recommend this approach, or if it would be better to keep it in the same table as before, or to have the installments that correspond to the same purchase in the same row (instead of a new row, as it is now), or some other alternative.&lt;/p&gt;\n\n&lt;p&gt;Also, another question I have is what would be the best way to track cases where there an error in the database and the records of the buyers are missing, but the sellers&amp;#39; records are present.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cx79i", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cx79i/data_warehouse_design_records_of_buyers_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cx79i/data_warehouse_design_records_of_buyers_and/", "subreddit_subscribers": 96365, "created_utc": 1680727334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am working on designing the architecture of a web app that is going to ask a bunch of input parameters from the user and perform joins, filters and aggregations on BigQuery.\n\nMy first idea was to have a flask app for the web part. Request comes in from the user, flask runs the function that pulls data from BQ and we process the data locally.\n\nHowever, this does not seem to be very performant. I would like to avoid having to read/write large tables into BigQuery so I am looking for a SQL-based solution. \n\nI am already using dbt for other projects but I am wondering if it really suits this use-case. \n\nI did stumble upon dbt server that would be able to run dbt operations in response to API Requests, but don't know how it would scale.\n\nAny thoughts ?", "author_fullname": "t2_kqzh0kz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data processing and web app", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dl8qj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680789584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am working on designing the architecture of a web app that is going to ask a bunch of input parameters from the user and perform joins, filters and aggregations on BigQuery.&lt;/p&gt;\n\n&lt;p&gt;My first idea was to have a flask app for the web part. Request comes in from the user, flask runs the function that pulls data from BQ and we process the data locally.&lt;/p&gt;\n\n&lt;p&gt;However, this does not seem to be very performant. I would like to avoid having to read/write large tables into BigQuery so I am looking for a SQL-based solution. &lt;/p&gt;\n\n&lt;p&gt;I am already using dbt for other projects but I am wondering if it really suits this use-case. &lt;/p&gt;\n\n&lt;p&gt;I did stumble upon dbt server that would be able to run dbt operations in response to API Requests, but don&amp;#39;t know how it would scale.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dl8qj", "is_robot_indexable": true, "report_reasons": null, "author": "JamieA28", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dl8qj/data_processing_and_web_app/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dl8qj/data_processing_and_web_app/", "subreddit_subscribers": 96365, "created_utc": 1680789584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Google BigQuery documentation discourages single row inserts (commonly needed in applications), since BigQuery is supposed to be used for bulk updates. ([Source](https://cloud.google.com/bigquery/docs/best-practices-performance-patterns#dml_statements_that_update_or_insert_single_rows)).\n\nFrom the documentation, they suggest that you instead can use Cloud SQL with federated queries, which [enables BigQuery to query data from the Cloud SQL database.](https://cloud.google.com/bigquery/docs/cloud-sql-federated-queries). However, the data ends up being stored in a traditional database, like MySQL, instead of BigQuery and loses thereby some features (I would imagine ???).\n\nAs an alternative, I thought about using redis as a \"man-in-the-middle\" to batch analytics data from the application (or multiple applications when deployed in a cluster), and then insert those every X minutes (as a load job, or is stream better???) into BigQuery.\nI can't seem to find many sources that mention such an approach, and I was wondering why not? Are there disadvantages that I'm not aware of which makes Cloud SQL a better choice in this situation?", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using redis to batch data from various sources and then bulk insert into BigQuery, is this common?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dh2ht", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680780174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google BigQuery documentation discourages single row inserts (commonly needed in applications), since BigQuery is supposed to be used for bulk updates. (&lt;a href=\"https://cloud.google.com/bigquery/docs/best-practices-performance-patterns#dml_statements_that_update_or_insert_single_rows\"&gt;Source&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;From the documentation, they suggest that you instead can use Cloud SQL with federated queries, which &lt;a href=\"https://cloud.google.com/bigquery/docs/cloud-sql-federated-queries\"&gt;enables BigQuery to query data from the Cloud SQL database.&lt;/a&gt;. However, the data ends up being stored in a traditional database, like MySQL, instead of BigQuery and loses thereby some features (I would imagine ???).&lt;/p&gt;\n\n&lt;p&gt;As an alternative, I thought about using redis as a &amp;quot;man-in-the-middle&amp;quot; to batch analytics data from the application (or multiple applications when deployed in a cluster), and then insert those every X minutes (as a load job, or is stream better???) into BigQuery.\nI can&amp;#39;t seem to find many sources that mention such an approach, and I was wondering why not? Are there disadvantages that I&amp;#39;m not aware of which makes Cloud SQL a better choice in this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?auto=webp&amp;v=enabled&amp;s=20d96e49b41bb7bd00ff56b8e9ed66dc6ed60231", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cefbc38d0f527d52311150f3e2a5ee0cd4294045", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23a8451c1ff5cb4b86d4b12437a301825fbdeb9f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ac4fd116586a05dcece0cf47055879bc2ac44aa", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=467e1aba873015b34b1a4e2b7b44a79b2ab7343c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f90c3f55c5ad89d8d04390c2907361929ba9d300", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2c9d97288732e216db7689a964c326a90bdbe29", "width": 1080, "height": 567}], "variants": {}, "id": "DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dh2ht", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dh2ht/using_redis_to_batch_data_from_various_sources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dh2ht/using_redis_to_batch_data_from_various_sources/", "subreddit_subscribers": 96365, "created_utc": 1680780174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. I\u2019m curious if there are any .NET/C# lurkers here like me.\n\nI\u2019m starting to build an equivalent of Apache Airflow/Prefect for .NET. I don\u2019t have much in my repos at all right now because I just started coding, but I\u2019m curious for any members here: would you use something like that? I\u2019m thinking about doing open-core: free if you host yourself, or you can pay me to host for you.\n\nThe .NET community surprised me and responded very positively to the idea: https://www.reddit.com/r/dotnet/comments/11wa5bx/net_modern_task_scheduler/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=ioscss&amp;utm_content=1&amp;utm_term=1\n\nRepo links:\n- Didact Engine : https://www.github.com/DidactHQ/didact-engine\n- Didact UI : https://www.github.com/DidactHQ/didact-ui\n\n[View Poll](https://www.reddit.com/poll/12d3db5)", "author_fullname": "t2_853j9w4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Airflow/Prefect Alternative Orchestration Platform for .NET/C#", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d3db5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680740996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I\u2019m curious if there are any .NET/C# lurkers here like me.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m starting to build an equivalent of Apache Airflow/Prefect for .NET. I don\u2019t have much in my repos at all right now because I just started coding, but I\u2019m curious for any members here: would you use something like that? I\u2019m thinking about doing open-core: free if you host yourself, or you can pay me to host for you.&lt;/p&gt;\n\n&lt;p&gt;The .NET community surprised me and responded very positively to the idea: &lt;a href=\"https://www.reddit.com/r/dotnet/comments/11wa5bx/net_modern_task_scheduler/?utm_source=share&amp;amp;utm_medium=ios_app&amp;amp;utm_name=ioscss&amp;amp;utm_content=1&amp;amp;utm_term=1\"&gt;https://www.reddit.com/r/dotnet/comments/11wa5bx/net_modern_task_scheduler/?utm_source=share&amp;amp;utm_medium=ios_app&amp;amp;utm_name=ioscss&amp;amp;utm_content=1&amp;amp;utm_term=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Repo links:\n- Didact Engine : &lt;a href=\"https://www.github.com/DidactHQ/didact-engine\"&gt;https://www.github.com/DidactHQ/didact-engine&lt;/a&gt;\n- Didact UI : &lt;a href=\"https://www.github.com/DidactHQ/didact-ui\"&gt;https://www.github.com/DidactHQ/didact-ui&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/12d3db5\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?auto=webp&amp;v=enabled&amp;s=621880b80ac0984531bf1b2cee3997ba21a3cdd9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30b9087aa3e76bb32a434a14280e82c3c7395608", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbc1fe746b513ad1aeefe9834a6772a7d5eb665b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c917900c6414ca3d37c49e3b63751d2e75b79bb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2e67510b47c414e9502e1232dd9e0912172c42a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9269608d055aaa24020be035b4ef5a9469f121a8", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a32bce45315ce1428533c9cdb114590be034598", "width": 1080, "height": 540}], "variants": {}, "id": "GREycnATKEH_PY8fctk9Qa6Kcg1tAo5YmufzMypRpYQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12d3db5", "is_robot_indexable": true, "report_reasons": null, "author": "SirLagsABot", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1680827396094, "options": [{"text": "Yeah, I\u2019d use this.", "id": "22420806"}, {"text": "No thanks.", "id": "22420807"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 51, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12d3db5/apache_airflowprefect_alternative_orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/12d3db5/apache_airflowprefect_alternative_orchestration/", "subreddit_subscribers": 96365, "created_utc": 1680740996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have Snowflake, and were using Tableau as the analytics and reporting tool. However the business has made a decision to instead move to Power BI, and due to the way it works there is a requirement to fully cache the data into Power BI every night.\n\nSo the insights team has set up the Power BI Service, which basically does a SELECT \\* on every fact and dim in the warehouse each night.\n\nMy question: we pay a lot of money to Snowflake. But based on the above approach, how much value are we actually receiving over a simple RDS?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake with Fully Cached Power BI - is there value?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d1i75", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680736724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have Snowflake, and were using Tableau as the analytics and reporting tool. However the business has made a decision to instead move to Power BI, and due to the way it works there is a requirement to fully cache the data into Power BI every night.&lt;/p&gt;\n\n&lt;p&gt;So the insights team has set up the Power BI Service, which basically does a SELECT * on every fact and dim in the warehouse each night.&lt;/p&gt;\n\n&lt;p&gt;My question: we pay a lot of money to Snowflake. But based on the above approach, how much value are we actually receiving over a simple RDS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12d1i75", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12d1i75/snowflake_with_fully_cached_power_bi_is_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12d1i75/snowflake_with_fully_cached_power_bi_is_there/", "subreddit_subscribers": 96365, "created_utc": 1680736724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I\u2019m not sure if this is the right subreddit to post to but I\u2019ve recently received a job offer and was wanted to get some insight on it.\n\nI currently live in DFW and have received an offer for a Data Engineer role for ~80k as my current contract is ending soon. Overall my background is in data analytics and modeling with 2 YoE (undergraduate and masters degree). This is a raise from my current role as an analyst but I have to be honest, I don\u2019t know if I have a ton of knowledge on data pipelining but I feel as though I am a relatively fast learner.\n\nI wanted to know if this is a reasonable offer because upon receiving it I have felt a little underwhelmed and even potentially undervalued. I have a feeling this may also be because of high amounts of socials praising high pay and wanted to know if that\u2019s gotten to my head.", "author_fullname": "t2_1hfbaj39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience &amp; Negotiation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cybtl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680729579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I\u2019m not sure if this is the right subreddit to post to but I\u2019ve recently received a job offer and was wanted to get some insight on it.&lt;/p&gt;\n\n&lt;p&gt;I currently live in DFW and have received an offer for a Data Engineer role for ~80k as my current contract is ending soon. Overall my background is in data analytics and modeling with 2 YoE (undergraduate and masters degree). This is a raise from my current role as an analyst but I have to be honest, I don\u2019t know if I have a ton of knowledge on data pipelining but I feel as though I am a relatively fast learner.&lt;/p&gt;\n\n&lt;p&gt;I wanted to know if this is a reasonable offer because upon receiving it I have felt a little underwhelmed and even potentially undervalued. I have a feeling this may also be because of high amounts of socials praising high pay and wanted to know if that\u2019s gotten to my head.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12cybtl", "is_robot_indexable": true, "report_reasons": null, "author": "UnsureSnake", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cybtl/experience_negotiation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cybtl/experience_negotiation/", "subreddit_subscribers": 96365, "created_utc": 1680729579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI have seen a lot of posts regarding the use cases of Docker but not really any on how it actually works. Many mention that it is great for dependency issues (one version of Python required for one process vs another) but how does Docker actually solve this?", "author_fullname": "t2_jr18wyyz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docker - Magic or Hype?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dteg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680806003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have seen a lot of posts regarding the use cases of Docker but not really any on how it actually works. Many mention that it is great for dependency issues (one version of Python required for one process vs another) but how does Docker actually solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dteg3", "is_robot_indexable": true, "report_reasons": null, "author": "Fintechie__", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dteg3/docker_magic_or_hype/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dteg3/docker_magic_or_hype/", "subreddit_subscribers": 96365, "created_utc": 1680806003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In AWS Glue (PySpark) I'm reading a 29GB CSV file from S3 then repartitioning to 204 partitions and finally writing out to S3. The Spark log for the FileScanRDD stage shows that a single executor is processing the entire 29GB and spilling a huge amount of data. To me this seems like Spark is using a  single executor to first read the entire CSV. I was under the impression that a  splittable format such as CSV could be read in chunks by each executor rather than going into a single executor. Is the only way to avoid the single-executor read to break up the input CSV into smaller files before reading? Also, why are  the 101GB disk and 40GB memory spill larger than the input size of 29GB? I'm  generally just trying to understand how the data transfer is happening  with respect to executors/partitions.\n\n&amp;#x200B;\n\n[FileScan Executor Metrics](https://preview.redd.it/s8gseh4mjasa1.png?width=1871&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fdb7fe4668506e2ece55a36057df3cd0208960b0)\n\n&amp;#x200B;\n\n[FileScan DAG](https://preview.redd.it/nung0qcojasa1.png?width=565&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=70ed7073d4608c93117de8fb4767a3cb9f57fe2b)\n\n&amp;#x200B;\n\n    import sys\n    from awsglue.transforms import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.context import SparkContext, SparkConf\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    import time\n    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n    \n    sc = SparkContext()\n    glueContext = GlueContext(sc)\n    spark = glueContext.spark_session\n    \n    \n    df = spark.read.format('csv').option('header',True)\\\n                                .option('multiline',True)\\\n                                .option(\"escape\", \"\\\"\")\\\n                                .option(\"delimiter\", \",\")\\\n                                .load(f's3://bucket/folder')\n    \n    df = df.repartition(numPartitions=204)\n    \n    \n    db = 'athena-db-name'\n    path = 's3://target_bucket/target_folder'\n    table = 'target_table_name'\n    folder = table.replace('_','-')\n    target_path = path + folder\n    \n    \n    df.write.saveAsTable(f\"`{db}`.{table}\", format='parquet', mode='overwrite', path=target_path)", "author_fullname": "t2_1evp2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark (AWS Glue) spill when reading large CSV file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 20, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nung0qcojasa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 132, "x": 108, "u": "https://preview.redd.it/nung0qcojasa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca92d05ca7ae22413fedb452ca6bdef04f34997d"}, {"y": 265, "x": 216, "u": "https://preview.redd.it/nung0qcojasa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de956270a3400fa6e6283e50d500906975001db1"}, {"y": 393, "x": 320, "u": "https://preview.redd.it/nung0qcojasa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8859cfb230ddf501ddf0044fd189fad47589abe"}], "s": {"y": 695, "x": 565, "u": "https://preview.redd.it/nung0qcojasa1.png?width=565&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=70ed7073d4608c93117de8fb4767a3cb9f57fe2b"}, "id": "nung0qcojasa1"}, "s8gseh4mjasa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 16, "x": 108, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7e753437a67bae601dd2e16fece2b648cc0f2ff"}, {"y": 32, "x": 216, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bda3090b9168e845d89555e379289c18be255c63"}, {"y": 47, "x": 320, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc98e1e06dea0ec99d3ec622e4217d074cf3959b"}, {"y": 95, "x": 640, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d70bfc4858eca2a3bf43c275b01c1762a6150d1"}, {"y": 142, "x": 960, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bc6879429dfbb8293c22095bc630b1b7988daba"}, {"y": 160, "x": 1080, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=481f83db3e97408cfb64fcf36606cd5770a9fea7"}], "s": {"y": 278, "x": 1871, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=1871&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fdb7fe4668506e2ece55a36057df3cd0208960b0"}, "id": "s8gseh4mjasa1"}}, "name": "t3_12dpot8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ffkA9zEVfvOCOc37wATyi335c6BIwRvXsujeDYMRxL4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680798718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In AWS Glue (PySpark) I&amp;#39;m reading a 29GB CSV file from S3 then repartitioning to 204 partitions and finally writing out to S3. The Spark log for the FileScanRDD stage shows that a single executor is processing the entire 29GB and spilling a huge amount of data. To me this seems like Spark is using a  single executor to first read the entire CSV. I was under the impression that a  splittable format such as CSV could be read in chunks by each executor rather than going into a single executor. Is the only way to avoid the single-executor read to break up the input CSV into smaller files before reading? Also, why are  the 101GB disk and 40GB memory spill larger than the input size of 29GB? I&amp;#39;m  generally just trying to understand how the data transfer is happening  with respect to executors/partitions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s8gseh4mjasa1.png?width=1871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fdb7fe4668506e2ece55a36057df3cd0208960b0\"&gt;FileScan Executor Metrics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nung0qcojasa1.png?width=565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=70ed7073d4608c93117de8fb4767a3cb9f57fe2b\"&gt;FileScan DAG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext, SparkConf\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport time\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\n\ndf = spark.read.format(&amp;#39;csv&amp;#39;).option(&amp;#39;header&amp;#39;,True)\\\n                            .option(&amp;#39;multiline&amp;#39;,True)\\\n                            .option(&amp;quot;escape&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;)\\\n                            .option(&amp;quot;delimiter&amp;quot;, &amp;quot;,&amp;quot;)\\\n                            .load(f&amp;#39;s3://bucket/folder&amp;#39;)\n\ndf = df.repartition(numPartitions=204)\n\n\ndb = &amp;#39;athena-db-name&amp;#39;\npath = &amp;#39;s3://target_bucket/target_folder&amp;#39;\ntable = &amp;#39;target_table_name&amp;#39;\nfolder = table.replace(&amp;#39;_&amp;#39;,&amp;#39;-&amp;#39;)\ntarget_path = path + folder\n\n\ndf.write.saveAsTable(f&amp;quot;`{db}`.{table}&amp;quot;, format=&amp;#39;parquet&amp;#39;, mode=&amp;#39;overwrite&amp;#39;, path=target_path)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dpot8", "is_robot_indexable": true, "report_reasons": null, "author": "LaminatedMisanthropy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dpot8/pyspark_aws_glue_spill_when_reading_large_csv_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dpot8/pyspark_aws_glue_spill_when_reading_large_csv_file/", "subreddit_subscribers": 96365, "created_utc": 1680798718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there anyway to set file sizes on write to 128mb without using coalesce() or repartition()?\n\nBtw, I\u2019m also interested in answers that use glue syntax aswell as pyspark.", "author_fullname": "t2_5fmit0v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark, Glue and Small/Big files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dot4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680796980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there anyway to set file sizes on write to 128mb without using coalesce() or repartition()?&lt;/p&gt;\n\n&lt;p&gt;Btw, I\u2019m also interested in answers that use glue syntax aswell as pyspark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dot4u", "is_robot_indexable": true, "report_reasons": null, "author": "gabbom_XCII", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dot4u/spark_glue_and_smallbig_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dot4u/spark_glue_and_smallbig_files/", "subreddit_subscribers": 96365, "created_utc": 1680796980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to tokenize some sensitive columns in my data warehouse. However, the tokenization needs to be format preserving so that business users can still carry our analysis without any issues. Are there any standard format preserving tokenization algorithm that I can use out of box or should I write my own custom tokenization algorithm?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any standard format preserving tokenization algorithm that we can use out of box?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dnhpn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680794354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to tokenize some sensitive columns in my data warehouse. However, the tokenization needs to be format preserving so that business users can still carry our analysis without any issues. Are there any standard format preserving tokenization algorithm that I can use out of box or should I write my own custom tokenization algorithm?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dnhpn", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dnhpn/are_there_any_standard_format_preserving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dnhpn/are_there_any_standard_format_preserving/", "subreddit_subscribers": 96365, "created_utc": 1680794354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, colorfulskull here. I'm a 3y swe, now moving into a de path in a mid-size startup. So, I've been working with several data scientists and analysts who love using Jupyter notebooks. I mean, I get it \u2013 they're great for prototyping, and it's so fast and easy to get results. But as someone who prefers working in a proper IDE and dev environment, I'm finding it hard when we move towards production. It's like going from smooth sailing to navigating a minefield!\n\nSo, I'm reaching out to the r/dataengineering ecosystem for guidance. What are some best practices you've found for working with notebooks, especially when moving towards production? Someone recommended me projects like nbdev, but not sure if that is the right path the follow. How do you strike a balance between the flexibility of notebooks and the structure of a traditional development environment?  Want to make sure I'm not taking the wrong steps, what are some blogs/individuals/newsletters that I should follow?\n\nI dumped all the questions I have in one post, hope it's not a bother. I'd love to hear your thoughts and experiences \ud83d\ude04\ud83d\ude80\ud83d\udcda", "author_fullname": "t2_43r4n5ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "tough time with notebooks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dn7ej", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680793767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, colorfulskull here. I&amp;#39;m a 3y swe, now moving into a de path in a mid-size startup. So, I&amp;#39;ve been working with several data scientists and analysts who love using Jupyter notebooks. I mean, I get it \u2013 they&amp;#39;re great for prototyping, and it&amp;#39;s so fast and easy to get results. But as someone who prefers working in a proper IDE and dev environment, I&amp;#39;m finding it hard when we move towards production. It&amp;#39;s like going from smooth sailing to navigating a minefield!&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m reaching out to the &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; ecosystem for guidance. What are some best practices you&amp;#39;ve found for working with notebooks, especially when moving towards production? Someone recommended me projects like nbdev, but not sure if that is the right path the follow. How do you strike a balance between the flexibility of notebooks and the structure of a traditional development environment?  Want to make sure I&amp;#39;m not taking the wrong steps, what are some blogs/individuals/newsletters that I should follow?&lt;/p&gt;\n\n&lt;p&gt;I dumped all the questions I have in one post, hope it&amp;#39;s not a bother. I&amp;#39;d love to hear your thoughts and experiences \ud83d\ude04\ud83d\ude80\ud83d\udcda&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dn7ej", "is_robot_indexable": true, "report_reasons": null, "author": "colorfulskull", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dn7ej/tough_time_with_notebooks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dn7ej/tough_time_with_notebooks/", "subreddit_subscribers": 96365, "created_utc": 1680793767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ay1q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Arrow String improvements in Pandas/Dask DataFrames", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_12djy0h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6uwjML35z-FxsE_pRboam-GDwq291V8TrPtVWQof1WA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680786747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/coiled-hq/pyarrow-strings-in-dask-dataframes-55a0c4871586", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?auto=webp&amp;v=enabled&amp;s=6859023178db536f94b55bb63382a7a09109032b", "width": 792, "height": 490}, "resolutions": [{"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9618b07d38c42c9b44e92644148b22822523cd9e", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e340c81948a4fba6552f26b728af8711788bc35", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ac1c66e5bf7be22194afbe2e088a490ce2f46dd", "width": 320, "height": 197}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4cd149bae3ae18eb7da5790f10a0c5f43a190ed", "width": 640, "height": 395}], "variants": {}, "id": "V5FocKkZS3WgTL0zjz6iujrafykJdPf8Om_OTahpOug"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "12djy0h", "is_robot_indexable": true, "report_reasons": null, "author": "mrocklin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12djy0h/arrow_string_improvements_in_pandasdask_dataframes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/coiled-hq/pyarrow-strings-in-dask-dataframes-55a0c4871586", "subreddit_subscribers": 96365, "created_utc": 1680786747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are running infrastructure as an agency for several clients. These clients have a low data maturity in most cases, setting the first steps in the data journey, and they range from 50-500 employees on average.\n\nFor the data stack, we mostly use Azure Data Factory, combined with a SQL database on DTU basis. ADF loads incremental data from the source into a 'staging' schema in SQL. Then ADF runs Stored Procedures to transfer the data from staging to a 'live' schema (this is still the 'raw' data, no transformations happen). \n\nAfter all pipelines that take data from the source have runned, we trigger Stored Procedures to transform the 'live' data into Dims and Facts.\n\nAll of this is logged and monitored in Elastic Search. The costs we make with this setup (elastic excluded) range from \u20ac50-\u20ac400 per month per client.\n\nMy questions are:\n- is this a logical setup?\n- what would be a next step for upscaling this to a more 'modern' or 'scalable' approach?", "author_fullname": "t2_bozno7ze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sanity check needed: Azure stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dax1e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680761107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are running infrastructure as an agency for several clients. These clients have a low data maturity in most cases, setting the first steps in the data journey, and they range from 50-500 employees on average.&lt;/p&gt;\n\n&lt;p&gt;For the data stack, we mostly use Azure Data Factory, combined with a SQL database on DTU basis. ADF loads incremental data from the source into a &amp;#39;staging&amp;#39; schema in SQL. Then ADF runs Stored Procedures to transfer the data from staging to a &amp;#39;live&amp;#39; schema (this is still the &amp;#39;raw&amp;#39; data, no transformations happen). &lt;/p&gt;\n\n&lt;p&gt;After all pipelines that take data from the source have runned, we trigger Stored Procedures to transform the &amp;#39;live&amp;#39; data into Dims and Facts.&lt;/p&gt;\n\n&lt;p&gt;All of this is logged and monitored in Elastic Search. The costs we make with this setup (elastic excluded) range from \u20ac50-\u20ac400 per month per client.&lt;/p&gt;\n\n&lt;p&gt;My questions are:\n- is this a logical setup?\n- what would be a next step for upscaling this to a more &amp;#39;modern&amp;#39; or &amp;#39;scalable&amp;#39; approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dax1e", "is_robot_indexable": true, "report_reasons": null, "author": "Royal_Statistician75", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dax1e/sanity_check_needed_azure_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dax1e/sanity_check_needed_azure_stack/", "subreddit_subscribers": 96365, "created_utc": 1680761107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a BI developer (SQL monkey) and I want to switch to a more technical direction.\n\nI don't use python, so one of logical ways to pursuit DE job is to become Data Analyst who uses SQL and python regularly. \n\nBut I am also considering some sysadmin-like  roles inside my company, that involve working with DHW and cloud network (bash), which seems like a DevOps job.\n\nAfaik, understanding cloud engineering and bash to balance data load/storage is essential for DE.\n\nShall I apply for Data Analyst positions or gain some DevOps knowledge  ?", "author_fullname": "t2_gy4d61n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DevOps knowledge beneficial to switch to DE from BI ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cww0x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680726652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a BI developer (SQL monkey) and I want to switch to a more technical direction.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t use python, so one of logical ways to pursuit DE job is to become Data Analyst who uses SQL and python regularly. &lt;/p&gt;\n\n&lt;p&gt;But I am also considering some sysadmin-like  roles inside my company, that involve working with DHW and cloud network (bash), which seems like a DevOps job.&lt;/p&gt;\n\n&lt;p&gt;Afaik, understanding cloud engineering and bash to balance data load/storage is essential for DE.&lt;/p&gt;\n\n&lt;p&gt;Shall I apply for Data Analyst positions or gain some DevOps knowledge  ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12cww0x", "is_robot_indexable": true, "report_reasons": null, "author": "SolariDoma", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cww0x/is_devops_knowledge_beneficial_to_switch_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cww0x/is_devops_knowledge_beneficial_to_switch_to_de/", "subreddit_subscribers": 96365, "created_utc": 1680726652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am trying to find the optimal program setup for machine learning training and forecasting large amount of financial time series data on a single computer.\n\nComputer: amd 5900x, 128gb ram, 4tb nvme, rtx 3070\n\nData :  Data size is over 1tb  and have individual daily files that needs to be read. over 500 millions of rows of data.\n\nWhat would you use?\n\npyspark, polars, any other alternatives?\n\nSpeed is important, can't load data to memory since it is more than 1tb.\n\nI am not sure about SAS or Alteryx.\n\nThank you.", "author_fullname": "t2_400w8tx5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimal setup for big data training on single computer/node.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dopeu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680796798.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am trying to find the optimal program setup for machine learning training and forecasting large amount of financial time series data on a single computer.&lt;/p&gt;\n\n&lt;p&gt;Computer: amd 5900x, 128gb ram, 4tb nvme, rtx 3070&lt;/p&gt;\n\n&lt;p&gt;Data :  Data size is over 1tb  and have individual daily files that needs to be read. over 500 millions of rows of data.&lt;/p&gt;\n\n&lt;p&gt;What would you use?&lt;/p&gt;\n\n&lt;p&gt;pyspark, polars, any other alternatives?&lt;/p&gt;\n\n&lt;p&gt;Speed is important, can&amp;#39;t load data to memory since it is more than 1tb.&lt;/p&gt;\n\n&lt;p&gt;I am not sure about SAS or Alteryx.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dopeu", "is_robot_indexable": true, "report_reasons": null, "author": "ozioh19", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dopeu/optimal_setup_for_big_data_training_on_single/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dopeu/optimal_setup_for_big_data_training_on_single/", "subreddit_subscribers": 96365, "created_utc": 1680796798.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}