{"kind": "Listing", "data": {"after": "t3_12df018", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Found out a query that I wrote was causing an issue with duplication. The duplication compounded therefore causing one of the tables to grow exponentially larger and larger each time. It\u2019s also on a scheduled run every hour. Problem ended up costing almost $30k\u2026.\n\nAnyone got any stories of when they fucked up?", "author_fullname": "t2_5ukitegd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I messed up today\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12da1uw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 98, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 98, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680758394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Found out a query that I wrote was causing an issue with duplication. The duplication compounded therefore causing one of the tables to grow exponentially larger and larger each time. It\u2019s also on a scheduled run every hour. Problem ended up costing almost $30k\u2026.&lt;/p&gt;\n\n&lt;p&gt;Anyone got any stories of when they fucked up?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12da1uw", "is_robot_indexable": true, "report_reasons": null, "author": "burningburnerbern", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12da1uw/i_messed_up_today/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12da1uw/i_messed_up_today/", "subreddit_subscribers": 96312, "created_utc": 1680758394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking at some foundational components for a new data platform. Looks like data lake + Spark is just all you need. Spark as a compute engine with its different APIs seems extremely powerful. \n\nWhat are the drawbacks? If self hosting I guess it's what comes with self hosting anything. If using a managed service like databricks I guess it's cost? \n\nAny insights?\n\nEDIT: Looks like there's a DBT connector for Spark. Has anyone used the two in combination? I've used dbt with RS, BQ, and SF but not Spark.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark/databricks seems amazing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctygq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 72, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 72, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680741476.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680720453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at some foundational components for a new data platform. Looks like data lake + Spark is just all you need. Spark as a compute engine with its different APIs seems extremely powerful. &lt;/p&gt;\n\n&lt;p&gt;What are the drawbacks? If self hosting I guess it&amp;#39;s what comes with self hosting anything. If using a managed service like databricks I guess it&amp;#39;s cost? &lt;/p&gt;\n\n&lt;p&gt;Any insights?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Looks like there&amp;#39;s a DBT connector for Spark. Has anyone used the two in combination? I&amp;#39;ve used dbt with RS, BQ, and SF but not Spark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ctygq", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 98, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ctygq/sparkdatabricks_seems_amazing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ctygq/sparkdatabricks_seems_amazing/", "subreddit_subscribers": 96312, "created_utc": 1680720453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering,\n\nI'm Matteo, and, over the last few months, I have been working with my co-founder and other folks from Goldman Sachs, Netflix, Palantir, and DBS Bank to simplify building data APIs. I have personally faced this problem myself multiple times, but, the inspiration to create a company out of it really came from this [Netflix article](https://netflixtechblog.com/bulldozer-batch-data-moving-from-data-warehouse-to-online-key-value-stores-41bac13863f8).\n\nYou know the story: you have tons of data locked in your data platform and RDBMS and suddenly,  a PM asks to integrate this data with your customer-facing app. Obviously, all in real-time. And the pain begins! You have to set up infrastructure to move and process the data in real-time (Kafka, Spark, Flink), provision a solid caching/serving layer, build APIs on top and, only at the end of all this, you can start integrating data with your mobile or web app! As if all this is not enough, because you are now serving data to customers, you have to put in place all the monitoring and recovery tools, just in case something goes wrong.\n\nThere must be an easier way !!!!!\n\nThat is what drove us to build Dozer. Dozer is a simple open-source Data APIs backend that allows you to source data in real-time from databases, data warehouses, files, etc., process it using SQL, store all the results in a caching layer, and automatically provide gRPC and REST APIs. Everything with just a bunch of SQL and YAML files. \n\nIn Dozer everything happens in real-time: we subscribe to CDC sources (i.e. Postgres CDC, Snowflake table streams, etc.), process all events using our Reactive SQL engine, and store the results in the cache. The advantage is that data in the serving layer is always pre-aggregated, and fresh, which helps us to guarantee constant low latency.\n\nWe are at a very early stage, but Dozer can already be downloaded from our [GitHub repo](https://github.com/getdozer/dozer). We have taken the decision to build it entirely in Rust, which gives us the ridiculous performance and the beauty of a self-contained binary.\n\nWe are now working on several features like cloud deployment, blue/green deployment of caches, data actions (aka real-time triggers in Typescript/Python), a nice UI, and many others.\n\nPlease try it out and let us know your feedback. We have set up a [samples-repository](https://github.com/getdozer/dozer-samples) for testing it out and a [Discord channel](https://discord.com/invite/3eWXBgJaEQ) in case you need help or would like to contribute ideas!\n\nThanks  \nMatteo", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Dozer: The Future of Data APIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12db1ol", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 56, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 56, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680761526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m Matteo, and, over the last few months, I have been working with my co-founder and other folks from Goldman Sachs, Netflix, Palantir, and DBS Bank to simplify building data APIs. I have personally faced this problem myself multiple times, but, the inspiration to create a company out of it really came from this &lt;a href=\"https://netflixtechblog.com/bulldozer-batch-data-moving-from-data-warehouse-to-online-key-value-stores-41bac13863f8\"&gt;Netflix article&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;You know the story: you have tons of data locked in your data platform and RDBMS and suddenly,  a PM asks to integrate this data with your customer-facing app. Obviously, all in real-time. And the pain begins! You have to set up infrastructure to move and process the data in real-time (Kafka, Spark, Flink), provision a solid caching/serving layer, build APIs on top and, only at the end of all this, you can start integrating data with your mobile or web app! As if all this is not enough, because you are now serving data to customers, you have to put in place all the monitoring and recovery tools, just in case something goes wrong.&lt;/p&gt;\n\n&lt;p&gt;There must be an easier way !!!!!&lt;/p&gt;\n\n&lt;p&gt;That is what drove us to build Dozer. Dozer is a simple open-source Data APIs backend that allows you to source data in real-time from databases, data warehouses, files, etc., process it using SQL, store all the results in a caching layer, and automatically provide gRPC and REST APIs. Everything with just a bunch of SQL and YAML files. &lt;/p&gt;\n\n&lt;p&gt;In Dozer everything happens in real-time: we subscribe to CDC sources (i.e. Postgres CDC, Snowflake table streams, etc.), process all events using our Reactive SQL engine, and store the results in the cache. The advantage is that data in the serving layer is always pre-aggregated, and fresh, which helps us to guarantee constant low latency.&lt;/p&gt;\n\n&lt;p&gt;We are at a very early stage, but Dozer can already be downloaded from our &lt;a href=\"https://github.com/getdozer/dozer\"&gt;GitHub repo&lt;/a&gt;. We have taken the decision to build it entirely in Rust, which gives us the ridiculous performance and the beauty of a self-contained binary.&lt;/p&gt;\n\n&lt;p&gt;We are now working on several features like cloud deployment, blue/green deployment of caches, data actions (aka real-time triggers in Typescript/Python), a nice UI, and many others.&lt;/p&gt;\n\n&lt;p&gt;Please try it out and let us know your feedback. We have set up a &lt;a href=\"https://github.com/getdozer/dozer-samples\"&gt;samples-repository&lt;/a&gt; for testing it out and a &lt;a href=\"https://discord.com/invite/3eWXBgJaEQ\"&gt;Discord channel&lt;/a&gt; in case you need help or would like to contribute ideas!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;br/&gt;\nMatteo&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?auto=webp&amp;v=enabled&amp;s=0f0e1e306e13d39799c2da671e752a19580ce5dd", "width": 1200, "height": 690}, "resolutions": [{"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50dae319be6f16eecb783a9893d0c94ba2ef4cc4", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e83cfa5581e0af59a47dabdff03443ffebed335", "width": 216, "height": 124}, {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87dec77a385aecc107507c8981ebe798331c6295", "width": 320, "height": 184}, {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3038e40ef300c6e7cf9d37bb3ea116bb594f3a4", "width": 640, "height": 368}, {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64f5bfd0d5d4a6ccf0a306d72beb8f2a6c9059b9", "width": 960, "height": 552}, {"url": "https://external-preview.redd.it/xDasFl1vz4jJ1dhpy6C4A3q_iaTf3PTHGVfE4yvK8w4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d98fd0b51a7d3ed426cdd58b63d45ea3d084437d", "width": 1080, "height": 621}], "variants": {}, "id": "sXGvQu_-ieSZjP2jlHJuyO9HHoMDsgg92289NrHtXT8"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "12db1ol", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12db1ol/dozer_the_future_of_data_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12db1ol/dozer_the_future_of_data_apis/", "subreddit_subscribers": 96312, "created_utc": 1680761526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you guys use any orchestrator with your spark structured streaming workloads. Airflow seems like is made for batch workloads. Any good suggestions ?", "author_fullname": "t2_62mycgca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestrator for streaming workloads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d3og3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680741705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you guys use any orchestrator with your spark structured streaming workloads. Airflow seems like is made for batch workloads. Any good suggestions ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12d3og3", "is_robot_indexable": true, "report_reasons": null, "author": "Wonderful_Original61", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12d3og3/orchestrator_for_streaming_workloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12d3og3/orchestrator_for_streaming_workloads/", "subreddit_subscribers": 96312, "created_utc": 1680741705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently using Redshift and implementing dbt as a transformation framework\u2014 I'm interested in the best way to create a \"development\" structure for code to be tested before being merged into production. [This](https://www.datafold.com/blog/how-to-setup-dbt-development-environments) is really the only resource I've found.\n\nIt seems like recommendations are either\n\n* Create a development schema per user\u2014 this seems like it could get out of hand.\n* Create a development database\u2014 not sure how to do this without fully replicating all data that currently exists. Snowflake has zero-copy clones, but there's no Redshift equivalent to my knowledge.\n* Some combination of the above.\n\nI'd love to hear what's worked and what hasn't... And if there's a Redshift/S3 specific way to make this simple and functional.", "author_fullname": "t2_82q10l6i3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to create a development \"environment\" in Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cvave", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680787209.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680723255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently using Redshift and implementing dbt as a transformation framework\u2014 I&amp;#39;m interested in the best way to create a &amp;quot;development&amp;quot; structure for code to be tested before being merged into production. &lt;a href=\"https://www.datafold.com/blog/how-to-setup-dbt-development-environments\"&gt;This&lt;/a&gt; is really the only resource I&amp;#39;ve found.&lt;/p&gt;\n\n&lt;p&gt;It seems like recommendations are either&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Create a development schema per user\u2014 this seems like it could get out of hand.&lt;/li&gt;\n&lt;li&gt;Create a development database\u2014 not sure how to do this without fully replicating all data that currently exists. Snowflake has zero-copy clones, but there&amp;#39;s no Redshift equivalent to my knowledge.&lt;/li&gt;\n&lt;li&gt;Some combination of the above.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear what&amp;#39;s worked and what hasn&amp;#39;t... And if there&amp;#39;s a Redshift/S3 specific way to make this simple and functional.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?auto=webp&amp;v=enabled&amp;s=6dcb69ffc7979d6f51a12b849b4b98abd4770fed", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e6c36f1745e23372f7aedf17dc37a5e29996093", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18f3a85da7997995d953c673ad784930f0d54c66", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a3ebbbf63daff5a2e1b0105765fb822f2ce2961", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cbd9a6dfee840ec4cff0f293c520603bf2a8e8af", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7357cccdb8e7c8ce495705e7157350775d26ba2c", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/CFEuqbW_XAMwnCmj6M8Gi9oxuZU5sXUSynYMqrxRZ4s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f49e3c90b978c5d24b63002846b68f86172cc1d", "width": 1080, "height": 567}], "variants": {}, "id": "cn1-9kKU3UI8ewdlfXJ1WUhQfwpOBR1qCe_i6bOWB4s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cvave", "is_robot_indexable": true, "report_reasons": null, "author": "day-tuh-day", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cvave/best_way_to_create_a_development_environment_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cvave/best_way_to_create_a_development_environment_in/", "subreddit_subscribers": 96312, "created_utc": 1680723255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you do dynamic tasks in Airflow? I know there is this concept of dynamic tasks mapping but is this really production ready? \n\nWhat I need is for example to run one task that grabs some file from S3 and based on this file create several tasks (and subsequent tasks as well, so not only 1 task). What we do now is we output the content of the file from step 1 into Airflow variable and then, based on the Variable generate the tasks in for loop. I do not know if this is a good practice? Does not really seem to me so. \n\nAlso, when there are tasks xyz in dag run A, then in the next dag run B there could be different tasks so the tasks from previous run will disappear \u00af\\_(\u30c4)_/\u00af. Is there any elegant solution to this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow dynamic tasks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctyar", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680720443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you do dynamic tasks in Airflow? I know there is this concept of dynamic tasks mapping but is this really production ready? &lt;/p&gt;\n\n&lt;p&gt;What I need is for example to run one task that grabs some file from S3 and based on this file create several tasks (and subsequent tasks as well, so not only 1 task). What we do now is we output the content of the file from step 1 into Airflow variable and then, based on the Variable generate the tasks in for loop. I do not know if this is a good practice? Does not really seem to me so. &lt;/p&gt;\n\n&lt;p&gt;Also, when there are tasks xyz in dag run A, then in the next dag run B there could be different tasks so the tasks from previous run will disappear \u00af_(\u30c4)_/\u00af. Is there any elegant solution to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ctyar", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ctyar/airflow_dynamic_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ctyar/airflow_dynamic_tasks/", "subreddit_subscribers": 96312, "created_utc": 1680720443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nMy task is to test Azure Data Pipelines.\n\n***My question is HOW?***\n\nCan you provide some resources?\n\nFor someone with experience, how would a test case look like? (give some examples please)\n\nContext:\n\nI recently joined a new project as QA Automation. In the initial phase of the project the pipeline will just copy data from a on-premise database into Azure Data Lake. Later, transformations and business rules will be added to the pipeline\n\nI have 0 experience with data pipelines and Azure Data Lake. I have experience with API testing, microservices, mobile (android and ios), end-to-end starting with creating testdata in backend and testing it in the UI.\n\nTools I've been using so far: Java, RestAssured, Cucumber, Appium (for mobile), a little bit of Gatling for performance, and of course Git, BitBucket, Jenkins, Azure Repo and Azure DevOps Pipelines.\n\nI am a bit stressed because I have to come up with a presentation proposing some tools for testing the Azure Data Pipelines (the internet suggested dbt or Great Expectations). How can I come up with pros and cons to each tool if I haven't worked with them yet? :)\n\nThank you!", "author_fullname": "t2_12t1ny", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing Azure Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dg9oy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680778182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;My task is to test Azure Data Pipelines.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;My question is HOW?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Can you provide some resources?&lt;/p&gt;\n\n&lt;p&gt;For someone with experience, how would a test case look like? (give some examples please)&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;/p&gt;\n\n&lt;p&gt;I recently joined a new project as QA Automation. In the initial phase of the project the pipeline will just copy data from a on-premise database into Azure Data Lake. Later, transformations and business rules will be added to the pipeline&lt;/p&gt;\n\n&lt;p&gt;I have 0 experience with data pipelines and Azure Data Lake. I have experience with API testing, microservices, mobile (android and ios), end-to-end starting with creating testdata in backend and testing it in the UI.&lt;/p&gt;\n\n&lt;p&gt;Tools I&amp;#39;ve been using so far: Java, RestAssured, Cucumber, Appium (for mobile), a little bit of Gatling for performance, and of course Git, BitBucket, Jenkins, Azure Repo and Azure DevOps Pipelines.&lt;/p&gt;\n\n&lt;p&gt;I am a bit stressed because I have to come up with a presentation proposing some tools for testing the Azure Data Pipelines (the internet suggested dbt or Great Expectations). How can I come up with pros and cons to each tool if I haven&amp;#39;t worked with them yet? :)&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dg9oy", "is_robot_indexable": true, "report_reasons": null, "author": "intranca", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dg9oy/testing_azure_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dg9oy/testing_azure_data_pipelines/", "subreddit_subscribers": 96312, "created_utc": 1680778182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Won 1, finished 3rd in my friends challenge that had 59 people, and my wife finished 3rd with it in her work bracket.  Curious if anyone else has ever used methods like this to win as I've never done this well on my own.  Also, I was actually nowhere close to winning the Kaggle competition, so I'm curious if the people who won that had some crazy good predictions.", "author_fullname": "t2_1ddx9ayn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For the first time in my life I won a march madness bracket and used a ton of Feature Engineering with basic XGboost.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_12d5euk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-LEbbB2lniOUFAO2cKG8KMtt7QeWx_nuJwebwEJNTcw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680745868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Won 1, finished 3rd in my friends challenge that had 59 people, and my wife finished 3rd with it in her work bracket.  Curious if anyone else has ever used methods like this to win as I&amp;#39;ve never done this well on my own.  Also, I was actually nowhere close to winning the Kaggle competition, so I&amp;#39;m curious if the people who won that had some crazy good predictions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/snowflake/predicting-the-unpredictable-march-madness-using-snowpark-and-hex-f16dc4f57add?source=friends_link&amp;sk=4c2216e26a4e4f24f86c4df03444d919", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?auto=webp&amp;v=enabled&amp;s=c26a2651b234be99d55c669e1a283b5635276400", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=643993ed8a95fd2699d5cd5d6fd6cda94e7ad570", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d6c0057e2f750e593ccfa7bde754f41e610f7ee", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85dcadad2629873ae6efda6e4db54354d8cf8eef", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=945eaba96e9b78c2184772223fe1d0155578a63f", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0acad35b072acc0e1c7485ace7479d78ade1b523", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/X3nLMVqGup_mAb0_JQr4bmUxQI0QGDmQ0oPYjnWu4JI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a859347350cd936f4ef90a48a28db83582feaf4b", "width": 1080, "height": 720}], "variants": {}, "id": "O6SJUM6dNR6FZjK52_mdT9LDRXkMJHCkY6JvyM8AD6A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12d5euk", "is_robot_indexable": true, "report_reasons": null, "author": "crom5805", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12d5euk/for_the_first_time_in_my_life_i_won_a_march/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/snowflake/predicting-the-unpredictable-march-madness-using-snowpark-and-hex-f16dc4f57add?source=friends_link&amp;sk=4c2216e26a4e4f24f86c4df03444d919", "subreddit_subscribers": 96312, "created_utc": 1680745868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Airflow noob and looking for critiques.  \n\nHave a requirement to send data in csv format daily to a handful of clients. In airflow, I have a 'modules' folder containing my custom functions/methods. A 'dag' folder for dags and 'extracts' folder containing the extract scripts. There's one extract script per client and it contains a function to query  our db and send to destination.  The dag i set it up with one @task per client, where it imports the extract script function and executes.  Any issues with that so far?  \n   \n\nI use one script per client because it's only a handful of clients and it'll be easier to quickly adjust to client requirements for now. What would I change or do, to the airflow/python structure, if I needed to scale to like 50 clients?  The extract scripts are generally the same where you import custom modules -&gt; set up connection to db and client destination-&gt; process queries and land in destination.  The queries are the same inasmuch I just have to alter the schema/filters between clients/client_ids.  \n\nIt's fine for now, but struggling to see how I would scale this up quickly while being quick to adjust. Would you loop through a client list in 1 extract script and adjust destination/query settings dynamically? Idk!  \n\n\n\nMuch appreciated all!", "author_fullname": "t2_szv0ygic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ctdtc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680719277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Airflow noob and looking for critiques.  &lt;/p&gt;\n\n&lt;p&gt;Have a requirement to send data in csv format daily to a handful of clients. In airflow, I have a &amp;#39;modules&amp;#39; folder containing my custom functions/methods. A &amp;#39;dag&amp;#39; folder for dags and &amp;#39;extracts&amp;#39; folder containing the extract scripts. There&amp;#39;s one extract script per client and it contains a function to query  our db and send to destination.  The dag i set it up with one @task per client, where it imports the extract script function and executes.  Any issues with that so far?  &lt;/p&gt;\n\n&lt;p&gt;I use one script per client because it&amp;#39;s only a handful of clients and it&amp;#39;ll be easier to quickly adjust to client requirements for now. What would I change or do, to the airflow/python structure, if I needed to scale to like 50 clients?  The extract scripts are generally the same where you import custom modules -&amp;gt; set up connection to db and client destination-&amp;gt; process queries and land in destination.  The queries are the same inasmuch I just have to alter the schema/filters between clients/client_ids.  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s fine for now, but struggling to see how I would scale this up quickly while being quick to adjust. Would you loop through a client list in 1 extract script and adjust destination/query settings dynamically? Idk!  &lt;/p&gt;\n\n&lt;p&gt;Much appreciated all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ctdtc", "is_robot_indexable": true, "report_reasons": null, "author": "Hippodick666420", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ctdtc/airflow_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ctdtc/airflow_architecture/", "subreddit_subscribers": 96312, "created_utc": 1680719277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI may be over thinking this, I've done a POC in Synapse Dedicated Pool and I have a question about hash distribution.\n\nSay I have two tables:\n\nOrder Fact\n- 100 million rows\n- hash distributed on OrderID\n\nCustomer Dimension\n- 1 million rows\n- hash distributed on Account No\n- has an identity column as the surrogate key\n\nI can join the fact to the dimension using the surrogate key.\n\nMy question is, am I correct to hash distribute the dimension on the Account No?  Or should I add in a couple of steps to allow me to hash distribute on the surrogate key?  Will this make the join faster, even though the table distribution would be the same?  What would you recommend as best practice?\n\nTIA \ud83d\ude42", "author_fullname": "t2_9h6gf9pp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to: Surrogate Key as Hash Distribution Key", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ct6ux", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680718877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I may be over thinking this, I&amp;#39;ve done a POC in Synapse Dedicated Pool and I have a question about hash distribution.&lt;/p&gt;\n\n&lt;p&gt;Say I have two tables:&lt;/p&gt;\n\n&lt;p&gt;Order Fact\n- 100 million rows\n- hash distributed on OrderID&lt;/p&gt;\n\n&lt;p&gt;Customer Dimension\n- 1 million rows\n- hash distributed on Account No\n- has an identity column as the surrogate key&lt;/p&gt;\n\n&lt;p&gt;I can join the fact to the dimension using the surrogate key.&lt;/p&gt;\n\n&lt;p&gt;My question is, am I correct to hash distribute the dimension on the Account No?  Or should I add in a couple of steps to allow me to hash distribute on the surrogate key?  Will this make the join faster, even though the table distribution would be the same?  What would you recommend as best practice?&lt;/p&gt;\n\n&lt;p&gt;TIA \ud83d\ude42&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ct6ux", "is_robot_indexable": true, "report_reasons": null, "author": "V10Matt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ct6ux/how_to_surrogate_key_as_hash_distribution_key/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ct6ux/how_to_surrogate_key_as_hash_distribution_key/", "subreddit_subscribers": 96312, "created_utc": 1680718877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have a doubt about how to build my data warehouse's data model. The company I work for is similar to a marketplace that connects sellers and buyers.\n\nHere's a dummy example of how the model of the company works if a buyer buys a product in 3 installments.\n\n&amp;#x200B;\n\ntransaction\\_table (example)\n\nhttps://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725\n\n&amp;#x200B;\n\n(the buyer's amount would be higher because a fee is charged to the buyer)\n\n&amp;#x200B;\n\nWhat would be the best way to model this in my data warehouse? Currently, I don't have a specific data warehouse set up, but rather views of the table in MySQL where I separate the table in the following way:\n\n&amp;#x200B;\n\ntransaction\\_buyer\\_view (example)\n\nhttps://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60\n\n&amp;#x200B;\n\ntransaction\\_seller\\_view (example)\n\nhttps://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75\n\n&amp;#x200B;\n\nSo, when I create a dashboard in Power BI, I retrieve information from either table depending on whether I need to meet a requirement for a team that needs to see information from the buyer's or seller's side. However, I feel that this may not be the most comfortable solution for working with the information. That's why I wanted to know if the best practices for designing a data warehouse would recommend this approach, or if it would be better to keep it in the same table as before, or to have the installments that correspond to the same purchase in the same row (instead of a new row, as it is now), or some other alternative.\n\nAlso, another question I have is what would be the best way to track cases where there an error in the database and the records of the buyers are missing, but the sellers' records are present.\n\n&amp;#x200B;\n\nThank you in advance!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data warehouse design: records of buyers and sellers in same or different table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zffzdz72o4sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2e0f5de5526f44455a37c609ea90b6442938910"}, {"y": 70, "x": 216, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62694771cf40fb8b29211cf3d04d26aca76c2c25"}, {"y": 105, "x": 320, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51782bd6a7b1f2de1a1be2414df68e43ad3906f0"}, {"y": 210, "x": 640, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e3c2a3620f30c59bf7130766c6fbdef1704a068"}, {"y": 315, "x": 960, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8134bb853de479b0f09e0484f85981d1e6be5eb3"}], "s": {"y": 325, "x": 990, "u": "https://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60"}, "id": "zffzdz72o4sa1"}, "9r9hfg1yn4sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0b0805a8ca305bbea5d323be35c74642fb0ffe5"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37d1e84fcea14857b98d112478823013f114bcf9"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b251a36cd5491052d8743ac86d65c83bd2ca834b"}, {"y": 378, "x": 640, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00640cbefe7b03bdd0bab94c7989e28579d74053"}, {"y": 567, "x": 960, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=108b0aa8813753a63ac384ec369a5d0e0886c728"}], "s": {"y": 598, "x": 1012, "u": "https://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725"}, "id": "9r9hfg1yn4sa1"}, "t9k8f7n4o4sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=27dab59246e45df6f8fab4334a2f6c90e96dc350"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0cc48b0b6af66c2f1cb335f2a6be1399144e700"}, {"y": 105, "x": 320, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f936fea5468277f034a58307535798fc4b7980b6"}, {"y": 210, "x": 640, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=023c3f8c1b1a2307f86b81dce273b5b0fd8be07f"}, {"y": 315, "x": 960, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=217b94a79f4b9d903ec0a80ae1aa921c04c39eff"}], "s": {"y": 327, "x": 994, "u": "https://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75"}, "id": "t9k8f7n4o4sa1"}}, "name": "t3_12cx79i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xvaDteTqhhImJzA-CGeD8yEAH51KPN3UhXKI5atxXMw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680727334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a doubt about how to build my data warehouse&amp;#39;s data model. The company I work for is similar to a marketplace that connects sellers and buyers.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a dummy example of how the model of the company works if a buyer buys a product in 3 installments.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;transaction_table (example)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725\"&gt;https://preview.redd.it/9r9hfg1yn4sa1.png?width=1012&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=65e8b37cb2d93244577d9b0f3670dd5b6530f725&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(the buyer&amp;#39;s amount would be higher because a fee is charged to the buyer)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to model this in my data warehouse? Currently, I don&amp;#39;t have a specific data warehouse set up, but rather views of the table in MySQL where I separate the table in the following way:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;transaction_buyer_view (example)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60\"&gt;https://preview.redd.it/zffzdz72o4sa1.png?width=990&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a6ee6165628bf1845393cf4a139805e7aaf67d60&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;transaction_seller_view (example)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75\"&gt;https://preview.redd.it/t9k8f7n4o4sa1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=01f13f61d13e18beae7bbab2eebd5c94133c6f75&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So, when I create a dashboard in Power BI, I retrieve information from either table depending on whether I need to meet a requirement for a team that needs to see information from the buyer&amp;#39;s or seller&amp;#39;s side. However, I feel that this may not be the most comfortable solution for working with the information. That&amp;#39;s why I wanted to know if the best practices for designing a data warehouse would recommend this approach, or if it would be better to keep it in the same table as before, or to have the installments that correspond to the same purchase in the same row (instead of a new row, as it is now), or some other alternative.&lt;/p&gt;\n\n&lt;p&gt;Also, another question I have is what would be the best way to track cases where there an error in the database and the records of the buyers are missing, but the sellers&amp;#39; records are present.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cx79i", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cx79i/data_warehouse_design_records_of_buyers_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cx79i/data_warehouse_design_records_of_buyers_and/", "subreddit_subscribers": 96312, "created_utc": 1680727334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am working on designing the architecture of a web app that is going to ask a bunch of input parameters from the user and perform joins, filters and aggregations on BigQuery.\n\nMy first idea was to have a flask app for the web part. Request comes in from the user, flask runs the function that pulls data from BQ and we process the data locally.\n\nHowever, this does not seem to be very performant. I would like to avoid having to read/write large tables into BigQuery so I am looking for a SQL-based solution. \n\nI am already using dbt for other projects but I am wondering if it really suits this use-case. \n\nI did stumble upon dbt server that would be able to run dbt operations in response to API Requests, but don't know how it would scale.\n\nAny thoughts ?", "author_fullname": "t2_kqzh0kz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data processing and web app", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dl8qj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680789584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am working on designing the architecture of a web app that is going to ask a bunch of input parameters from the user and perform joins, filters and aggregations on BigQuery.&lt;/p&gt;\n\n&lt;p&gt;My first idea was to have a flask app for the web part. Request comes in from the user, flask runs the function that pulls data from BQ and we process the data locally.&lt;/p&gt;\n\n&lt;p&gt;However, this does not seem to be very performant. I would like to avoid having to read/write large tables into BigQuery so I am looking for a SQL-based solution. &lt;/p&gt;\n\n&lt;p&gt;I am already using dbt for other projects but I am wondering if it really suits this use-case. &lt;/p&gt;\n\n&lt;p&gt;I did stumble upon dbt server that would be able to run dbt operations in response to API Requests, but don&amp;#39;t know how it would scale.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dl8qj", "is_robot_indexable": true, "report_reasons": null, "author": "JamieA28", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dl8qj/data_processing_and_web_app/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dl8qj/data_processing_and_web_app/", "subreddit_subscribers": 96312, "created_utc": 1680789584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Google BigQuery documentation discourages single row inserts (commonly needed in applications), since BigQuery is supposed to be used for bulk updates. ([Source](https://cloud.google.com/bigquery/docs/best-practices-performance-patterns#dml_statements_that_update_or_insert_single_rows)).\n\nFrom the documentation, they suggest that you instead can use Cloud SQL with federated queries, which [enables BigQuery to query data from the Cloud SQL database.](https://cloud.google.com/bigquery/docs/cloud-sql-federated-queries). However, the data ends up being stored in a traditional database, like MySQL, instead of BigQuery and loses thereby some features (I would imagine ???).\n\nAs an alternative, I thought about using redis as a \"man-in-the-middle\" to batch analytics data from the application (or multiple applications when deployed in a cluster), and then insert those every X minutes (as a load job, or is stream better???) into BigQuery.\nI can't seem to find many sources that mention such an approach, and I was wondering why not? Are there disadvantages that I'm not aware of which makes Cloud SQL a better choice in this situation?", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using redis to batch data from various sources and then bulk insert into BigQuery, is this common?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dh2ht", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680780174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google BigQuery documentation discourages single row inserts (commonly needed in applications), since BigQuery is supposed to be used for bulk updates. (&lt;a href=\"https://cloud.google.com/bigquery/docs/best-practices-performance-patterns#dml_statements_that_update_or_insert_single_rows\"&gt;Source&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;From the documentation, they suggest that you instead can use Cloud SQL with federated queries, which &lt;a href=\"https://cloud.google.com/bigquery/docs/cloud-sql-federated-queries\"&gt;enables BigQuery to query data from the Cloud SQL database.&lt;/a&gt;. However, the data ends up being stored in a traditional database, like MySQL, instead of BigQuery and loses thereby some features (I would imagine ???).&lt;/p&gt;\n\n&lt;p&gt;As an alternative, I thought about using redis as a &amp;quot;man-in-the-middle&amp;quot; to batch analytics data from the application (or multiple applications when deployed in a cluster), and then insert those every X minutes (as a load job, or is stream better???) into BigQuery.\nI can&amp;#39;t seem to find many sources that mention such an approach, and I was wondering why not? Are there disadvantages that I&amp;#39;m not aware of which makes Cloud SQL a better choice in this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?auto=webp&amp;v=enabled&amp;s=20d96e49b41bb7bd00ff56b8e9ed66dc6ed60231", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cefbc38d0f527d52311150f3e2a5ee0cd4294045", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23a8451c1ff5cb4b86d4b12437a301825fbdeb9f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ac4fd116586a05dcece0cf47055879bc2ac44aa", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=467e1aba873015b34b1a4e2b7b44a79b2ab7343c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f90c3f55c5ad89d8d04390c2907361929ba9d300", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2c9d97288732e216db7689a964c326a90bdbe29", "width": 1080, "height": 567}], "variants": {}, "id": "DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dh2ht", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dh2ht/using_redis_to_batch_data_from_various_sources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dh2ht/using_redis_to_batch_data_from_various_sources/", "subreddit_subscribers": 96312, "created_utc": 1680780174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. I\u2019m curious if there are any .NET/C# lurkers here like me.\n\nI\u2019m starting to build an equivalent of Apache Airflow/Prefect for .NET. I don\u2019t have much in my repos at all right now because I just started coding, but I\u2019m curious for any members here: would you use something like that? I\u2019m thinking about doing open-core: free if you host yourself, or you can pay me to host for you.\n\nThe .NET community surprised me and responded very positively to the idea: https://www.reddit.com/r/dotnet/comments/11wa5bx/net_modern_task_scheduler/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=ioscss&amp;utm_content=1&amp;utm_term=1\n\nRepo links:\n- Didact Engine : https://www.github.com/DidactHQ/didact-engine\n- Didact UI : https://www.github.com/DidactHQ/didact-ui\n\n[View Poll](https://www.reddit.com/poll/12d3db5)", "author_fullname": "t2_853j9w4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Airflow/Prefect Alternative Orchestration Platform for .NET/C#", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d3db5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680740996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I\u2019m curious if there are any .NET/C# lurkers here like me.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m starting to build an equivalent of Apache Airflow/Prefect for .NET. I don\u2019t have much in my repos at all right now because I just started coding, but I\u2019m curious for any members here: would you use something like that? I\u2019m thinking about doing open-core: free if you host yourself, or you can pay me to host for you.&lt;/p&gt;\n\n&lt;p&gt;The .NET community surprised me and responded very positively to the idea: &lt;a href=\"https://www.reddit.com/r/dotnet/comments/11wa5bx/net_modern_task_scheduler/?utm_source=share&amp;amp;utm_medium=ios_app&amp;amp;utm_name=ioscss&amp;amp;utm_content=1&amp;amp;utm_term=1\"&gt;https://www.reddit.com/r/dotnet/comments/11wa5bx/net_modern_task_scheduler/?utm_source=share&amp;amp;utm_medium=ios_app&amp;amp;utm_name=ioscss&amp;amp;utm_content=1&amp;amp;utm_term=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Repo links:\n- Didact Engine : &lt;a href=\"https://www.github.com/DidactHQ/didact-engine\"&gt;https://www.github.com/DidactHQ/didact-engine&lt;/a&gt;\n- Didact UI : &lt;a href=\"https://www.github.com/DidactHQ/didact-ui\"&gt;https://www.github.com/DidactHQ/didact-ui&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/12d3db5\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?auto=webp&amp;v=enabled&amp;s=621880b80ac0984531bf1b2cee3997ba21a3cdd9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30b9087aa3e76bb32a434a14280e82c3c7395608", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbc1fe746b513ad1aeefe9834a6772a7d5eb665b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c917900c6414ca3d37c49e3b63751d2e75b79bb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2e67510b47c414e9502e1232dd9e0912172c42a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9269608d055aaa24020be035b4ef5a9469f121a8", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qVjmrop-SVxsyS1ZeitzCqVaSlJQ5jQIQA0WMrztUwo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a32bce45315ce1428533c9cdb114590be034598", "width": 1080, "height": 540}], "variants": {}, "id": "GREycnATKEH_PY8fctk9Qa6Kcg1tAo5YmufzMypRpYQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12d3db5", "is_robot_indexable": true, "report_reasons": null, "author": "SirLagsABot", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1680827396094, "options": [{"text": "Yeah, I\u2019d use this.", "id": "22420806"}, {"text": "No thanks.", "id": "22420807"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 50, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12d3db5/apache_airflowprefect_alternative_orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/12d3db5/apache_airflowprefect_alternative_orchestration/", "subreddit_subscribers": 96312, "created_utc": 1680740996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have Snowflake, and were using Tableau as the analytics and reporting tool. However the business has made a decision to instead move to Power BI, and due to the way it works there is a requirement to fully cache the data into Power BI every night.\n\nSo the insights team has set up the Power BI Service, which basically does a SELECT \\* on every fact and dim in the warehouse each night.\n\nMy question: we pay a lot of money to Snowflake. But based on the above approach, how much value are we actually receiving over a simple RDS?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake with Fully Cached Power BI - is there value?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d1i75", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680736724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have Snowflake, and were using Tableau as the analytics and reporting tool. However the business has made a decision to instead move to Power BI, and due to the way it works there is a requirement to fully cache the data into Power BI every night.&lt;/p&gt;\n\n&lt;p&gt;So the insights team has set up the Power BI Service, which basically does a SELECT * on every fact and dim in the warehouse each night.&lt;/p&gt;\n\n&lt;p&gt;My question: we pay a lot of money to Snowflake. But based on the above approach, how much value are we actually receiving over a simple RDS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12d1i75", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12d1i75/snowflake_with_fully_cached_power_bi_is_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12d1i75/snowflake_with_fully_cached_power_bi_is_there/", "subreddit_subscribers": 96312, "created_utc": 1680736724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I\u2019m not sure if this is the right subreddit to post to but I\u2019ve recently received a job offer and was wanted to get some insight on it.\n\nI currently live in DFW and have received an offer for a Data Engineer role for ~80k as my current contract is ending soon. Overall my background is in data analytics and modeling with 2 YoE (undergraduate and masters degree). This is a raise from my current role as an analyst but I have to be honest, I don\u2019t know if I have a ton of knowledge on data pipelining but I feel as though I am a relatively fast learner.\n\nI wanted to know if this is a reasonable offer because upon receiving it I have felt a little underwhelmed and even potentially undervalued. I have a feeling this may also be because of high amounts of socials praising high pay and wanted to know if that\u2019s gotten to my head.", "author_fullname": "t2_1hfbaj39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience &amp; Negotiation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cybtl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680729579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I\u2019m not sure if this is the right subreddit to post to but I\u2019ve recently received a job offer and was wanted to get some insight on it.&lt;/p&gt;\n\n&lt;p&gt;I currently live in DFW and have received an offer for a Data Engineer role for ~80k as my current contract is ending soon. Overall my background is in data analytics and modeling with 2 YoE (undergraduate and masters degree). This is a raise from my current role as an analyst but I have to be honest, I don\u2019t know if I have a ton of knowledge on data pipelining but I feel as though I am a relatively fast learner.&lt;/p&gt;\n\n&lt;p&gt;I wanted to know if this is a reasonable offer because upon receiving it I have felt a little underwhelmed and even potentially undervalued. I have a feeling this may also be because of high amounts of socials praising high pay and wanted to know if that\u2019s gotten to my head.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12cybtl", "is_robot_indexable": true, "report_reasons": null, "author": "UnsureSnake", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cybtl/experience_negotiation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cybtl/experience_negotiation/", "subreddit_subscribers": 96312, "created_utc": 1680729579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to get the columns on which a spark transformation depends without parsing the SQL query. Is there a way to get it from the explain plan or some other method directly in Spark?", "author_fullname": "t2_3yaxcuy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resolve column dependencies in Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cr9tx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680714953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to get the columns on which a spark transformation depends without parsing the SQL query. Is there a way to get it from the explain plan or some other method directly in Spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12cr9tx", "is_robot_indexable": true, "report_reasons": null, "author": "midasadim", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12cr9tx/resolve_column_dependencies_in_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12cr9tx/resolve_column_dependencies_in_spark/", "subreddit_subscribers": 96312, "created_utc": 1680714953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there anyway to set file sizes on write to 128mb without using coalesce() or repartition()?\n\nBtw, I\u2019m also interested in answers that use glue syntax aswell as pyspark.", "author_fullname": "t2_5fmit0v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark, Glue and Small/Big files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12dot4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680796980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there anyway to set file sizes on write to 128mb without using coalesce() or repartition()?&lt;/p&gt;\n\n&lt;p&gt;Btw, I\u2019m also interested in answers that use glue syntax aswell as pyspark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dot4u", "is_robot_indexable": true, "report_reasons": null, "author": "gabbom_XCII", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dot4u/spark_glue_and_smallbig_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dot4u/spark_glue_and_smallbig_files/", "subreddit_subscribers": 96312, "created_utc": 1680796980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am trying to find the optimal program setup for machine learning training and forecasting large amount of financial time series data on a single computer.\n\nComputer: amd 5900x, 128gb ram, 4tb nvme, rtx 3070\n\nData :  Data size is over 1tb  and have individual daily files that needs to be read. over 500 millions of rows of data.\n\nWhat would you use?\n\npyspark, polars, any other alternatives?\n\nSpeed is important, can't load data to memory since it is more than 1tb.\n\nI am not sure about SAS or Alteryx.\n\nThank you.", "author_fullname": "t2_400w8tx5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimal setup for big data training on single computer/node.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12dopeu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680796798.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am trying to find the optimal program setup for machine learning training and forecasting large amount of financial time series data on a single computer.&lt;/p&gt;\n\n&lt;p&gt;Computer: amd 5900x, 128gb ram, 4tb nvme, rtx 3070&lt;/p&gt;\n\n&lt;p&gt;Data :  Data size is over 1tb  and have individual daily files that needs to be read. over 500 millions of rows of data.&lt;/p&gt;\n\n&lt;p&gt;What would you use?&lt;/p&gt;\n\n&lt;p&gt;pyspark, polars, any other alternatives?&lt;/p&gt;\n\n&lt;p&gt;Speed is important, can&amp;#39;t load data to memory since it is more than 1tb.&lt;/p&gt;\n\n&lt;p&gt;I am not sure about SAS or Alteryx.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dopeu", "is_robot_indexable": true, "report_reasons": null, "author": "ozioh19", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dopeu/optimal_setup_for_big_data_training_on_single/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dopeu/optimal_setup_for_big_data_training_on_single/", "subreddit_subscribers": 96312, "created_utc": 1680796798.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to tokenize some sensitive columns in my data warehouse. However, the tokenization needs to be format preserving so that business users can still carry our analysis without any issues. Are there any standard format preserving tokenization algorithm that I can use out of box or should I write my own custom tokenization algorithm?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any standard format preserving tokenization algorithm that we can use out of box?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12dnhpn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680794354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to tokenize some sensitive columns in my data warehouse. However, the tokenization needs to be format preserving so that business users can still carry our analysis without any issues. Are there any standard format preserving tokenization algorithm that I can use out of box or should I write my own custom tokenization algorithm?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dnhpn", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dnhpn/are_there_any_standard_format_preserving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dnhpn/are_there_any_standard_format_preserving/", "subreddit_subscribers": 96312, "created_utc": 1680794354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake vs Redshift: a comprehensive guide on choosing your cloud data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12dkst1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/E7S3Y4PGDYtRiG7Qx7vB_-NFu3PcyJ9ljzm69C6Ws3A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680788599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/snowflake-vs-redshift", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?auto=webp&amp;v=enabled&amp;s=47a86384027e35d5146c9faa79795835d7a8d550", "width": 1800, "height": 946}, "resolutions": [{"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=789e3825d5a199333c23ac750d8dd304cc73a331", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=978c798f81ef9e752dee778def8ecb74e3ce938f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6e45f6c05585fce3fb85f409d71d7e1e156c645", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e3f92c8c124d0017104970b7f52c32604059077", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2dcfe0591ca1ac1a3859ed799e9a0336c4bca90b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=026bcd859fd79edaf025c8df1554c501a2585eac", "width": 1080, "height": 567}], "variants": {}, "id": "FcKPoXUUPVO80Peja_IMgPPk998zQAeEIgojWPAIXeE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12dkst1", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12dkst1/snowflake_vs_redshift_a_comprehensive_guide_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/snowflake-vs-redshift", "subreddit_subscribers": 96312, "created_utc": 1680788599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ay1q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Arrow String improvements in Pandas/Dask DataFrames", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_12djy0h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6uwjML35z-FxsE_pRboam-GDwq291V8TrPtVWQof1WA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680786747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/coiled-hq/pyarrow-strings-in-dask-dataframes-55a0c4871586", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?auto=webp&amp;v=enabled&amp;s=6859023178db536f94b55bb63382a7a09109032b", "width": 792, "height": 490}, "resolutions": [{"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9618b07d38c42c9b44e92644148b22822523cd9e", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e340c81948a4fba6552f26b728af8711788bc35", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ac1c66e5bf7be22194afbe2e088a490ce2f46dd", "width": 320, "height": 197}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4cd149bae3ae18eb7da5790f10a0c5f43a190ed", "width": 640, "height": 395}], "variants": {}, "id": "V5FocKkZS3WgTL0zjz6iujrafykJdPf8Om_OTahpOug"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "12djy0h", "is_robot_indexable": true, "report_reasons": null, "author": "mrocklin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12djy0h/arrow_string_improvements_in_pandasdask_dataframes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/coiled-hq/pyarrow-strings-in-dask-dataframes-55a0c4871586", "subreddit_subscribers": 96312, "created_utc": 1680786747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious to hear how others test data pipelines that spread across multiple tools.\n\n\\- Do you create dev and staging environments for each layer?\n\n\\- Do you run the whole pipeline during development?\n\n\\- Do you write unit, integration, or end-to-end tests?\n\n\\- Do you review data differences across the pipeline?\n\nI wrote an [article](https://www.datafold.com/blog/testing-data-pipelines) discussing some challenges to testing each layer: storage, integration, orchestration, transformation, BI, activation\u2026\n\nOne of my takes is that data teams will keep moving most of the complex transformation logic to the transformation layer (notably dbt):\n\n\\- ELT pipelines move transformation out from the integration layer\n\n\\- the metrics layer moves transformations out of the BI layer\n\n\\- the activation layer already advocates for transforming data with dbt\n\nIf raw data enters the transformation layer and ready-to-be-consumed data exits, can you test end-to-end pipelines from the transformation layer?", "author_fullname": "t2_975og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing the modern data stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12djkq1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680786231.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680785962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious to hear how others test data pipelines that spread across multiple tools.&lt;/p&gt;\n\n&lt;p&gt;- Do you create dev and staging environments for each layer?&lt;/p&gt;\n\n&lt;p&gt;- Do you run the whole pipeline during development?&lt;/p&gt;\n\n&lt;p&gt;- Do you write unit, integration, or end-to-end tests?&lt;/p&gt;\n\n&lt;p&gt;- Do you review data differences across the pipeline?&lt;/p&gt;\n\n&lt;p&gt;I wrote an &lt;a href=\"https://www.datafold.com/blog/testing-data-pipelines\"&gt;article&lt;/a&gt; discussing some challenges to testing each layer: storage, integration, orchestration, transformation, BI, activation\u2026&lt;/p&gt;\n\n&lt;p&gt;One of my takes is that data teams will keep moving most of the complex transformation logic to the transformation layer (notably dbt):&lt;/p&gt;\n\n&lt;p&gt;- ELT pipelines move transformation out from the integration layer&lt;/p&gt;\n\n&lt;p&gt;- the metrics layer moves transformations out of the BI layer&lt;/p&gt;\n\n&lt;p&gt;- the activation layer already advocates for transforming data with dbt&lt;/p&gt;\n\n&lt;p&gt;If raw data enters the transformation layer and ready-to-be-consumed data exits, can you test end-to-end pipelines from the transformation layer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?auto=webp&amp;v=enabled&amp;s=629e981b19b1f5a09b7eca4029116bb93f9ffc34", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c1b1fa08a10ecc2430d2298bcabf0718ae941de", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9afafbfd738711eb984df3e8bd957b4413cddb98", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9cb4920d04a54371115a4c625830dde9b2bf377", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=935d9a143280253de2d05b9ac7c3a169e0bf0ed8", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44dbe71446119d6bb4ced5e7b697ac68e35f1337", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/boPF5YrwqrSsEyxakvag8-TAY5TVtcpAShR-eAk1uzY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41cf4c548a336e53bb91e25656580d6cf778f934", "width": 1080, "height": 607}], "variants": {}, "id": "EpOJBL6tEGJlfDwVwF5pZQ7zInpkiLXsFmo98IemLk4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12djkq1", "is_robot_indexable": true, "report_reasons": null, "author": "arimbr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12djkq1/testing_the_modern_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12djkq1/testing_the_modern_data_stack/", "subreddit_subscribers": 96312, "created_utc": 1680785962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently a data engineer at a unicorn tech company, and I started applying a couple of months ago to see if I could get a job offer in this economy.  I have 5 years of experience, and before my current job, I was a data engineer at a smaller tech company and a non-tech company in the fortune 500.\n\n\n\n\n\n\nI only applied to about 20-25 jobs in the past couple of months, since there really isn't that many job openings in my area.  Which is strange since I live in one of the big cities for tech jobs.  LinkedIn in my area is the same promoted jobs, so there appears to be more jobs available than there really are.\n\n\n\n\nOut of about 25 applications, I got one interview at a very small tech startup with a decent amount of funding.  I did really well in the recruiter and programming rounds, but I couldn't get a final round interview since there is a lot of competition for jobs.\n\n\n\n\nIt seems that even smaller tech companies and startups don't really care if you worked at a big tech company for several years and if you interviewed well, since there are so many engineers on the job market.", "author_fullname": "t2_3v6ob2bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineers, what has been your experience applying for jobs in this economy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dii09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680783746.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680783508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently a data engineer at a unicorn tech company, and I started applying a couple of months ago to see if I could get a job offer in this economy.  I have 5 years of experience, and before my current job, I was a data engineer at a smaller tech company and a non-tech company in the fortune 500.&lt;/p&gt;\n\n&lt;p&gt;I only applied to about 20-25 jobs in the past couple of months, since there really isn&amp;#39;t that many job openings in my area.  Which is strange since I live in one of the big cities for tech jobs.  LinkedIn in my area is the same promoted jobs, so there appears to be more jobs available than there really are.&lt;/p&gt;\n\n&lt;p&gt;Out of about 25 applications, I got one interview at a very small tech startup with a decent amount of funding.  I did really well in the recruiter and programming rounds, but I couldn&amp;#39;t get a final round interview since there is a lot of competition for jobs.&lt;/p&gt;\n\n&lt;p&gt;It seems that even smaller tech companies and startups don&amp;#39;t really care if you worked at a big tech company for several years and if you interviewed well, since there are so many engineers on the job market.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dii09", "is_robot_indexable": true, "report_reasons": null, "author": "data_preprocessing", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12dii09/data_engineers_what_has_been_your_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dii09/data_engineers_what_has_been_your_experience/", "subreddit_subscribers": 96312, "created_utc": 1680783508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, i have a production airflow orchestrating 4 clusters on pre 2.0 version. Our scheduler is dying constantly and no one knows why.\n\nWe have over 500k entries in x-com table, and probably as much in other tables, so our db has probably never been cleaned. I belive that to be slowing us, among other things, but senior engineer in charge of my team said its not the case ( guy is known for not caring too much). \n\nAny expert here to confirm or deny this? We have a ton of other bad practices, but this would be the quickest to clean up for sure.", "author_fullname": "t2_4smbd7xs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow meta-db cleaning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12df018", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680774766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i have a production airflow orchestrating 4 clusters on pre 2.0 version. Our scheduler is dying constantly and no one knows why.&lt;/p&gt;\n\n&lt;p&gt;We have over 500k entries in x-com table, and probably as much in other tables, so our db has probably never been cleaned. I belive that to be slowing us, among other things, but senior engineer in charge of my team said its not the case ( guy is known for not caring too much). &lt;/p&gt;\n\n&lt;p&gt;Any expert here to confirm or deny this? We have a ton of other bad practices, but this would be the quickest to clean up for sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12df018", "is_robot_indexable": true, "report_reasons": null, "author": "Sneakyfrog112", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12df018/airflow_metadb_cleaning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12df018/airflow_metadb_cleaning/", "subreddit_subscribers": 96312, "created_utc": 1680774766.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}