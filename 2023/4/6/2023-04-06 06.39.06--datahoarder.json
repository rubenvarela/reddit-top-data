{"kind": "Listing", "data": {"after": "t3_12d4sxc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone, I\u2019m part of Fight for the Future, a digital rights org that\u2019s been fighting for ebook access.\n\nFollowing  the recent ruling against the Internet Archive (and libraries in  general), we\u2019re organizing an in-person rally outside IA\u2019s San Francisco  headquarters. We want to get loud and demonstrate the popular support  for libraries and their ability to own, lend, and preserve ebooks.\n\nIf  you\u2019re in the SF area and have been following this case, we\u2019d love to  see you this Saturday at 11am. We\u2019ll have signs but encourage you to  BYOS as well! Here\u2019s a link to RSVP: [https://actionnetwork.org/events/dont-delete-our-books-rally-in-san-francisco](https://actionnetwork.org/events/dont-delete-our-books-rally-in-san-francisco)\n\nThanks, hope to see some of you there.", "author_fullname": "t2_6ncim", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rally to support Internet Archive in SF on Saturday", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ct6wl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 612, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 612, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680718879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I\u2019m part of Fight for the Future, a digital rights org that\u2019s been fighting for ebook access.&lt;/p&gt;\n\n&lt;p&gt;Following  the recent ruling against the Internet Archive (and libraries in  general), we\u2019re organizing an in-person rally outside IA\u2019s San Francisco  headquarters. We want to get loud and demonstrate the popular support  for libraries and their ability to own, lend, and preserve ebooks.&lt;/p&gt;\n\n&lt;p&gt;If  you\u2019re in the SF area and have been following this case, we\u2019d love to  see you this Saturday at 11am. We\u2019ll have signs but encourage you to  BYOS as well! Here\u2019s a link to RSVP: &lt;a href=\"https://actionnetwork.org/events/dont-delete-our-books-rally-in-san-francisco\"&gt;https://actionnetwork.org/events/dont-delete-our-books-rally-in-san-francisco&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks, hope to see some of you there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1kdCSboRP9L798pwV44r8v6B3fn6XKQRiH0_6OI9FTs.jpg?auto=webp&amp;v=enabled&amp;s=6a0d48fe5f3f2e457c6533ff3898cdb06cad83f4", "width": 1044, "height": 609}, "resolutions": [{"url": "https://external-preview.redd.it/1kdCSboRP9L798pwV44r8v6B3fn6XKQRiH0_6OI9FTs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae75a5bd3a8297bc92bbe4f4e02cd8803ee13b23", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/1kdCSboRP9L798pwV44r8v6B3fn6XKQRiH0_6OI9FTs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4801024549889833039217918c37148402277df", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/1kdCSboRP9L798pwV44r8v6B3fn6XKQRiH0_6OI9FTs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89242a225888d5d2b1d2a12ff2b80f30817adaea", "width": 320, "height": 186}, {"url": "https://external-preview.redd.it/1kdCSboRP9L798pwV44r8v6B3fn6XKQRiH0_6OI9FTs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60a40260230bdc902ce0e26aaa1c86fcf5a98b9e", "width": 640, "height": 373}, {"url": "https://external-preview.redd.it/1kdCSboRP9L798pwV44r8v6B3fn6XKQRiH0_6OI9FTs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c940038c7b6be1928773d4fff112d15d5195c0b", "width": 960, "height": 560}], "variants": {}, "id": "1aCJ5vBn2mNaAADMcDA-J6AMGJOhkbrkMAh7L1_AxJA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12ct6wl", "is_robot_indexable": true, "report_reasons": null, "author": "fightforthefuture", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12ct6wl/rally_to_support_internet_archive_in_sf_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12ct6wl/rally_to_support_internet_archive_in_sf_on/", "subreddit_subscribers": 676791, "created_utc": 1680718879.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "to download images from a pinterest board I am using the extension mentioned above, but something happens that it keeps replicating the image several times in different resolutions. \n\nThanks if anyone knows how to fix this, or at least suggest me another good extension that downloads the images from the board with good quality\n\nanother help can be to recommend me some program to delete repeated images from a windows folder", "author_fullname": "t2_6oian2l9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does anyone know of any good extensions that download all images from a pinterest board, or at least know how the image assistant image batch downloader works", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ch6gu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 145, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 145, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680694160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;to download images from a pinterest board I am using the extension mentioned above, but something happens that it keeps replicating the image several times in different resolutions. &lt;/p&gt;\n\n&lt;p&gt;Thanks if anyone knows how to fix this, or at least suggest me another good extension that downloads the images from the board with good quality&lt;/p&gt;\n\n&lt;p&gt;another help can be to recommend me some program to delete repeated images from a windows folder&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12ch6gu", "is_robot_indexable": true, "report_reasons": null, "author": "designygued3s", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12ch6gu/does_anyone_know_of_any_good_extensions_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12ch6gu/does_anyone_know_of_any_good_extensions_that/", "subreddit_subscribers": 676791, "created_utc": 1680694160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a big family and they all have their own photos, videos, notes, etc. We've also been digitizing a lot of family history.\n\n--\n\nI'm looking at help with:\n\n**1:** Organizing the data\n\n**2:** storing meta data\n\n--\n\n**1:** In terms of organizing, right now it's just a big dump.  It's basically just a big folder on a NAS and it's not well organized.  Everything is folder based and everyone has their own naming conversation, or just generic names.  \n\n--\n\n\n**2:** The other problem is I don't know how to store meta-data.  \n\nFor example: If my grandma tells me a story about a photo, how do I keep that information with the photo?  \n\nExample 2: Or simple documenting who is in the video and the location and date it was shot, etc.\n\n(caveat: you can store meta-data in some photo formats, but not everything is a photo.  Some will be videos, sound files, text documents, etc)\n\n--\n\nFor meta-data, I want it to be in a \"open format\" that is non-propriety, since propriety software might not work in future.  (My grandpa had all his photos in a commercial photo album that ran on windows 95 and it was a huge pain to extract that data since it was all in some closed source janky database).\n\n\n\n\n\n--\n\nWe want to pass these digital files down though the generations.  So hard to know what the future holds, but want to try our best to make them accessible and not lock ourselves into some specific software choice.\n\n\n--\n\n\nAny tips from the horde on best way to tackle these problems?  Has anyone else done a project like this ?", "author_fullname": "t2_9gtvi5rc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on managing a shared digital \"archive\" for the family?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cnhak", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680707503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a big family and they all have their own photos, videos, notes, etc. We&amp;#39;ve also been digitizing a lot of family history.&lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n\n&lt;p&gt;I&amp;#39;m looking at help with:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1:&lt;/strong&gt; Organizing the data&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2:&lt;/strong&gt; storing meta data&lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n\n&lt;p&gt;&lt;strong&gt;1:&lt;/strong&gt; In terms of organizing, right now it&amp;#39;s just a big dump.  It&amp;#39;s basically just a big folder on a NAS and it&amp;#39;s not well organized.  Everything is folder based and everyone has their own naming conversation, or just generic names.  &lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n\n&lt;p&gt;&lt;strong&gt;2:&lt;/strong&gt; The other problem is I don&amp;#39;t know how to store meta-data.  &lt;/p&gt;\n\n&lt;p&gt;For example: If my grandma tells me a story about a photo, how do I keep that information with the photo?  &lt;/p&gt;\n\n&lt;p&gt;Example 2: Or simple documenting who is in the video and the location and date it was shot, etc.&lt;/p&gt;\n\n&lt;p&gt;(caveat: you can store meta-data in some photo formats, but not everything is a photo.  Some will be videos, sound files, text documents, etc)&lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n\n&lt;p&gt;For meta-data, I want it to be in a &amp;quot;open format&amp;quot; that is non-propriety, since propriety software might not work in future.  (My grandpa had all his photos in a commercial photo album that ran on windows 95 and it was a huge pain to extract that data since it was all in some closed source janky database).&lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n\n&lt;p&gt;We want to pass these digital files down though the generations.  So hard to know what the future holds, but want to try our best to make them accessible and not lock ourselves into some specific software choice.&lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n\n&lt;p&gt;Any tips from the horde on best way to tackle these problems?  Has anyone else done a project like this ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12cnhak", "is_robot_indexable": true, "report_reasons": null, "author": "ZjY5MjFk", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cnhak/thoughts_on_managing_a_shared_digital_archive_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cnhak/thoughts_on_managing_a_shared_digital_archive_for/", "subreddit_subscribers": 676791, "created_utc": 1680707503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need an external hard drive with at least 3-4TB. In my country these are the two options (if I order from abroad there is a rather pricy tax). My Passport is considerably more expensive. Does it mean it is more reliable ? will the data be safer ? does is live longer ? Thanks in advance. Sorry for my bad English.", "author_fullname": "t2_601cgodk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I get WD Elements or My Password ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12canmh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680677151.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680675417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need an external hard drive with at least 3-4TB. In my country these are the two options (if I order from abroad there is a rather pricy tax). My Passport is considerably more expensive. Does it mean it is more reliable ? will the data be safer ? does is live longer ? Thanks in advance. Sorry for my bad English.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12canmh", "is_robot_indexable": true, "report_reasons": null, "author": "ro2ro", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12canmh/should_i_get_wd_elements_or_my_password/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12canmh/should_i_get_wd_elements_or_my_password/", "subreddit_subscribers": 676791, "created_utc": 1680675417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nSome time ago, I had a conversation here where I mentioned a Python script I wrote and use to check for bit rot in my archives.  It's kind of simple really: it hashes files in target directories and stores the hashes (plus last modified times) of the files in a database.  On subsequent runs, it recalculates the checksums and compares them (as well as the last modified information) and reports mismatches.  It reports checksum mismatches as bit rot unless the file system last modified is newer, in which case it silently updates the database on the assumption that the file simply changed (and if the file system last modified happens to be older than what's in the database then that's reported as a possible file system problem).  \n\nNothing fancy, and nothing you couldn't do with some shell scripts, but it is perhaps a little nicer to use and has some configuration flexibility.\n\nIt's always been just something for me, but I finally got around to polishing and enhancing it a bit and putting it up on my GitHub.  \n\nIf it sounds like something you might be interested in, have a look:\n\n[https://github.com/fzammetti/python/tree/main/FileIntegrityCheckerScript](https://github.com/fzammetti/python/tree/main/FileIntegrityCheckerScript)\n\n(and as I wrote this, I realized that I probably want a configuration option for whether to report a checksum mismatch as bit rot regardless of the last modified info, so that for directories where it's truly an archive, where you wouldn't ever expect the files to change, that can be reported since it'll effectively be missed right now... I'll work on that)", "author_fullname": "t2_6q4oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File Integrity Checker Script", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cukum", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680721716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Some time ago, I had a conversation here where I mentioned a Python script I wrote and use to check for bit rot in my archives.  It&amp;#39;s kind of simple really: it hashes files in target directories and stores the hashes (plus last modified times) of the files in a database.  On subsequent runs, it recalculates the checksums and compares them (as well as the last modified information) and reports mismatches.  It reports checksum mismatches as bit rot unless the file system last modified is newer, in which case it silently updates the database on the assumption that the file simply changed (and if the file system last modified happens to be older than what&amp;#39;s in the database then that&amp;#39;s reported as a possible file system problem).  &lt;/p&gt;\n\n&lt;p&gt;Nothing fancy, and nothing you couldn&amp;#39;t do with some shell scripts, but it is perhaps a little nicer to use and has some configuration flexibility.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s always been just something for me, but I finally got around to polishing and enhancing it a bit and putting it up on my GitHub.  &lt;/p&gt;\n\n&lt;p&gt;If it sounds like something you might be interested in, have a look:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/fzammetti/python/tree/main/FileIntegrityCheckerScript\"&gt;https://github.com/fzammetti/python/tree/main/FileIntegrityCheckerScript&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(and as I wrote this, I realized that I probably want a configuration option for whether to report a checksum mismatch as bit rot regardless of the last modified info, so that for directories where it&amp;#39;s truly an archive, where you wouldn&amp;#39;t ever expect the files to change, that can be reported since it&amp;#39;ll effectively be missed right now... I&amp;#39;ll work on that)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Xo44xbUMHZPkHGMKw9eKJwnuXUgbK0ccfFA-ZlVQcjE.jpg?auto=webp&amp;v=enabled&amp;s=14585cff9577f838810ffc919516fcb371b3f220", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Xo44xbUMHZPkHGMKw9eKJwnuXUgbK0ccfFA-ZlVQcjE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56716d3e42dfc2180dfd69ec928d437ef6c5520e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Xo44xbUMHZPkHGMKw9eKJwnuXUgbK0ccfFA-ZlVQcjE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70e92846c42c8eabf6bf7d9ce982c0d09d215cca", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Xo44xbUMHZPkHGMKw9eKJwnuXUgbK0ccfFA-ZlVQcjE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1344a838ae95af40600bbb9090376e019e5ce723", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Xo44xbUMHZPkHGMKw9eKJwnuXUgbK0ccfFA-ZlVQcjE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c7a84cce4865f99f9b4cca255d13e239dfb56c9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Xo44xbUMHZPkHGMKw9eKJwnuXUgbK0ccfFA-ZlVQcjE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d6ee47a0a2afefe53c607dfc0501de738ccc05d0", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Xo44xbUMHZPkHGMKw9eKJwnuXUgbK0ccfFA-ZlVQcjE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f12f635629428415a2f123b83749363b0829a4e0", "width": 1080, "height": 540}], "variants": {}, "id": "SGKtmJubJppojVXVbF-UHGsxCH_GOIdzCk9lmw2elYM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cukum", "is_robot_indexable": true, "report_reasons": null, "author": "fzammetti", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cukum/file_integrity_checker_script/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cukum/file_integrity_checker_script/", "subreddit_subscribers": 676791, "created_utc": 1680721716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://status.mycloud.com/os4\n\n\"Contact Support\" button is also down.\n\nI use this to run Plex on my TV. Anyone have a better suggestion for a Plex Media Server?", "author_fullname": "t2_v653fh9g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital MyCloud has been down for 3 days", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cnj64", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680707610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://status.mycloud.com/os4\"&gt;https://status.mycloud.com/os4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Contact Support&amp;quot; button is also down.&lt;/p&gt;\n\n&lt;p&gt;I use this to run Plex on my TV. Anyone have a better suggestion for a Plex Media Server?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cnj64", "is_robot_indexable": true, "report_reasons": null, "author": "HowIsYourBreathing", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cnj64/western_digital_mycloud_has_been_down_for_3_days/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cnj64/western_digital_mycloud_has_been_down_for_3_days/", "subreddit_subscribers": 676791, "created_utc": 1680707610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have this External HDD. It's WD My Passport 4TB. I bought it in maybe late 2018, early 2019 but it died less than a year earlier. Just never works when I plug it into any laptop.\n\nToday, I thought \"what the hell? Might as well open it and see what's inside\"\n\nSo I opened the plastic cover, but couldn't open it further as none of my screw drivers work on these weird screws (what type are they? Those little screws on the PCB thingy)\n\nI said what the hell again, and I inserted the screw driver just next to the usb port. There was a spark, and the drive was working again! Of course, it only worked as long as I kept the screw driver there.\n\nNow, I can't afford a new one since it costs easily 8-10 times the price I bought it for. (My country's economy is fucked). So I'm hoping to try to save this.\n\nI'm thinking that the PCB thingy is the problem since there was no problem with transferring data after I got it working.\n\nWhat is this part called? And can I find a replacement for it and swap it? \n\n\n[IMG-20230405-101007.jpg](https://postimg.cc/tnJRby6d)", "author_fullname": "t2_p6rta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hoping to save this External HDD. What is this part called and can I replace it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cbyu6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680679292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have this External HDD. It&amp;#39;s WD My Passport 4TB. I bought it in maybe late 2018, early 2019 but it died less than a year earlier. Just never works when I plug it into any laptop.&lt;/p&gt;\n\n&lt;p&gt;Today, I thought &amp;quot;what the hell? Might as well open it and see what&amp;#39;s inside&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;So I opened the plastic cover, but couldn&amp;#39;t open it further as none of my screw drivers work on these weird screws (what type are they? Those little screws on the PCB thingy)&lt;/p&gt;\n\n&lt;p&gt;I said what the hell again, and I inserted the screw driver just next to the usb port. There was a spark, and the drive was working again! Of course, it only worked as long as I kept the screw driver there.&lt;/p&gt;\n\n&lt;p&gt;Now, I can&amp;#39;t afford a new one since it costs easily 8-10 times the price I bought it for. (My country&amp;#39;s economy is fucked). So I&amp;#39;m hoping to try to save this.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking that the PCB thingy is the problem since there was no problem with transferring data after I got it working.&lt;/p&gt;\n\n&lt;p&gt;What is this part called? And can I find a replacement for it and swap it? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://postimg.cc/tnJRby6d\"&gt;IMG-20230405-101007.jpg&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Y_uak6yX4ulM0mAGLZycHp8nKOE_hOe8tLGMGz7ZLpg.jpg?auto=webp&amp;v=enabled&amp;s=4df7f446a0cfe45ca3cc8c5021a176243c402652", "width": 360, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/Y_uak6yX4ulM0mAGLZycHp8nKOE_hOe8tLGMGz7ZLpg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7043f7723bb44d8f70bc1b744a7ef90ce4734403", "width": 108, "height": 216}, {"url": "https://external-preview.redd.it/Y_uak6yX4ulM0mAGLZycHp8nKOE_hOe8tLGMGz7ZLpg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4147ff234d75b61e622636b1c1c556fefc5b32c", "width": 216, "height": 432}, {"url": "https://external-preview.redd.it/Y_uak6yX4ulM0mAGLZycHp8nKOE_hOe8tLGMGz7ZLpg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af85688212ce415775c2c2661f38d985cbdb0051", "width": 320, "height": 640}], "variants": {}, "id": "t3qZKn6L5fhQ3A06D-0RO73JqlKWuo_Qr-SjNrX49jc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cbyu6", "is_robot_indexable": true, "report_reasons": null, "author": "HarimaToshirou", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cbyu6/hoping_to_save_this_external_hdd_what_is_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cbyu6/hoping_to_save_this_external_hdd_what_is_this/", "subreddit_subscribers": 676791, "created_utc": 1680679292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Pretty interesting documentary covering the transition from one technology to another which many of us take for granted\u2026\n\nhttps://youtu.be/1MGjFKs9bnU", "author_fullname": "t2_2l7vdc8f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FarewellEtaoinShrdlu", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d4o0g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680744050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty interesting documentary covering the transition from one technology to another which many of us take for granted\u2026&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/1MGjFKs9bnU\"&gt;https://youtu.be/1MGjFKs9bnU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PA5i4jiCDBWD316MFNg_G6jf53VZl1IgivqKmNWKTyU.jpg?auto=webp&amp;v=enabled&amp;s=1960e49ff04bd11408ca39b3c8c1daf8cd8cc999", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/PA5i4jiCDBWD316MFNg_G6jf53VZl1IgivqKmNWKTyU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1157f04552a182f9155992c2cd7e40f351bbc4ce", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/PA5i4jiCDBWD316MFNg_G6jf53VZl1IgivqKmNWKTyU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff356b1bbeb88a3953ffe6491745b3650ebdedaf", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/PA5i4jiCDBWD316MFNg_G6jf53VZl1IgivqKmNWKTyU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2617f2178c3d1684d02f61072174e5317e2d4102", "width": 320, "height": 240}], "variants": {}, "id": "tyn_vKgpvcMVKPlUjKYva-8WVKSR6j0_c-Y5Ym3XEwY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12d4o0g", "is_robot_indexable": true, "report_reasons": null, "author": "qlippoth513", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12d4o0g/farewelletaoinshrdlu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12d4o0g/farewelletaoinshrdlu/", "subreddit_subscribers": 676791, "created_utc": 1680744050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My Google Drive storage is almost full. I believe most of it is due to files that I've uploaded to a folder, shared the folder with someone else, then later deleted that folder from my own GDrive. Despite my deletions of folders, Drive keeps the files inside these folders to a) continue to provide the files to the people with whom they've been shared and b) force us to buy more storage. :/\n\nIs there a filter to display files that account for my storage quota that exist in shared-deleted-folders?  \nPerhaps a third-party plugin to do this?  \n\n\nThanks in advance,  \nDax.  \n\n\n**UPDATE:** ***I've discovered the underlying problem.*** I have folders which I assigned to other users' accounts by using Transfer Ownership. The trouble is that the files inside those folders are still assigned to me! When I deleted those folders, I removed them from my GDrive folder tree, but the files are still \"mine\".  \nI'm hoping there is a solution to delete files that belong to you that are located in folders that don't belong to you.\n\nhttps://preview.redd.it/sg3j9zepu5sa1.png?width=2643&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=05ebcf8e846a3e86265d7938a55e364eed00abe5", "author_fullname": "t2_ucssf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive retains files inside deleted shared folders", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "media_metadata": {"sg3j9zepu5sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/sg3j9zepu5sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ab81918235980d60a8a8aeea8b0bfc5ecc4491"}, {"y": 84, "x": 216, "u": "https://preview.redd.it/sg3j9zepu5sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f6757ab8b4e085c592a68bd626a572ce5a95006"}, {"y": 124, "x": 320, "u": "https://preview.redd.it/sg3j9zepu5sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=163b1ad1634cd75145a8c861a120fc1b0ce339e3"}, {"y": 248, "x": 640, "u": "https://preview.redd.it/sg3j9zepu5sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b8f6e7ea52e45d0daffe63f05131076aa5c1ed2"}, {"y": 373, "x": 960, "u": "https://preview.redd.it/sg3j9zepu5sa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77104e174537c1217969d75695e0957b044e902b"}, {"y": 420, "x": 1080, "u": "https://preview.redd.it/sg3j9zepu5sa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1575e172a926d147c30228cbd35bfabfe0c355f9"}], "s": {"y": 1028, "x": 2643, "u": "https://preview.redd.it/sg3j9zepu5sa1.png?width=2643&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=05ebcf8e846a3e86265d7938a55e364eed00abe5"}, "id": "sg3j9zepu5sa1"}}, "name": "t3_12d1jiy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ksaAzSKZJACtR3IDx8KNnziL5T06NMJnSLN4vXPNXcs.jpg", "edited": 1680741860.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680736809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Google Drive storage is almost full. I believe most of it is due to files that I&amp;#39;ve uploaded to a folder, shared the folder with someone else, then later deleted that folder from my own GDrive. Despite my deletions of folders, Drive keeps the files inside these folders to a) continue to provide the files to the people with whom they&amp;#39;ve been shared and b) force us to buy more storage. :/&lt;/p&gt;\n\n&lt;p&gt;Is there a filter to display files that account for my storage quota that exist in shared-deleted-folders?&lt;br/&gt;\nPerhaps a third-party plugin to do this?  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance,&lt;br/&gt;\nDax.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;I&amp;#39;ve discovered the underlying problem.&lt;/em&gt;&lt;/strong&gt; I have folders which I assigned to other users&amp;#39; accounts by using Transfer Ownership. The trouble is that the files inside those folders are still assigned to me! When I deleted those folders, I removed them from my GDrive folder tree, but the files are still &amp;quot;mine&amp;quot;.&lt;br/&gt;\nI&amp;#39;m hoping there is a solution to delete files that belong to you that are located in folders that don&amp;#39;t belong to you.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/sg3j9zepu5sa1.png?width=2643&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=05ebcf8e846a3e86265d7938a55e364eed00abe5\"&gt;https://preview.redd.it/sg3j9zepu5sa1.png?width=2643&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=05ebcf8e846a3e86265d7938a55e364eed00abe5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12d1jiy", "is_robot_indexable": true, "report_reasons": null, "author": "daxliniere", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12d1jiy/google_drive_retains_files_inside_deleted_shared/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12d1jiy/google_drive_retains_files_inside_deleted_shared/", "subreddit_subscribers": 676791, "created_utc": 1680736809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know this is a long shot but I have a couple  IXSystems X10 HA systems that came from IX with 32gb of RAM in each HA node. I'm looking to upgrade this and IX has told me they cannot give me any proprietary information such as max ram per slot/system or max supported memory speed (seriously...)\n\n\nAnyone here have any experience with these or that has seen one with a large amount of RAM? Each node in each chassis is running 2x 16gb DDR4-2400 ECC Sodimm, and I'm trying to avoid having to buy a bunch of different sizes and speeds to see what works.\n\nPretty fucking ridiculous that they won't share this basic information on a 3 year old $15,000 server if you ask me...", "author_fullname": "t2_6bubo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IXSystems X10 Memory Upgrades", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cyh03", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680729893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is a long shot but I have a couple  IXSystems X10 HA systems that came from IX with 32gb of RAM in each HA node. I&amp;#39;m looking to upgrade this and IX has told me they cannot give me any proprietary information such as max ram per slot/system or max supported memory speed (seriously...)&lt;/p&gt;\n\n&lt;p&gt;Anyone here have any experience with these or that has seen one with a large amount of RAM? Each node in each chassis is running 2x 16gb DDR4-2400 ECC Sodimm, and I&amp;#39;m trying to avoid having to buy a bunch of different sizes and speeds to see what works.&lt;/p&gt;\n\n&lt;p&gt;Pretty fucking ridiculous that they won&amp;#39;t share this basic information on a 3 year old $15,000 server if you ask me...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "176TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cyh03", "is_robot_indexable": true, "report_reasons": null, "author": "ycatsce", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12cyh03/ixsystems_x10_memory_upgrades/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cyh03/ixsystems_x10_memory_upgrades/", "subreddit_subscribers": 676791, "created_utc": 1680729893.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My relatively new and not much used  **WD80EFAX** is doing click(ing) of death. I have shucked it just to see if maybe it will work like that but its the same. As its filled with helium, the last resort of opening it and trying to move the heads is out of the question.  Anything else I can do before shooting it? Thank you!", "author_fullname": "t2_8g6po", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Click of death - WD80EFAX (256 MB Cache) - 8TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cs1sz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680716532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My relatively new and not much used  &lt;strong&gt;WD80EFAX&lt;/strong&gt; is doing click(ing) of death. I have shucked it just to see if maybe it will work like that but its the same. As its filled with helium, the last resort of opening it and trying to move the heads is out of the question.  Anything else I can do before shooting it? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cs1sz", "is_robot_indexable": true, "report_reasons": null, "author": "Genie52", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cs1sz/click_of_death_wd80efax_256_mb_cache_8tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cs1sz/click_of_death_wd80efax_256_mb_cache_8tb/", "subreddit_subscribers": 676791, "created_utc": 1680716532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nHello everyone.\n\nI bought a pendrive to use as an mp3 archive, to be used with a karaoke machine.\n\nSo after downloading with the pc some mp3s, I insert them into the Pendrive, and then plug the pendrive into the karaoke machine.\n\nWhich initially plays the songs and then after a few days does not recognize the pendrive.\n\nI tried formatting the pen drive, but to no avail.\n\nThen I bought another identical pen drive and, likewise, it is recognized by Karaoke for a few days and then no longer works.\n\nHowever, in the pc both pendrives work regularly.\n\nWhere could the problem be?", "author_fullname": "t2_sou1g6gm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Usb is read for a limited time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cjqjh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680699918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone.&lt;/p&gt;\n\n&lt;p&gt;I bought a pendrive to use as an mp3 archive, to be used with a karaoke machine.&lt;/p&gt;\n\n&lt;p&gt;So after downloading with the pc some mp3s, I insert them into the Pendrive, and then plug the pendrive into the karaoke machine.&lt;/p&gt;\n\n&lt;p&gt;Which initially plays the songs and then after a few days does not recognize the pendrive.&lt;/p&gt;\n\n&lt;p&gt;I tried formatting the pen drive, but to no avail.&lt;/p&gt;\n\n&lt;p&gt;Then I bought another identical pen drive and, likewise, it is recognized by Karaoke for a few days and then no longer works.&lt;/p&gt;\n\n&lt;p&gt;However, in the pc both pendrives work regularly.&lt;/p&gt;\n\n&lt;p&gt;Where could the problem be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cjqjh", "is_robot_indexable": true, "report_reasons": null, "author": "Maleficent_Quote_392", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cjqjh/usb_is_read_for_a_limited_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cjqjh/usb_is_read_for_a_limited_time/", "subreddit_subscribers": 676791, "created_utc": 1680699918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If I understand correctly, DrivePool's file duplication more or less duplicates each file to a \"random\" drive selected from the pool. Due to this, it can only tolerate 1 drive failure in the pool before losing data (assuming 2x duplication). I understand why this limitation exists, but I was wondering if it was possible to mimic RAID10-like drive mirroring rather than just duplicating to a random drive in the pool.\n\nI don't have enough spare disks to test this, but my assumption is that you can set up multiple 2-drive pools with duplication enabled (\"RAID 1\"), then using hierarchical pooling you can pool all these together into a main pool (\"RAID 0\"). This way all data is still duplicated, but the pool is much more resistant to drive failures (1 from each pool rather than only 1 total). I understand DrivePool doesn't stripe the data, so the performance characteristics wouldn't be the same as true RAID10, but I only care about the resiliency.\n\nDoes this setup work, does anyone use it this way? If so, are there any downsides to setting up the pools like this?", "author_fullname": "t2_5kann", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mimicking RAID10 failure resiliency in DrivePool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d2qfr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680739515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I understand correctly, DrivePool&amp;#39;s file duplication more or less duplicates each file to a &amp;quot;random&amp;quot; drive selected from the pool. Due to this, it can only tolerate 1 drive failure in the pool before losing data (assuming 2x duplication). I understand why this limitation exists, but I was wondering if it was possible to mimic RAID10-like drive mirroring rather than just duplicating to a random drive in the pool.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have enough spare disks to test this, but my assumption is that you can set up multiple 2-drive pools with duplication enabled (&amp;quot;RAID 1&amp;quot;), then using hierarchical pooling you can pool all these together into a main pool (&amp;quot;RAID 0&amp;quot;). This way all data is still duplicated, but the pool is much more resistant to drive failures (1 from each pool rather than only 1 total). I understand DrivePool doesn&amp;#39;t stripe the data, so the performance characteristics wouldn&amp;#39;t be the same as true RAID10, but I only care about the resiliency.&lt;/p&gt;\n\n&lt;p&gt;Does this setup work, does anyone use it this way? If so, are there any downsides to setting up the pools like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "34TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12d2qfr", "is_robot_indexable": true, "report_reasons": null, "author": "Hakkin", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12d2qfr/mimicking_raid10_failure_resiliency_in_drivepool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12d2qfr/mimicking_raid10_failure_resiliency_in_drivepool/", "subreddit_subscribers": 676791, "created_utc": 1680739515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was making a backup using robocopy in CMD, but sadly the PC shut down. Now I wonder if using the same command again will rewrite the files , it tried to do the first time or will I end up with corrputed files?\n\nMy command :\nRobocopy \"A\" \"B\"   /xj /e /COPYALL /ZB /DCOPY:T /r:1 /w:1 /mt:128", "author_fullname": "t2_3lyupz8a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can an arborted robocopy command be redone without any drawbacks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cp8uf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680711011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was making a backup using robocopy in CMD, but sadly the PC shut down. Now I wonder if using the same command again will rewrite the files , it tried to do the first time or will I end up with corrputed files?&lt;/p&gt;\n\n&lt;p&gt;My command :\nRobocopy &amp;quot;A&amp;quot; &amp;quot;B&amp;quot;   /xj /e /COPYALL /ZB /DCOPY:T /r:1 /w:1 /mt:128&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cp8uf", "is_robot_indexable": true, "report_reasons": null, "author": "seronlover", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cp8uf/can_an_arborted_robocopy_command_be_redone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cp8uf/can_an_arborted_robocopy_command_be_redone/", "subreddit_subscribers": 676791, "created_utc": 1680711011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All,\n\nSo I met a guy who's autistic daughter loves the families Singing Douglas Fir Christmas Tree. (like this one: [https://www.youtube.com/watch?v=NxNWgfFZmRs](https://www.youtube.com/watch?v=NxNWgfFZmRs)) \n\nThe tree operates via a cassette player and an aux cord. The cassette tape that came with his setup has started dying and he has been having a ton of trouble getting it ripped and working. When he did rip it to his computer the tree only sang, but did not dance. So I think it may be left channel sing, right channel dance. \n\nThis may be the single most random thing to try to find, but if anyone had archived it, it would be on r/DataHoarder. Does anyone have a two channel audio file of the song / dance? Or could point me in the right direction? \n\nThanks!", "author_fullname": "t2_342bg6eb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[File Request] Singing Christmas Douglas Fir song / dance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cx3kh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "File Reqest", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680727113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;So I met a guy who&amp;#39;s autistic daughter loves the families Singing Douglas Fir Christmas Tree. (like this one: &lt;a href=\"https://www.youtube.com/watch?v=NxNWgfFZmRs\"&gt;https://www.youtube.com/watch?v=NxNWgfFZmRs&lt;/a&gt;) &lt;/p&gt;\n\n&lt;p&gt;The tree operates via a cassette player and an aux cord. The cassette tape that came with his setup has started dying and he has been having a ton of trouble getting it ripped and working. When he did rip it to his computer the tree only sang, but did not dance. So I think it may be left channel sing, right channel dance. &lt;/p&gt;\n\n&lt;p&gt;This may be the single most random thing to try to find, but if anyone had archived it, it would be on &lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt;. Does anyone have a two channel audio file of the song / dance? Or could point me in the right direction? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GUFIoBRJNlOYJHLcrn4JN-_kHeXZZ2IbRAk7j-yUoL4.jpg?auto=webp&amp;v=enabled&amp;s=dc88acc39b6dcf5fc78b16de834645442c2bb90f", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GUFIoBRJNlOYJHLcrn4JN-_kHeXZZ2IbRAk7j-yUoL4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4452d064c44808c8800ad82c53629a10d083c01d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GUFIoBRJNlOYJHLcrn4JN-_kHeXZZ2IbRAk7j-yUoL4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70d9beb738a108f3682959c1f280806f79ce3145", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GUFIoBRJNlOYJHLcrn4JN-_kHeXZZ2IbRAk7j-yUoL4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffae2eced3bca39884050043205c324bd9db38a2", "width": 320, "height": 240}], "variants": {}, "id": "z0mDjGh9mBmXujPtUx1P7g_Vy_Nvjo3pSEOgGiXNAxs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12cx3kh", "is_robot_indexable": true, "report_reasons": null, "author": "SpaceRex1776", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cx3kh/file_request_singing_christmas_douglas_fir_song/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cx3kh/file_request_singing_christmas_douglas_fir_song/", "subreddit_subscribers": 676791, "created_utc": 1680727113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to simply back up my 3tb of data on my pc - most is archive stuff, like videos, templates, etc. If I need something, I would copy it to my 2tb m.2 ssd, and use it there till I'm done.   \n\n\nHave been told the following:  \n1) Just add those 2 drives to your pc and create your backup copies (If a fire happens on your house, NAS/DAS also dies either way, so not worth buying them)  \n2) A Das is enough. You create a copy of your data, when needed you just attach it to whatever device you need it, and it's faster than nas.  \n3) A NAS has so many more uses, like connect to both laptop and desktop through network, etc, which has so much more potential If you're gonna spend money, just go for something better.  \n\n\nSo, which is it? Any advice is appreciated :)", "author_fullname": "t2_or8du", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bought 16tb (2x8tb) - Now what?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cufyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680721438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to simply back up my 3tb of data on my pc - most is archive stuff, like videos, templates, etc. If I need something, I would copy it to my 2tb m.2 ssd, and use it there till I&amp;#39;m done.   &lt;/p&gt;\n\n&lt;p&gt;Have been told the following:&lt;br/&gt;\n1) Just add those 2 drives to your pc and create your backup copies (If a fire happens on your house, NAS/DAS also dies either way, so not worth buying them)&lt;br/&gt;\n2) A Das is enough. You create a copy of your data, when needed you just attach it to whatever device you need it, and it&amp;#39;s faster than nas.&lt;br/&gt;\n3) A NAS has so many more uses, like connect to both laptop and desktop through network, etc, which has so much more potential If you&amp;#39;re gonna spend money, just go for something better.  &lt;/p&gt;\n\n&lt;p&gt;So, which is it? Any advice is appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cufyd", "is_robot_indexable": true, "report_reasons": null, "author": "Mangomagno123", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cufyd/bought_16tb_2x8tb_now_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cufyd/bought_16tb_2x8tb_now_what/", "subreddit_subscribers": 676791, "created_utc": 1680721438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello.  I'm planning on buying several shows/movies in dvd and blu-ray. The thing is though, I'm worried about the possibility of defects in some of the discs. I've heard experiences from a few people about how they encountered disc-skipping and other errors in the middle of watching one/some of their discs, even though the whole set was brand-new and the disc had no visible imperfections.\n\nWith that said, is there a way to test or scan discs for defects, skipping, etc.?\n\n\"Bruteforcing\" it by watching the entirety of every disc is out of the question. I simply do not have the free-time to go through that much media within the 30-day return period.\n\nThe reason I ask here of all places is because I couldn't find many answers elsewhere online. Usually it's just other people on forums complaining about the disc-skipping issues they have. I figured that this subreddit, which deals with collecting &amp; maybe verifying data/media, must surely have some people with technical expertise in the matter.\n\nAny insight or suggestions would be greatly appreciated.", "author_fullname": "t2_rwuwsvku", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any methods for detecting errors in physical media? (DVD's/Blu-Rays)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d2fwk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680738876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.  I&amp;#39;m planning on buying several shows/movies in dvd and blu-ray. The thing is though, I&amp;#39;m worried about the possibility of defects in some of the discs. I&amp;#39;ve heard experiences from a few people about how they encountered disc-skipping and other errors in the middle of watching one/some of their discs, even though the whole set was brand-new and the disc had no visible imperfections.&lt;/p&gt;\n\n&lt;p&gt;With that said, is there a way to test or scan discs for defects, skipping, etc.?&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Bruteforcing&amp;quot; it by watching the entirety of every disc is out of the question. I simply do not have the free-time to go through that much media within the 30-day return period.&lt;/p&gt;\n\n&lt;p&gt;The reason I ask here of all places is because I couldn&amp;#39;t find many answers elsewhere online. Usually it&amp;#39;s just other people on forums complaining about the disc-skipping issues they have. I figured that this subreddit, which deals with collecting &amp;amp; maybe verifying data/media, must surely have some people with technical expertise in the matter.&lt;/p&gt;\n\n&lt;p&gt;Any insight or suggestions would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12d2fwk", "is_robot_indexable": true, "report_reasons": null, "author": "tatertoter10", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12d2fwk/any_methods_for_detecting_errors_in_physical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12d2fwk/any_methods_for_detecting_errors_in_physical/", "subreddit_subscribers": 676791, "created_utc": 1680738876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It seems like my Seagate 5 TB portable backup HDD (Model: SRD0NF1; P/N: 2N1AP8-500; 2020; 3 partitions [encrypted APFS + exFAT + encrypted old HFS+(journal)] just died a hour ago. My 4 computers (2 MBPs (2012 Mojave &amp; 2020's Big Sur) + 2 PCs [Linux/Debian bullseye and 64-bit Windows 10]) don't see the connected drive anymore. \n\nEarlier today, I was doing a Time Machine back up fine in 2020 MBP. And then, I tried to do it again. macOS Big Sur got stuck with its animated colorful pinwheel. I tried to abort and eject, but it failed. I pulled its old school USB cable connection to make MBP respond. I rebooted and retried. It never saw the drive even though the HDD's light blinked. I tried it on another (older) MBP's Mojave, and it never saw it but its light blinked only once right after physically connecting. Same with my Linux/Debian and 64-bit W10 PCs. My Debian's dmesg -T showed failures it seems as shown in https://paste2.org/xeHxaxKN. \n\nAlso, I can feel the drive vibration after connecting and seeing its white light up either once or blink. I'm going to leave the drive physically connected to see if the drive will ever show up.\n\nWhat do you think? Dead/Broken? Warranty expired last year according to Seagate's web site with the serial number. Time for a new one? If so, then which reliable brand and model to get to replace it for cheap? :(\n\nThank you for reading and hopefully answering soon. :)", "author_fullname": "t2_4a27h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dead external Seagate 5 TB portable HDD from 2020 (ordered at that year IIRC)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cyvxz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": "", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680761200.0, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680730798.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems like my Seagate 5 TB portable backup HDD (Model: SRD0NF1; P/N: 2N1AP8-500; 2020; 3 partitions [encrypted APFS + exFAT + encrypted old HFS+(journal)] just died a hour ago. My 4 computers (2 MBPs (2012 Mojave &amp;amp; 2020&amp;#39;s Big Sur) + 2 PCs [Linux/Debian bullseye and 64-bit Windows 10]) don&amp;#39;t see the connected drive anymore. &lt;/p&gt;\n\n&lt;p&gt;Earlier today, I was doing a Time Machine back up fine in 2020 MBP. And then, I tried to do it again. macOS Big Sur got stuck with its animated colorful pinwheel. I tried to abort and eject, but it failed. I pulled its old school USB cable connection to make MBP respond. I rebooted and retried. It never saw the drive even though the HDD&amp;#39;s light blinked. I tried it on another (older) MBP&amp;#39;s Mojave, and it never saw it but its light blinked only once right after physically connecting. Same with my Linux/Debian and 64-bit W10 PCs. My Debian&amp;#39;s dmesg -T showed failures it seems as shown in &lt;a href=\"https://paste2.org/xeHxaxKN\"&gt;https://paste2.org/xeHxaxKN&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Also, I can feel the drive vibration after connecting and seeing its white light up either once or blink. I&amp;#39;m going to leave the drive physically connected to see if the drive will ever show up.&lt;/p&gt;\n\n&lt;p&gt;What do you think? Dead/Broken? Warranty expired last year according to Seagate&amp;#39;s web site with the serial number. Time for a new one? If so, then which reliable brand and model to get to replace it for cheap? :(&lt;/p&gt;\n\n&lt;p&gt;Thank you for reading and hopefully answering soon. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Where's the big big floppy disk(ette) flair? :P", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cyvxz", "is_robot_indexable": true, "report_reasons": null, "author": "antdude", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12cyvxz/dead_external_seagate_5_tb_portable_hdd_from_2020/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cyvxz/dead_external_seagate_5_tb_portable_hdd_from_2020/", "subreddit_subscribers": 676791, "created_utc": 1680730798.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone have experience with add-on drives for a tape library? \n\nI have a two drive library with one IBM drive already populated that came with it from the manufacturer. I am looking to upgrade to LTO-8 and would prefer not to pay the manufacturer markup for their add-on drive and source a \"generic\" drive.\n\nFrom what I can tell, the firmware, form-factor and just about everything else for LTO-6 SAS HH drives look identical.\n\nI have heard differing opinions on this. An LTO repair tech has told me it doesn't matter while a large LTO reseller has told me you must use the add-on drive from the manufacturer.\n\nFor reference, the library is an Overland Storage T24 LTO-6 device.", "author_fullname": "t2_3skvm79t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO Tape Library and Add-on Drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cqgs3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680713391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have experience with add-on drives for a tape library? &lt;/p&gt;\n\n&lt;p&gt;I have a two drive library with one IBM drive already populated that came with it from the manufacturer. I am looking to upgrade to LTO-8 and would prefer not to pay the manufacturer markup for their add-on drive and source a &amp;quot;generic&amp;quot; drive.&lt;/p&gt;\n\n&lt;p&gt;From what I can tell, the firmware, form-factor and just about everything else for LTO-6 SAS HH drives look identical.&lt;/p&gt;\n\n&lt;p&gt;I have heard differing opinions on this. An LTO repair tech has told me it doesn&amp;#39;t matter while a large LTO reseller has told me you must use the add-on drive from the manufacturer.&lt;/p&gt;\n\n&lt;p&gt;For reference, the library is an Overland Storage T24 LTO-6 device.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cqgs3", "is_robot_indexable": true, "report_reasons": null, "author": "svwer", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cqgs3/lto_tape_library_and_addon_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cqgs3/lto_tape_library_and_addon_drives/", "subreddit_subscribers": 676791, "created_utc": 1680713391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Like the title says I bought some of these seagate refurbished drives. I got two 16TB for $329 and two 12TB for $108 each. I know I\u2019ll probably get a lot of crap for both seagate and being refurb drives. But for the price and Amazon return policy I\u2019m feeling pretty good. So, don\u2019t care. \n\nHowever, I have 90 days to return. My question is what is the best method to stress test these? I can obviously run CrystalDisk info and look at the SMART data there. I haven\u2019t even taken them out of the box yet as I\u2019m waiting for my HBA to come from eBay. \n\nI\u2019d like to run some sort of read/write test. I\u2019m fine if it takes a few days or something but I\u2019d rather not do a test that is going to take weeks.\n\nI\u2019m on Windows 10 with basically zero knowledge of command prompt. I free program or something would be ideal. ELi5 also appreciated.", "author_fullname": "t2_8vyg2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just bought a couple Seagate Exos refurbs and I\u2019m curious what long term stress test you run on them before using them.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12covv1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1680710322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "a.co", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title says I bought some of these seagate refurbished drives. I got two 16TB for $329 and two 12TB for $108 each. I know I\u2019ll probably get a lot of crap for both seagate and being refurb drives. But for the price and Amazon return policy I\u2019m feeling pretty good. So, don\u2019t care. &lt;/p&gt;\n\n&lt;p&gt;However, I have 90 days to return. My question is what is the best method to stress test these? I can obviously run CrystalDisk info and look at the SMART data there. I haven\u2019t even taken them out of the box yet as I\u2019m waiting for my HBA to come from eBay. &lt;/p&gt;\n\n&lt;p&gt;I\u2019d like to run some sort of read/write test. I\u2019m fine if it takes a few days or something but I\u2019d rather not do a test that is going to take weeks.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m on Windows 10 with basically zero knowledge of command prompt. I free program or something would be ideal. ELi5 also appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://a.co/d/dQEFLgN", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "36TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12covv1", "is_robot_indexable": true, "report_reasons": null, "author": "hunterl1990", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12covv1/just_bought_a_couple_seagate_exos_refurbs_and_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://a.co/d/dQEFLgN", "subreddit_subscribers": 676791, "created_utc": 1680710322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im perplexed in this area because I lose files intermittently and i cant ever really figure out why. So to further my research and to feed the mind; I turn to you guys.  \n\n\nMost guides state that you simply need to close Google Drive on windows and delete the cache content. I currently have my cache folder on a 8Tb drive just for Google Drive cache. This drive gets filled up IMPRESSIVELY quick, and of course; mostly due to my dumping of data to Google Drive. \n\nIve always wondered if my internet speed was a factor. Cox sucks, so im at 1gb Down, and 30 MB Up.\n\nClearly I can gather more then I can begin to offload. I typically have cool down periods for the PC where it might be ripping, but not uploading because synching is still doing its thing filling the drive.  \n\n\nso, currently got 600 files left to synch. got 200 GB left of 8Tb. If i close Google Drive (as recommended when deleting cache) do you fellas think im going to lose those 600 files; or the files that are being worked on of the 600? do they all go to Lost and Found?  \n\n\njust curious and thinking out loud. Let me know what you all think. Thank you.", "author_fullname": "t2_4msgmj8l4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows Google Drive Question - Deleting Cache while items are synching.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cotg6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680710181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im perplexed in this area because I lose files intermittently and i cant ever really figure out why. So to further my research and to feed the mind; I turn to you guys.  &lt;/p&gt;\n\n&lt;p&gt;Most guides state that you simply need to close Google Drive on windows and delete the cache content. I currently have my cache folder on a 8Tb drive just for Google Drive cache. This drive gets filled up IMPRESSIVELY quick, and of course; mostly due to my dumping of data to Google Drive. &lt;/p&gt;\n\n&lt;p&gt;Ive always wondered if my internet speed was a factor. Cox sucks, so im at 1gb Down, and 30 MB Up.&lt;/p&gt;\n\n&lt;p&gt;Clearly I can gather more then I can begin to offload. I typically have cool down periods for the PC where it might be ripping, but not uploading because synching is still doing its thing filling the drive.  &lt;/p&gt;\n\n&lt;p&gt;so, currently got 600 files left to synch. got 200 GB left of 8Tb. If i close Google Drive (as recommended when deleting cache) do you fellas think im going to lose those 600 files; or the files that are being worked on of the 600? do they all go to Lost and Found?  &lt;/p&gt;\n\n&lt;p&gt;just curious and thinking out loud. Let me know what you all think. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2ByteModiffier - UNLIMITED GOOGLE DRIVE", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cotg6", "is_robot_indexable": true, "report_reasons": null, "author": "SudoAcidAlchamy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12cotg6/windows_google_drive_question_deleting_cache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cotg6/windows_google_drive_question_deleting_cache/", "subreddit_subscribers": 676791, "created_utc": 1680710181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What are your experiences with both?  I'm now dealing with WD for a 2nd time and I must say I am disappointed in the performance of their support program.  This project has an order of $2,500+ worth of drives, one of which is DOA and due to the recent hack, they can't perform a RMA or a return.  Not ideal and had to order a drive elsewhere but none the less I am disappointed.    \n\n\nMy previous experience with WD was with 2 shucked drives.  One was 100% dead and the other is on its way out.  2nd drive they wouldn't support even though on the sticker it says \"internal hard drive\" as well as a warranty on the sticker.  Are they mad to insinuate a internal use drive that they strap a shitty board that accepts power and USB is \"not usable if shucked\"?  Kind of a annoying considering they are just rebadged Red drives.    \n\n\nRegardless of my anecdotes the experiences have been less than desirable so I would love to see how your experiences have been whether it be Seagate or WD.", "author_fullname": "t2_d65h6qka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Support:: Seagate VS WD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ckd1o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680701245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your experiences with both?  I&amp;#39;m now dealing with WD for a 2nd time and I must say I am disappointed in the performance of their support program.  This project has an order of $2,500+ worth of drives, one of which is DOA and due to the recent hack, they can&amp;#39;t perform a RMA or a return.  Not ideal and had to order a drive elsewhere but none the less I am disappointed.    &lt;/p&gt;\n\n&lt;p&gt;My previous experience with WD was with 2 shucked drives.  One was 100% dead and the other is on its way out.  2nd drive they wouldn&amp;#39;t support even though on the sticker it says &amp;quot;internal hard drive&amp;quot; as well as a warranty on the sticker.  Are they mad to insinuate a internal use drive that they strap a shitty board that accepts power and USB is &amp;quot;not usable if shucked&amp;quot;?  Kind of a annoying considering they are just rebadged Red drives.    &lt;/p&gt;\n\n&lt;p&gt;Regardless of my anecdotes the experiences have been less than desirable so I would love to see how your experiences have been whether it be Seagate or WD.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12ckd1o", "is_robot_indexable": true, "report_reasons": null, "author": "Maciluminous", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12ckd1o/support_seagate_vs_wd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12ckd1o/support_seagate_vs_wd/", "subreddit_subscribers": 676791, "created_utc": 1680701245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm just using this as an example:\n\n[https://forums.sherdog.com/threads/bkfc-president-francis-ngannou-is-%E2%80%9Casking-for-unrealistic-money%E2%80%9D.4285974/](https://forums.sherdog.com/threads/bkfc-president-francis-ngannou-is-%E2%80%9Casking-for-unrealistic-money%E2%80%9D.4285974/)\n\nAs I'm attempting to download similar content from a thread generated through another Xenforo hosted platform.\n\nI've tried a chrome extension called \"web scrapper\" (webscraper.io) and it seemed it worked initially but when I clicked on the data collected it just spat out some code for \"failed\".\n\nI tried another scrapper called httrack, which rendered something similar.\n\nThis is not something I need to do frequently and I'm not a proficient coder, so I just need to get this done really on this single occasion - with that being said perhaps someone could provide guidelines how to \"scrape\" the content of that link above with link/name of the relevant scape tool?\n\ni.e. the page it links to, and subsequent pages within that thread?\n\nMuch appreciated.", "author_fullname": "t2_6oqtcupo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can anyone provide step by step guidelines how to \"scrape\" this content?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12cd7lf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680682969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just using this as an example:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forums.sherdog.com/threads/bkfc-president-francis-ngannou-is-%E2%80%9Casking-for-unrealistic-money%E2%80%9D.4285974/\"&gt;https://forums.sherdog.com/threads/bkfc-president-francis-ngannou-is-%E2%80%9Casking-for-unrealistic-money%E2%80%9D.4285974/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As I&amp;#39;m attempting to download similar content from a thread generated through another Xenforo hosted platform.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried a chrome extension called &amp;quot;web scrapper&amp;quot; (webscraper.io) and it seemed it worked initially but when I clicked on the data collected it just spat out some code for &amp;quot;failed&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I tried another scrapper called httrack, which rendered something similar.&lt;/p&gt;\n\n&lt;p&gt;This is not something I need to do frequently and I&amp;#39;m not a proficient coder, so I just need to get this done really on this single occasion - with that being said perhaps someone could provide guidelines how to &amp;quot;scrape&amp;quot; the content of that link above with link/name of the relevant scape tool?&lt;/p&gt;\n\n&lt;p&gt;i.e. the page it links to, and subsequent pages within that thread?&lt;/p&gt;\n\n&lt;p&gt;Much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BkOQVJgmFj7B3Mxa5Jq4vbFOTd9YC05Lt3wk8MchC-g.jpg?auto=webp&amp;v=enabled&amp;s=7ce4321138bbaa23761411bbf6262cfdbc2b1df4", "width": 96, "height": 96}, "resolutions": [], "variants": {}, "id": "vwQ0kqdMtdBXIGl6y5wNAbqWGOaDIEik01-44wtBpwk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12cd7lf", "is_robot_indexable": true, "report_reasons": null, "author": "Express-Bike-2836", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12cd7lf/can_anyone_provide_step_by_step_guidelines_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12cd7lf/can_anyone_provide_step_by_step_guidelines_how_to/", "subreddit_subscribers": 676791, "created_utc": 1680682969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello people. \nI have an old desktop pc I am gone use as a server. But I don\u2019t trust the PSU to run 24/7. So I\u2019m looking for a new one. \n\nI need to connect 15 drives and a PSU there can handle 20 will be preferred. I don\u2019t need the 6+2 pci power. And the mb only have a 4 pin cpu power. Beside the 24pin. \n\nI have a lot of Corsair PSU 4x15pin cables . And was wondering if I can buy a new Corsair PSU and use them. Some how. \n\nOr maybe split a pci/cpu power into some 15pin Sata. I know is a risky move. And there is a lot of bad products out there. But maybe your guy\u2019s know a way to do it properly. That I don\u2019t. \n\nAnd I prefer a cheap way to do it.", "author_fullname": "t2_6mm1xxyyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a new PSU for my data pc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d5co1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680745722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello people. \nI have an old desktop pc I am gone use as a server. But I don\u2019t trust the PSU to run 24/7. So I\u2019m looking for a new one. &lt;/p&gt;\n\n&lt;p&gt;I need to connect 15 drives and a PSU there can handle 20 will be preferred. I don\u2019t need the 6+2 pci power. And the mb only have a 4 pin cpu power. Beside the 24pin. &lt;/p&gt;\n\n&lt;p&gt;I have a lot of Corsair PSU 4x15pin cables . And was wondering if I can buy a new Corsair PSU and use them. Some how. &lt;/p&gt;\n\n&lt;p&gt;Or maybe split a pci/cpu power into some 15pin Sata. I know is a risky move. And there is a lot of bad products out there. But maybe your guy\u2019s know a way to do it properly. That I don\u2019t. &lt;/p&gt;\n\n&lt;p&gt;And I prefer a cheap way to do it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12d5co1", "is_robot_indexable": true, "report_reasons": null, "author": "DSandholm", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12d5co1/looking_for_a_new_psu_for_my_data_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12d5co1/looking_for_a_new_psu_for_my_data_pc/", "subreddit_subscribers": 676791, "created_utc": 1680745722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_6bch24w8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Everything not saved will be lost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_12d4sxc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/p_UBu7vLCIlxkROoahkDwQnb9KKpVHOc-Z1Vt2bP4yQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680744373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/comics/comments/12cna56/everything_not_saved_will_be_lost/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DmCpBW5AhzTO0wm5cj9Cuk9Jh49vcz1n1jfKPnP-xh8.png?auto=webp&amp;v=enabled&amp;s=98046f69266ded08089d6c92edd87497fde3921e", "width": 2000, "height": 2218}, "resolutions": [{"url": "https://external-preview.redd.it/DmCpBW5AhzTO0wm5cj9Cuk9Jh49vcz1n1jfKPnP-xh8.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4c519873e8982986161b1c19a299b41a3416b77", "width": 108, "height": 119}, {"url": "https://external-preview.redd.it/DmCpBW5AhzTO0wm5cj9Cuk9Jh49vcz1n1jfKPnP-xh8.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=290c077c2d23ee10644afd2cba0fd2c199ee88cc", "width": 216, "height": 239}, {"url": "https://external-preview.redd.it/DmCpBW5AhzTO0wm5cj9Cuk9Jh49vcz1n1jfKPnP-xh8.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e252c185b50d37448e2a7490da948d1abf760bc", "width": 320, "height": 354}, {"url": "https://external-preview.redd.it/DmCpBW5AhzTO0wm5cj9Cuk9Jh49vcz1n1jfKPnP-xh8.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74edd02c910caa2f7b778f5bf827054159f98051", "width": 640, "height": 709}, {"url": "https://external-preview.redd.it/DmCpBW5AhzTO0wm5cj9Cuk9Jh49vcz1n1jfKPnP-xh8.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87a7827f8b21f7563e21be6c3b248baccc9e8d20", "width": 960, "height": 1064}, {"url": "https://external-preview.redd.it/DmCpBW5AhzTO0wm5cj9Cuk9Jh49vcz1n1jfKPnP-xh8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3960f55cb4251b081e59886b7bc12be9cdc2d9ea", "width": 1080, "height": 1197}], "variants": {}, "id": "uva8goHot84KAgZUELzjSNJiAYedxil9H5bt7B2VmD4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12d4sxc", "is_robot_indexable": true, "report_reasons": null, "author": "burger4d", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12d4sxc/everything_not_saved_will_be_lost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/comics/comments/12cna56/everything_not_saved_will_be_lost/", "subreddit_subscribers": 676791, "created_utc": 1680744373.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}