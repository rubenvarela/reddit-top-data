{"kind": "Listing", "data": {"after": null, "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I felt the need to let everyone on this subreddit know I got my dream job offer.\n\nYou gave me a bollocking for calling OLAP cubes outdated. I'm sorry I pissed all of you off. \n\nYou pointed out I'm applying for the wrong jobs, and the platform engineering roles are sometimes hidden in devops and software engineering adverts.\n\nYou advised that an in-person second stage interview is likely to be a whiteboarding session when I didn't know what to expect.\n\nI made it!\n\nThank You!", "author_fullname": "t2_r7bmwiee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got the job!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129w7vp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 123, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 123, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680477286.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680465531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I felt the need to let everyone on this subreddit know I got my dream job offer.&lt;/p&gt;\n\n&lt;p&gt;You gave me a bollocking for calling OLAP cubes outdated. I&amp;#39;m sorry I pissed all of you off. &lt;/p&gt;\n\n&lt;p&gt;You pointed out I&amp;#39;m applying for the wrong jobs, and the platform engineering roles are sometimes hidden in devops and software engineering adverts.&lt;/p&gt;\n\n&lt;p&gt;You advised that an in-person second stage interview is likely to be a whiteboarding session when I didn&amp;#39;t know what to expect.&lt;/p&gt;\n\n&lt;p&gt;I made it!&lt;/p&gt;\n\n&lt;p&gt;Thank You!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "129w7vp", "is_robot_indexable": true, "report_reasons": null, "author": "va1kyrja-kara", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129w7vp/i_got_the_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129w7vp/i_got_the_job/", "subreddit_subscribers": 95485, "created_utc": 1680465531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI developed a python package to build ETL flows/dags. Each flow is defined as class. Its good for visualizing and running your flows and is notebook friendly.\n\n    # example.py\n    from flowrunner import BaseFlow, step, start, end\n    \n    class ExampleFlow(BaseFlow):\n        @start\n        @step(next=['method2', 'method3'])\n        def method1(self):\n            self.a = 1\n    \n        @step(next=['method4'])\n        def method2(self):\n            self.a += 1\n    \n        @step(next=['method4'])\n        def method3(self):\n            self.a += 2\n    \n        @end\n        @step\n        def method4(self):\n            self.a += 3\n            print(\"output of flow is:\", self.a)\n\nRunning the following display command method gives this output\n\n    ExampleFlow().display()\n\nhttps://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=051b993d2f97627fded2d5bb60ad143be662a540\n\n&amp;#x200B;\n\nRepo link: [https://github.com/prithvijitguha/flowrunner](https://github.com/prithvijitguha/flowrunner)\n\nPyPI link: [https://pypi.org/project/flowrunner/](https://pypi.org/project/flowrunner/)\n\nDocumentation link: [https://flowrunner.readthedocs.io/en/latest/](https://flowrunner.readthedocs.io/en/latest/)\n\nLet me know what you think!\n\nFeedback is welcome :)", "author_fullname": "t2_7u46udxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Package to build ETL flows/dags", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bgfx0yu1bgra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 97, "x": 108, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=710a4eb6b0556220d6d2ee356f3e4e0a010187f4"}, {"y": 195, "x": 216, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91f11849aa746c67ff61c5632b4855e1a408ff05"}, {"y": 289, "x": 320, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20897286cb7faad832c2a442c1ab6d079b1247d6"}], "s": {"y": 378, "x": 418, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=051b993d2f97627fded2d5bb60ad143be662a540"}, "id": "bgfx0yu1bgra1"}}, "name": "t3_129f92i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/x5YGC3CyaESrvCzhSudgubSeF1q0qVZQGGI59D1J_1g.jpg", "edited": 1680432501.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1680424992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I developed a python package to build ETL flows/dags. Each flow is defined as class. Its good for visualizing and running your flows and is notebook friendly.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# example.py\nfrom flowrunner import BaseFlow, step, start, end\n\nclass ExampleFlow(BaseFlow):\n    @start\n    @step(next=[&amp;#39;method2&amp;#39;, &amp;#39;method3&amp;#39;])\n    def method1(self):\n        self.a = 1\n\n    @step(next=[&amp;#39;method4&amp;#39;])\n    def method2(self):\n        self.a += 1\n\n    @step(next=[&amp;#39;method4&amp;#39;])\n    def method3(self):\n        self.a += 2\n\n    @end\n    @step\n    def method4(self):\n        self.a += 3\n        print(&amp;quot;output of flow is:&amp;quot;, self.a)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Running the following display command method gives this output&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ExampleFlow().display()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=051b993d2f97627fded2d5bb60ad143be662a540\"&gt;https://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=051b993d2f97627fded2d5bb60ad143be662a540&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Repo link: &lt;a href=\"https://github.com/prithvijitguha/flowrunner\"&gt;https://github.com/prithvijitguha/flowrunner&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;PyPI link: &lt;a href=\"https://pypi.org/project/flowrunner/\"&gt;https://pypi.org/project/flowrunner/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Documentation link: &lt;a href=\"https://flowrunner.readthedocs.io/en/latest/\"&gt;https://flowrunner.readthedocs.io/en/latest/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n\n&lt;p&gt;Feedback is welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?auto=webp&amp;v=enabled&amp;s=7acd1b010da49901af2ff523e0c4c4b74734a776", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d567a311847d745ac71d4672b5dbffa8f4ff9e70", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a4e0d1259d13b7930932de40e3af8d141f7827c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35acbea26dca43d4e12b054cf4a2fcd1444ac1a3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aeba398e72ae3196a8c56622b201503608d8a208", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f70d407971a59a3400ec9f306fd15bfb01535639", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ddace866d62d51118274d2d0e1f711a9a94de10", "width": 1080, "height": 540}], "variants": {}, "id": "ILxhIEm5fbc5DVSA_rDAPFSK-cgoksPDe143i61rHoA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "129f92i", "is_robot_indexable": true, "report_reasons": null, "author": "Revolutionary-Bat176", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129f92i/python_package_to_build_etl_flowsdags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129f92i/python_package_to_build_etl_flowsdags/", "subreddit_subscribers": 95485, "created_utc": 1680424992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The current stack i am working on is mainly databricks.\nRecently i got two interesting setups regarding the parsing of raw text files. \n\n- For the first project, we got a DB dump of all tables combined altogether in a single .csv file, with the table name as the first field. The trick was that some DB rows, due to their length, landed in multiples rows in the text file dump. Using `awk` and an healthy dose of chatgpt, i could get the parsing done pretty quickly without running into an \"Out of Memory\" error.\n\n\n- For the second project,  the text file DB dump consisted in a single line text file with fixed-width fields. There, the `fold` unix command just made the things easier.\n\nThus my question : how often do you use those unix packages to get the stuff done, knowing that spark is of no help in those contexts ? I'm just curious  what are the alternatives with regard to performance.\n\nThanks a lot in advance for your kind feedback.", "author_fullname": "t2_wvt8wjs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling messy text file as raw data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129ouy7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680450782.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680450039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The current stack i am working on is mainly databricks.\nRecently i got two interesting setups regarding the parsing of raw text files. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;For the first project, we got a DB dump of all tables combined altogether in a single .csv file, with the table name as the first field. The trick was that some DB rows, due to their length, landed in multiples rows in the text file dump. Using &lt;code&gt;awk&lt;/code&gt; and an healthy dose of chatgpt, i could get the parsing done pretty quickly without running into an &amp;quot;Out of Memory&amp;quot; error.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For the second project,  the text file DB dump consisted in a single line text file with fixed-width fields. There, the &lt;code&gt;fold&lt;/code&gt; unix command just made the things easier.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thus my question : how often do you use those unix packages to get the stuff done, knowing that spark is of no help in those contexts ? I&amp;#39;m just curious  what are the alternatives with regard to performance.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot in advance for your kind feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "129ouy7", "is_robot_indexable": true, "report_reasons": null, "author": "clementalweddingaoui", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129ouy7/handling_messy_text_file_as_raw_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129ouy7/handling_messy_text_file_as_raw_data/", "subreddit_subscribers": 95485, "created_utc": 1680450039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all. I\u2019m an AWS SolArc by trade but big data and data lakes are not my area of expertise. I\u2019ve traditionally been more on the Ops and Dev side of the house. But we don\u2019t always get to pick the projects we are handed and I\u2019ve been tasked with building out a data lake for my Org. \n\nWe\u2019ve got about 10 Lines Of Business whose data we need to ingest, something close to 100 tables total. Data size in the terabytes when it\u2019s all said and done.\n\nQuery engine is assumed to be Athena. Analytics will be either Tableau or Quicksight. \n\nMy current sketch has 3 buckets: \n\n1) Raw data bucket.  \n2) Transformed Data Bucket. \n3) Curated Data Bucket\n\nTurn on Intelligent Tiering for all the buckets. Bucket format would follow\u2026. \n\n&lt;bucket&gt;/&lt;LineOfBusiness&gt;/&lt;DatabaseName&gt;/table=&lt;tableName&gt;/&lt;partitions&gt;/&lt;DataFiles&gt;.parquet\n\nFor Time Series data we are looking at Kinesis Firehose to continually write new files, partitioned on year/month/day of ingest. Backing up the raw json data to the raw data bucket and writing the transformed, batched, data to the transformed bucket. Batch 5 minutes at a time, possibly less of lots of partitions, 128MB target file size, parquet format. \n\nFor the non time series data, my plan was two fold:\n\nEnable CDC on the tables and ingest the CDC data as time series data. Batch to 128MB or 5 minutes, write out as parquet files. Expose the CDC data as a table to be queried incase people want to run queries against data churn. \n\nRun daily snapshots of the tables and dump them to S3 using Glue Jobs. These snapshots would have a partition field of \u201csnapshot_date\u201d. We would also have an Athena View that would always point to the latest snapshot by defining the view with a where clause and use the GetDate function to figure out the current date minus one day. The idea being people\nCan write queries against the view and it\u2019ll always pick up yesterdays snapshot as the most current without them needing to actually modify their queries. Expose these snapshots as tables so that people can reference them. My coworker calls these non series tables \u201cenrichment data\u201d though I don\u2019t know if that\u2019s an industry term or a him-term.\n\nMy questions: \n\n1) Does the above seem reasonable? Any problems? My DBA hates the idea of us taking daily snapshots, he would prefer that we use the CDC data and an Athena Apache Iceberg table to run upserts &amp; merges, I have concerns about the performance of doing those constantly but I don\u2019t know if those concerns are warranted? I also don\u2019t love the idea of us doing those modifications non-stop without having a backup available. \n\n2) Most of our relational DBs are MSFT SQL Server, is there a better/best way to get that data into S3 other than Glue Jobs and just letting them doing Select * on a loop to chunk the data and upload it 100,000 rows at a time or something? \n\n3) Are there strong opinions on whether we should keep the data normalized or should we denormalize it? I was thinking about doing all the joins as part of the glue job to convert from raw data to transformed\nData but I wasn\u2019t sure if there was a downside to doing so?\n\n4) does glue table type really matter? I saw that Lake Formation Governed Tables and Athena Iceberg tables both support automatic performance optimization through data compaction, but is the performance noticeably different? We don\u2019t, today, have any PII data that needs protected so not having LF\u2019s row/column level security isn\u2019t a concern -today- if we went with a traditional hive table or Iceberg table. \n\n5) Are we missing anything? Seriously assume I\u2019m a Dumbass who knows nothing. This is a huge project for me and I want to make sure we aren\u2019t walking into a minefield accidentally", "author_fullname": "t2_me6kj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a new data lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129uytu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680463140.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680462901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. I\u2019m an AWS SolArc by trade but big data and data lakes are not my area of expertise. I\u2019ve traditionally been more on the Ops and Dev side of the house. But we don\u2019t always get to pick the projects we are handed and I\u2019ve been tasked with building out a data lake for my Org. &lt;/p&gt;\n\n&lt;p&gt;We\u2019ve got about 10 Lines Of Business whose data we need to ingest, something close to 100 tables total. Data size in the terabytes when it\u2019s all said and done.&lt;/p&gt;\n\n&lt;p&gt;Query engine is assumed to be Athena. Analytics will be either Tableau or Quicksight. &lt;/p&gt;\n\n&lt;p&gt;My current sketch has 3 buckets: &lt;/p&gt;\n\n&lt;p&gt;1) Raw data bucket.&lt;br/&gt;\n2) Transformed Data Bucket. \n3) Curated Data Bucket&lt;/p&gt;\n\n&lt;p&gt;Turn on Intelligent Tiering for all the buckets. Bucket format would follow\u2026. &lt;/p&gt;\n\n&lt;p&gt;&amp;lt;bucket&amp;gt;/&amp;lt;LineOfBusiness&amp;gt;/&amp;lt;DatabaseName&amp;gt;/table=&amp;lt;tableName&amp;gt;/&amp;lt;partitions&amp;gt;/&amp;lt;DataFiles&amp;gt;.parquet&lt;/p&gt;\n\n&lt;p&gt;For Time Series data we are looking at Kinesis Firehose to continually write new files, partitioned on year/month/day of ingest. Backing up the raw json data to the raw data bucket and writing the transformed, batched, data to the transformed bucket. Batch 5 minutes at a time, possibly less of lots of partitions, 128MB target file size, parquet format. &lt;/p&gt;\n\n&lt;p&gt;For the non time series data, my plan was two fold:&lt;/p&gt;\n\n&lt;p&gt;Enable CDC on the tables and ingest the CDC data as time series data. Batch to 128MB or 5 minutes, write out as parquet files. Expose the CDC data as a table to be queried incase people want to run queries against data churn. &lt;/p&gt;\n\n&lt;p&gt;Run daily snapshots of the tables and dump them to S3 using Glue Jobs. These snapshots would have a partition field of \u201csnapshot_date\u201d. We would also have an Athena View that would always point to the latest snapshot by defining the view with a where clause and use the GetDate function to figure out the current date minus one day. The idea being people\nCan write queries against the view and it\u2019ll always pick up yesterdays snapshot as the most current without them needing to actually modify their queries. Expose these snapshots as tables so that people can reference them. My coworker calls these non series tables \u201cenrichment data\u201d though I don\u2019t know if that\u2019s an industry term or a him-term.&lt;/p&gt;\n\n&lt;p&gt;My questions: &lt;/p&gt;\n\n&lt;p&gt;1) Does the above seem reasonable? Any problems? My DBA hates the idea of us taking daily snapshots, he would prefer that we use the CDC data and an Athena Apache Iceberg table to run upserts &amp;amp; merges, I have concerns about the performance of doing those constantly but I don\u2019t know if those concerns are warranted? I also don\u2019t love the idea of us doing those modifications non-stop without having a backup available. &lt;/p&gt;\n\n&lt;p&gt;2) Most of our relational DBs are MSFT SQL Server, is there a better/best way to get that data into S3 other than Glue Jobs and just letting them doing Select * on a loop to chunk the data and upload it 100,000 rows at a time or something? &lt;/p&gt;\n\n&lt;p&gt;3) Are there strong opinions on whether we should keep the data normalized or should we denormalize it? I was thinking about doing all the joins as part of the glue job to convert from raw data to transformed\nData but I wasn\u2019t sure if there was a downside to doing so?&lt;/p&gt;\n\n&lt;p&gt;4) does glue table type really matter? I saw that Lake Formation Governed Tables and Athena Iceberg tables both support automatic performance optimization through data compaction, but is the performance noticeably different? We don\u2019t, today, have any PII data that needs protected so not having LF\u2019s row/column level security isn\u2019t a concern -today- if we went with a traditional hive table or Iceberg table. &lt;/p&gt;\n\n&lt;p&gt;5) Are we missing anything? Seriously assume I\u2019m a Dumbass who knows nothing. This is a huge project for me and I want to make sure we aren\u2019t walking into a minefield accidentally&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "129uytu", "is_robot_indexable": true, "report_reasons": null, "author": "Flakmaster92", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129uytu/building_a_new_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129uytu/building_a_new_data_lake/", "subreddit_subscribers": 95485, "created_utc": 1680462901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nFor nearly three years, a colleague and I have been serving as data engineers on a contractual basis(we are from a south asian country) for a US healthcare system. Our workload has been extensive, including the use of SSIS, Tableau, MDS, and PowerShell from the beginning. Over time, we also took on backend development responsibilities for specific use cases. Currently, we are exclusively handling the pipeline migration from SSIS to Prefect, Spark, and DBT because we are the only individuals who are familiar with the \"modern stack\" (let me make it very clear we are in no way experts but we have been learning and applying a lot of stuff on the fly)\n\nIn summary, we have been working tirelessly for a relatively low salary of approximately $700 per month. Do you believe that we could apply directly to the healthcare system and be hired? Are non-US citizens allowed to work with healthcare data and do you think it would be a good idea if I talk with my manager (US) about it?", "author_fullname": "t2_vgi0we9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Data Engineers with Experience in a US Healthcare System Secure Direct Employment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1298yz5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680407145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;For nearly three years, a colleague and I have been serving as data engineers on a contractual basis(we are from a south asian country) for a US healthcare system. Our workload has been extensive, including the use of SSIS, Tableau, MDS, and PowerShell from the beginning. Over time, we also took on backend development responsibilities for specific use cases. Currently, we are exclusively handling the pipeline migration from SSIS to Prefect, Spark, and DBT because we are the only individuals who are familiar with the &amp;quot;modern stack&amp;quot; (let me make it very clear we are in no way experts but we have been learning and applying a lot of stuff on the fly)&lt;/p&gt;\n\n&lt;p&gt;In summary, we have been working tirelessly for a relatively low salary of approximately $700 per month. Do you believe that we could apply directly to the healthcare system and be hired? Are non-US citizens allowed to work with healthcare data and do you think it would be a good idea if I talk with my manager (US) about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1298yz5", "is_robot_indexable": true, "report_reasons": null, "author": "cosmic_lurker", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1298yz5/can_data_engineers_with_experience_in_a_us/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1298yz5/can_data_engineers_with_experience_in_a_us/", "subreddit_subscribers": 95485, "created_utc": 1680407145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cog in a wheel of IT for a bank\n\n\nJust finished migrating 200TB of CSVs into ADLS. Now have azure databricks to implement medallion architecture and lakehouse model. \n\nTalking to vendors, what RoRs should I be looking for?\n\nOur main business unit is an enterprise analytics team and they don\u2019t straightforwardly let us know their measures/KPIs.", "author_fullname": "t2_5j62j6h7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roles and Responsibilities for a lakehouse project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129k4of", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680478259.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680439239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cog in a wheel of IT for a bank&lt;/p&gt;\n\n&lt;p&gt;Just finished migrating 200TB of CSVs into ADLS. Now have azure databricks to implement medallion architecture and lakehouse model. &lt;/p&gt;\n\n&lt;p&gt;Talking to vendors, what RoRs should I be looking for?&lt;/p&gt;\n\n&lt;p&gt;Our main business unit is an enterprise analytics team and they don\u2019t straightforwardly let us know their measures/KPIs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "129k4of", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-88760", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129k4of/roles_and_responsibilities_for_a_lakehouse_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129k4of/roles_and_responsibilities_for_a_lakehouse_project/", "subreddit_subscribers": 95485, "created_utc": 1680439239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been a management consultant for 14 years, I've always been interested in learning new things which has led to me having experience in strategy, innovation, process design, change management, product management (physical &amp; digital), SDLC, and Technical project management. However,  I do not code.\n\nI would say my biggest value at this point is that I can look across very large ambiguous problems and understand what it takes to develop successful solutions across all silos. I also can develop strategies that can actually be executed because I know what it takes and where the dangers are and then go execute them across different parts of a business. \n\nBUT things aren't that rosy, the consulting industry is becoming a race to the bottom and the roles that I would be really good at in a normal company are filled with Ivy League MBA types. I was quite literally a poor orphan and scraped out a Bachelors in Finance at a state school working 3 jobs at a time. So I find my self hitting my head on a glass ceiling in the strategy or large transformation areas of businesses when I apply because they are filled with people with the education I listed above and the accompanying educational expectations.\n\nASK: With my current background, would learning to code python do anything for me?  I imagine I would be a very unique individual skill wise, but is it marketable? A friend of mine suggested I become a solutions architect tied to software sales due to all of my experience with clients. I think I'd need some certs in AWS and Databricks Engineering. Another thought I had was what I could do if I learned enough to train my own AI models. I could start a consulting firm around helping companies get value out of AI. I can think of 500 use cases for AI in the corporate world, I just can't build them. Any other ideas?", "author_fullname": "t2_unefgpk9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "14 Years experience in consulting, what could I do with learning to code python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12a779x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680490106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been a management consultant for 14 years, I&amp;#39;ve always been interested in learning new things which has led to me having experience in strategy, innovation, process design, change management, product management (physical &amp;amp; digital), SDLC, and Technical project management. However,  I do not code.&lt;/p&gt;\n\n&lt;p&gt;I would say my biggest value at this point is that I can look across very large ambiguous problems and understand what it takes to develop successful solutions across all silos. I also can develop strategies that can actually be executed because I know what it takes and where the dangers are and then go execute them across different parts of a business. &lt;/p&gt;\n\n&lt;p&gt;BUT things aren&amp;#39;t that rosy, the consulting industry is becoming a race to the bottom and the roles that I would be really good at in a normal company are filled with Ivy League MBA types. I was quite literally a poor orphan and scraped out a Bachelors in Finance at a state school working 3 jobs at a time. So I find my self hitting my head on a glass ceiling in the strategy or large transformation areas of businesses when I apply because they are filled with people with the education I listed above and the accompanying educational expectations.&lt;/p&gt;\n\n&lt;p&gt;ASK: With my current background, would learning to code python do anything for me?  I imagine I would be a very unique individual skill wise, but is it marketable? A friend of mine suggested I become a solutions architect tied to software sales due to all of my experience with clients. I think I&amp;#39;d need some certs in AWS and Databricks Engineering. Another thought I had was what I could do if I learned enough to train my own AI models. I could start a consulting firm around helping companies get value out of AI. I can think of 500 use cases for AI in the corporate world, I just can&amp;#39;t build them. Any other ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12a779x", "is_robot_indexable": true, "report_reasons": null, "author": "RedHawkChop", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12a779x/14_years_experience_in_consulting_what_could_i_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12a779x/14_years_experience_in_consulting_what_could_i_do/", "subreddit_subscribers": 95485, "created_utc": 1680490106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say you built a data warehouse (DW) for a few reports.\nNow you are serving many BI teams with multiple report on the same database.\n\nOne more reporting requests comes along the way.\n\nBut the reporting queries are becoming inefficient. You need to change the design schema to make it more efficient. (aggregation, denormalize, add more columns etc ) \n\nThe cost for serving those reports are also rising.\n\nWhat is most common reason you would consider to redesign a schema?\n\nIs it a common practice? How often have you done it?", "author_fullname": "t2_7t9felxr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often do you redesign a data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12a64bq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680487393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say you built a data warehouse (DW) for a few reports.\nNow you are serving many BI teams with multiple report on the same database.&lt;/p&gt;\n\n&lt;p&gt;One more reporting requests comes along the way.&lt;/p&gt;\n\n&lt;p&gt;But the reporting queries are becoming inefficient. You need to change the design schema to make it more efficient. (aggregation, denormalize, add more columns etc ) &lt;/p&gt;\n\n&lt;p&gt;The cost for serving those reports are also rising.&lt;/p&gt;\n\n&lt;p&gt;What is most common reason you would consider to redesign a schema?&lt;/p&gt;\n\n&lt;p&gt;Is it a common practice? How often have you done it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12a64bq", "is_robot_indexable": true, "report_reasons": null, "author": "query_optimization", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12a64bq/how_often_do_you_redesign_a_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12a64bq/how_often_do_you_redesign_a_data_warehouse/", "subreddit_subscribers": 95485, "created_utc": 1680487393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am studying for a DP exam since the company I am working with are heavy users of Azure.  Is it worth going for DP203 or should DP 900 suffice and learn the rest on the job? \n\nIs the DP 203 extremely hard?  I hear it has more industry credibility than the DP 900.", "author_fullname": "t2_o3vqiyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP 900 vs DP 203", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12a5qkn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680486481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am studying for a DP exam since the company I am working with are heavy users of Azure.  Is it worth going for DP203 or should DP 900 suffice and learn the rest on the job? &lt;/p&gt;\n\n&lt;p&gt;Is the DP 203 extremely hard?  I hear it has more industry credibility than the DP 900.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12a5qkn", "is_robot_indexable": true, "report_reasons": null, "author": "InvestingNerd2020", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12a5qkn/dp_900_vs_dp_203/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12a5qkn/dp_900_vs_dp_203/", "subreddit_subscribers": 95485, "created_utc": 1680486481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "creating visualizers for a local network packet spark kafka stream.  \nIs cassandra or hbase optimal options to link sql into javascript ui?\n\nsorting best practice options.  \nideas / library workflows ? appreciate, thanks !", "author_fullname": "t2_b356h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best practice? spark kafka streaming visualizer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129yef9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680470040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;creating visualizers for a local network packet spark kafka stream.&lt;br/&gt;\nIs cassandra or hbase optimal options to link sql into javascript ui?&lt;/p&gt;\n\n&lt;p&gt;sorting best practice options.&lt;br/&gt;\nideas / library workflows ? appreciate, thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "129yef9", "is_robot_indexable": true, "report_reasons": null, "author": "Kubrickann", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129yef9/best_practice_spark_kafka_streaming_visualizer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129yef9/best_practice_spark_kafka_streaming_visualizer/", "subreddit_subscribers": 95485, "created_utc": 1680470040.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}