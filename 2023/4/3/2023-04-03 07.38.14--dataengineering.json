{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I felt the need to let everyone on this subreddit know I got my dream job offer.\n\nYou gave me a bollocking for calling OLAP cubes outdated. I'm sorry I pissed all of you off. \n\nYou pointed out I'm applying for the wrong jobs, and the platform engineering roles are sometimes hidden in devops and software engineering adverts.\n\nYou advised that an in-person second stage interview is likely to be a whiteboarding session when I didn't know what to expect.\n\nI made it!\n\nThank You!", "author_fullname": "t2_r7bmwiee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got the job!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129w7vp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 162, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 162, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680477286.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680465531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I felt the need to let everyone on this subreddit know I got my dream job offer.&lt;/p&gt;\n\n&lt;p&gt;You gave me a bollocking for calling OLAP cubes outdated. I&amp;#39;m sorry I pissed all of you off. &lt;/p&gt;\n\n&lt;p&gt;You pointed out I&amp;#39;m applying for the wrong jobs, and the platform engineering roles are sometimes hidden in devops and software engineering adverts.&lt;/p&gt;\n\n&lt;p&gt;You advised that an in-person second stage interview is likely to be a whiteboarding session when I didn&amp;#39;t know what to expect.&lt;/p&gt;\n\n&lt;p&gt;I made it!&lt;/p&gt;\n\n&lt;p&gt;Thank You!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "129w7vp", "is_robot_indexable": true, "report_reasons": null, "author": "va1kyrja-kara", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129w7vp/i_got_the_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129w7vp/i_got_the_job/", "subreddit_subscribers": 95522, "created_utc": 1680465531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI developed a python package to build ETL flows/dags. Each flow is defined as class. Its good for visualizing and running your flows and is notebook friendly.\n\n    # example.py\n    from flowrunner import BaseFlow, step, start, end\n    \n    class ExampleFlow(BaseFlow):\n        @start\n        @step(next=['method2', 'method3'])\n        def method1(self):\n            self.a = 1\n    \n        @step(next=['method4'])\n        def method2(self):\n            self.a += 1\n    \n        @step(next=['method4'])\n        def method3(self):\n            self.a += 2\n    \n        @end\n        @step\n        def method4(self):\n            self.a += 3\n            print(\"output of flow is:\", self.a)\n\nRunning the following display command method gives this output\n\n    ExampleFlow().display()\n\nhttps://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=051b993d2f97627fded2d5bb60ad143be662a540\n\n&amp;#x200B;\n\nRepo link: [https://github.com/prithvijitguha/flowrunner](https://github.com/prithvijitguha/flowrunner)\n\nPyPI link: [https://pypi.org/project/flowrunner/](https://pypi.org/project/flowrunner/)\n\nDocumentation link: [https://flowrunner.readthedocs.io/en/latest/](https://flowrunner.readthedocs.io/en/latest/)\n\nLet me know what you think!\n\nFeedback is welcome :)", "author_fullname": "t2_7u46udxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Package to build ETL flows/dags", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bgfx0yu1bgra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 97, "x": 108, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=710a4eb6b0556220d6d2ee356f3e4e0a010187f4"}, {"y": 195, "x": 216, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91f11849aa746c67ff61c5632b4855e1a408ff05"}, {"y": 289, "x": 320, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20897286cb7faad832c2a442c1ab6d079b1247d6"}], "s": {"y": 378, "x": 418, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=051b993d2f97627fded2d5bb60ad143be662a540"}, "id": "bgfx0yu1bgra1"}}, "name": "t3_129f92i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/x5YGC3CyaESrvCzhSudgubSeF1q0qVZQGGI59D1J_1g.jpg", "edited": 1680432501.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1680424992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I developed a python package to build ETL flows/dags. Each flow is defined as class. Its good for visualizing and running your flows and is notebook friendly.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# example.py\nfrom flowrunner import BaseFlow, step, start, end\n\nclass ExampleFlow(BaseFlow):\n    @start\n    @step(next=[&amp;#39;method2&amp;#39;, &amp;#39;method3&amp;#39;])\n    def method1(self):\n        self.a = 1\n\n    @step(next=[&amp;#39;method4&amp;#39;])\n    def method2(self):\n        self.a += 1\n\n    @step(next=[&amp;#39;method4&amp;#39;])\n    def method3(self):\n        self.a += 2\n\n    @end\n    @step\n    def method4(self):\n        self.a += 3\n        print(&amp;quot;output of flow is:&amp;quot;, self.a)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Running the following display command method gives this output&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ExampleFlow().display()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=051b993d2f97627fded2d5bb60ad143be662a540\"&gt;https://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=051b993d2f97627fded2d5bb60ad143be662a540&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Repo link: &lt;a href=\"https://github.com/prithvijitguha/flowrunner\"&gt;https://github.com/prithvijitguha/flowrunner&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;PyPI link: &lt;a href=\"https://pypi.org/project/flowrunner/\"&gt;https://pypi.org/project/flowrunner/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Documentation link: &lt;a href=\"https://flowrunner.readthedocs.io/en/latest/\"&gt;https://flowrunner.readthedocs.io/en/latest/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n\n&lt;p&gt;Feedback is welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?auto=webp&amp;v=enabled&amp;s=7acd1b010da49901af2ff523e0c4c4b74734a776", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d567a311847d745ac71d4672b5dbffa8f4ff9e70", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a4e0d1259d13b7930932de40e3af8d141f7827c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35acbea26dca43d4e12b054cf4a2fcd1444ac1a3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aeba398e72ae3196a8c56622b201503608d8a208", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f70d407971a59a3400ec9f306fd15bfb01535639", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ddace866d62d51118274d2d0e1f711a9a94de10", "width": 1080, "height": 540}], "variants": {}, "id": "ILxhIEm5fbc5DVSA_rDAPFSK-cgoksPDe143i61rHoA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "129f92i", "is_robot_indexable": true, "report_reasons": null, "author": "Revolutionary-Bat176", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129f92i/python_package_to_build_etl_flowsdags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129f92i/python_package_to_build_etl_flowsdags/", "subreddit_subscribers": 95522, "created_utc": 1680424992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all. I\u2019m an AWS SolArc by trade but big data and data lakes are not my area of expertise. I\u2019ve traditionally been more on the Ops and Dev side of the house. But we don\u2019t always get to pick the projects we are handed and I\u2019ve been tasked with building out a data lake for my Org. \n\nWe\u2019ve got about 10 Lines Of Business whose data we need to ingest, something close to 100 tables total. Data size in the terabytes when it\u2019s all said and done.\n\nQuery engine is assumed to be Athena. Analytics will be either Tableau or Quicksight. \n\nMy current sketch has 3 buckets: \n\n1) Raw data bucket.  \n2) Transformed Data Bucket. \n3) Curated Data Bucket\n\nTurn on Intelligent Tiering for all the buckets. Bucket format would follow\u2026. \n\n&lt;bucket&gt;/&lt;LineOfBusiness&gt;/&lt;DatabaseName&gt;/table=&lt;tableName&gt;/&lt;partitions&gt;/&lt;DataFiles&gt;.parquet\n\nFor Time Series data we are looking at Kinesis Firehose to continually write new files, partitioned on year/month/day of ingest. Backing up the raw json data to the raw data bucket and writing the transformed, batched, data to the transformed bucket. Batch 5 minutes at a time, possibly less of lots of partitions, 128MB target file size, parquet format. \n\nFor the non time series data, my plan was two fold:\n\nEnable CDC on the tables and ingest the CDC data as time series data. Batch to 128MB or 5 minutes, write out as parquet files. Expose the CDC data as a table to be queried incase people want to run queries against data churn. \n\nRun daily snapshots of the tables and dump them to S3 using Glue Jobs. These snapshots would have a partition field of \u201csnapshot_date\u201d. We would also have an Athena View that would always point to the latest snapshot by defining the view with a where clause and use the GetDate function to figure out the current date minus one day. The idea being people\nCan write queries against the view and it\u2019ll always pick up yesterdays snapshot as the most current without them needing to actually modify their queries. Expose these snapshots as tables so that people can reference them. My coworker calls these non series tables \u201cenrichment data\u201d though I don\u2019t know if that\u2019s an industry term or a him-term.\n\nMy questions: \n\n1) Does the above seem reasonable? Any problems? My DBA hates the idea of us taking daily snapshots, he would prefer that we use the CDC data and an Athena Apache Iceberg table to run upserts &amp; merges, I have concerns about the performance of doing those constantly but I don\u2019t know if those concerns are warranted? I also don\u2019t love the idea of us doing those modifications non-stop without having a backup available. \n\n2) Most of our relational DBs are MSFT SQL Server, is there a better/best way to get that data into S3 other than Glue Jobs and just letting them doing Select * on a loop to chunk the data and upload it 100,000 rows at a time or something? \n\n3) Are there strong opinions on whether we should keep the data normalized or should we denormalize it? I was thinking about doing all the joins as part of the glue job to convert from raw data to transformed\nData but I wasn\u2019t sure if there was a downside to doing so?\n\n4) does glue table type really matter? I saw that Lake Formation Governed Tables and Athena Iceberg tables both support automatic performance optimization through data compaction, but is the performance noticeably different? We don\u2019t, today, have any PII data that needs protected so not having LF\u2019s row/column level security isn\u2019t a concern -today- if we went with a traditional hive table or Iceberg table. \n\n5) Are we missing anything? Seriously assume I\u2019m a Dumbass who knows nothing. This is a huge project for me and I want to make sure we aren\u2019t walking into a minefield accidentally", "author_fullname": "t2_me6kj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a new data lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129uytu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680463140.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680462901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. I\u2019m an AWS SolArc by trade but big data and data lakes are not my area of expertise. I\u2019ve traditionally been more on the Ops and Dev side of the house. But we don\u2019t always get to pick the projects we are handed and I\u2019ve been tasked with building out a data lake for my Org. &lt;/p&gt;\n\n&lt;p&gt;We\u2019ve got about 10 Lines Of Business whose data we need to ingest, something close to 100 tables total. Data size in the terabytes when it\u2019s all said and done.&lt;/p&gt;\n\n&lt;p&gt;Query engine is assumed to be Athena. Analytics will be either Tableau or Quicksight. &lt;/p&gt;\n\n&lt;p&gt;My current sketch has 3 buckets: &lt;/p&gt;\n\n&lt;p&gt;1) Raw data bucket.&lt;br/&gt;\n2) Transformed Data Bucket. \n3) Curated Data Bucket&lt;/p&gt;\n\n&lt;p&gt;Turn on Intelligent Tiering for all the buckets. Bucket format would follow\u2026. &lt;/p&gt;\n\n&lt;p&gt;&amp;lt;bucket&amp;gt;/&amp;lt;LineOfBusiness&amp;gt;/&amp;lt;DatabaseName&amp;gt;/table=&amp;lt;tableName&amp;gt;/&amp;lt;partitions&amp;gt;/&amp;lt;DataFiles&amp;gt;.parquet&lt;/p&gt;\n\n&lt;p&gt;For Time Series data we are looking at Kinesis Firehose to continually write new files, partitioned on year/month/day of ingest. Backing up the raw json data to the raw data bucket and writing the transformed, batched, data to the transformed bucket. Batch 5 minutes at a time, possibly less of lots of partitions, 128MB target file size, parquet format. &lt;/p&gt;\n\n&lt;p&gt;For the non time series data, my plan was two fold:&lt;/p&gt;\n\n&lt;p&gt;Enable CDC on the tables and ingest the CDC data as time series data. Batch to 128MB or 5 minutes, write out as parquet files. Expose the CDC data as a table to be queried incase people want to run queries against data churn. &lt;/p&gt;\n\n&lt;p&gt;Run daily snapshots of the tables and dump them to S3 using Glue Jobs. These snapshots would have a partition field of \u201csnapshot_date\u201d. We would also have an Athena View that would always point to the latest snapshot by defining the view with a where clause and use the GetDate function to figure out the current date minus one day. The idea being people\nCan write queries against the view and it\u2019ll always pick up yesterdays snapshot as the most current without them needing to actually modify their queries. Expose these snapshots as tables so that people can reference them. My coworker calls these non series tables \u201cenrichment data\u201d though I don\u2019t know if that\u2019s an industry term or a him-term.&lt;/p&gt;\n\n&lt;p&gt;My questions: &lt;/p&gt;\n\n&lt;p&gt;1) Does the above seem reasonable? Any problems? My DBA hates the idea of us taking daily snapshots, he would prefer that we use the CDC data and an Athena Apache Iceberg table to run upserts &amp;amp; merges, I have concerns about the performance of doing those constantly but I don\u2019t know if those concerns are warranted? I also don\u2019t love the idea of us doing those modifications non-stop without having a backup available. &lt;/p&gt;\n\n&lt;p&gt;2) Most of our relational DBs are MSFT SQL Server, is there a better/best way to get that data into S3 other than Glue Jobs and just letting them doing Select * on a loop to chunk the data and upload it 100,000 rows at a time or something? &lt;/p&gt;\n\n&lt;p&gt;3) Are there strong opinions on whether we should keep the data normalized or should we denormalize it? I was thinking about doing all the joins as part of the glue job to convert from raw data to transformed\nData but I wasn\u2019t sure if there was a downside to doing so?&lt;/p&gt;\n\n&lt;p&gt;4) does glue table type really matter? I saw that Lake Formation Governed Tables and Athena Iceberg tables both support automatic performance optimization through data compaction, but is the performance noticeably different? We don\u2019t, today, have any PII data that needs protected so not having LF\u2019s row/column level security isn\u2019t a concern -today- if we went with a traditional hive table or Iceberg table. &lt;/p&gt;\n\n&lt;p&gt;5) Are we missing anything? Seriously assume I\u2019m a Dumbass who knows nothing. This is a huge project for me and I want to make sure we aren\u2019t walking into a minefield accidentally&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "129uytu", "is_robot_indexable": true, "report_reasons": null, "author": "Flakmaster92", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129uytu/building_a_new_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129uytu/building_a_new_data_lake/", "subreddit_subscribers": 95522, "created_utc": 1680462901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The current stack i am working on is mainly databricks.\nRecently i got two interesting setups regarding the parsing of raw text files. \n\n- For the first project, we got a DB dump of all tables combined altogether in a single .csv file, with the table name as the first field. The trick was that some DB rows, due to their length, landed in multiples rows in the text file dump. Using `awk` and an healthy dose of chatgpt, i could get the parsing done pretty quickly without running into an \"Out of Memory\" error.\n\n\n- For the second project,  the text file DB dump consisted in a single line text file with fixed-width fields. There, the `fold` unix command just made the things easier.\n\nThus my question : how often do you use those unix packages to get the stuff done, knowing that spark is of no help in those contexts ? I'm just curious  what are the alternatives with regard to performance.\n\nThanks a lot in advance for your kind feedback.", "author_fullname": "t2_wvt8wjs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling messy text file as raw data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129ouy7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680450782.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680450039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The current stack i am working on is mainly databricks.\nRecently i got two interesting setups regarding the parsing of raw text files. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;For the first project, we got a DB dump of all tables combined altogether in a single .csv file, with the table name as the first field. The trick was that some DB rows, due to their length, landed in multiples rows in the text file dump. Using &lt;code&gt;awk&lt;/code&gt; and an healthy dose of chatgpt, i could get the parsing done pretty quickly without running into an &amp;quot;Out of Memory&amp;quot; error.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For the second project,  the text file DB dump consisted in a single line text file with fixed-width fields. There, the &lt;code&gt;fold&lt;/code&gt; unix command just made the things easier.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thus my question : how often do you use those unix packages to get the stuff done, knowing that spark is of no help in those contexts ? I&amp;#39;m just curious  what are the alternatives with regard to performance.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot in advance for your kind feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "129ouy7", "is_robot_indexable": true, "report_reasons": null, "author": "clementalweddingaoui", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129ouy7/handling_messy_text_file_as_raw_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129ouy7/handling_messy_text_file_as_raw_data/", "subreddit_subscribers": 95522, "created_utc": 1680450039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say you built a data warehouse (DW) for a few reports.\nNow you are serving many BI teams with multiple report on the same database.\n\nOne more reporting requests comes along the way.\n\nBut the reporting queries are becoming inefficient. You need to change the design schema to make it more efficient. (aggregation, denormalize, add more columns etc ) \n\nThe cost for serving those reports are also rising.\n\nWhat is most common reason you would consider to redesign a schema?\n\nIs it a common practice? How often have you done it?", "author_fullname": "t2_7t9felxr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often do you redesign a data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12a64bq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680487393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say you built a data warehouse (DW) for a few reports.\nNow you are serving many BI teams with multiple report on the same database.&lt;/p&gt;\n\n&lt;p&gt;One more reporting requests comes along the way.&lt;/p&gt;\n\n&lt;p&gt;But the reporting queries are becoming inefficient. You need to change the design schema to make it more efficient. (aggregation, denormalize, add more columns etc ) &lt;/p&gt;\n\n&lt;p&gt;The cost for serving those reports are also rising.&lt;/p&gt;\n\n&lt;p&gt;What is most common reason you would consider to redesign a schema?&lt;/p&gt;\n\n&lt;p&gt;Is it a common practice? How often have you done it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12a64bq", "is_robot_indexable": true, "report_reasons": null, "author": "query_optimization", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12a64bq/how_often_do_you_redesign_a_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12a64bq/how_often_do_you_redesign_a_data_warehouse/", "subreddit_subscribers": 95522, "created_utc": 1680487393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cog in a wheel of IT for a bank\n\n\nJust finished migrating 200TB of CSVs into ADLS. Now have azure databricks to implement medallion architecture and lakehouse model. \n\nTalking to vendors, what RoRs should I be looking for?\n\nOur main business unit is an enterprise analytics team and they don\u2019t straightforwardly let us know their measures/KPIs.", "author_fullname": "t2_5j62j6h7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roles and Responsibilities for a lakehouse project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129k4of", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680478259.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680439239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cog in a wheel of IT for a bank&lt;/p&gt;\n\n&lt;p&gt;Just finished migrating 200TB of CSVs into ADLS. Now have azure databricks to implement medallion architecture and lakehouse model. &lt;/p&gt;\n\n&lt;p&gt;Talking to vendors, what RoRs should I be looking for?&lt;/p&gt;\n\n&lt;p&gt;Our main business unit is an enterprise analytics team and they don\u2019t straightforwardly let us know their measures/KPIs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "129k4of", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-88760", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129k4of/roles_and_responsibilities_for_a_lakehouse_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129k4of/roles_and_responsibilities_for_a_lakehouse_project/", "subreddit_subscribers": 95522, "created_utc": 1680439239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Preferably open source, preferably free, definitely hosted in-house, but anything that isn't too ridiculously expensive would be considered.\n\nThere's so many options, most of which either a) have zero documentation, not even system requirements, b) won't tell you what it costs until after a demo (that will inevitably be a waste of time with high pressure sales pitches), c) obscenely expensive (with subscription fees), or d) so complicated to set up as to be impossible (see a)).", "author_fullname": "t2_c8eislj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can anybody recommend a document management system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12a969n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680495488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preferably open source, preferably free, definitely hosted in-house, but anything that isn&amp;#39;t too ridiculously expensive would be considered.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s so many options, most of which either a) have zero documentation, not even system requirements, b) won&amp;#39;t tell you what it costs until after a demo (that will inevitably be a waste of time with high pressure sales pitches), c) obscenely expensive (with subscription fees), or d) so complicated to set up as to be impossible (see a)).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12a969n", "is_robot_indexable": true, "report_reasons": null, "author": "DwaywelayTOP", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12a969n/can_anybody_recommend_a_document_management_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12a969n/can_anybody_recommend_a_document_management_system/", "subreddit_subscribers": 95522, "created_utc": 1680495488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been a management consultant for 14 years, I've always been interested in learning new things which has led to me having experience in strategy, innovation, process design, change management, product management (physical &amp; digital), SDLC, and Technical project management. However,  I do not code.\n\nI would say my biggest value at this point is that I can look across very large ambiguous problems and understand what it takes to develop successful solutions across all silos. I also can develop strategies that can actually be executed because I know what it takes and where the dangers are and then go execute them across different parts of a business. \n\nBUT things aren't that rosy, the consulting industry is becoming a race to the bottom and the roles that I would be really good at in a normal company are filled with Ivy League MBA types. I was quite literally a poor orphan and scraped out a Bachelors in Finance at a state school working 3 jobs at a time. So I find my self hitting my head on a glass ceiling in the strategy or large transformation areas of businesses when I apply because they are filled with people with the education I listed above and the accompanying educational expectations.\n\nASK: With my current background, would learning to code python do anything for me?  I imagine I would be a very unique individual skill wise, but is it marketable? A friend of mine suggested I become a solutions architect tied to software sales due to all of my experience with clients. I think I'd need some certs in AWS and Databricks Engineering. Another thought I had was what I could do if I learned enough to train my own AI models. I could start a consulting firm around helping companies get value out of AI. I can think of 500 use cases for AI in the corporate world, I just can't build them. Any other ideas?", "author_fullname": "t2_unefgpk9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "14 Years experience in consulting, what could I do with learning to code python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12a779x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680490106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been a management consultant for 14 years, I&amp;#39;ve always been interested in learning new things which has led to me having experience in strategy, innovation, process design, change management, product management (physical &amp;amp; digital), SDLC, and Technical project management. However,  I do not code.&lt;/p&gt;\n\n&lt;p&gt;I would say my biggest value at this point is that I can look across very large ambiguous problems and understand what it takes to develop successful solutions across all silos. I also can develop strategies that can actually be executed because I know what it takes and where the dangers are and then go execute them across different parts of a business. &lt;/p&gt;\n\n&lt;p&gt;BUT things aren&amp;#39;t that rosy, the consulting industry is becoming a race to the bottom and the roles that I would be really good at in a normal company are filled with Ivy League MBA types. I was quite literally a poor orphan and scraped out a Bachelors in Finance at a state school working 3 jobs at a time. So I find my self hitting my head on a glass ceiling in the strategy or large transformation areas of businesses when I apply because they are filled with people with the education I listed above and the accompanying educational expectations.&lt;/p&gt;\n\n&lt;p&gt;ASK: With my current background, would learning to code python do anything for me?  I imagine I would be a very unique individual skill wise, but is it marketable? A friend of mine suggested I become a solutions architect tied to software sales due to all of my experience with clients. I think I&amp;#39;d need some certs in AWS and Databricks Engineering. Another thought I had was what I could do if I learned enough to train my own AI models. I could start a consulting firm around helping companies get value out of AI. I can think of 500 use cases for AI in the corporate world, I just can&amp;#39;t build them. Any other ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12a779x", "is_robot_indexable": true, "report_reasons": null, "author": "RedHawkChop", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12a779x/14_years_experience_in_consulting_what_could_i_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12a779x/14_years_experience_in_consulting_what_could_i_do/", "subreddit_subscribers": 95522, "created_utc": 1680490106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am studying for a DP exam since the company I am working with are heavy users of Azure.  Is it worth going for DP203 or should DP 900 suffice and learn the rest on the job? \n\nIs the DP 203 extremely hard?  I hear it has more industry credibility than the DP 900.", "author_fullname": "t2_o3vqiyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP 900 vs DP 203", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12a5qkn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680486481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am studying for a DP exam since the company I am working with are heavy users of Azure.  Is it worth going for DP203 or should DP 900 suffice and learn the rest on the job? &lt;/p&gt;\n\n&lt;p&gt;Is the DP 203 extremely hard?  I hear it has more industry credibility than the DP 900.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12a5qkn", "is_robot_indexable": true, "report_reasons": null, "author": "InvestingNerd2020", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12a5qkn/dp_900_vs_dp_203/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12a5qkn/dp_900_vs_dp_203/", "subreddit_subscribers": 95522, "created_utc": 1680486481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\n&amp;#x200B;\n\nTLDR; we are doing a lot of forecasting on time series data and are currently using SQL databases for storing the data. We are wanting to switch over to time series databases like InfluxDB or AWS Timestream.\n\n&amp;#x200B;\n\nAny suggestions ? Do you have any experience with time series DBs? Is the switch worth it ?", "author_fullname": "t2_2i1av4aw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using time series databases ? InfluxDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12aca4z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680505165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TLDR; we are doing a lot of forecasting on time series data and are currently using SQL databases for storing the data. We are wanting to switch over to time series databases like InfluxDB or AWS Timestream.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions ? Do you have any experience with time series DBs? Is the switch worth it ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12aca4z", "is_robot_indexable": true, "report_reasons": null, "author": "younggamech", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12aca4z/using_time_series_databases_influxdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12aca4z/using_time_series_databases_influxdb/", "subreddit_subscribers": 95522, "created_utc": 1680505165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nThe analyst comes to you (the data engineer) saying I need this data, and you say OK. You give them the data and then the analyst comes to you again saying they actually need to change something in data. How can you avoid this back &amp; forth and work better together as a team? By getting on the same page about the complete analytics lifecycle.\n\nFollowing this structure helps data engineers better understand their data and improve the effectiveness of the data analytics work of their team.\n\nData Analytics Lifecycle, here it goes\n\n### 1. Discovery\n\nWhat problem you are solving and what business outcomes you wish to see.\n\n### 2. Data preparation\n\nDecide which data sources will be useful for the analysis, collect the data from all these disparate sources, and load it into a data analytics sandbox so it can be used for prototyping.\n\n### 3. Model planning\n\nA model of relationship between variables that can help you understand the probability of an event happening. It could be a SQL model, statistical model, or machine learning model.\n\n### 4. Building and executing the model\n\nOnce you know what your models should look like, you can build them and begin to draw inferences from your modeled data. \n\nFor the sake of brevity, let's skip it in this short post (I'll write another post about it or you can read about [building different types of models on this post](https://www.rudderstack.com/learn/data-analytics/data-analytics-lifecycle/))\n\n### 6. Operationalizing\n\nOnce the stakeholders are happy with the analysis, you can execute the same model outside of the analytics sandbox on a production dataset.\n\nQuestions/thoughts? Share your challenges/success-stories following this lifecycle?\n\nP.S. If you like this, I will write one post every week to share similar primers on different data engineering topics. Do share the topics and support my open-source work at [RudderStack](https://github.com/rudderlabs/rudder-server)", "author_fullname": "t2_cbh6ollo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analytics Lifecycle", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": true, "name": "t3_12ac3qm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/iigc1vjbRvP8Fefid_WFlcNs1xNPLJw94V2aQ1G8fB0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680504564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The analyst comes to you (the data engineer) saying I need this data, and you say OK. You give them the data and then the analyst comes to you again saying they actually need to change something in data. How can you avoid this back &amp;amp; forth and work better together as a team? By getting on the same page about the complete analytics lifecycle.&lt;/p&gt;\n\n&lt;p&gt;Following this structure helps data engineers better understand their data and improve the effectiveness of the data analytics work of their team.&lt;/p&gt;\n\n&lt;p&gt;Data Analytics Lifecycle, here it goes&lt;/p&gt;\n\n&lt;h3&gt;1. Discovery&lt;/h3&gt;\n\n&lt;p&gt;What problem you are solving and what business outcomes you wish to see.&lt;/p&gt;\n\n&lt;h3&gt;2. Data preparation&lt;/h3&gt;\n\n&lt;p&gt;Decide which data sources will be useful for the analysis, collect the data from all these disparate sources, and load it into a data analytics sandbox so it can be used for prototyping.&lt;/p&gt;\n\n&lt;h3&gt;3. Model planning&lt;/h3&gt;\n\n&lt;p&gt;A model of relationship between variables that can help you understand the probability of an event happening. It could be a SQL model, statistical model, or machine learning model.&lt;/p&gt;\n\n&lt;h3&gt;4. Building and executing the model&lt;/h3&gt;\n\n&lt;p&gt;Once you know what your models should look like, you can build them and begin to draw inferences from your modeled data. &lt;/p&gt;\n\n&lt;p&gt;For the sake of brevity, let&amp;#39;s skip it in this short post (I&amp;#39;ll write another post about it or you can read about &lt;a href=\"https://www.rudderstack.com/learn/data-analytics/data-analytics-lifecycle/\"&gt;building different types of models on this post&lt;/a&gt;)&lt;/p&gt;\n\n&lt;h3&gt;6. Operationalizing&lt;/h3&gt;\n\n&lt;p&gt;Once the stakeholders are happy with the analysis, you can execute the same model outside of the analytics sandbox on a production dataset.&lt;/p&gt;\n\n&lt;p&gt;Questions/thoughts? Share your challenges/success-stories following this lifecycle?&lt;/p&gt;\n\n&lt;p&gt;P.S. If you like this, I will write one post every week to share similar primers on different data engineering topics. Do share the topics and support my open-source work at &lt;a href=\"https://github.com/rudderlabs/rudder-server\"&gt;RudderStack&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/a77mj57vqnra1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/a77mj57vqnra1.jpg?auto=webp&amp;v=enabled&amp;s=77d48322341c735504b62daf7969772082fac332", "width": 1200, "height": 757}, "resolutions": [{"url": "https://preview.redd.it/a77mj57vqnra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d249b72c835dd666db4ff988f2076488f3f67947", "width": 108, "height": 68}, {"url": "https://preview.redd.it/a77mj57vqnra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b521055f571e3abfc8e4751eb49385560d4d971c", "width": 216, "height": 136}, {"url": "https://preview.redd.it/a77mj57vqnra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b2cb56fac77d7f18bf4a9de3a635d1a99f09ec6", "width": 320, "height": 201}, {"url": "https://preview.redd.it/a77mj57vqnra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3058293189bb1cfd1cad895ab589df21521436f", "width": 640, "height": 403}, {"url": "https://preview.redd.it/a77mj57vqnra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=601c33ca894536483d7588452372ce8eb16c0639", "width": 960, "height": 605}, {"url": "https://preview.redd.it/a77mj57vqnra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5b26df3fc373749ab8496ebc4807861f8ff4a9b", "width": 1080, "height": 681}], "variants": {}, "id": "lV8U2Wxliqx_99B53OAPWtHmm9qiZg6h-xnyKx8ZWp0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ac3qm", "is_robot_indexable": true, "report_reasons": null, "author": "ephemeral404", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ac3qm/data_analytics_lifecycle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/a77mj57vqnra1.jpg", "subreddit_subscribers": 95522, "created_utc": 1680504564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to migrate a Database with multiple schemas from SQL Server to Databricks Delta Lake. What is the equivalent of SQL Server's schema in databricks? In the unity catalog database or schema is the same and we put tables directly in the databricks database. Do I need to create multiple databases to account for each schema in SQL Server?", "author_fullname": "t2_c9y3v2w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Server to Databricks Delta Lake Unity Catalog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12abg1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680502385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to migrate a Database with multiple schemas from SQL Server to Databricks Delta Lake. What is the equivalent of SQL Server&amp;#39;s schema in databricks? In the unity catalog database or schema is the same and we put tables directly in the databricks database. Do I need to create multiple databases to account for each schema in SQL Server?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12abg1s", "is_robot_indexable": true, "report_reasons": null, "author": "mdghouse1986", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12abg1s/sql_server_to_databricks_delta_lake_unity_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12abg1s/sql_server_to_databricks_delta_lake_unity_catalog/", "subreddit_subscribers": 95522, "created_utc": 1680502385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Connection here is for postgresql and we are running a test for null values.  \n\n\nHere is the sample code for Airflow:  \n\n\n    from airflow import DAG\n    from airflow.operators.python_operator import PythonOperator\n    from datetime import datetime\n    import psycopg2\n    \n    def check_null_or_empty():\n        # Set up a connection to the PostgreSQL database\n        conn = psycopg2.connect(\n            host=\"your_host_name\",\n            database=\"your_database_name\",\n            user=\"your_username\",\n            password=\"your_password\"\n        )\n    \n        # Open a cursor to perform database operations\n        cur = conn.cursor()\n    \n        # Define the query to run\n        query = \"SELECT COUNT(*) FROM users WHERE column_name IS NULL OR column_name=''\"\n    \n        # Execute the query\n        cur.execute(query)\n    \n        # Fetch the results\n        result = cur.fetchone()[0]\n    \n        # Close the cursor and the connection\n        cur.close()\n        conn.close()\n    \n        # Raise an error if the result is not 0\n        if result != 0:\n            raise ValueError(f\"Data quality check failed. {result} rows have null or empty values in column_name.\")\n        else:\n            print(\"Data quality check passed. All rows have a value in column_name.\")\n    \n    # Define the DAG\n    dag = DAG('data_quality_dag', description='Data quality check for null or empty values in users.column_name',\n              schedule_interval='@daily', start_date=datetime(2023, 4, 1))\n    \n    # Define the task\n    check_null_or_empty_task = PythonOperator(task_id='check_null_or_empty', python_callable=check_null_or_empty, dag=dag)\n    \n    # Define the task order\n    check_null_or_empty_task\n\nNot lets take the example of GE:  \n\n\n    name: data_quality_check\n    \n    steps:\n      - name: check_null_or_empty\n        action:\n          module_name: great_expectations.dataset.sqlalchemy_dataset\n          class_name: SqlAlchemyDataset\n          datasource: your_postgresql_datasource\n          batch_kwargs:\n            table: users\n            schema: public\n          expectation_suite_name: data_quality_check\n          action_list_operator: OR\n          failing_expectation_results_in_triggered_actions: True\n          action_on_failure:\n            exit: True\n            message: \"Data quality check failed. {{ result['result'] }} rows have null or empty values in column_name.\"\n        expectations:\n          - expectation_type: not_null\n            kwargs:\n              column: column_name\n          - expectation_type: expect_column_values_to_not_be_null\n            kwargs:\n              column: column_name\n          - expectation_type: expect_column_values_to_not_be_empty\n            kwargs:\n              column: column_name\n    \n    datasources:\n      your_postgresql_datasource:\n        class_name: Datasource\n        module_name: great_expectations.datasource\n        credentials:\n          username: your_username\n          password: your_password\n        module_name: great_expectations.datasource\n        class_name: SqlAlchemyDatasource\n        data_asset_type:\n          module_name: great_expectations.dataset\n          class_name: SqlAlchemyDataset\n        credentials:\n          username: your_username\n          password: your_password\n        drivername: postgresql\n        host: your_host_name\n        port: 5432\n        database: your_database_name\n        schema: public", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Sample Code] : Data quality null check for Airflow vs GreatExpectations [GE]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12aao4m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680499971.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Connection here is for postgresql and we are running a test for null values.  &lt;/p&gt;\n\n&lt;p&gt;Here is the sample code for Airflow:  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime\nimport psycopg2\n\ndef check_null_or_empty():\n    # Set up a connection to the PostgreSQL database\n    conn = psycopg2.connect(\n        host=&amp;quot;your_host_name&amp;quot;,\n        database=&amp;quot;your_database_name&amp;quot;,\n        user=&amp;quot;your_username&amp;quot;,\n        password=&amp;quot;your_password&amp;quot;\n    )\n\n    # Open a cursor to perform database operations\n    cur = conn.cursor()\n\n    # Define the query to run\n    query = &amp;quot;SELECT COUNT(*) FROM users WHERE column_name IS NULL OR column_name=&amp;#39;&amp;#39;&amp;quot;\n\n    # Execute the query\n    cur.execute(query)\n\n    # Fetch the results\n    result = cur.fetchone()[0]\n\n    # Close the cursor and the connection\n    cur.close()\n    conn.close()\n\n    # Raise an error if the result is not 0\n    if result != 0:\n        raise ValueError(f&amp;quot;Data quality check failed. {result} rows have null or empty values in column_name.&amp;quot;)\n    else:\n        print(&amp;quot;Data quality check passed. All rows have a value in column_name.&amp;quot;)\n\n# Define the DAG\ndag = DAG(&amp;#39;data_quality_dag&amp;#39;, description=&amp;#39;Data quality check for null or empty values in users.column_name&amp;#39;,\n          schedule_interval=&amp;#39;@daily&amp;#39;, start_date=datetime(2023, 4, 1))\n\n# Define the task\ncheck_null_or_empty_task = PythonOperator(task_id=&amp;#39;check_null_or_empty&amp;#39;, python_callable=check_null_or_empty, dag=dag)\n\n# Define the task order\ncheck_null_or_empty_task\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Not lets take the example of GE:  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;name: data_quality_check\n\nsteps:\n  - name: check_null_or_empty\n    action:\n      module_name: great_expectations.dataset.sqlalchemy_dataset\n      class_name: SqlAlchemyDataset\n      datasource: your_postgresql_datasource\n      batch_kwargs:\n        table: users\n        schema: public\n      expectation_suite_name: data_quality_check\n      action_list_operator: OR\n      failing_expectation_results_in_triggered_actions: True\n      action_on_failure:\n        exit: True\n        message: &amp;quot;Data quality check failed. {{ result[&amp;#39;result&amp;#39;] }} rows have null or empty values in column_name.&amp;quot;\n    expectations:\n      - expectation_type: not_null\n        kwargs:\n          column: column_name\n      - expectation_type: expect_column_values_to_not_be_null\n        kwargs:\n          column: column_name\n      - expectation_type: expect_column_values_to_not_be_empty\n        kwargs:\n          column: column_name\n\ndatasources:\n  your_postgresql_datasource:\n    class_name: Datasource\n    module_name: great_expectations.datasource\n    credentials:\n      username: your_username\n      password: your_password\n    module_name: great_expectations.datasource\n    class_name: SqlAlchemyDatasource\n    data_asset_type:\n      module_name: great_expectations.dataset\n      class_name: SqlAlchemyDataset\n    credentials:\n      username: your_username\n      password: your_password\n    drivername: postgresql\n    host: your_host_name\n    port: 5432\n    database: your_database_name\n    schema: public\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12aao4m", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12aao4m/sample_code_data_quality_null_check_for_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12aao4m/sample_code_data_quality_null_check_for_airflow/", "subreddit_subscribers": 95522, "created_utc": 1680499971.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently working as a controls engineering  (1yr) with a degree in industrial engineering, minor in data analysis. I\u2019m just curious if anyone has made a similar jump, how you did it, and how you would improve it if you did it again? Thanks for reading!", "author_fullname": "t2_zl24e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Controls Engineer Migration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12aajgp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680499580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently working as a controls engineering  (1yr) with a degree in industrial engineering, minor in data analysis. I\u2019m just curious if anyone has made a similar jump, how you did it, and how you would improve it if you did it again? Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12aajgp", "is_robot_indexable": true, "report_reasons": null, "author": "MrGreat_Value", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12aajgp/controls_engineer_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12aajgp/controls_engineer_migration/", "subreddit_subscribers": 95522, "created_utc": 1680499580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. \n\nI'm starting to learn DuckDB and I'm having some problems and I don't find much information about it on the internet.\n\nI'm following the following tutorial for beginners: [https://marclamberti.com/blog/duckdb-getting-started-for-beginners/](https://marclamberti.com/blog/duckdb-getting-started-for-beginners/)\n\nRight at the beginning, I tried converting csv files to parquet with the following command:\n\n    import glob\n    PATH = 'stock_market_data/nasdaq'\n    for filename in glob.iglob(f'{PATH}/csv/*.csv'):\n    dest = f'{PATH}/parquet/{filename.split(\"/\")[-1][:-4]}.parquet'\n    conn.execute(f\"\"\"\n    COPY (SELECT * FROM read_csv('{filename}', header=True, dateformat='%d-%m-%Y', columns={{'Date': 'DATE', 'Low': 'DOUBLE', 'Open': 'DOUBLE', 'Volume': 'BIGINT', 'High': 'DOUBLE', 'Close': 'DOUBLE', 'AdjustedClose': 'DOUBLE'}}, filename=True)) \n    TO '{dest}' (FORMAT 'parquet')\"\"\")\n\nThen I get the following error:\n\n    IOException                               Traceback (most recent call last)\n    Cell In[14], line 6\n          4 for filename in glob.iglob(f'{PATH}/csv/*.csv'):\n          5     dest = f'{PATH}/parquet/{filename.split(\"/\")[-1][:-4]}.parquet'\n    ----&gt; 6     conn.execute(f\"\"\"COPY (SELECT * \n          7         FROM read_csv('{filename}', \n          8         header=True, \n          9         dateformat='%d-%m-%Y', \n         10         columns={{'Date': 'DATE', \n         11             'Low': 'DOUBLE', \n         12             'Open': 'DOUBLE', \n         13             'Volume': 'BIGINT', \n         14             'High': 'DOUBLE', \n         15             'Close': 'DOUBLE', \n         16             'AdjustedClose': 'DOUBLE'}}, \n         17         filename=True)) \n         18         TO '{dest}' (FORMAT 'parquet')\"\"\")\n    \n    IOException: \n\nThe error is just \"IOException\" and **no further information** **is given**.\n\nI tried looking up the IOException error regarding DuckDB and found nothing even on the project's git page. Could someone help me or give me a direction of what this error could be?\n\nThanks in advance.", "author_fullname": "t2_o6dai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB beginner needs help: IOException error.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12aabel", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680498904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m starting to learn DuckDB and I&amp;#39;m having some problems and I don&amp;#39;t find much information about it on the internet.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m following the following tutorial for beginners: &lt;a href=\"https://marclamberti.com/blog/duckdb-getting-started-for-beginners/\"&gt;https://marclamberti.com/blog/duckdb-getting-started-for-beginners/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Right at the beginning, I tried converting csv files to parquet with the following command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import glob\nPATH = &amp;#39;stock_market_data/nasdaq&amp;#39;\nfor filename in glob.iglob(f&amp;#39;{PATH}/csv/*.csv&amp;#39;):\ndest = f&amp;#39;{PATH}/parquet/{filename.split(&amp;quot;/&amp;quot;)[-1][:-4]}.parquet&amp;#39;\nconn.execute(f&amp;quot;&amp;quot;&amp;quot;\nCOPY (SELECT * FROM read_csv(&amp;#39;{filename}&amp;#39;, header=True, dateformat=&amp;#39;%d-%m-%Y&amp;#39;, columns={{&amp;#39;Date&amp;#39;: &amp;#39;DATE&amp;#39;, &amp;#39;Low&amp;#39;: &amp;#39;DOUBLE&amp;#39;, &amp;#39;Open&amp;#39;: &amp;#39;DOUBLE&amp;#39;, &amp;#39;Volume&amp;#39;: &amp;#39;BIGINT&amp;#39;, &amp;#39;High&amp;#39;: &amp;#39;DOUBLE&amp;#39;, &amp;#39;Close&amp;#39;: &amp;#39;DOUBLE&amp;#39;, &amp;#39;AdjustedClose&amp;#39;: &amp;#39;DOUBLE&amp;#39;}}, filename=True)) \nTO &amp;#39;{dest}&amp;#39; (FORMAT &amp;#39;parquet&amp;#39;)&amp;quot;&amp;quot;&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then I get the following error:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;IOException                               Traceback (most recent call last)\nCell In[14], line 6\n      4 for filename in glob.iglob(f&amp;#39;{PATH}/csv/*.csv&amp;#39;):\n      5     dest = f&amp;#39;{PATH}/parquet/{filename.split(&amp;quot;/&amp;quot;)[-1][:-4]}.parquet&amp;#39;\n----&amp;gt; 6     conn.execute(f&amp;quot;&amp;quot;&amp;quot;COPY (SELECT * \n      7         FROM read_csv(&amp;#39;{filename}&amp;#39;, \n      8         header=True, \n      9         dateformat=&amp;#39;%d-%m-%Y&amp;#39;, \n     10         columns={{&amp;#39;Date&amp;#39;: &amp;#39;DATE&amp;#39;, \n     11             &amp;#39;Low&amp;#39;: &amp;#39;DOUBLE&amp;#39;, \n     12             &amp;#39;Open&amp;#39;: &amp;#39;DOUBLE&amp;#39;, \n     13             &amp;#39;Volume&amp;#39;: &amp;#39;BIGINT&amp;#39;, \n     14             &amp;#39;High&amp;#39;: &amp;#39;DOUBLE&amp;#39;, \n     15             &amp;#39;Close&amp;#39;: &amp;#39;DOUBLE&amp;#39;, \n     16             &amp;#39;AdjustedClose&amp;#39;: &amp;#39;DOUBLE&amp;#39;}}, \n     17         filename=True)) \n     18         TO &amp;#39;{dest}&amp;#39; (FORMAT &amp;#39;parquet&amp;#39;)&amp;quot;&amp;quot;&amp;quot;)\n\nIOException: \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The error is just &amp;quot;IOException&amp;quot; and &lt;strong&gt;no further information&lt;/strong&gt; &lt;strong&gt;is given&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I tried looking up the IOException error regarding DuckDB and found nothing even on the project&amp;#39;s git page. Could someone help me or give me a direction of what this error could be?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/b73Sh7_BD7rOx6F2PoaD_pAUr_mj6Dxu2U82g_8FlBI.jpg?auto=webp&amp;v=enabled&amp;s=34752b7312e9a9b74c4facceeac2707bff0aff4e", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/b73Sh7_BD7rOx6F2PoaD_pAUr_mj6Dxu2U82g_8FlBI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6500f1c303945a97711a5c1025e85012a2ea5e75", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/b73Sh7_BD7rOx6F2PoaD_pAUr_mj6Dxu2U82g_8FlBI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc85504e077e20324b388c42e4b87e122ec21f71", "width": 216, "height": 216}], "variants": {}, "id": "EwxLy3sukWDHbLi0uHuOH1gmt62GQPEJMy3KLFFWgLM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12aabel", "is_robot_indexable": true, "report_reasons": null, "author": "andreylabanca", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12aabel/duckdb_beginner_needs_help_ioexception_error/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12aabel/duckdb_beginner_needs_help_ioexception_error/", "subreddit_subscribers": 95522, "created_utc": 1680498904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to understand the difference between spark.kubernetes.executor.request.cores and spark.executor.cores?\n\nI am running spark 3.2 on Kubernetes. So in YARN I was using spark.executor.cores to set the core per executor. \n\nDoes spark.kubernetes.executor.request.cores takes precedence for executor cores when used on Kubernetes? \nAnd should I not use spark.executor.cores on Kubernetes?", "author_fullname": "t2_7ht6c4ggj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark on Kubernetes config", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12a7ebg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680490640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to understand the difference between spark.kubernetes.executor.request.cores and spark.executor.cores?&lt;/p&gt;\n\n&lt;p&gt;I am running spark 3.2 on Kubernetes. So in YARN I was using spark.executor.cores to set the core per executor. &lt;/p&gt;\n\n&lt;p&gt;Does spark.kubernetes.executor.request.cores takes precedence for executor cores when used on Kubernetes? \nAnd should I not use spark.executor.cores on Kubernetes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12a7ebg", "is_robot_indexable": true, "report_reasons": null, "author": "Weird_Implement_2960", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12a7ebg/spark_on_kubernetes_config/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12a7ebg/spark_on_kubernetes_config/", "subreddit_subscribers": 95522, "created_utc": 1680490640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "creating visualizers for a local network packet spark kafka stream.  \nIs cassandra or hbase optimal options to link sql into javascript ui?\n\nsorting best practice options.  \nideas / library workflows ? appreciate, thanks !", "author_fullname": "t2_b356h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best practice? spark kafka streaming visualizer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129yef9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680470040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;creating visualizers for a local network packet spark kafka stream.&lt;br/&gt;\nIs cassandra or hbase optimal options to link sql into javascript ui?&lt;/p&gt;\n\n&lt;p&gt;sorting best practice options.&lt;br/&gt;\nideas / library workflows ? appreciate, thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "129yef9", "is_robot_indexable": true, "report_reasons": null, "author": "Kubrickann", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129yef9/best_practice_spark_kafka_streaming_visualizer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129yef9/best_practice_spark_kafka_streaming_visualizer/", "subreddit_subscribers": 95522, "created_utc": 1680470040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What kind of technical coding assessment should I expect from a smaller startup? Will it be difficult SQL and Python questions? Or should I just focus on SQL? It looks like a lot of data modeling and data governance from the job description.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Staff Data Engineer/Architect Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12a83k8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680492505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What kind of technical coding assessment should I expect from a smaller startup? Will it be difficult SQL and Python questions? Or should I just focus on SQL? It looks like a lot of data modeling and data governance from the job description.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12a83k8", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12a83k8/staff_data_engineerarchitect_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12a83k8/staff_data_engineerarchitect_interview/", "subreddit_subscribers": 95522, "created_utc": 1680492505.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}