{"kind": "Listing", "data": {"after": "t3_12bsx8t", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://sqlmesh.com/\n\nSQLMesh has native support for reading dbt projects. \n\nIt allows you to build safe incremental models with SQL. No Jinja required. Courtesy of SQLglot. \n\nComes bundled with DuckDB for testing. \n\nIt looks like a more pleasant experience. \n\nThoughts?", "author_fullname": "t2_74pfheof", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A dbt killer is born (SQLMesh)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12b6fgb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680575088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://sqlmesh.com/\"&gt;https://sqlmesh.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;SQLMesh has native support for reading dbt projects. &lt;/p&gt;\n\n&lt;p&gt;It allows you to build safe incremental models with SQL. No Jinja required. Courtesy of SQLglot. &lt;/p&gt;\n\n&lt;p&gt;Comes bundled with DuckDB for testing. &lt;/p&gt;\n\n&lt;p&gt;It looks like a more pleasant experience. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12b6fgb", "is_robot_indexable": true, "report_reasons": null, "author": "No_Equivalent5942", "discussion_type": null, "num_comments": 72, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12b6fgb/a_dbt_killer_is_born_sqlmesh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12b6fgb/a_dbt_killer_is_born_sqlmesh/", "subreddit_subscribers": 95812, "created_utc": 1680575088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, DEs.\n\nI recently tried using Great Expectations on my data processing pipelines in Databricks (written in PySpark). I\u2019ve heard about this library a lot, but in reality seems not promising. \n\nFor starters, nearly all methods there can take only one column as argument - so for example you cannot check duplicates by multiple columns without concatenation. Secondly, it seems like it consumes a lot of resources - once we included it in script, it began utilise 5 workers at once. \n\nMy question is: how do you approach your data tests? Do you write them yourself or leverage GE/some other open-source framework?", "author_fullname": "t2_i0q1ptpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Great expectations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bblq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680592196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, DEs.&lt;/p&gt;\n\n&lt;p&gt;I recently tried using Great Expectations on my data processing pipelines in Databricks (written in PySpark). I\u2019ve heard about this library a lot, but in reality seems not promising. &lt;/p&gt;\n\n&lt;p&gt;For starters, nearly all methods there can take only one column as argument - so for example you cannot check duplicates by multiple columns without concatenation. Secondly, it seems like it consumes a lot of resources - once we included it in script, it began utilise 5 workers at once. &lt;/p&gt;\n\n&lt;p&gt;My question is: how do you approach your data tests? Do you write them yourself or leverage GE/some other open-source framework?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bblq0", "is_robot_indexable": true, "report_reasons": null, "author": "ye11owmonster", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bblq0/great_expectations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bblq0/great_expectations/", "subreddit_subscribers": 95812, "created_utc": 1680592196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \n\nI know projects are not that important but I have a to fun building them and I thought maybe someone else is interested in some of mine. \n\nSo basically this is a very simple Data Lakehouse deployed in Docker containers, which uses Iceberg, Trino, Minio and a Hive Metastore. Since someone maybe directly wants to play with some data I have built an init container which creates an Iceberg table based on a parquet file in the object storage. Furthermore there is a BI Service pre configured to visualize it. \n\nI thought this project might be interesting to some of you who have only worked with traditional Data Warehouses (not that I am an expert with \"new types\" of storages) or want a more real life like storage, without paying a cloud provider, for your own Data projects. \n\nHere is the Github repo: [https://github.com/dominikhei/Local-Data-LakeHouse](https://github.com/dominikhei/Local-Data-LakeHouse)\n\nFeedback is well appreciated :)", "author_fullname": "t2_v219tksh", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Project showcase: sample Data Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12buxtq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680639043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;I know projects are not that important but I have a to fun building them and I thought maybe someone else is interested in some of mine. &lt;/p&gt;\n\n&lt;p&gt;So basically this is a very simple Data Lakehouse deployed in Docker containers, which uses Iceberg, Trino, Minio and a Hive Metastore. Since someone maybe directly wants to play with some data I have built an init container which creates an Iceberg table based on a parquet file in the object storage. Furthermore there is a BI Service pre configured to visualize it. &lt;/p&gt;\n\n&lt;p&gt;I thought this project might be interesting to some of you who have only worked with traditional Data Warehouses (not that I am an expert with &amp;quot;new types&amp;quot; of storages) or want a more real life like storage, without paying a cloud provider, for your own Data projects. &lt;/p&gt;\n\n&lt;p&gt;Here is the Github repo: &lt;a href=\"https://github.com/dominikhei/Local-Data-LakeHouse\"&gt;https://github.com/dominikhei/Local-Data-LakeHouse&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feedback is well appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?auto=webp&amp;v=enabled&amp;s=a58192c664108607ded3daf9478385a01fbd0ecc", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d0d0e069191b2d1de8fceddc2d8d67b6afdcfa6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0a1d064f39318c98ded16c5bb222f58d2359e5a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=583b60681f50e05fd927771f145bb86a48ed662e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f897b46d33f821bc4292cb42076abfee2a38226a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c36c9251e310b24d30ca2ed0c1dabe70c3421f3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aUdWRxdZCKN5dS8DekIK3lpAlbNrWFszJ8sBSZBhESw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=25561e443cddffff0a625175d488742cc40d37ca", "width": 1080, "height": 540}], "variants": {}, "id": "FLYBfmpfBaqdgNeA_3kK487GKlC9BiAetvGC4EzqLpo"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12buxtq", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Hand-577", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12buxtq/project_showcase_sample_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12buxtq/project_showcase_sample_data_lakehouse/", "subreddit_subscribers": 95812, "created_utc": 1680639043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm on lesson 4.9L of the specified course, and I'm curious why when we SELECT the new name for user\\_id to be user, this one turns purple. I noticed this also happens in other lessons when I fill in the new 'user' name.\n\nhttps://preview.redd.it/4bo29daf7tra1.png?width=795&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0a44ea4895b8fdf92d3e959997779b28414a6065", "author_fullname": "t2_g39jwfp1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering with Databricks course - clarification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 37, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4bo29daf7tra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 28, "x": 108, "u": "https://preview.redd.it/4bo29daf7tra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=227654083faf206322a5662be9fb51db6a26f9c7"}, {"y": 57, "x": 216, "u": "https://preview.redd.it/4bo29daf7tra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=037fd5a4a698b7588ee38fc9c1b49a1bfcaf7e12"}, {"y": 84, "x": 320, "u": "https://preview.redd.it/4bo29daf7tra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bf7e04e9876a5c8688dbc9152fd5ac02c4a07d3"}, {"y": 169, "x": 640, "u": "https://preview.redd.it/4bo29daf7tra1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13e4d2a4f9a53c61eeaa3f20d87bef8e300dbaaa"}], "s": {"y": 211, "x": 795, "u": "https://preview.redd.it/4bo29daf7tra1.png?width=795&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0a44ea4895b8fdf92d3e959997779b28414a6065"}, "id": "4bo29daf7tra1"}}, "name": "t3_12bao84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/-QpepBClsfO2fK7abeBKiPybBtsQFsLT5tOMVFh5R_4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680588713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m on lesson 4.9L of the specified course, and I&amp;#39;m curious why when we SELECT the new name for user_id to be user, this one turns purple. I noticed this also happens in other lessons when I fill in the new &amp;#39;user&amp;#39; name.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4bo29daf7tra1.png?width=795&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0a44ea4895b8fdf92d3e959997779b28414a6065\"&gt;https://preview.redd.it/4bo29daf7tra1.png?width=795&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0a44ea4895b8fdf92d3e959997779b28414a6065&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bao84", "is_robot_indexable": true, "report_reasons": null, "author": "Background-Ball5978", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bao84/data_engineering_with_databricks_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bao84/data_engineering_with_databricks_course/", "subreddit_subscribers": 95812, "created_utc": 1680588713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. I just joined a very small startup and looking for setting up very basic analytics stack. Currently excel is the king.\n\nThe product is basically the website and a simple online platform. My goal is to access data reagarding the platform and query the DB, mainly for data exploration.\n\nQuerying directly the production DB doesn't look like a smart idea. In previous jobs I had we addressed the issue in 2 ways : a) building a DWH and b) make a perfect replica of the production DB for data purposes.\n\nI am awake the DWH would be the best solution but I need a solution with very low budget and very easy to managed. I'm the only person working on it and I have lots of other tasks to work on. I am mainly a data analyst with some coding experience. I can get some support from engineering but very limited.\n\nAny suggestion on how to to approach the issue in the most simple way? I am fine with getting something that kind of work, show results and then next year iterate in a more structured way", "author_fullname": "t2_9gx56cnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small startup (10p) - how to managed production database data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bk8f6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680617224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I just joined a very small startup and looking for setting up very basic analytics stack. Currently excel is the king.&lt;/p&gt;\n\n&lt;p&gt;The product is basically the website and a simple online platform. My goal is to access data reagarding the platform and query the DB, mainly for data exploration.&lt;/p&gt;\n\n&lt;p&gt;Querying directly the production DB doesn&amp;#39;t look like a smart idea. In previous jobs I had we addressed the issue in 2 ways : a) building a DWH and b) make a perfect replica of the production DB for data purposes.&lt;/p&gt;\n\n&lt;p&gt;I am awake the DWH would be the best solution but I need a solution with very low budget and very easy to managed. I&amp;#39;m the only person working on it and I have lots of other tasks to work on. I am mainly a data analyst with some coding experience. I can get some support from engineering but very limited.&lt;/p&gt;\n\n&lt;p&gt;Any suggestion on how to to approach the issue in the most simple way? I am fine with getting something that kind of work, show results and then next year iterate in a more structured way&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bk8f6", "is_robot_indexable": true, "report_reasons": null, "author": "Kokubo-ubo", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bk8f6/small_startup_10p_how_to_managed_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bk8f6/small_startup_10p_how_to_managed_production/", "subreddit_subscribers": 95812, "created_utc": 1680617224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi people of internet,\n\nI am hopping to get an answer to my question in title. We are a small to medium business and our data amount is smaller than 200gb. So i have googled so much that now I am even confused as to which one to choose in Azure to build our data warehouse. \n\nWe want to create our data warehouse in Azure and choose the right database for that. I have never used Azure portal before so i have been doing some research about it over the last several days to choose a database option for our data warehouse but cannot get my head around as what option is good for us. Can anyone please help me understand which one would be right tool for us?", "author_fullname": "t2_2b7yft3b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure SQL Database (Single &amp; serverless) or Dedicated SQL Pools (Formerly SQL DW) for Data warehousing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12brosk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680632506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people of internet,&lt;/p&gt;\n\n&lt;p&gt;I am hopping to get an answer to my question in title. We are a small to medium business and our data amount is smaller than 200gb. So i have googled so much that now I am even confused as to which one to choose in Azure to build our data warehouse. &lt;/p&gt;\n\n&lt;p&gt;We want to create our data warehouse in Azure and choose the right database for that. I have never used Azure portal before so i have been doing some research about it over the last several days to choose a database option for our data warehouse but cannot get my head around as what option is good for us. Can anyone please help me understand which one would be right tool for us?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12brosk", "is_robot_indexable": true, "report_reasons": null, "author": "DznFatih", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12brosk/azure_sql_database_single_serverless_or_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12brosk/azure_sql_database_single_serverless_or_dedicated/", "subreddit_subscribers": 95812, "created_utc": 1680632506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After evaluating few solutions in the market: We were in the market to hunt for a solution which will cost under 10k (yearly) considering the cost of opensource will be similar considering DE resource and maintenance cost etc  \n1. [MonteCarlo](https://montecarlodata.com) \\- Super duper expensive - Unable to hosting in Google Cloud  \n2. [BigEye](https://bigeye.com) \\- Good features  \n3. [Metaplane](https://metaplane.dev) \\- Overall good package but when compared to catalog and other features it lost the ground.   \n4. [Atlan](https://atlan.com) \\- Only catalog and was super expensive for us. (around 30k+)  \n\n\n[Decube](https://decube.io) was the one we boiled down to - fairly a new company based in Singapore and provides hosting option too on all clouds.  \n\n\nBefore decube, we were using [Soda.io](https://Soda.io) but it was kind of challenging for us to map the lineage and downstream impact which resulted in search of another solution.", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts around decube.io (data observability and catalog platform)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bfzkg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680607412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After evaluating few solutions in the market: We were in the market to hunt for a solution which will cost under 10k (yearly) considering the cost of opensource will be similar considering DE resource and maintenance cost etc&lt;br/&gt;\n1. &lt;a href=\"https://montecarlodata.com\"&gt;MonteCarlo&lt;/a&gt; - Super duper expensive - Unable to hosting in Google Cloud&lt;br/&gt;\n2. &lt;a href=\"https://bigeye.com\"&gt;BigEye&lt;/a&gt; - Good features&lt;br/&gt;\n3. &lt;a href=\"https://metaplane.dev\"&gt;Metaplane&lt;/a&gt; - Overall good package but when compared to catalog and other features it lost the ground.&lt;br/&gt;\n4. &lt;a href=\"https://atlan.com\"&gt;Atlan&lt;/a&gt; - Only catalog and was super expensive for us. (around 30k+)  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://decube.io\"&gt;Decube&lt;/a&gt; was the one we boiled down to - fairly a new company based in Singapore and provides hosting option too on all clouds.  &lt;/p&gt;\n\n&lt;p&gt;Before decube, we were using &lt;a href=\"https://Soda.io\"&gt;Soda.io&lt;/a&gt; but it was kind of challenging for us to map the lineage and downstream impact which resulted in search of another solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bfzkg", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bfzkg/thoughts_around_decubeio_data_observability_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bfzkg/thoughts_around_decubeio_data_observability_and/", "subreddit_subscribers": 95812, "created_utc": 1680607412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am downloading publicly available provider information from the Centers for Medicare &amp; Medicaid Services. The file is published monthly. Any entity can change names or address without an update to the identifier. I am appending a 'data month' to the data.  \n\n\nI want to have a table (oracle) that helps the user indicate if there has been a change to any of 3 fields that are not the main identifier (address, entity name, business name). In that case I would like to have the most recent information and a second row with the old address, name, ect. The user could use the data download month to see the changes over time on a particular ID.   \n\n\nI can think of 2 ways to do this. Make the key be the identifier plus the other fields I care about. Update the 'downloaded month field' based on the key if it exists otherwise import. Or query the existing data set, append the new data, leverage pandas to check for duplicates on the columns, truncate the existing table, insert the entire table. The monthly files are approx 15k rows.   \n\n\nAnother option would be to just import everything always and then use other methods to filter down to what I want to see. I am learning best practices for table design and ETL and would appreciate some guidance.", "author_fullname": "t2_f1q7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL/Table Design Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bphlz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680628048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am downloading publicly available provider information from the Centers for Medicare &amp;amp; Medicaid Services. The file is published monthly. Any entity can change names or address without an update to the identifier. I am appending a &amp;#39;data month&amp;#39; to the data.  &lt;/p&gt;\n\n&lt;p&gt;I want to have a table (oracle) that helps the user indicate if there has been a change to any of 3 fields that are not the main identifier (address, entity name, business name). In that case I would like to have the most recent information and a second row with the old address, name, ect. The user could use the data download month to see the changes over time on a particular ID.   &lt;/p&gt;\n\n&lt;p&gt;I can think of 2 ways to do this. Make the key be the identifier plus the other fields I care about. Update the &amp;#39;downloaded month field&amp;#39; based on the key if it exists otherwise import. Or query the existing data set, append the new data, leverage pandas to check for duplicates on the columns, truncate the existing table, insert the entire table. The monthly files are approx 15k rows.   &lt;/p&gt;\n\n&lt;p&gt;Another option would be to just import everything always and then use other methods to filter down to what I want to see. I am learning best practices for table design and ETL and would appreciate some guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bphlz", "is_robot_indexable": true, "report_reasons": null, "author": "machinegunke11y", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bphlz/etltable_design_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bphlz/etltable_design_question/", "subreddit_subscribers": 95812, "created_utc": 1680628048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, how do you enable access to analytical data for your developers to build data products (real-time or near real-time). How do you build APIs (does your analytics team write the query and give to engineering team or they write it themselves) \n\nAnd do you use a specialised database for enabling that access or a normal warehouse like Redshift would work. We looked at Clickhouse, Druid. And were wondering which is more cost efficient. \n\n&amp;#x200B;\n\nContext - We're a clinic management application with EMR for doctors and we're creating an analytics dashboard for the doctors to look at business data and health data for their patients. It will have multiple visualisations - E.g. Patients seen, same over time, longitudinal data for a patient etc\n\nInfo - We're using MongoDB as our application database, Redshift for analytics", "author_fullname": "t2_71me4x17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data access for customer facing analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bkk5z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680617952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, how do you enable access to analytical data for your developers to build data products (real-time or near real-time). How do you build APIs (does your analytics team write the query and give to engineering team or they write it themselves) &lt;/p&gt;\n\n&lt;p&gt;And do you use a specialised database for enabling that access or a normal warehouse like Redshift would work. We looked at Clickhouse, Druid. And were wondering which is more cost efficient. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Context - We&amp;#39;re a clinic management application with EMR for doctors and we&amp;#39;re creating an analytics dashboard for the doctors to look at business data and health data for their patients. It will have multiple visualisations - E.g. Patients seen, same over time, longitudinal data for a patient etc&lt;/p&gt;\n\n&lt;p&gt;Info - We&amp;#39;re using MongoDB as our application database, Redshift for analytics&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bkk5z", "is_robot_indexable": true, "report_reasons": null, "author": "rationaleuser", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bkk5z/data_access_for_customer_facing_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bkk5z/data_access_for_customer_facing_analytics/", "subreddit_subscribers": 95812, "created_utc": 1680617952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A company called Basedash is doing a survey about databases, ORMs, data warehouses, etc. and giving away a 27\" Apple display. \n\nI just did the survey and got a referral link, so check it out. Not affiliated with Basedash.\n\nhttps://2023.stateofdb.com/?referral=W25L9R", "author_fullname": "t2_a1jey", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "State of Databases 2023 Survey + Giveaway", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bn51b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680623331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A company called Basedash is doing a survey about databases, ORMs, data warehouses, etc. and giving away a 27&amp;quot; Apple display. &lt;/p&gt;\n\n&lt;p&gt;I just did the survey and got a referral link, so check it out. Not affiliated with Basedash.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://2023.stateofdb.com/?referral=W25L9R\"&gt;https://2023.stateofdb.com/?referral=W25L9R&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lc901qHSbRlSRbels1IOfOnIrI5piIbQsr9udY1p1o8.jpg?auto=webp&amp;v=enabled&amp;s=06c6a5bf03d03f8bd0c1615493b779dd87af1364", "width": 887, "height": 584}, "resolutions": [{"url": "https://external-preview.redd.it/lc901qHSbRlSRbels1IOfOnIrI5piIbQsr9udY1p1o8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e81d5747bb989416ad2c7297c411e662736e414b", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/lc901qHSbRlSRbels1IOfOnIrI5piIbQsr9udY1p1o8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63ed50be66100cfcec0d053a4bf3a26d706583d0", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/lc901qHSbRlSRbels1IOfOnIrI5piIbQsr9udY1p1o8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e89cdf7be7814cc6cfe2e77928baa16700e95ffd", "width": 320, "height": 210}, {"url": "https://external-preview.redd.it/lc901qHSbRlSRbels1IOfOnIrI5piIbQsr9udY1p1o8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=851a8022bc82c753df36b718ab665d6694af4957", "width": 640, "height": 421}], "variants": {}, "id": "632qflb-BS9xIHlJIX17ofamRFIIefHb51FR-CPdMRU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bn51b", "is_robot_indexable": true, "report_reasons": null, "author": "TheLoveBoat", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bn51b/state_of_databases_2023_survey_giveaway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bn51b/state_of_databases_2023_survey_giveaway/", "subreddit_subscribers": 95812, "created_utc": 1680623331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry for the self-promotion, but I wanted to share my publication with you where I cover all topics related to data engineering on a weekly basis. It's essentially a collection of my personal notes, as there are countless articles, tools, and other resources being released every day, and I want to keep track of the most interesting ones in one place. If you're interested in checking it out, you can find the link here: [**https://patrikbraborec.substack.com/p/data-news-23**](https://patrikbraborec.substack.com/p/data-news-23)", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data news - new tools, techniques, research, and industry insights in data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bcp4s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680596525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry for the self-promotion, but I wanted to share my publication with you where I cover all topics related to data engineering on a weekly basis. It&amp;#39;s essentially a collection of my personal notes, as there are countless articles, tools, and other resources being released every day, and I want to keep track of the most interesting ones in one place. If you&amp;#39;re interested in checking it out, you can find the link here: &lt;a href=\"https://patrikbraborec.substack.com/p/data-news-23\"&gt;&lt;strong&gt;https://patrikbraborec.substack.com/p/data-news-23&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TscxDWFwK6fkcvE-OBb8EGxXROGkrqGGElDnjPhVenw.jpg?auto=webp&amp;v=enabled&amp;s=169c1efec067f06a9c56c49dad7e2c1c2783d8a4", "width": 617, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/TscxDWFwK6fkcvE-OBb8EGxXROGkrqGGElDnjPhVenw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd886e4b2870793bfbfec4b8f760a07d3edc4dd3", "width": 108, "height": 105}, {"url": "https://external-preview.redd.it/TscxDWFwK6fkcvE-OBb8EGxXROGkrqGGElDnjPhVenw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a816917da15c379bc4acb70925925596f96d510", "width": 216, "height": 210}, {"url": "https://external-preview.redd.it/TscxDWFwK6fkcvE-OBb8EGxXROGkrqGGElDnjPhVenw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bd15ec1198a92bbe7d6bead181ab6ade430e3e7", "width": 320, "height": 311}], "variants": {}, "id": "nBTc879rAhvUO8SIpz7LSu1qiRqAr6j8rD6F0TXEH-w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12bcp4s", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bcp4s/data_news_new_tools_techniques_research_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bcp4s/data_news_new_tools_techniques_research_and/", "subreddit_subscribers": 95812, "created_utc": 1680596525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there,  \n\n\nWe currently have created a pyspark pipeline and now we are trying to figure out where our job slows down so we can track performance over time. We are aware of how pyspark lazily evaluates but not sure how to factor that in.  \n\n\nSay we wanted create a run time over a function that returns a dataframe, do we have to call an action on that dataframe then a timing point to figure out the actual runtime of that function? Otherwise it feels like we are just timing the unevaluated dataframe.  \n\n\nHas anyone tackled this before? Would be keen to hear options  \n\n\nThanks", "author_fullname": "t2_7xwk8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark lazy evaluation andd logging", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12b9217", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680582914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,  &lt;/p&gt;\n\n&lt;p&gt;We currently have created a pyspark pipeline and now we are trying to figure out where our job slows down so we can track performance over time. We are aware of how pyspark lazily evaluates but not sure how to factor that in.  &lt;/p&gt;\n\n&lt;p&gt;Say we wanted create a run time over a function that returns a dataframe, do we have to call an action on that dataframe then a timing point to figure out the actual runtime of that function? Otherwise it feels like we are just timing the unevaluated dataframe.  &lt;/p&gt;\n\n&lt;p&gt;Has anyone tackled this before? Would be keen to hear options  &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12b9217", "is_robot_indexable": true, "report_reasons": null, "author": "Manyreason", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12b9217/pyspark_lazy_evaluation_andd_logging/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12b9217/pyspark_lazy_evaluation_andd_logging/", "subreddit_subscribers": 95812, "created_utc": 1680582914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE's,\n\nJust looking for some advice in terms of the best solutions to ingest the data for every event that is located in the STFP. \n\nThe process i have come up with is setting up a logic app that pulls any new json file that has been dropped into the location, looks every minute for these files and moves them into blob storage. \n\nFrom there using databricks to flatten the JSON into a dataframe and pushing it into another location with the correct dtypes and naming. \n\nIf anyone has a better way of doing that please let me know, or if you've worked with eagleeye before and have a working solution in azure that would be great!", "author_fullname": "t2_eibi1lc4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone in this thread worked with Eagleeye Solutions for a Loyalty Program, basically ingesting the data from an SFTP? {Azure]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bv75l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680639577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE&amp;#39;s,&lt;/p&gt;\n\n&lt;p&gt;Just looking for some advice in terms of the best solutions to ingest the data for every event that is located in the STFP. &lt;/p&gt;\n\n&lt;p&gt;The process i have come up with is setting up a logic app that pulls any new json file that has been dropped into the location, looks every minute for these files and moves them into blob storage. &lt;/p&gt;\n\n&lt;p&gt;From there using databricks to flatten the JSON into a dataframe and pushing it into another location with the correct dtypes and naming. &lt;/p&gt;\n\n&lt;p&gt;If anyone has a better way of doing that please let me know, or if you&amp;#39;ve worked with eagleeye before and have a working solution in azure that would be great!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bv75l", "is_robot_indexable": true, "report_reasons": null, "author": "cosmic-destiny", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bv75l/has_anyone_in_this_thread_worked_with_eagleeye/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bv75l/has_anyone_in_this_thread_worked_with_eagleeye/", "subreddit_subscribers": 95812, "created_utc": 1680639577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am writing a Spark dataframe to Hive external table using insertInto. The table is partitioned on multiple columns. Something like category/sub_category.\n\nHow can I make sure to write 10 files under each sub_category directory. The data is huge (1 year of data), If I do repartition(10) the job is taking too long. If I repartition on category and sub_category columns together, there is only 1  file getting creating under each sub_category directory.\n\nWhat I am missing? My understanding is if I repartition based on multiple columns and provide numPartition too, it should work.\n\n(df\n.repartion(10, \"category\", \"sub_category\")\n.insertInto(\"schema.tableName\"))\n\nIs this the right way to achieve what I want?", "author_fullname": "t2_gzyg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing Spark dataframe to Hive external table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bqkbw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680630201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am writing a Spark dataframe to Hive external table using insertInto. The table is partitioned on multiple columns. Something like category/sub_category.&lt;/p&gt;\n\n&lt;p&gt;How can I make sure to write 10 files under each sub_category directory. The data is huge (1 year of data), If I do repartition(10) the job is taking too long. If I repartition on category and sub_category columns together, there is only 1  file getting creating under each sub_category directory.&lt;/p&gt;\n\n&lt;p&gt;What I am missing? My understanding is if I repartition based on multiple columns and provide numPartition too, it should work.&lt;/p&gt;\n\n&lt;p&gt;(df\n.repartion(10, &amp;quot;category&amp;quot;, &amp;quot;sub_category&amp;quot;)\n.insertInto(&amp;quot;schema.tableName&amp;quot;))&lt;/p&gt;\n\n&lt;p&gt;Is this the right way to achieve what I want?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bqkbw", "is_robot_indexable": true, "report_reasons": null, "author": "ps2931", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bqkbw/writing_spark_dataframe_to_hive_external_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bqkbw/writing_spark_dataframe_to_hive_external_table/", "subreddit_subscribers": 95812, "created_utc": 1680630201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Helloe everyone student here on internship, my subject is about the creation of a data quality framework, and while i am curretnly making a POC i can't help but feel that the subject has no main standards (concerning the process of making it or the general way a data quality should look like), i have created a flow for how the data should go (will probably use airflow for it, but i still don't have the hang on when it is necessary to use airflow and when it's not).\n\nI am curretnly working on an oracle database where i retrieve the data then extract some descriptive statistics, then check for uniqueness (still not well implemented) and the last step are a bunch of rules validation that i just made for the sake of the POC (future plan is to make a business rule engine).\n\nI was thrown a new term called Data Catalog, been searching around and i still don't really understand what does it do (Apache Atlas) and what's the point of it inside of a data quality framework.\n\nCan anyone explain or guide me to ressources to how relevant it is in general ?\n\nN.B : I am still learning what a data quality framework is, it's kinda hard since i find mostly tools like great expectations or pandas-profiling but no general guidelines of how it looks.", "author_fullname": "t2_fcv4rhtp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a data catalog and why would it matter inside a data quality framework ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bqccj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680629756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Helloe everyone student here on internship, my subject is about the creation of a data quality framework, and while i am curretnly making a POC i can&amp;#39;t help but feel that the subject has no main standards (concerning the process of making it or the general way a data quality should look like), i have created a flow for how the data should go (will probably use airflow for it, but i still don&amp;#39;t have the hang on when it is necessary to use airflow and when it&amp;#39;s not).&lt;/p&gt;\n\n&lt;p&gt;I am curretnly working on an oracle database where i retrieve the data then extract some descriptive statistics, then check for uniqueness (still not well implemented) and the last step are a bunch of rules validation that i just made for the sake of the POC (future plan is to make a business rule engine).&lt;/p&gt;\n\n&lt;p&gt;I was thrown a new term called Data Catalog, been searching around and i still don&amp;#39;t really understand what does it do (Apache Atlas) and what&amp;#39;s the point of it inside of a data quality framework.&lt;/p&gt;\n\n&lt;p&gt;Can anyone explain or guide me to ressources to how relevant it is in general ?&lt;/p&gt;\n\n&lt;p&gt;N.B : I am still learning what a data quality framework is, it&amp;#39;s kinda hard since i find mostly tools like great expectations or pandas-profiling but no general guidelines of how it looks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bqccj", "is_robot_indexable": true, "report_reasons": null, "author": "Still-W1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bqccj/what_is_a_data_catalog_and_why_would_it_matter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bqccj/what_is_a_data_catalog_and_why_would_it_matter/", "subreddit_subscribers": 95812, "created_utc": 1680629756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are companies still using the licensed ETL tools? like - Informatica, Datastage etc.", "author_fullname": "t2_6kmo2ecy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bq5oe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680629403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are companies still using the licensed ETL tools? like - Informatica, Datastage etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bq5oe", "is_robot_indexable": true, "report_reasons": null, "author": "soujoshi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bq5oe/etl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bq5oe/etl_tools/", "subreddit_subscribers": 95812, "created_utc": 1680629403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, former data engineer here.\n\nI am working on solving some problems I observed at my last startup related to data pipelines and specifically file/report download and distribution. Are any of the following relatable? If not, something else? (Have nothing to sell...yet)\n\n1. File drops in remote locations are unpredictable but are time critical so processing has to start ASAP.\n2. Failed file downloads must retry or be escalated. Similarly, late file drops become a business/operations risks and must be escalated\n3. Sometimes partners will drop the wrong file (i.e. yesterday's file, wrong format, missing data) and detecting problems with reports/files occurs late in your pipeline or leads to incorrect outcomes\n4. Multiple consumers across the business /engineering depend on the same daily/weekly/monthly reports and all need a copy\n5. Customers want data/reports uploaded to specific storage locations they manage, requiring setup and configuration. Maybe you don't support all storage types? Changing of upload location is not self serve to customer but require support/engineering.\n6. Support/engineering sometimes has to check on file/report content and need to access a particular file in a secure environment\n7. Most of the above become support team challenges before they become engineering challenges\n\nDo any of these seem like relatable challenges? If not, anything else?", "author_fullname": "t2_11or0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do any of these file management problems ring true?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bq0zl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680629143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, former data engineer here.&lt;/p&gt;\n\n&lt;p&gt;I am working on solving some problems I observed at my last startup related to data pipelines and specifically file/report download and distribution. Are any of the following relatable? If not, something else? (Have nothing to sell...yet)&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;File drops in remote locations are unpredictable but are time critical so processing has to start ASAP.&lt;/li&gt;\n&lt;li&gt;Failed file downloads must retry or be escalated. Similarly, late file drops become a business/operations risks and must be escalated&lt;/li&gt;\n&lt;li&gt;Sometimes partners will drop the wrong file (i.e. yesterday&amp;#39;s file, wrong format, missing data) and detecting problems with reports/files occurs late in your pipeline or leads to incorrect outcomes&lt;/li&gt;\n&lt;li&gt;Multiple consumers across the business /engineering depend on the same daily/weekly/monthly reports and all need a copy&lt;/li&gt;\n&lt;li&gt;Customers want data/reports uploaded to specific storage locations they manage, requiring setup and configuration. Maybe you don&amp;#39;t support all storage types? Changing of upload location is not self serve to customer but require support/engineering.&lt;/li&gt;\n&lt;li&gt;Support/engineering sometimes has to check on file/report content and need to access a particular file in a secure environment&lt;/li&gt;\n&lt;li&gt;Most of the above become support team challenges before they become engineering challenges&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Do any of these seem like relatable challenges? If not, anything else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bq0zl", "is_robot_indexable": true, "report_reasons": null, "author": "harryblueberry", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bq0zl/do_any_of_these_file_management_problems_ring_true/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bq0zl/do_any_of_these_file_management_problems_ring_true/", "subreddit_subscribers": 95812, "created_utc": 1680629143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Monte Carlo Data, creator of the data observability category, announced its first-ever brand mascot, \u201cBilli the Observabili-Bear,\u201d with an elementary school roadshow to educate future data engineers about the dangers of data downtime.  \n\n\n\u201cBilli\u2019s Observability Bonanza,\u201d developed in partnership with California Public Schools, aims to teach children between the ages of 5 and 12 about the importance of data reliability and how a proactive approach to data observability can guard their pipelines against bad data.\n\nhttps://preview.redd.it/2023bzyhdwra1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=38dfea93e6365c93d888aacaeee09c81907bb32f\n\n[https://www.montecarlodata.com/blog-billi-the-observabili-bear](https://www.montecarlodata.com/blog-billi-the-observabili-bear)", "author_fullname": "t2_a49okn69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Start 'em young - Data Engineer April Fools'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2023bzyhdwra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/2023bzyhdwra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f351da9f002a40258690372666ce696e45e11e3"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/2023bzyhdwra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f58442d5009ad10605faf1dd6aa5c8d818d1f84"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/2023bzyhdwra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a37dd3225f10a2f8acbac467e557054ce3033438"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/2023bzyhdwra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e873679a5a3e0a556589d10ab9cfa13cd8e5e4ec"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/2023bzyhdwra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4386c33a4220dc011f263c8bb52e0e1675964494"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/2023bzyhdwra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f7506258f7c49da92e580e96db62fc0effc3cc5"}], "s": {"y": 900, "x": 1600, "u": "https://preview.redd.it/2023bzyhdwra1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=38dfea93e6365c93d888aacaeee09c81907bb32f"}, "id": "2023bzyhdwra1"}}, "name": "t3_12bp1nm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rzHH1abzMmJOys0ossVND7rCZql_-F21K4NVwYiGYXc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1680627134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Monte Carlo Data, creator of the data observability category, announced its first-ever brand mascot, \u201cBilli the Observabili-Bear,\u201d with an elementary school roadshow to educate future data engineers about the dangers of data downtime.  &lt;/p&gt;\n\n&lt;p&gt;\u201cBilli\u2019s Observability Bonanza,\u201d developed in partnership with California Public Schools, aims to teach children between the ages of 5 and 12 about the importance of data reliability and how a proactive approach to data observability can guard their pipelines against bad data.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2023bzyhdwra1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=38dfea93e6365c93d888aacaeee09c81907bb32f\"&gt;https://preview.redd.it/2023bzyhdwra1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=38dfea93e6365c93d888aacaeee09c81907bb32f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.montecarlodata.com/blog-billi-the-observabili-bear\"&gt;https://www.montecarlodata.com/blog-billi-the-observabili-bear&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CTqlYuwNZrRGpjEtS_58iYk2FkH9bA9juC6gSXsNjso.jpg?auto=webp&amp;v=enabled&amp;s=ee83eba504bdb0b19188dc3b11d6dc09ec94d1b3", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/CTqlYuwNZrRGpjEtS_58iYk2FkH9bA9juC6gSXsNjso.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d02565bdbd08e8d8b6284f6f193b4f18653c1575", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/CTqlYuwNZrRGpjEtS_58iYk2FkH9bA9juC6gSXsNjso.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1dd9058631a960f94452be80d64d660b9062dfd", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/CTqlYuwNZrRGpjEtS_58iYk2FkH9bA9juC6gSXsNjso.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a552424c5b76f5bde5e95aaf7642751651ac7bf", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/CTqlYuwNZrRGpjEtS_58iYk2FkH9bA9juC6gSXsNjso.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03cc8a2242d4ba936fb1d82d3febeb9d214e4d01", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/CTqlYuwNZrRGpjEtS_58iYk2FkH9bA9juC6gSXsNjso.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c2d3a4accc1e142b5fdeadbb2a752385858f8b7", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/CTqlYuwNZrRGpjEtS_58iYk2FkH9bA9juC6gSXsNjso.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=461f86f824939a45b388764e5d80fdeacd5feb9e", "width": 1080, "height": 607}], "variants": {}, "id": "Ry3wvXgA9xEJ8YWyYQoPcXIMO475oBLgcjuECQ2s5xE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12bp1nm", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Substance2185", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bp1nm/start_em_young_data_engineer_april_fools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bp1nm/start_em_young_data_engineer_april_fools/", "subreddit_subscribers": 95812, "created_utc": 1680627134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do Graviton instances lower costs for Spark on EMR on AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_12bizxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ciUy--NPtMtqmxuSUKNX8WvsfYrzyzwTte9MjCINO8w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680614459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/dev-genius/do-graviton-instances-lower-costs-for-spark-on-emr-on-aws-5b702fcc3918", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?auto=webp&amp;v=enabled&amp;s=7a93e2ea8de4833cd263a771c6d0ca3b503236a1", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a6f23cd652e063462949b7690546fed3ef909d9", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3451b6032b461d1a39011580dd1164fe252c16d", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=843b58b922c0903ae9ace8bc45a932c63bf3b0ff", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f800e0805e29d54dfbf05d114de15b9fba9f9aec", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64a7b5c87ac713c84fc97af201ad82370f4a9892", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e173defc2034d8655cc4219b10d05b37f8c7e06", "width": 1080, "height": 720}], "variants": {}, "id": "GXQGN3ovHexJ6yB6Fc-1A30IAkMeaGAm6uczHH4azmg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12bizxr", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bizxr/do_graviton_instances_lower_costs_for_spark_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/dev-genius/do-graviton-instances-lower-costs-for-spark-on-emr-on-aws-5b702fcc3918", "subreddit_subscribers": 95812, "created_utc": 1680614459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\nDo you have any tips on how to effectively communicate with business leaders that their current data architecture is inefficient and that a new one can be made? \nWhat factors should I highlight? \nDoes anyone have any stories of how they improved a company's data systems, whether by building a data warehouse, or just tweaking something in the existing system?", "author_fullname": "t2_5cejpp03", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to convince stakeholders to change their tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bgf2l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680608484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,\nDo you have any tips on how to effectively communicate with business leaders that their current data architecture is inefficient and that a new one can be made? \nWhat factors should I highlight? \nDoes anyone have any stories of how they improved a company&amp;#39;s data systems, whether by building a data warehouse, or just tweaking something in the existing system?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bgf2l", "is_robot_indexable": true, "report_reasons": null, "author": "early-earl", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bgf2l/how_to_convince_stakeholders_to_change_their_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bgf2l/how_to_convince_stakeholders_to_change_their_tools/", "subreddit_subscribers": 95812, "created_utc": 1680608484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, this may seem trivial, but just wanted to clarify and see how you all approach data integration with a data lake.\n\nSo, lets say you are starting from scratch and are tasked with integrating data from a source RDMS (Adventure Works for example) and your data lake. Note that the Adventure Works Database has 68 tables, excluding the dbo schema.\n\nQuestions on how you would approach this:\n\n1. Would you look to integrate all 68 tables from the DB into the Data Lake? So that the Raw and Clean data lake zones would keep the 68 tables in the same normal form that exists in the RDMS. Only in my Curated Zone, I would have business logic and denormalization to create data marts or analytics tables?\n2. Would a folder structure like this be appropriate: *adventure\\_works/schema/table\\_name/partitions/file.snappy.parquet?*\n3. How would you manage upserts in the lake? If an order status changes from open to closed, would you overwrite that record in the data lake with the new status? If not, how would track this change in the lake?\n\nThanks!", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Source Data Integration into a Data Lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12buhgr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680638861.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680638147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, this may seem trivial, but just wanted to clarify and see how you all approach data integration with a data lake.&lt;/p&gt;\n\n&lt;p&gt;So, lets say you are starting from scratch and are tasked with integrating data from a source RDMS (Adventure Works for example) and your data lake. Note that the Adventure Works Database has 68 tables, excluding the dbo schema.&lt;/p&gt;\n\n&lt;p&gt;Questions on how you would approach this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Would you look to integrate all 68 tables from the DB into the Data Lake? So that the Raw and Clean data lake zones would keep the 68 tables in the same normal form that exists in the RDMS. Only in my Curated Zone, I would have business logic and denormalization to create data marts or analytics tables?&lt;/li&gt;\n&lt;li&gt;Would a folder structure like this be appropriate: &lt;em&gt;adventure_works/schema/table_name/partitions/file.snappy.parquet?&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;How would you manage upserts in the lake? If an order status changes from open to closed, would you overwrite that record in the data lake with the new status? If not, how would track this change in the lake?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12buhgr", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12buhgr/source_data_integration_into_a_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12buhgr/source_data_integration_into_a_data_lake/", "subreddit_subscribers": 95812, "created_utc": 1680638147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m5mja", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Burstable vs non-burstable AWS instance types for data engineering workloads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_12btcue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HVOGVjMNyTgJVlc3NMAQPc-z6iYTS4C5nD9O4LcTULs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680635899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/coiled-hq/burstable-vs-non-burstable-aws-instance-types-for-data-engineering-workloads-540b7f10f6eb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?auto=webp&amp;v=enabled&amp;s=37963b8df59a95702f1521537b89b4c91f11f91b", "width": 600, "height": 371}, "resolutions": [{"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=034ccd70c7da5a52b6c12f37cf8e754abf3d276c", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=990c1441d5c93bc5e69073fc0c7085783709d90d", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/bya6PCZ704PxPDZ4OIZdYk4-_QV8oyKPUyqkCRlyPMQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20c76277991df844d275216801d8f0ee6f22a551", "width": 320, "height": 197}], "variants": {}, "id": "827n3XVLrcD58rUAY4TDhu4SGblYDmn0dgw0DPUW4ac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12btcue", "is_robot_indexable": true, "report_reasons": null, "author": "dchudz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12btcue/burstable_vs_nonburstable_aws_instance_types_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/coiled-hq/burstable-vs-non-burstable-aws-instance-types-for-data-engineering-workloads-540b7f10f6eb", "subreddit_subscribers": 95812, "created_utc": 1680635899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sync Autotuner Reduced Our EMR Cost by 25%", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": false, "name": "t3_12bqpb1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ka4pwfiQcQw23bE1Og9iY9mIir1DTA-LFy9iqMx35fQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680630481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/insiderengineering/sync-autotuner-reduced-our-amazon-emr-cost-by-25-percent-2412d168b3e7", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?auto=webp&amp;v=enabled&amp;s=165734a513856dcbcfca1dcedfd75ba343f393af", "width": 1200, "height": 764}, "resolutions": [{"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b740836ea9f7dff60cc2d599ddbd76e848b33b5", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69b26db2206345170f9f91911e8b0e42dfb61129", "width": 216, "height": 137}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75539abe1d1b05761927687a5ab3847b6f5c72fe", "width": 320, "height": 203}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6b463b6933f9b529cb83f6bb404a2ac5db5874a", "width": 640, "height": 407}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=834c8ab069beab301c4a571e1259d95c204f1688", "width": 960, "height": 611}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d7c2f0201e8f2e7e9b238ada9b4a8420354c1b8", "width": 1080, "height": 687}], "variants": {}, "id": "zlUxK1rEQ-lVb4Dfo0VPiK5bI-Z5MBhCuLwGXfR83-M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12bqpb1", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bqpb1/sync_autotuner_reduced_our_emr_cost_by_25/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/insiderengineering/sync-autotuner-reduced-our-amazon-emr-cost-by-25-percent-2412d168b3e7", "subreddit_subscribers": 95812, "created_utc": 1680630481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any recommendations regarding how to set up my environment.  Specifically speaking, if I have WSL2 set up, where do I install which software (spark, Python - Anaconda, etc.)\n\nI know \"it depends\", but that isn't super informative.\n\nAny general advice would be helpful.\n\nI'm running to the use case where things can be installed in multiple places and need softwares to talk to each other. Also certain things need linux, or are better at it.\n\nAny general principles would be helpful.", "author_fullname": "t2_y8an8ku", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Environment Setup - Installing Apps/Softwares on WSL2 vs. Windows Host Machine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12blld0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680620177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any recommendations regarding how to set up my environment.  Specifically speaking, if I have WSL2 set up, where do I install which software (spark, Python - Anaconda, etc.)&lt;/p&gt;\n\n&lt;p&gt;I know &amp;quot;it depends&amp;quot;, but that isn&amp;#39;t super informative.&lt;/p&gt;\n\n&lt;p&gt;Any general advice would be helpful.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running to the use case where things can be installed in multiple places and need softwares to talk to each other. Also certain things need linux, or are better at it.&lt;/p&gt;\n\n&lt;p&gt;Any general principles would be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12blld0", "is_robot_indexable": true, "report_reasons": null, "author": "dataoveropinions", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12blld0/environment_setup_installing_appssoftwares_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12blld0/environment_setup_installing_appssoftwares_on/", "subreddit_subscribers": 95812, "created_utc": 1680620177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\nI have a case where I need to stream (or batch every 1 min) new records in BQ stream table into Redis. I think I should use CF gen 2 triggered by BQ event, but I'm not sure how to read only new records in BQ table.\nDo you have idea what's the best way to solve this?", "author_fullname": "t2_j32k9s23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream data from BQ Stream Table into Redis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bsx8t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680635004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,\nI have a case where I need to stream (or batch every 1 min) new records in BQ stream table into Redis. I think I should use CF gen 2 triggered by BQ event, but I&amp;#39;m not sure how to read only new records in BQ table.\nDo you have idea what&amp;#39;s the best way to solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bsx8t", "is_robot_indexable": true, "report_reasons": null, "author": "Away_Efficiency_5837", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bsx8t/stream_data_from_bq_stream_table_into_redis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bsx8t/stream_data_from_bq_stream_table_into_redis/", "subreddit_subscribers": 95812, "created_utc": 1680635004.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}