{"kind": "Listing", "data": {"after": "t3_12bq5oe", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After a few years and with the hype gone, it has become apparent that MLOps overlap more with Data Engineering than most people believed.\n\nI wrote my thoughts on the matter and the awesome people of the MLOps community were kind enough to host them on their blog as a guest post. You can find the post here:\n\n[https://mlops.community/mlops-is-mostly-data-engineering/](https://mlops.community/mlops-is-mostly-data-engineering/)", "author_fullname": "t2_fb1s1pke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MLOps is 98% Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12asp78", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 190, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 190, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680545604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After a few years and with the hype gone, it has become apparent that MLOps overlap more with Data Engineering than most people believed.&lt;/p&gt;\n\n&lt;p&gt;I wrote my thoughts on the matter and the awesome people of the MLOps community were kind enough to host them on their blog as a guest post. You can find the post here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://mlops.community/mlops-is-mostly-data-engineering/\"&gt;https://mlops.community/mlops-is-mostly-data-engineering/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bVjWIf0rrdbCKPd_Xzn3W6W1rfXxA5DUGfQ4vOGB5OI.jpg?auto=webp&amp;v=enabled&amp;s=0d81c32dcfb93ea8d28ca754c33326a81e0c224e", "width": 700, "height": 467}, "resolutions": [{"url": "https://external-preview.redd.it/bVjWIf0rrdbCKPd_Xzn3W6W1rfXxA5DUGfQ4vOGB5OI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=744fe705c8924b38a5cb5812da49becb067c8ef7", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/bVjWIf0rrdbCKPd_Xzn3W6W1rfXxA5DUGfQ4vOGB5OI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ee10e129b5a63d0843b3853372afea177faaddd", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/bVjWIf0rrdbCKPd_Xzn3W6W1rfXxA5DUGfQ4vOGB5OI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=454128534ee7c02c4eb208d07b925cfb60adaa53", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/bVjWIf0rrdbCKPd_Xzn3W6W1rfXxA5DUGfQ4vOGB5OI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=322afdd685f97911331712d0c443345267967c7b", "width": 640, "height": 426}], "variants": {}, "id": "jLhsBd_EDLM0YfBqf4uOPydrZ4w5DXOH2lHMgyZ6lXw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12asp78", "is_robot_indexable": true, "report_reasons": null, "author": "cpardl", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12asp78/mlops_is_98_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12asp78/mlops_is_98_data_engineering/", "subreddit_subscribers": 95784, "created_utc": 1680545604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://sqlmesh.com/\n\nSQLMesh has native support for reading dbt projects. \n\nIt allows you to build safe incremental models with SQL. No Jinja required. Courtesy of SQLglot. \n\nComes bundled with DuckDB for testing. \n\nIt looks like a more pleasant experience. \n\nThoughts?", "author_fullname": "t2_74pfheof", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A dbt killer is born (SQLMesh)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12b6fgb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680575088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://sqlmesh.com/\"&gt;https://sqlmesh.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;SQLMesh has native support for reading dbt projects. &lt;/p&gt;\n\n&lt;p&gt;It allows you to build safe incremental models with SQL. No Jinja required. Courtesy of SQLglot. &lt;/p&gt;\n\n&lt;p&gt;Comes bundled with DuckDB for testing. &lt;/p&gt;\n\n&lt;p&gt;It looks like a more pleasant experience. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12b6fgb", "is_robot_indexable": true, "report_reasons": null, "author": "No_Equivalent5942", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12b6fgb/a_dbt_killer_is_born_sqlmesh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12b6fgb/a_dbt_killer_is_born_sqlmesh/", "subreddit_subscribers": 95784, "created_utc": 1680575088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, DEs.\n\nI recently tried using Great Expectations on my data processing pipelines in Databricks (written in PySpark). I\u2019ve heard about this library a lot, but in reality seems not promising. \n\nFor starters, nearly all methods there can take only one column as argument - so for example you cannot check duplicates by multiple columns without concatenation. Secondly, it seems like it consumes a lot of resources - once we included it in script, it began utilise 5 workers at once. \n\nMy question is: how do you approach your data tests? Do you write them yourself or leverage GE/some other open-source framework?", "author_fullname": "t2_i0q1ptpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Great expectations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bblq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680592196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, DEs.&lt;/p&gt;\n\n&lt;p&gt;I recently tried using Great Expectations on my data processing pipelines in Databricks (written in PySpark). I\u2019ve heard about this library a lot, but in reality seems not promising. &lt;/p&gt;\n\n&lt;p&gt;For starters, nearly all methods there can take only one column as argument - so for example you cannot check duplicates by multiple columns without concatenation. Secondly, it seems like it consumes a lot of resources - once we included it in script, it began utilise 5 workers at once. &lt;/p&gt;\n\n&lt;p&gt;My question is: how do you approach your data tests? Do you write them yourself or leverage GE/some other open-source framework?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bblq0", "is_robot_indexable": true, "report_reasons": null, "author": "ye11owmonster", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bblq0/great_expectations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bblq0/great_expectations/", "subreddit_subscribers": 95784, "created_utc": 1680592196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm on lesson 4.9L of the specified course, and I'm curious why when we SELECT the new name for user\\_id to be user, this one turns purple. I noticed this also happens in other lessons when I fill in the new 'user' name.\n\nhttps://preview.redd.it/4bo29daf7tra1.png?width=795&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0a44ea4895b8fdf92d3e959997779b28414a6065", "author_fullname": "t2_g39jwfp1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering with Databricks course - clarification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 37, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4bo29daf7tra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 28, "x": 108, "u": "https://preview.redd.it/4bo29daf7tra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=227654083faf206322a5662be9fb51db6a26f9c7"}, {"y": 57, "x": 216, "u": "https://preview.redd.it/4bo29daf7tra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=037fd5a4a698b7588ee38fc9c1b49a1bfcaf7e12"}, {"y": 84, "x": 320, "u": "https://preview.redd.it/4bo29daf7tra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bf7e04e9876a5c8688dbc9152fd5ac02c4a07d3"}, {"y": 169, "x": 640, "u": "https://preview.redd.it/4bo29daf7tra1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13e4d2a4f9a53c61eeaa3f20d87bef8e300dbaaa"}], "s": {"y": 211, "x": 795, "u": "https://preview.redd.it/4bo29daf7tra1.png?width=795&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0a44ea4895b8fdf92d3e959997779b28414a6065"}, "id": "4bo29daf7tra1"}}, "name": "t3_12bao84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/-QpepBClsfO2fK7abeBKiPybBtsQFsLT5tOMVFh5R_4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680588713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m on lesson 4.9L of the specified course, and I&amp;#39;m curious why when we SELECT the new name for user_id to be user, this one turns purple. I noticed this also happens in other lessons when I fill in the new &amp;#39;user&amp;#39; name.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4bo29daf7tra1.png?width=795&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0a44ea4895b8fdf92d3e959997779b28414a6065\"&gt;https://preview.redd.it/4bo29daf7tra1.png?width=795&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0a44ea4895b8fdf92d3e959997779b28414a6065&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bao84", "is_robot_indexable": true, "report_reasons": null, "author": "Background-Ball5978", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bao84/data_engineering_with_databricks_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bao84/data_engineering_with_databricks_course/", "subreddit_subscribers": 95784, "created_utc": 1680588713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started out doing a lot of data infrastructure work and python/airflow development during my career as a data engineer.\n\n\n\n\nHowever, I was transferred to a new team at the beginning of the year where I manage several ELT pipelines and a very large SQL codebase.  Most of my development nowadays is in SQL, and I haven't been doing much python or devops work compared to the past.\n\n\n\n\n\nHow can I get back to python/data infrastructure work?  The vast majority of data engineering jobs in my area are only SQL, and my company isn't hiring data infrastructure focused data engineers currently.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get a python/data infrastructure focused data engineering role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12awqmv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680553738.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started out doing a lot of data infrastructure work and python/airflow development during my career as a data engineer.&lt;/p&gt;\n\n&lt;p&gt;However, I was transferred to a new team at the beginning of the year where I manage several ELT pipelines and a very large SQL codebase.  Most of my development nowadays is in SQL, and I haven&amp;#39;t been doing much python or devops work compared to the past.&lt;/p&gt;\n\n&lt;p&gt;How can I get back to python/data infrastructure work?  The vast majority of data engineering jobs in my area are only SQL, and my company isn&amp;#39;t hiring data infrastructure focused data engineers currently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12awqmv", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12awqmv/how_to_get_a_pythondata_infrastructure_focused/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12awqmv/how_to_get_a_pythondata_infrastructure_focused/", "subreddit_subscribers": 95784, "created_utc": 1680553738.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, how do you enable access to analytical data for your developers to build data products (real-time or near real-time). How do you build APIs (does your analytics team write the query and give to engineering team or they write it themselves) \n\nAnd do you use a specialised database for enabling that access or a normal warehouse like Redshift would work. We looked at Clickhouse, Druid. And were wondering which is more cost efficient. \n\n&amp;#x200B;\n\nContext - We're a clinic management application with EMR for doctors and we're creating an analytics dashboard for the doctors to look at business data and health data for their patients. It will have multiple visualisations - E.g. Patients seen, same over time, longitudinal data for a patient etc\n\nInfo - We're using MongoDB as our application database, Redshift for analytics", "author_fullname": "t2_71me4x17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data access for customer facing analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bkk5z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680617952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, how do you enable access to analytical data for your developers to build data products (real-time or near real-time). How do you build APIs (does your analytics team write the query and give to engineering team or they write it themselves) &lt;/p&gt;\n\n&lt;p&gt;And do you use a specialised database for enabling that access or a normal warehouse like Redshift would work. We looked at Clickhouse, Druid. And were wondering which is more cost efficient. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Context - We&amp;#39;re a clinic management application with EMR for doctors and we&amp;#39;re creating an analytics dashboard for the doctors to look at business data and health data for their patients. It will have multiple visualisations - E.g. Patients seen, same over time, longitudinal data for a patient etc&lt;/p&gt;\n\n&lt;p&gt;Info - We&amp;#39;re using MongoDB as our application database, Redshift for analytics&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bkk5z", "is_robot_indexable": true, "report_reasons": null, "author": "rationaleuser", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bkk5z/data_access_for_customer_facing_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bkk5z/data_access_for_customer_facing_analytics/", "subreddit_subscribers": 95784, "created_utc": 1680617952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. I just joined a very small startup and looking for setting up very basic analytics stack. Currently excel is the king.\n\nThe product is basically the website and a simple online platform. My goal is to access data reagarding the platform and query the DB, mainly for data exploration.\n\nQuerying directly the production DB doesn't look like a smart idea. In previous jobs I had we addressed the issue in 2 ways : a) building a DWH and b) make a perfect replica of the production DB for data purposes.\n\nI am awake the DWH would be the best solution but I need a solution with very low budget and very easy to managed. I'm the only person working on it and I have lots of other tasks to work on. I am mainly a data analyst with some coding experience. I can get some support from engineering but very limited.\n\nAny suggestion on how to to approach the issue in the most simple way? I am fine with getting something that kind of work, show results and then next year iterate in a more structured way", "author_fullname": "t2_9gx56cnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small startup (10p) - how to managed production database data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bk8f6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680617224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I just joined a very small startup and looking for setting up very basic analytics stack. Currently excel is the king.&lt;/p&gt;\n\n&lt;p&gt;The product is basically the website and a simple online platform. My goal is to access data reagarding the platform and query the DB, mainly for data exploration.&lt;/p&gt;\n\n&lt;p&gt;Querying directly the production DB doesn&amp;#39;t look like a smart idea. In previous jobs I had we addressed the issue in 2 ways : a) building a DWH and b) make a perfect replica of the production DB for data purposes.&lt;/p&gt;\n\n&lt;p&gt;I am awake the DWH would be the best solution but I need a solution with very low budget and very easy to managed. I&amp;#39;m the only person working on it and I have lots of other tasks to work on. I am mainly a data analyst with some coding experience. I can get some support from engineering but very limited.&lt;/p&gt;\n\n&lt;p&gt;Any suggestion on how to to approach the issue in the most simple way? I am fine with getting something that kind of work, show results and then next year iterate in a more structured way&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bk8f6", "is_robot_indexable": true, "report_reasons": null, "author": "Kokubo-ubo", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bk8f6/small_startup_10p_how_to_managed_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bk8f6/small_startup_10p_how_to_managed_production/", "subreddit_subscribers": 95784, "created_utc": 1680617224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After evaluating few solutions in the market: We were in the market to hunt for a solution which will cost under 10k (yearly) considering the cost of opensource will be similar considering DE resource and maintenance cost etc  \n1. [MonteCarlo](https://montecarlodata.com) \\- Super duper expensive - Unable to hosting in Google Cloud  \n2. [BigEye](https://bigeye.com) \\- Good features  \n3. [Metaplane](https://metaplane.dev) \\- Overall good package but when compared to catalog and other features it lost the ground.   \n4. [Atlan](https://atlan.com) \\- Only catalog and was super expensive for us. (around 30k+)  \n\n\n[Decube](https://decube.io) was the one we boiled down to - fairly a new company based in Singapore and provides hosting option too on all clouds.  \n\n\nBefore decube, we were using [Soda.io](https://Soda.io) but it was kind of challenging for us to map the lineage and downstream impact which resulted in search of another solution.", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts around decube.io (data observability and catalog platform)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bfzkg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680607412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After evaluating few solutions in the market: We were in the market to hunt for a solution which will cost under 10k (yearly) considering the cost of opensource will be similar considering DE resource and maintenance cost etc&lt;br/&gt;\n1. &lt;a href=\"https://montecarlodata.com\"&gt;MonteCarlo&lt;/a&gt; - Super duper expensive - Unable to hosting in Google Cloud&lt;br/&gt;\n2. &lt;a href=\"https://bigeye.com\"&gt;BigEye&lt;/a&gt; - Good features&lt;br/&gt;\n3. &lt;a href=\"https://metaplane.dev\"&gt;Metaplane&lt;/a&gt; - Overall good package but when compared to catalog and other features it lost the ground.&lt;br/&gt;\n4. &lt;a href=\"https://atlan.com\"&gt;Atlan&lt;/a&gt; - Only catalog and was super expensive for us. (around 30k+)  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://decube.io\"&gt;Decube&lt;/a&gt; was the one we boiled down to - fairly a new company based in Singapore and provides hosting option too on all clouds.  &lt;/p&gt;\n\n&lt;p&gt;Before decube, we were using &lt;a href=\"https://Soda.io\"&gt;Soda.io&lt;/a&gt; but it was kind of challenging for us to map the lineage and downstream impact which resulted in search of another solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bfzkg", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bfzkg/thoughts_around_decubeio_data_observability_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bfzkg/thoughts_around_decubeio_data_observability_and/", "subreddit_subscribers": 95784, "created_utc": 1680607412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there,  \n\n\nWe currently have created a pyspark pipeline and now we are trying to figure out where our job slows down so we can track performance over time. We are aware of how pyspark lazily evaluates but not sure how to factor that in.  \n\n\nSay we wanted create a run time over a function that returns a dataframe, do we have to call an action on that dataframe then a timing point to figure out the actual runtime of that function? Otherwise it feels like we are just timing the unevaluated dataframe.  \n\n\nHas anyone tackled this before? Would be keen to hear options  \n\n\nThanks", "author_fullname": "t2_7xwk8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark lazy evaluation andd logging", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12b9217", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680582914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,  &lt;/p&gt;\n\n&lt;p&gt;We currently have created a pyspark pipeline and now we are trying to figure out where our job slows down so we can track performance over time. We are aware of how pyspark lazily evaluates but not sure how to factor that in.  &lt;/p&gt;\n\n&lt;p&gt;Say we wanted create a run time over a function that returns a dataframe, do we have to call an action on that dataframe then a timing point to figure out the actual runtime of that function? Otherwise it feels like we are just timing the unevaluated dataframe.  &lt;/p&gt;\n\n&lt;p&gt;Has anyone tackled this before? Would be keen to hear options  &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12b9217", "is_robot_indexable": true, "report_reasons": null, "author": "Manyreason", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12b9217/pyspark_lazy_evaluation_andd_logging/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12b9217/pyspark_lazy_evaluation_andd_logging/", "subreddit_subscribers": 95784, "created_utc": 1680582914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This post discusses about some key skills/technolgy that are used widely in the data engineering landscape. Your dream employment as a data engineer will be within reach with these skills.\n\n[https://blogs.sibyabin.tech/dataengineering/tech/hot-skills-for-dataengineers-in-2023/](https://blogs.sibyabin.tech/dataengineering/tech/hot-skills-for-dataengineers-in-2023/)\n\nhttps://preview.redd.it/di2d0h160qra1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c6c4c0f74bd53a48b98c101bd33ec4e9ab5bf492", "author_fullname": "t2_c4myu4hh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Top skills in 2023 that will help you land your dream data engineering job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"di2d0h160qra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/di2d0h160qra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5247a922299cdbe277e8351f7bfb8f0f5b7b2a1c"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/di2d0h160qra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b86475c8a235afeab92510580356faf01392d74"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/di2d0h160qra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e2fa2bab321aa1a6512d4485525b897d8b3accf"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/di2d0h160qra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e977885e0fb5e1d2a411dd811ce674c91f4ed7b"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/di2d0h160qra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab8a939055b3c6bc1a7b6a81861e9ee8b5cf4895"}], "s": {"y": 768, "x": 1024, "u": "https://preview.redd.it/di2d0h160qra1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c6c4c0f74bd53a48b98c101bd33ec4e9ab5bf492"}, "id": "di2d0h160qra1"}}, "name": "t3_12ausn7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZljpALCa3lpAajP_hvV-AJTMxUvlPOXpr_qYwVOdSKY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680549803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post discusses about some key skills/technolgy that are used widely in the data engineering landscape. Your dream employment as a data engineer will be within reach with these skills.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blogs.sibyabin.tech/dataengineering/tech/hot-skills-for-dataengineers-in-2023/\"&gt;https://blogs.sibyabin.tech/dataengineering/tech/hot-skills-for-dataengineers-in-2023/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/di2d0h160qra1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c6c4c0f74bd53a48b98c101bd33ec4e9ab5bf492\"&gt;https://preview.redd.it/di2d0h160qra1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c6c4c0f74bd53a48b98c101bd33ec4e9ab5bf492&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ausn7", "is_robot_indexable": true, "report_reasons": null, "author": "Satm2021", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12ausn7/top_skills_in_2023_that_will_help_you_land_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ausn7/top_skills_in_2023_that_will_help_you_land_your/", "subreddit_subscribers": 95784, "created_utc": 1680549803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\nDo you have any tips on how to effectively communicate with business leaders that their current data architecture is inefficient and that a new one can be made? \nWhat factors should I highlight? \nDoes anyone have any stories of how they improved a company's data systems, whether by building a data warehouse, or just tweaking something in the existing system?", "author_fullname": "t2_5cejpp03", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to convince stakeholders to change their tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bgf2l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680608484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,\nDo you have any tips on how to effectively communicate with business leaders that their current data architecture is inefficient and that a new one can be made? \nWhat factors should I highlight? \nDoes anyone have any stories of how they improved a company&amp;#39;s data systems, whether by building a data warehouse, or just tweaking something in the existing system?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bgf2l", "is_robot_indexable": true, "report_reasons": null, "author": "early-earl", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bgf2l/how_to_convince_stakeholders_to_change_their_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bgf2l/how_to_convince_stakeholders_to_change_their_tools/", "subreddit_subscribers": 95784, "created_utc": 1680608484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am downloading publicly available provider information from the Centers for Medicare &amp; Medicaid Services. The file is published monthly. Any entity can change names or address without an update to the identifier. I am appending a 'data month' to the data.  \n\n\nI want to have a table (oracle) that helps the user indicate if there has been a change to any of 3 fields that are not the main identifier (address, entity name, business name). In that case I would like to have the most recent information and a second row with the old address, name, ect. The user could use the data download month to see the changes over time on a particular ID.   \n\n\nI can think of 2 ways to do this. Make the key be the identifier plus the other fields I care about. Update the 'downloaded month field' based on the key if it exists otherwise import. Or query the existing data set, append the new data, leverage pandas to check for duplicates on the columns, truncate the existing table, insert the entire table. The monthly files are approx 15k rows.   \n\n\nAnother option would be to just import everything always and then use other methods to filter down to what I want to see. I am learning best practices for table design and ETL and would appreciate some guidance.", "author_fullname": "t2_f1q7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL/Table Design Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12bphlz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680628048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am downloading publicly available provider information from the Centers for Medicare &amp;amp; Medicaid Services. The file is published monthly. Any entity can change names or address without an update to the identifier. I am appending a &amp;#39;data month&amp;#39; to the data.  &lt;/p&gt;\n\n&lt;p&gt;I want to have a table (oracle) that helps the user indicate if there has been a change to any of 3 fields that are not the main identifier (address, entity name, business name). In that case I would like to have the most recent information and a second row with the old address, name, ect. The user could use the data download month to see the changes over time on a particular ID.   &lt;/p&gt;\n\n&lt;p&gt;I can think of 2 ways to do this. Make the key be the identifier plus the other fields I care about. Update the &amp;#39;downloaded month field&amp;#39; based on the key if it exists otherwise import. Or query the existing data set, append the new data, leverage pandas to check for duplicates on the columns, truncate the existing table, insert the entire table. The monthly files are approx 15k rows.   &lt;/p&gt;\n\n&lt;p&gt;Another option would be to just import everything always and then use other methods to filter down to what I want to see. I am learning best practices for table design and ETL and would appreciate some guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bphlz", "is_robot_indexable": true, "report_reasons": null, "author": "machinegunke11y", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bphlz/etltable_design_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bphlz/etltable_design_question/", "subreddit_subscribers": 95784, "created_utc": 1680628048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A company called Basedash is doing a survey about databases, ORMs, data warehouses, etc. and giving away a 27\" Apple display. \n\nI just did the survey and got a referral link, so check it out. Not affiliated with Basedash.\n\nhttps://2023.stateofdb.com/?referral=W25L9R", "author_fullname": "t2_a1jey", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "State of Databases 2023 Survey + Giveaway", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bn51b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680623331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A company called Basedash is doing a survey about databases, ORMs, data warehouses, etc. and giving away a 27&amp;quot; Apple display. &lt;/p&gt;\n\n&lt;p&gt;I just did the survey and got a referral link, so check it out. Not affiliated with Basedash.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://2023.stateofdb.com/?referral=W25L9R\"&gt;https://2023.stateofdb.com/?referral=W25L9R&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lc901qHSbRlSRbels1IOfOnIrI5piIbQsr9udY1p1o8.jpg?auto=webp&amp;v=enabled&amp;s=06c6a5bf03d03f8bd0c1615493b779dd87af1364", "width": 887, "height": 584}, "resolutions": [{"url": "https://external-preview.redd.it/lc901qHSbRlSRbels1IOfOnIrI5piIbQsr9udY1p1o8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e81d5747bb989416ad2c7297c411e662736e414b", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/lc901qHSbRlSRbels1IOfOnIrI5piIbQsr9udY1p1o8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63ed50be66100cfcec0d053a4bf3a26d706583d0", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/lc901qHSbRlSRbels1IOfOnIrI5piIbQsr9udY1p1o8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e89cdf7be7814cc6cfe2e77928baa16700e95ffd", "width": 320, "height": 210}, {"url": "https://external-preview.redd.it/lc901qHSbRlSRbels1IOfOnIrI5piIbQsr9udY1p1o8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=851a8022bc82c753df36b718ab665d6694af4957", "width": 640, "height": 421}], "variants": {}, "id": "632qflb-BS9xIHlJIX17ofamRFIIefHb51FR-CPdMRU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bn51b", "is_robot_indexable": true, "report_reasons": null, "author": "TheLoveBoat", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bn51b/state_of_databases_2023_survey_giveaway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bn51b/state_of_databases_2023_survey_giveaway/", "subreddit_subscribers": 95784, "created_utc": 1680623331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am working for 1 year now in a german company on data topics and managing projects for several data driven usecases as well as establishing data products in our company.\n\nI want to go with my team to a nice conference - as I have a strong Java/Software Engineering background - I am not very familiar with popular conferences data engineering / data in general.\n\nCan someone give me advice about nice conferences in Europe which are worth to visit?\n\nI would prefer a general, vendor agnostic conference.", "author_fullname": "t2_jky21", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Conferences 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12axbt2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680554913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am working for 1 year now in a german company on data topics and managing projects for several data driven usecases as well as establishing data products in our company.&lt;/p&gt;\n\n&lt;p&gt;I want to go with my team to a nice conference - as I have a strong Java/Software Engineering background - I am not very familiar with popular conferences data engineering / data in general.&lt;/p&gt;\n\n&lt;p&gt;Can someone give me advice about nice conferences in Europe which are worth to visit?&lt;/p&gt;\n\n&lt;p&gt;I would prefer a general, vendor agnostic conference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12axbt2", "is_robot_indexable": true, "report_reasons": null, "author": "mikugo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12axbt2/data_engineering_conferences_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12axbt2/data_engineering_conferences_2023/", "subreddit_subscribers": 95784, "created_utc": 1680554913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do Graviton instances lower costs for Spark on EMR on AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_12bizxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ciUy--NPtMtqmxuSUKNX8WvsfYrzyzwTte9MjCINO8w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680614459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/dev-genius/do-graviton-instances-lower-costs-for-spark-on-emr-on-aws-5b702fcc3918", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?auto=webp&amp;v=enabled&amp;s=7a93e2ea8de4833cd263a771c6d0ca3b503236a1", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a6f23cd652e063462949b7690546fed3ef909d9", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3451b6032b461d1a39011580dd1164fe252c16d", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=843b58b922c0903ae9ace8bc45a932c63bf3b0ff", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f800e0805e29d54dfbf05d114de15b9fba9f9aec", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64a7b5c87ac713c84fc97af201ad82370f4a9892", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/gCvkcugr2WV6fXamp9oqR-fz5qa1AnPhewaWxFMcZYg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e173defc2034d8655cc4219b10d05b37f8c7e06", "width": 1080, "height": 720}], "variants": {}, "id": "GXQGN3ovHexJ6yB6Fc-1A30IAkMeaGAm6uczHH4azmg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12bizxr", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bizxr/do_graviton_instances_lower_costs_for_spark_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/dev-genius/do-graviton-instances-lower-costs-for-spark-on-emr-on-aws-5b702fcc3918", "subreddit_subscribers": 95784, "created_utc": 1680614459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry for the self-promotion, but I wanted to share my publication with you where I cover all topics related to data engineering on a weekly basis. It's essentially a collection of my personal notes, as there are countless articles, tools, and other resources being released every day, and I want to keep track of the most interesting ones in one place. If you're interested in checking it out, you can find the link here: [**https://patrikbraborec.substack.com/p/data-news-23**](https://patrikbraborec.substack.com/p/data-news-23)", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data news - new tools, techniques, research, and industry insights in data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12bcp4s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680596525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry for the self-promotion, but I wanted to share my publication with you where I cover all topics related to data engineering on a weekly basis. It&amp;#39;s essentially a collection of my personal notes, as there are countless articles, tools, and other resources being released every day, and I want to keep track of the most interesting ones in one place. If you&amp;#39;re interested in checking it out, you can find the link here: &lt;a href=\"https://patrikbraborec.substack.com/p/data-news-23\"&gt;&lt;strong&gt;https://patrikbraborec.substack.com/p/data-news-23&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TscxDWFwK6fkcvE-OBb8EGxXROGkrqGGElDnjPhVenw.jpg?auto=webp&amp;v=enabled&amp;s=169c1efec067f06a9c56c49dad7e2c1c2783d8a4", "width": 617, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/TscxDWFwK6fkcvE-OBb8EGxXROGkrqGGElDnjPhVenw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd886e4b2870793bfbfec4b8f760a07d3edc4dd3", "width": 108, "height": 105}, {"url": "https://external-preview.redd.it/TscxDWFwK6fkcvE-OBb8EGxXROGkrqGGElDnjPhVenw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a816917da15c379bc4acb70925925596f96d510", "width": 216, "height": 210}, {"url": "https://external-preview.redd.it/TscxDWFwK6fkcvE-OBb8EGxXROGkrqGGElDnjPhVenw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bd15ec1198a92bbe7d6bead181ab6ade430e3e7", "width": 320, "height": 311}], "variants": {}, "id": "nBTc879rAhvUO8SIpz7LSu1qiRqAr6j8rD6F0TXEH-w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12bcp4s", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bcp4s/data_news_new_tools_techniques_research_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bcp4s/data_news_new_tools_techniques_research_and/", "subreddit_subscribers": 95784, "created_utc": 1680596525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone , i hope everybody doing well , so i facing a problem : i discovered programming since 2018 a year before my bacalureat, so i decided to  dive in it domain , the first year in college , i had no laptop, i remember when we writing c programs in paper , the entire year i learned the fundamentals of c , networking , os , vb.net , some algebra , the second year i switeched to another school ( high school of technology) , i chosee to study data science and business intelligent , i liked everything  starting from python pandas java uml , the summer come , i need to pass an intership , i did  a desktop app with access because , i don't want web dev so the next year  i m happy we have machine learning in the programme  every day i practice what i learned the second half of my second year ( third year after bac) we have an end project of the year , i chosed to implement a mobile app with flutter second summer another intership in ml i was happy , this year ( last year) we back to study software enginering i hated back to java ,i have to pass a final intership in web devlopement , so the  problem is i dont have the neccessary skills for web  dev , and i want to launch a career in data engineering so what i m supposed to do in intership forgetting data and focusing in back end to have good grade in my bachlor pr starting learning data eng ?", "author_fullname": "t2_efwj3j67", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career in data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12b7k8b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680578134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone , i hope everybody doing well , so i facing a problem : i discovered programming since 2018 a year before my bacalureat, so i decided to  dive in it domain , the first year in college , i had no laptop, i remember when we writing c programs in paper , the entire year i learned the fundamentals of c , networking , os , vb.net , some algebra , the second year i switeched to another school ( high school of technology) , i chosee to study data science and business intelligent , i liked everything  starting from python pandas java uml , the summer come , i need to pass an intership , i did  a desktop app with access because , i don&amp;#39;t want web dev so the next year  i m happy we have machine learning in the programme  every day i practice what i learned the second half of my second year ( third year after bac) we have an end project of the year , i chosed to implement a mobile app with flutter second summer another intership in ml i was happy , this year ( last year) we back to study software enginering i hated back to java ,i have to pass a final intership in web devlopement , so the  problem is i dont have the neccessary skills for web  dev , and i want to launch a career in data engineering so what i m supposed to do in intership forgetting data and focusing in back end to have good grade in my bachlor pr starting learning data eng ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12b7k8b", "is_robot_indexable": true, "report_reasons": null, "author": "TopUnit9269", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12b7k8b/career_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12b7k8b/career_in_data_engineering/", "subreddit_subscribers": 95784, "created_utc": 1680578134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I see Polars has bindings for Rust, Python, and NodeJS. Is there any interoperability between them?\n\nIf I have a Python or Rust backend serving data out to NodeJS frontend apps, is there any way of working natively in Polars (vs serializing/deserializing JSON back and forth)?\n\nLike a \u201cPolars API\u201d or something where backend and frontend are just taking Polars?\n\nOr, perhaps even if we still have to serialize/deserialize over HTTP, there would still be a benefit to having the backend and frontend working with the same data concepts.", "author_fullname": "t2_nz4bc66t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars in Python and NodeJS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12b578m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680572132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see Polars has bindings for Rust, Python, and NodeJS. Is there any interoperability between them?&lt;/p&gt;\n\n&lt;p&gt;If I have a Python or Rust backend serving data out to NodeJS frontend apps, is there any way of working natively in Polars (vs serializing/deserializing JSON back and forth)?&lt;/p&gt;\n\n&lt;p&gt;Like a \u201cPolars API\u201d or something where backend and frontend are just taking Polars?&lt;/p&gt;\n\n&lt;p&gt;Or, perhaps even if we still have to serialize/deserialize over HTTP, there would still be a benefit to having the backend and frontend working with the same data concepts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12b578m", "is_robot_indexable": true, "report_reasons": null, "author": "PlausibleNinja", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12b578m/polars_in_python_and_nodejs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12b578m/polars_in_python_and_nodejs/", "subreddit_subscribers": 95784, "created_utc": 1680572132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys! First post here in this sub.  \n\n\nI'd like some advice in how to improve our Data Architeture in at our client. Currently I'm a intern at a a consulting firm in Brazil and we are exploring some options on how to improve our clients Architeture.  \n\n\nIr goes as follow:  \n\n\n1. We have a virtual machina which extracts data from SAP and store it at our Data Warehouse (SQL Server) in excract tables.  \n\n2. We consume this data at our ETL pipelines via Azure Data Factory, currently using Data Flow to perform transformations to  such as Joins, Filters... as much as the client requires.  \n\n3. The data is then stored at a temporary table in which we perform some validation and then stored again at our DW  with a view for that specific client/business process which is then made avalaible from the client to consult of feed into his/hers BI tool.  \n\n\nWe are trying to move away from Data Flow (2) into Databricks but are still figuring out the detalis of such a move and if it is a viable option at all. I've done some data transformation with databricks instead of Data Flow but mostly used Pandas instead of PySpark and as far as I've heard, the later is much more efficient at dealing with larger datasets.   \n\n\nTables extracted from SAP can have from 10k to 10kk rows, so I pretty sure any given process may need some exemptions according to data size.  \n\n\n  \nAny tips are appreciated!!", "author_fullname": "t2_4hycmkaa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help improving Data Architeture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12b08cb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680560914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys! First post here in this sub.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like some advice in how to improve our Data Architeture in at our client. Currently I&amp;#39;m a intern at a a consulting firm in Brazil and we are exploring some options on how to improve our clients Architeture.  &lt;/p&gt;\n\n&lt;p&gt;Ir goes as follow:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;We have a virtual machina which extracts data from SAP and store it at our Data Warehouse (SQL Server) in excract tables.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We consume this data at our ETL pipelines via Azure Data Factory, currently using Data Flow to perform transformations to  such as Joins, Filters... as much as the client requires.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The data is then stored at a temporary table in which we perform some validation and then stored again at our DW  with a view for that specific client/business process which is then made avalaible from the client to consult of feed into his/hers BI tool.  &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We are trying to move away from Data Flow (2) into Databricks but are still figuring out the detalis of such a move and if it is a viable option at all. I&amp;#39;ve done some data transformation with databricks instead of Data Flow but mostly used Pandas instead of PySpark and as far as I&amp;#39;ve heard, the later is much more efficient at dealing with larger datasets.   &lt;/p&gt;\n\n&lt;p&gt;Tables extracted from SAP can have from 10k to 10kk rows, so I pretty sure any given process may need some exemptions according to data size.  &lt;/p&gt;\n\n&lt;p&gt;Any tips are appreciated!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12b08cb", "is_robot_indexable": true, "report_reasons": null, "author": "Lopokik", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12b08cb/help_improving_data_architeture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12b08cb/help_improving_data_architeture/", "subreddit_subscribers": 95784, "created_utc": 1680560914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have 7 slices, a 3.5GB CSV file (uncompressed) with 35m rows and looking into applying AWS recommendations for COPY to data faster into Redshift.\n\nIt seems like when RedShift encounters an uncompressed single file, they'll fan out better than if handed a compressed split files, despite recommending that.\n\n|Files|Compression|Format|STL\\_LOAD\\_COMMITS|Elapsed|\n|:-|:-|:-|:-|:-|\n|1|UNCOMPRESSED|CSV|55|1m|\n|7|UNCOMPRESSED|CSV|56|2.6m|\n|60|UNCOMPRESSED|CSV|60|1.9m|\n|1|GZIP|CSV|1|2.9m|\n|7|GZIP|CSV|7|2.1m|\n|60|GZIP|CSV|60|2.5m|\n|1|ZSTD|CSV|1|3m|\n|1|UNCOMPRESSED|PARQUET|3|1.57m|\n|7|UNCOMPRESSED|PARQUET|7|0.9m|\n|70|UNCOMPRESSED|PARQUET|70|1.7m|\n|1|GZIP|PARQUET|2|1.7m|\n|7|GZIP|PARQUET|7|0.6m|\n|70|GZIP|PARQUET|70|1.7m|\n|1|SNAPPY|PARQUET|2|1.7m|\n|7|SNAPPY|PARQUET|7|1.1m|\n|1|UNCOMPRESSED|AVRO|1|13.7m|\n\nI would've loved to also test PARQUET with BZ4 compression since RedShift claims that it can do concurrent reads against it, but nothing I'm familiar with seem to be able to write with that.\n\nThe only thing meaningfully outperforming a single UNCOMPRESSED CSV seem to be GZIP PARQUET split to the same number of files as slices.\n\nHas anyone else benchmarked formats and compression on import speed? I would've expected better results switching away from the most naive solution.", "author_fullname": "t2_3bvnd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "COPY to Redshift seems negatively affected by both compression and file splitting with UNCOMPRESSED CSV as peak performance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12br5ab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680631382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have 7 slices, a 3.5GB CSV file (uncompressed) with 35m rows and looking into applying AWS recommendations for COPY to data faster into Redshift.&lt;/p&gt;\n\n&lt;p&gt;It seems like when RedShift encounters an uncompressed single file, they&amp;#39;ll fan out better than if handed a compressed split files, despite recommending that.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Files&lt;/th&gt;\n&lt;th align=\"left\"&gt;Compression&lt;/th&gt;\n&lt;th align=\"left\"&gt;Format&lt;/th&gt;\n&lt;th align=\"left\"&gt;STL_LOAD_COMMITS&lt;/th&gt;\n&lt;th align=\"left\"&gt;Elapsed&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;55&lt;/td&gt;\n&lt;td align=\"left\"&gt;1m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;56&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.6m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.9m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.9m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.1m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.5m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;ZSTD&lt;/td&gt;\n&lt;td align=\"left\"&gt;CSV&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;3m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.57m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.9m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.7m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.7m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.6m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;GZIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.7m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;SNAPPY&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.7m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;SNAPPY&lt;/td&gt;\n&lt;td align=\"left\"&gt;PARQUET&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.1m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;UNCOMPRESSED&lt;/td&gt;\n&lt;td align=\"left\"&gt;AVRO&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.7m&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I would&amp;#39;ve loved to also test PARQUET with BZ4 compression since RedShift claims that it can do concurrent reads against it, but nothing I&amp;#39;m familiar with seem to be able to write with that.&lt;/p&gt;\n\n&lt;p&gt;The only thing meaningfully outperforming a single UNCOMPRESSED CSV seem to be GZIP PARQUET split to the same number of files as slices.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else benchmarked formats and compression on import speed? I would&amp;#39;ve expected better results switching away from the most naive solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12br5ab", "is_robot_indexable": true, "report_reasons": null, "author": "kitsunde", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12br5ab/copy_to_redshift_seems_negatively_affected_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12br5ab/copy_to_redshift_seems_negatively_affected_by/", "subreddit_subscribers": 95784, "created_utc": 1680631382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is everyone's opinions regarding the crazy rise in demand for data engineering, and the gradual cooling of the market? Has it been a difficult adjustment for anyone?\n\nI'm in my late twenties, and the only thing I've really experienced is an upward momentum in opportunity and wages in this field. Back in early 2022, I was getting thrown opportunities for exciting jobs with high salaries. At least $150k plus TC with just 2-3 years of experience. \n\nNow, it seems like $150k TC is the higher limit of the jobs available, and companies are getting more selective. At least for remote work. \n\nDo you think software engineer/data engineer compensation will hover around the $100-150k range for mid-level? and do you think this is how it's going to stay?\n\nI'm thinking that to get to a higher TC above that, I'll either have to be an even higher level IC or go into management.", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think the job market for the data engineering field will look like in the next 5 years?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12br054", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680631107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is everyone&amp;#39;s opinions regarding the crazy rise in demand for data engineering, and the gradual cooling of the market? Has it been a difficult adjustment for anyone?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in my late twenties, and the only thing I&amp;#39;ve really experienced is an upward momentum in opportunity and wages in this field. Back in early 2022, I was getting thrown opportunities for exciting jobs with high salaries. At least $150k plus TC with just 2-3 years of experience. &lt;/p&gt;\n\n&lt;p&gt;Now, it seems like $150k TC is the higher limit of the jobs available, and companies are getting more selective. At least for remote work. &lt;/p&gt;\n\n&lt;p&gt;Do you think software engineer/data engineer compensation will hover around the $100-150k range for mid-level? and do you think this is how it&amp;#39;s going to stay?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking that to get to a higher TC above that, I&amp;#39;ll either have to be an even higher level IC or go into management.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12br054", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12br054/what_do_you_think_the_job_market_for_the_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12br054/what_do_you_think_the_job_market_for_the_data/", "subreddit_subscribers": 95784, "created_utc": 1680631107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sync Autotuner Reduced Our EMR Cost by 25%", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": true, "name": "t3_12bqpb1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ka4pwfiQcQw23bE1Og9iY9mIir1DTA-LFy9iqMx35fQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680630481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/insiderengineering/sync-autotuner-reduced-our-amazon-emr-cost-by-25-percent-2412d168b3e7", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?auto=webp&amp;v=enabled&amp;s=165734a513856dcbcfca1dcedfd75ba343f393af", "width": 1200, "height": 764}, "resolutions": [{"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b740836ea9f7dff60cc2d599ddbd76e848b33b5", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69b26db2206345170f9f91911e8b0e42dfb61129", "width": 216, "height": 137}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75539abe1d1b05761927687a5ab3847b6f5c72fe", "width": 320, "height": 203}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6b463b6933f9b529cb83f6bb404a2ac5db5874a", "width": 640, "height": 407}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=834c8ab069beab301c4a571e1259d95c204f1688", "width": 960, "height": 611}, {"url": "https://external-preview.redd.it/2uHCKXbvgqzb-h5b-LtXJ9x4mcTv1_WUfBBhpvcw4-k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d7c2f0201e8f2e7e9b238ada9b4a8420354c1b8", "width": 1080, "height": 687}], "variants": {}, "id": "zlUxK1rEQ-lVb4Dfo0VPiK5bI-Z5MBhCuLwGXfR83-M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12bqpb1", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bqpb1/sync_autotuner_reduced_our_emr_cost_by_25/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/insiderengineering/sync-autotuner-reduced-our-amazon-emr-cost-by-25-percent-2412d168b3e7", "subreddit_subscribers": 95784, "created_utc": 1680630481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am writing a Spark dataframe to Hive external table using insertInto. The table is partitioned on multiple columns. Something like category/sub_category.\n\nHow can I make sure to write 10 files under each sub_category directory. The data is huge (1 year of data), If I do repartition(10) the job is taking too long. If I repartition on category and sub_category columns together, there is only 1  file getting creating under each sub_category directory.\n\nWhat I am missing? My understanding is if I repartition based on multiple columns and provide numPartition too, it should work.\n\n(df\n.repartion(10, \"category\", \"sub_category\")\n.insertInto(\"schema.tableName\"))\n\nIs this the right way to achieve what I want?", "author_fullname": "t2_gzyg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing Spark dataframe to Hive external table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12bqkbw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680630201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am writing a Spark dataframe to Hive external table using insertInto. The table is partitioned on multiple columns. Something like category/sub_category.&lt;/p&gt;\n\n&lt;p&gt;How can I make sure to write 10 files under each sub_category directory. The data is huge (1 year of data), If I do repartition(10) the job is taking too long. If I repartition on category and sub_category columns together, there is only 1  file getting creating under each sub_category directory.&lt;/p&gt;\n\n&lt;p&gt;What I am missing? My understanding is if I repartition based on multiple columns and provide numPartition too, it should work.&lt;/p&gt;\n\n&lt;p&gt;(df\n.repartion(10, &amp;quot;category&amp;quot;, &amp;quot;sub_category&amp;quot;)\n.insertInto(&amp;quot;schema.tableName&amp;quot;))&lt;/p&gt;\n\n&lt;p&gt;Is this the right way to achieve what I want?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bqkbw", "is_robot_indexable": true, "report_reasons": null, "author": "ps2931", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bqkbw/writing_spark_dataframe_to_hive_external_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bqkbw/writing_spark_dataframe_to_hive_external_table/", "subreddit_subscribers": 95784, "created_utc": 1680630201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Helloe everyone student here on internship, my subject is about the creation of a data quality framework, and while i am curretnly making a POC i can't help but feel that the subject has no main standards (concerning the process of making it or the general way a data quality should look like), i have created a flow for how the data should go (will probably use airflow for it, but i still don't have the hang on when it is necessary to use airflow and when it's not).\n\nI am curretnly working on an oracle database where i retrieve the data then extract some descriptive statistics, then check for uniqueness (still not well implemented) and the last step are a bunch of rules validation that i just made for the sake of the POC (future plan is to make a business rule engine).\n\nI was thrown a new term called Data Catalog, been searching around and i still don't really understand what does it do (Apache Atlas) and what's the point of it inside of a data quality framework.\n\nCan anyone explain or guide me to ressources to how relevant it is in general ?\n\nN.B : I am still learning what a data quality framework is, it's kinda hard since i find mostly tools like great expectations or pandas-profiling but no general guidelines of how it looks.", "author_fullname": "t2_fcv4rhtp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a data catalog and why would it matter inside a data quality framework ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12bqccj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680629756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Helloe everyone student here on internship, my subject is about the creation of a data quality framework, and while i am curretnly making a POC i can&amp;#39;t help but feel that the subject has no main standards (concerning the process of making it or the general way a data quality should look like), i have created a flow for how the data should go (will probably use airflow for it, but i still don&amp;#39;t have the hang on when it is necessary to use airflow and when it&amp;#39;s not).&lt;/p&gt;\n\n&lt;p&gt;I am curretnly working on an oracle database where i retrieve the data then extract some descriptive statistics, then check for uniqueness (still not well implemented) and the last step are a bunch of rules validation that i just made for the sake of the POC (future plan is to make a business rule engine).&lt;/p&gt;\n\n&lt;p&gt;I was thrown a new term called Data Catalog, been searching around and i still don&amp;#39;t really understand what does it do (Apache Atlas) and what&amp;#39;s the point of it inside of a data quality framework.&lt;/p&gt;\n\n&lt;p&gt;Can anyone explain or guide me to ressources to how relevant it is in general ?&lt;/p&gt;\n\n&lt;p&gt;N.B : I am still learning what a data quality framework is, it&amp;#39;s kinda hard since i find mostly tools like great expectations or pandas-profiling but no general guidelines of how it looks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12bqccj", "is_robot_indexable": true, "report_reasons": null, "author": "Still-W1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bqccj/what_is_a_data_catalog_and_why_would_it_matter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bqccj/what_is_a_data_catalog_and_why_would_it_matter/", "subreddit_subscribers": 95784, "created_utc": 1680629756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are companies still using the licensed ETL tools? like - Informatica, Datastage etc.", "author_fullname": "t2_6kmo2ecy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12bq5oe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680629403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are companies still using the licensed ETL tools? like - Informatica, Datastage etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12bq5oe", "is_robot_indexable": true, "report_reasons": null, "author": "soujoshi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12bq5oe/etl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12bq5oe/etl_tools/", "subreddit_subscribers": 95784, "created_utc": 1680629403.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}