{"kind": "Listing", "data": {"after": "t3_128quom", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_91cznmt5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive does a surprise rollout of file limits, locking out some users | Ars Technica", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1284hmi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 709, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 709, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PImAAAgxHX82kGJmvweGNP2cfyEr66H9lgrRJruPqck.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680307021.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arstechnica.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arstechnica.com/gadgets/2023/03/google-drive-does-a-surprise-rollout-of-file-limits-locking-out-some-users/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IJKnDsMWx7MVXiILkRdYOF37kOBxGK4-W-C_GEV_0ts.jpg?auto=webp&amp;v=enabled&amp;s=6d973b775efb0a906320d15b1538c86b30aae26e", "width": 760, "height": 380}, "resolutions": [{"url": "https://external-preview.redd.it/IJKnDsMWx7MVXiILkRdYOF37kOBxGK4-W-C_GEV_0ts.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2780073c6a768710daf4a82e080c7e5dae5689fa", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/IJKnDsMWx7MVXiILkRdYOF37kOBxGK4-W-C_GEV_0ts.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adcd3b52e18295d4a4236edf3ee02fdce7c1865a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/IJKnDsMWx7MVXiILkRdYOF37kOBxGK4-W-C_GEV_0ts.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e0ff63e8277352619bdd4f09c468d75906db40e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/IJKnDsMWx7MVXiILkRdYOF37kOBxGK4-W-C_GEV_0ts.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d75c81f4847b9b568141414914373bc6831928df", "width": 640, "height": 320}], "variants": {}, "id": "XoqN2Ai_h3uEm5idVvQ_jt9_eu0tR8hc4R5yuKdt_4s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1284hmi", "is_robot_indexable": true, "report_reasons": null, "author": "TinyHalogenBird", "discussion_type": null, "num_comments": 121, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1284hmi/google_drive_does_a_surprise_rollout_of_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arstechnica.com/gadgets/2023/03/google-drive-does-a-surprise-rollout-of-file-limits-locking-out-some-users/", "subreddit_subscribers": 676330, "created_utc": 1680307021.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Ever since the [DPReview closure announcement](https://www.reddit.com/r/DataHoarder/comments/11xlnk3/dpreviewcom_to_close_on_april_10_after_25_years/) we were thinking how to preserve the 25 years of valuable DPReview camera data, and so many threads here were committed to this effort.\n\nWhile I'm happy we were able to save everything to Archive.org, we all know it's not very usable by the general public.. so how do keep the DPR data accessible after it's gone?\n\nWell, please welcome: [digicamfinder.com](https://digicamfinder.com/)\n\nThe best way to keep it safe going forward, is to have the community own it, **so we open sourced** **it**: [github.com/open-product-data/digital-cameras](https://github.com/open-product-data/digital-cameras) (hoping this way the \"next amazon\" can't take it down)\n\n\\+ made some improvements (the DPR isn't really mobile friendly or fast, so we felt it was important to modernize the experience a bit).\n\nThoughts or ideas? + really looking for some contribution love!\n\n&amp;#x200B;\n\nP.S. yes, I'm aware of a number of attempts to make product data open-sourced, but none have the power of the photo geeks behind it \ud83e\udd78", "author_fullname": "t2_8uzv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We open-sourced DPReview archived camera data (meet DigicamFinder)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128487z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 127, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 127, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680312415.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680306473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever since the &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/11xlnk3/dpreviewcom_to_close_on_april_10_after_25_years/\"&gt;DPReview closure announcement&lt;/a&gt; we were thinking how to preserve the 25 years of valuable DPReview camera data, and so many threads here were committed to this effort.&lt;/p&gt;\n\n&lt;p&gt;While I&amp;#39;m happy we were able to save everything to Archive.org, we all know it&amp;#39;s not very usable by the general public.. so how do keep the DPR data accessible after it&amp;#39;s gone?&lt;/p&gt;\n\n&lt;p&gt;Well, please welcome: &lt;a href=\"https://digicamfinder.com/\"&gt;digicamfinder.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The best way to keep it safe going forward, is to have the community own it, &lt;strong&gt;so we open sourced&lt;/strong&gt; &lt;strong&gt;it&lt;/strong&gt;: &lt;a href=\"https://github.com/open-product-data/digital-cameras\"&gt;github.com/open-product-data/digital-cameras&lt;/a&gt; (hoping this way the &amp;quot;next amazon&amp;quot; can&amp;#39;t take it down)&lt;/p&gt;\n\n&lt;p&gt;+ made some improvements (the DPR isn&amp;#39;t really mobile friendly or fast, so we felt it was important to modernize the experience a bit).&lt;/p&gt;\n\n&lt;p&gt;Thoughts or ideas? + really looking for some contribution love!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;P.S. yes, I&amp;#39;m aware of a number of attempts to make product data open-sourced, but none have the power of the photo geeks behind it \ud83e\udd78&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128487z", "is_robot_indexable": true, "report_reasons": null, "author": "petergreeen", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128487z/we_opensourced_dpreview_archived_camera_data_meet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128487z/we_opensourced_dpreview_archived_camera_data_meet/", "subreddit_subscribers": 676330, "created_utc": 1680306473.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've two HDDs that my family pulled from old desktop PCs as we moved to laptops. \n\nCan someone tell me which connections I would need to use these as external HDDs and access their data? Secondly, will Avast (or Windows Defender) be sufficient for protection? Or should I install another software before I connect those old HDDs?", "author_fullname": "t2_acappcie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Need help: Getting personal photos out of two decade-old HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"dsa04q8qa9ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/dsa04q8qa9ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0292642343b6d40c2ef49efb89373b79a939539f"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/dsa04q8qa9ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cfcc5679f69ef6ada9b282a4100fd5fafa05624"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/dsa04q8qa9ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cfec37c3048967b3365ccc94d269310c37b3d28"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/dsa04q8qa9ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac29b41eeef05671a9b17316dbbd6597861699d3"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/dsa04q8qa9ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a1167d8d4b9da8743d85b84ec4cfefc0f651432"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/dsa04q8qa9ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f63ea77ace6be3b75fb350aa3fb1790b0c2c11c4"}], "s": {"y": 3000, "x": 4000, "u": "https://preview.redd.it/dsa04q8qa9ra1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3672737abe75104cba834ccd3d8d940120c7b45a"}, "id": "dsa04q8qa9ra1"}, "r9v7w3pqa9ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/r9v7w3pqa9ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=494e665e7854b6b38f4d451b5b0a24adccc56a62"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/r9v7w3pqa9ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c948d96f0a364b4acc2fa9151ce071a2436513ed"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/r9v7w3pqa9ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e7202b29e1e30f3d46f2430ab65d78d47c13f91"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/r9v7w3pqa9ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=894e6478cb5882aedd5d8e7aac2e5cb121550133"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/r9v7w3pqa9ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e12010764c558a5b5d6139cb590f1bfab83a472"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/r9v7w3pqa9ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=304301585e80565e8b867dc93ced3e43cd31e299"}], "s": {"y": 3000, "x": 4000, "u": "https://preview.redd.it/r9v7w3pqa9ra1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d083138951984e4ba5c8289468a8cfb0e6e88fc6"}, "id": "r9v7w3pqa9ra1"}}, "name": "t3_128j5zf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 70, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "dsa04q8qa9ra1", "id": 258204634}, {"media_id": "r9v7w3pqa9ra1", "id": 258204635}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 70, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/fFrOktJ5dNt8ue7Jmk8yfmfyHFlXHfyFSXStTm0utl4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680347646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve two HDDs that my family pulled from old desktop PCs as we moved to laptops. &lt;/p&gt;\n\n&lt;p&gt;Can someone tell me which connections I would need to use these as external HDDs and access their data? Secondly, will Avast (or Windows Defender) be sufficient for protection? Or should I install another software before I connect those old HDDs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/128j5zf", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128j5zf", "is_robot_indexable": true, "report_reasons": null, "author": "Eccentric_Idea", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128j5zf/need_help_getting_personal_photos_out_of_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/128j5zf", "subreddit_subscribers": 676330, "created_utc": 1680347646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Howdy! I want to setup a 36 drive XFS array using Rocky Linux (similar to RHEL) for storing my home lab VM backups. I am using 36x18TB drives and the Supermicro CSE-847. Since it doesn't seem like the smartest idea to use a 36-drive wide RAID6 array \u2013 due to parity calculations in case of failures \u2013 I instead thought about something like 4x9-drive RAID6 arrays and then somehow make it one big pool? I also considered using RAID10 instead, but capacity is more important to me.\n\nSo far I setup Linux according to [MinIO x Intel's storage base line setup](https://min.io/resources/docs/CPG-MinIO-implementation-guide.pdf) and did some `mail` MTA setup but have been struggling finding information about how to:\n\n* Setup SMART scans to report drive failures \u2013 using this in `smartd.conf` right now but unsure whether it's actually a sufficient configuration to report `REALLOCATED SECTORS` early enough: `DEVICESCAN -a -o off -S on -W 5 -s (S/../../2/12|L/../.[02468]/7/08) -m root -M exec /usr/libexec/smartmontools/smartdnotify -n standby,10,q`\n* How can I configure the 4 arrays as a single big pool? Something like ZFS allows for a net capacity of 504TB\n* How to scrub RAID and alert for failures?\n* How to configure two hot-spare drives for automatic failover for any of the 4 RAID arrays\n* Anything else I forgot? Or is it utopian to try and achieve these things with bare metal Linux?\n\n\nWhile I love ZFS with TrueNAS \u2013 which takes care of all the above \u2013 I cannot use it here since XFS it required to leverage Veeam's storage savings using Fast Clone.  I also mainly want to learn about how to configure the on-board tools since there seems to be little condensed information out there. Thanks for any advice!", "author_fullname": "t2_da3zq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to design big 36-drive XFS array and setup Linux to report failures?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128igrb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680347051.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680345717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy! I want to setup a 36 drive XFS array using Rocky Linux (similar to RHEL) for storing my home lab VM backups. I am using 36x18TB drives and the Supermicro CSE-847. Since it doesn&amp;#39;t seem like the smartest idea to use a 36-drive wide RAID6 array \u2013 due to parity calculations in case of failures \u2013 I instead thought about something like 4x9-drive RAID6 arrays and then somehow make it one big pool? I also considered using RAID10 instead, but capacity is more important to me.&lt;/p&gt;\n\n&lt;p&gt;So far I setup Linux according to &lt;a href=\"https://min.io/resources/docs/CPG-MinIO-implementation-guide.pdf\"&gt;MinIO x Intel&amp;#39;s storage base line setup&lt;/a&gt; and did some &lt;code&gt;mail&lt;/code&gt; MTA setup but have been struggling finding information about how to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Setup SMART scans to report drive failures \u2013 using this in &lt;code&gt;smartd.conf&lt;/code&gt; right now but unsure whether it&amp;#39;s actually a sufficient configuration to report &lt;code&gt;REALLOCATED SECTORS&lt;/code&gt; early enough: &lt;code&gt;DEVICESCAN -a -o off -S on -W 5 -s (S/../../2/12|L/../.[02468]/7/08) -m root -M exec /usr/libexec/smartmontools/smartdnotify -n standby,10,q&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;How can I configure the 4 arrays as a single big pool? Something like ZFS allows for a net capacity of 504TB&lt;/li&gt;\n&lt;li&gt;How to scrub RAID and alert for failures?&lt;/li&gt;\n&lt;li&gt;How to configure two hot-spare drives for automatic failover for any of the 4 RAID arrays&lt;/li&gt;\n&lt;li&gt;Anything else I forgot? Or is it utopian to try and achieve these things with bare metal Linux?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While I love ZFS with TrueNAS \u2013 which takes care of all the above \u2013 I cannot use it here since XFS it required to leverage Veeam&amp;#39;s storage savings using Fast Clone.  I also mainly want to learn about how to configure the on-board tools since there seems to be little condensed information out there. Thanks for any advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128igrb", "is_robot_indexable": true, "report_reasons": null, "author": "Teilchen", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128igrb/how_to_design_big_36drive_xfs_array_and_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128igrb/how_to_design_big_36drive_xfs_array_and_setup/", "subreddit_subscribers": 676330, "created_utc": 1680345717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks \n\nHaving a clear out and found this old Zipp 100 drive.\n\nBefore I dispose of it. I wondered if anyone here might have a need/use for it?\n\nI'm based in the UK so I've got a UK PSU for it (though its only a 5V 1A supply so should be easy to replace).\n\nAlso have the original serial cable for it. No hardware available to test functionality so you'd be taking a punt.\n\nThanks for listening.", "author_fullname": "t2_ejzq73ie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have a need for a zip drive (100Mb version)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128numl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680358910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks &lt;/p&gt;\n\n&lt;p&gt;Having a clear out and found this old Zipp 100 drive.&lt;/p&gt;\n\n&lt;p&gt;Before I dispose of it. I wondered if anyone here might have a need/use for it?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m based in the UK so I&amp;#39;ve got a UK PSU for it (though its only a 5V 1A supply so should be easy to replace).&lt;/p&gt;\n\n&lt;p&gt;Also have the original serial cable for it. No hardware available to test functionality so you&amp;#39;d be taking a punt.&lt;/p&gt;\n\n&lt;p&gt;Thanks for listening.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "128numl", "is_robot_indexable": true, "report_reasons": null, "author": "Naked-Daveth", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128numl/anyone_have_a_need_for_a_zip_drive_100mb_version/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128numl/anyone_have_a_need_for_a_zip_drive_100mb_version/", "subreddit_subscribers": 676330, "created_utc": 1680358910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was just clicking around the various sales that are going on and noticed some easystore SSD branding.  Never saw it before!  I was just use to the WD SSD branding of: blue, green, and gold.  Quick search, seems like the external was launched in 2018 and the internal sky blue was launched in 2022 and I didn't hear about either until today.  \n  \nhttps://www.westerndigital.com/products/internal-drives/wd-easystore-sata-2-5-ssd#WDBAGU4800ANC-WRBB  \n  \nhttps://www.westerndigital.com/products/portable-drives/wd-easystore-usb-3-0-ssd#WDBAYN0010BBK-WEBB", "author_fullname": "t2_fwx9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "easystore internal and external SSD! When did this happen?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_127zk2q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680296978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was just clicking around the various sales that are going on and noticed some easystore SSD branding.  Never saw it before!  I was just use to the WD SSD branding of: blue, green, and gold.  Quick search, seems like the external was launched in 2018 and the internal sky blue was launched in 2022 and I didn&amp;#39;t hear about either until today.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-easystore-sata-2-5-ssd#WDBAGU4800ANC-WRBB\"&gt;https://www.westerndigital.com/products/internal-drives/wd-easystore-sata-2-5-ssd#WDBAGU4800ANC-WRBB&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.westerndigital.com/products/portable-drives/wd-easystore-usb-3-0-ssd#WDBAYN0010BBK-WEBB\"&gt;https://www.westerndigital.com/products/portable-drives/wd-easystore-usb-3-0-ssd#WDBAYN0010BBK-WEBB&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wOcQ4j1ejKvA4eDgzgLY-BG-Pz5Nm83UrMqfkZ4oMm4.jpg?auto=webp&amp;v=enabled&amp;s=e2e6b5a8340824263b94441aaf34be7d6ac3f4a8", "width": 1680, "height": 1680}, "resolutions": [{"url": "https://external-preview.redd.it/wOcQ4j1ejKvA4eDgzgLY-BG-Pz5Nm83UrMqfkZ4oMm4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5ad7c01e2df0075350973264ab56bfccf0c2b2f", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/wOcQ4j1ejKvA4eDgzgLY-BG-Pz5Nm83UrMqfkZ4oMm4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6028552c001ba25b493b000dc520cb7ee6241673", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/wOcQ4j1ejKvA4eDgzgLY-BG-Pz5Nm83UrMqfkZ4oMm4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08ffa9c1e4e4e02d66a23da3742498521f9f58c6", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/wOcQ4j1ejKvA4eDgzgLY-BG-Pz5Nm83UrMqfkZ4oMm4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=698e4f7eb83eed6eddc0cb4627dcdf31a10fa251", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/wOcQ4j1ejKvA4eDgzgLY-BG-Pz5Nm83UrMqfkZ4oMm4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a832b4044e61aca48bd0a2445d73d40692005d11", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/wOcQ4j1ejKvA4eDgzgLY-BG-Pz5Nm83UrMqfkZ4oMm4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4ac1c1de6ef9488c9355c57def6af45b932ed14", "width": 1080, "height": 1080}], "variants": {}, "id": "wD6D9Q8PvJY8Q5TvvNPE8qiCnsTL656rezGx1f7Md3U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "127zk2q", "is_robot_indexable": true, "report_reasons": null, "author": "Nyteowls", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/127zk2q/easystore_internal_and_external_ssd_when_did_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/127zk2q/easystore_internal_and_external_ssd_when_did_this/", "subreddit_subscribers": 676330, "created_utc": 1680296978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi I am planning to build a NAS with one of my old tower PC  (Dell OptiPlex 790)  along with TrueNas Scale. Currently I am debating with either buying new drives or recertified drives from server part deal. Here are the two choices at the moment. \n\nOption1 - 3 x 8TB New drives which cost around 120$ each. I plan to set the up in a zpool of 3 with raid 5. I will plan to buy a hot swap spare drive later down the road (within a year when I see a good deal most likely)\n\nOption2 - 3 x 14TB of Recertified Drives from server part deal which is also around 130$ each. For this, since this is used drives, I understand there is some risk involved and they are more likely to break than brand new ones. I plan to set them up 2 of them in zpool raid 1 and leave one as hot spare in case one break.\n\nBoth would give me around the same space (14TB vs 16TB). I would like to ask fellow data holder what are other trade off I may have missed as well as your thoughts and experience with buying re-certified drives from server part deals. I guess I also want to understand if recertified drives on average can last 60% of what a new drive can (since they are \\~60% of the price/TB). Any opinion and advice would be greatly appreciated.", "author_fullname": "t2_aixiv8km", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Buying New Vs Used Drive From Server Part Deal", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128sm0f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680369610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am planning to build a NAS with one of my old tower PC  (Dell OptiPlex 790)  along with TrueNas Scale. Currently I am debating with either buying new drives or recertified drives from server part deal. Here are the two choices at the moment. &lt;/p&gt;\n\n&lt;p&gt;Option1 - 3 x 8TB New drives which cost around 120$ each. I plan to set the up in a zpool of 3 with raid 5. I will plan to buy a hot swap spare drive later down the road (within a year when I see a good deal most likely)&lt;/p&gt;\n\n&lt;p&gt;Option2 - 3 x 14TB of Recertified Drives from server part deal which is also around 130$ each. For this, since this is used drives, I understand there is some risk involved and they are more likely to break than brand new ones. I plan to set them up 2 of them in zpool raid 1 and leave one as hot spare in case one break.&lt;/p&gt;\n\n&lt;p&gt;Both would give me around the same space (14TB vs 16TB). I would like to ask fellow data holder what are other trade off I may have missed as well as your thoughts and experience with buying re-certified drives from server part deals. I guess I also want to understand if recertified drives on average can last 60% of what a new drive can (since they are ~60% of the price/TB). Any opinion and advice would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128sm0f", "is_robot_indexable": true, "report_reasons": null, "author": "TheWeebSavior", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128sm0f/buying_new_vs_used_drive_from_server_part_deal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128sm0f/buying_new_vs_used_drive_from_server_part_deal/", "subreddit_subscribers": 676330, "created_utc": 1680369610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Have second hand nvme, want to use it with confidence. Read extensively on secure wiping nvme to tbh the discussions are nearly all unresolved and complex. \n\nWhat is a simple explanation of the best way to nuke an nvme drive of any potential malware so it can be used in a system confidently?   \n\n\nThank you!", "author_fullname": "t2_94wx1d2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Securely nuke nvme the simple version ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128kdfy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680350727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have second hand nvme, want to use it with confidence. Read extensively on secure wiping nvme to tbh the discussions are nearly all unresolved and complex. &lt;/p&gt;\n\n&lt;p&gt;What is a simple explanation of the best way to nuke an nvme drive of any potential malware so it can be used in a system confidently?   &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128kdfy", "is_robot_indexable": true, "report_reasons": null, "author": "vingallomnr", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128kdfy/securely_nuke_nvme_the_simple_version/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128kdfy/securely_nuke_nvme_the_simple_version/", "subreddit_subscribers": 676330, "created_utc": 1680350727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to run some data analytics on my messages and I'm having a hard time figuring out how to download the raw google data from the google messages app. Does anyone know how to go about doing this correctly?", "author_fullname": "t2_4nij32tt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "downloading google messages raw data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128uq42", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680374256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to run some data analytics on my messages and I&amp;#39;m having a hard time figuring out how to download the raw google data from the google messages app. Does anyone know how to go about doing this correctly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128uq42", "is_robot_indexable": true, "report_reasons": null, "author": "lewibs", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128uq42/downloading_google_messages_raw_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128uq42/downloading_google_messages_raw_data/", "subreddit_subscribers": 676330, "created_utc": 1680374256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://slickdeals.net/f/16549945-2x-western-digital-red-plus-14tb-hard-drive-360?src=jfy&amp;prop=rcmid-742900dc869b55275de67616266fce3f](https://slickdeals.net/f/16549945-2x-western-digital-red-plus-14tb-hard-drive-360?src=jfy&amp;prop=rcmid-742900dc869b55275de67616266fce3f)", "author_fullname": "t2_x9ussa5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good 14TB price", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128pmhu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hot price deal", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680362902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://slickdeals.net/f/16549945-2x-western-digital-red-plus-14tb-hard-drive-360?src=jfy&amp;amp;prop=rcmid-742900dc869b55275de67616266fce3f\"&gt;https://slickdeals.net/f/16549945-2x-western-digital-red-plus-14tb-hard-drive-360?src=jfy&amp;amp;prop=rcmid-742900dc869b55275de67616266fce3f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/a2Nb7GuYid-c0WC9LH8qCoyy-aQwr4B0PrLxwkvdW-8.jpg?auto=webp&amp;v=enabled&amp;s=4db1a339c2b470ff74a39e95127e2a069868b19a", "width": 1300, "height": 1300}, "resolutions": [{"url": "https://external-preview.redd.it/a2Nb7GuYid-c0WC9LH8qCoyy-aQwr4B0PrLxwkvdW-8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d3adc158bf8546261eb32fbe7982273391049c4", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/a2Nb7GuYid-c0WC9LH8qCoyy-aQwr4B0PrLxwkvdW-8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f2c14daf15b9a407d55048be157c9d120fff35b", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/a2Nb7GuYid-c0WC9LH8qCoyy-aQwr4B0PrLxwkvdW-8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=831de5ac5e18659f4ad05d60e09381dcde2aae68", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/a2Nb7GuYid-c0WC9LH8qCoyy-aQwr4B0PrLxwkvdW-8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34e7c8af767371ce3bf7fcf41d6bfdbb3941babb", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/a2Nb7GuYid-c0WC9LH8qCoyy-aQwr4B0PrLxwkvdW-8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45412d88c2fe4aa2ea9fa8d3396a636e87d905bc", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/a2Nb7GuYid-c0WC9LH8qCoyy-aQwr4B0PrLxwkvdW-8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aeee17b5031de69124b3c81cf759fea6fac3d42d", "width": 1080, "height": 1080}], "variants": {}, "id": "61cXmVU39-zm8F9J0zZx0Etu7qSEYyJBllzjLcDW42I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "128pmhu", "is_robot_indexable": true, "report_reasons": null, "author": "devilbob69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/128pmhu/good_14tb_price/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128pmhu/good_14tb_price/", "subreddit_subscribers": 676330, "created_utc": 1680362902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there,\n\nIngesting some old beta tapes and need some insight into what equipment I\u2019ll need to retain the most quality. I\u2019ve got:\n\nSeveral Beta Tapes\nFireWire\nSony J-30 SDL Dec\nAdobe Premiere\n\nHoping to get an NTSC 29.97 interlaced output.\n\nAny ideas as to what else I need or any tips on the best method to get this done?\n\nThanks!", "author_fullname": "t2_17zqy87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting Beta Tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1288dop", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680315955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;Ingesting some old beta tapes and need some insight into what equipment I\u2019ll need to retain the most quality. I\u2019ve got:&lt;/p&gt;\n\n&lt;p&gt;Several Beta Tapes\nFireWire\nSony J-30 SDL Dec\nAdobe Premiere&lt;/p&gt;\n\n&lt;p&gt;Hoping to get an NTSC 29.97 interlaced output.&lt;/p&gt;\n\n&lt;p&gt;Any ideas as to what else I need or any tips on the best method to get this done?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1288dop", "is_robot_indexable": true, "report_reasons": null, "author": "Jschwartz567", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1288dop/ingesting_beta_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1288dop/ingesting_beta_tapes/", "subreddit_subscribers": 676330, "created_utc": 1680315955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 36TB total across 4 hard drives in a Qnap TR004 DAS and am finally looking for a decent affordable online (cloud) backup option. \n\nWhat are some cost effective solutions?\n\n\nThank you", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Online backup options (36TB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_127zy1q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680297744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 36TB total across 4 hard drives in a Qnap TR004 DAS and am finally looking for a decent affordable online (cloud) backup option. &lt;/p&gt;\n\n&lt;p&gt;What are some cost effective solutions?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "127zy1q", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/127zy1q/online_backup_options_36tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/127zy1q/online_backup_options_36tb/", "subreddit_subscribers": 676330, "created_utc": 1680297744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI heard that the WD Red's are loud? Which is strange to me because I have a 4TB Red purchased maybe 2 years ago, and I can never hear it in my system. The reason I am wondering is because I wish to purchase a 16TB MyBook and utilize it for archival storage/backups. But people are saying they're loud and that anything over 8TB from WD uses CMR.\n\nAny advice?", "author_fullname": "t2_88kwqyl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Red's are loud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128uuu2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680374549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I heard that the WD Red&amp;#39;s are loud? Which is strange to me because I have a 4TB Red purchased maybe 2 years ago, and I can never hear it in my system. The reason I am wondering is because I wish to purchase a 16TB MyBook and utilize it for archival storage/backups. But people are saying they&amp;#39;re loud and that anything over 8TB from WD uses CMR.&lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128uuu2", "is_robot_indexable": true, "report_reasons": null, "author": "techtimee", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128uuu2/wd_reds_are_loud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128uuu2/wd_reds_are_loud/", "subreddit_subscribers": 676330, "created_utc": 1680374549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am Building my server rack and have a ATX psu from bequiet and a motherboard ASRock 980DE3/U3S3. I need a rack that is already equipped with 5 slots for HDD.\n\nThank you very much for your ideas!", "author_fullname": "t2_in69h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which 4HU Rack for 5x 3,5\u201c HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128rdg8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680366855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am Building my server rack and have a ATX psu from bequiet and a motherboard ASRock 980DE3/U3S3. I need a rack that is already equipped with 5 slots for HDD.&lt;/p&gt;\n\n&lt;p&gt;Thank you very much for your ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128rdg8", "is_robot_indexable": true, "report_reasons": null, "author": "_rkey", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128rdg8/which_4hu_rack_for_5x_35_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128rdg8/which_4hu_rack_for_5x_35_hdd/", "subreddit_subscribers": 676330, "created_utc": 1680366855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 2 TB. I should have paid a week ago. As long as everything seems to be fine.", "author_fullname": "t2_alxlg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long does Google Drive take to delete data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128gxmo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680341177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 2 TB. I should have paid a week ago. As long as everything seems to be fine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128gxmo", "is_robot_indexable": true, "report_reasons": null, "author": "Dimoks", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128gxmo/how_long_does_google_drive_take_to_delete_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128gxmo/how_long_does_google_drive_take_to_delete_data/", "subreddit_subscribers": 676330, "created_utc": 1680341177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_isd7i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CrystalDiskInfo HDD shows \"Caution\", possible to fix or time for a new HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_128an88", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nZF8RY5fT2cEUR_pF-CVTdynXI77ihax9x4q6oDueOw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680322023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/b9izac6i67ra1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/b9izac6i67ra1.jpg?auto=webp&amp;v=enabled&amp;s=1681e4e26d3a14ba2376b124c6068a93e9633998", "width": 836, "height": 1011}, "resolutions": [{"url": "https://preview.redd.it/b9izac6i67ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3adb421a2e3ce10ce5c54c33ee353898e2096c8c", "width": 108, "height": 130}, {"url": "https://preview.redd.it/b9izac6i67ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee5512a0bb7d1c0ca7db14ed2069bf7c65fe0c92", "width": 216, "height": 261}, {"url": "https://preview.redd.it/b9izac6i67ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66ff2727285afc6a0d48aa195e4b9678baa44eb3", "width": 320, "height": 386}, {"url": "https://preview.redd.it/b9izac6i67ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6999c57d61250ad72b398a23473106f4554c9d08", "width": 640, "height": 773}], "variants": {}, "id": "SyJ2hpHHVm0hFPyFx0FrHBpCp_MFFe7IMdzhtCdA3BM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128an88", "is_robot_indexable": true, "report_reasons": null, "author": "lord_churchill", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128an88/crystaldiskinfo_hdd_shows_caution_possible_to_fix/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/b9izac6i67ra1.jpg", "subreddit_subscribers": 676330, "created_utc": 1680322023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Assuming you are in the same financial situation, and physical space.", "author_fullname": "t2_2m1nrgmm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Knowing what you know now, how would you have started differently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1285jnj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680308967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Assuming you are in the same financial situation, and physical space.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1285jnj", "is_robot_indexable": true, "report_reasons": null, "author": "FIDST", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1285jnj/knowing_what_you_know_now_how_would_you_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1285jnj/knowing_what_you_know_now_how_would_you_have/", "subreddit_subscribers": 676330, "created_utc": 1680308967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Its a real hassel to try and be as efficient as possible when burning blu-ray disks, and keeping tracks of things... So i came up with this script that tries to iterate folder size in parent directory and gives suggestions on grouping directories togheter for most efficincy (if you know any other application that can be run in terminal macOS/Linux feel free to comment).\n\nThis script takes and argument `-d` or `--dir` if you dont want it to default to current working directory. It also creates a log file in the specified directory so your versioning becomes easier, with a timestamp.\n\nOnly dependency is termcolor. Tested on Python 3.10\n\nPlease dont run on any production machine, coding is not my living so it can be full of bugs.\n\nSuggestions are welcome.\n\n    import os\n    from datetime import datetime\n    from termcolor import colored\n    import argparse\n    \n    MAX_GROUP_SIZE = 25 * 1024 * 1024 * 1024  # 25 GB in bytes\n    SPLIT_THRESHOLD = MAX_GROUP_SIZE + 1024 * 1024 * 1024  # 1 GB buffer\n    \n    def group_directories(parent_dir, log_file):\n        # Get the size of each directory and sort them by size\n        directories = []\n        for dir_name in os.listdir(parent_dir):\n            dir_path = os.path.join(parent_dir, dir_name)\n            if os.path.isdir(dir_path):\n                size = sum(os.path.getsize(os.path.join(dir_path, f)) for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f)))\n                directories.append((dir_name, size))\n        directories.sort(key=lambda d: d[1], reverse=True)\n    \n        # Load the previous log if it exists\n        if os.path.exists(log_file):\n            with open(log_file, 'r') as f:\n                existing_dirs = set(line.strip() for line in f)\n        else:\n            existing_dirs = set()\n    \n        # Group the directories into sets that fit onto a 25 GB Blu-ray disc\n        groups = []\n        current_group = []\n        current_size = 0\n        for dir_name, size in directories:\n            if dir_name in existing_dirs:\n                continue\n    \n            if size &gt; SPLIT_THRESHOLD:\n                groups.append([(dir_name, size)])\n                continue\n    \n            if current_size + size &gt; MAX_GROUP_SIZE:\n                groups.append(current_group)\n                current_group = []\n                current_size = 0\n            current_group.append((dir_name, size))\n            current_size += size\n    \n        if current_group:\n            groups.append(current_group)\n    \n        # Update the log with the new directories and date\n        with open(log_file, 'a') as f:\n            now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            f.write(f'{now}\\n')\n            for dir_name, size in directories:\n                if dir_name not in existing_dirs:\n                    f.write(f'{dir_name}\\n')\n    \n        return groups\n    \n    if __name__ == '__main__':\n        parser = argparse.ArgumentParser(description='Group directories into sets that fit onto a 25 GB Blu-ray disc.')\n        parser.add_argument('-d', '--dir', type=str, help='Parent directory', default='.')\n        args = parser.parse_args()\n    \n        parent_dir = args.dir\n        log_file = os.path.join(parent_dir, 'dir_log.txt')\n    \n        # Group the directories into sets that fit onto a 25 GB Blu-ray disc\n        groups = group_directories(parent_dir, log_file)\n    \n        # Print the groups and any warnings\n        for i, group in enumerate(groups, 1):\n            group_size = sum(d[1] for d in group)\n            print(f'Group {i} ({group_size / 1024 / 1024 / 1024:.2f} GB):')\n            for dir_name, size in group:\n                print(f'- {dir_name}')\n    \n            if group_size &gt; MAX_GROUP_SIZE:\n                warning = 'Warning: Group is larger than 25 GB. Consider splitting it with zip.'\n                print(colored(warning, 'red'))\n    \n        print(f'{len(groups)} groups found.')\n\n&amp;#x200B;", "author_fullname": "t2_qv9fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions on my take on space efficiency for the 3-2-1 rule? (In my case burning blu-rays)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_127znk6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680297170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Its a real hassel to try and be as efficient as possible when burning blu-ray disks, and keeping tracks of things... So i came up with this script that tries to iterate folder size in parent directory and gives suggestions on grouping directories togheter for most efficincy (if you know any other application that can be run in terminal macOS/Linux feel free to comment).&lt;/p&gt;\n\n&lt;p&gt;This script takes and argument &lt;code&gt;-d&lt;/code&gt; or &lt;code&gt;--dir&lt;/code&gt; if you dont want it to default to current working directory. It also creates a log file in the specified directory so your versioning becomes easier, with a timestamp.&lt;/p&gt;\n\n&lt;p&gt;Only dependency is termcolor. Tested on Python 3.10&lt;/p&gt;\n\n&lt;p&gt;Please dont run on any production machine, coding is not my living so it can be full of bugs.&lt;/p&gt;\n\n&lt;p&gt;Suggestions are welcome.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import os\nfrom datetime import datetime\nfrom termcolor import colored\nimport argparse\n\nMAX_GROUP_SIZE = 25 * 1024 * 1024 * 1024  # 25 GB in bytes\nSPLIT_THRESHOLD = MAX_GROUP_SIZE + 1024 * 1024 * 1024  # 1 GB buffer\n\ndef group_directories(parent_dir, log_file):\n    # Get the size of each directory and sort them by size\n    directories = []\n    for dir_name in os.listdir(parent_dir):\n        dir_path = os.path.join(parent_dir, dir_name)\n        if os.path.isdir(dir_path):\n            size = sum(os.path.getsize(os.path.join(dir_path, f)) for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f)))\n            directories.append((dir_name, size))\n    directories.sort(key=lambda d: d[1], reverse=True)\n\n    # Load the previous log if it exists\n    if os.path.exists(log_file):\n        with open(log_file, &amp;#39;r&amp;#39;) as f:\n            existing_dirs = set(line.strip() for line in f)\n    else:\n        existing_dirs = set()\n\n    # Group the directories into sets that fit onto a 25 GB Blu-ray disc\n    groups = []\n    current_group = []\n    current_size = 0\n    for dir_name, size in directories:\n        if dir_name in existing_dirs:\n            continue\n\n        if size &amp;gt; SPLIT_THRESHOLD:\n            groups.append([(dir_name, size)])\n            continue\n\n        if current_size + size &amp;gt; MAX_GROUP_SIZE:\n            groups.append(current_group)\n            current_group = []\n            current_size = 0\n        current_group.append((dir_name, size))\n        current_size += size\n\n    if current_group:\n        groups.append(current_group)\n\n    # Update the log with the new directories and date\n    with open(log_file, &amp;#39;a&amp;#39;) as f:\n        now = datetime.now().strftime(&amp;#39;%Y-%m-%d %H:%M:%S&amp;#39;)\n        f.write(f&amp;#39;{now}\\n&amp;#39;)\n        for dir_name, size in directories:\n            if dir_name not in existing_dirs:\n                f.write(f&amp;#39;{dir_name}\\n&amp;#39;)\n\n    return groups\n\nif __name__ == &amp;#39;__main__&amp;#39;:\n    parser = argparse.ArgumentParser(description=&amp;#39;Group directories into sets that fit onto a 25 GB Blu-ray disc.&amp;#39;)\n    parser.add_argument(&amp;#39;-d&amp;#39;, &amp;#39;--dir&amp;#39;, type=str, help=&amp;#39;Parent directory&amp;#39;, default=&amp;#39;.&amp;#39;)\n    args = parser.parse_args()\n\n    parent_dir = args.dir\n    log_file = os.path.join(parent_dir, &amp;#39;dir_log.txt&amp;#39;)\n\n    # Group the directories into sets that fit onto a 25 GB Blu-ray disc\n    groups = group_directories(parent_dir, log_file)\n\n    # Print the groups and any warnings\n    for i, group in enumerate(groups, 1):\n        group_size = sum(d[1] for d in group)\n        print(f&amp;#39;Group {i} ({group_size / 1024 / 1024 / 1024:.2f} GB):&amp;#39;)\n        for dir_name, size in group:\n            print(f&amp;#39;- {dir_name}&amp;#39;)\n\n        if group_size &amp;gt; MAX_GROUP_SIZE:\n            warning = &amp;#39;Warning: Group is larger than 25 GB. Consider splitting it with zip.&amp;#39;\n            print(colored(warning, &amp;#39;red&amp;#39;))\n\n    print(f&amp;#39;{len(groups)} groups found.&amp;#39;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB raidz2", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "127znk6", "is_robot_indexable": true, "report_reasons": null, "author": "Kuken500", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/127znk6/suggestions_on_my_take_on_space_efficiency_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/127znk6/suggestions_on_my_take_on_space_efficiency_for/", "subreddit_subscribers": 676330, "created_utc": 1680297170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://www.newegg.com/seagate-exos-x12-st12000nm0117-12tb/p/1Z4-002P-022Y8](https://www.newegg.com/seagate-exos-x12-st12000nm0117-12tb/p/1Z4-002P-022Y8)\n\nCan't seem to find any specific long term warranty listed and while that's just short of $10/TB I'm concerned that one could fail after a month and I'm out of luck.....", "author_fullname": "t2_3dioeubg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have any experience with refurbished drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_128yoq2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680382829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.newegg.com/seagate-exos-x12-st12000nm0117-12tb/p/1Z4-002P-022Y8\"&gt;https://www.newegg.com/seagate-exos-x12-st12000nm0117-12tb/p/1Z4-002P-022Y8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Can&amp;#39;t seem to find any specific long term warranty listed and while that&amp;#39;s just short of $10/TB I&amp;#39;m concerned that one could fail after a month and I&amp;#39;m out of luck.....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "24TB Raid 6 NAS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128yoq2", "is_robot_indexable": true, "report_reasons": null, "author": "herkalurk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/128yoq2/anyone_have_any_experience_with_refurbished_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128yoq2/anyone_have_any_experience_with_refurbished_drives/", "subreddit_subscribers": 676330, "created_utc": 1680382829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nI'm intending to use my old PC as a media/backup server, I already hooked up some of my spare drives and after an unnecessarily painful process that is sharing a hard drive as a share over LAN with Ubuntu all I need now is a way to connect remaining hard drives. I have more hard drives than SATA ports though, so my question is:\n\n**How do I expand the amount of sata ports? Are PCIe sata extension boards any good or are there any other ways to do it?**\n\nAny advice or recommendations will be much appreciated!", "author_fullname": "t2_ko6z68e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Old PC and lots of hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_128ynip", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680382756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m intending to use my old PC as a media/backup server, I already hooked up some of my spare drives and after an unnecessarily painful process that is sharing a hard drive as a share over LAN with Ubuntu all I need now is a way to connect remaining hard drives. I have more hard drives than SATA ports though, so my question is:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How do I expand the amount of sata ports? Are PCIe sata extension boards any good or are there any other ways to do it?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Any advice or recommendations will be much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Share the wealth, brother!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128ynip", "is_robot_indexable": true, "report_reasons": null, "author": "DrIvoPingasnik", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/128ynip/old_pc_and_lots_of_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128ynip/old_pc_and_lots_of_hard_drives/", "subreddit_subscribers": 676330, "created_utc": 1680382756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "something similar to fxtwitter, except that if the original tweet is deleted, the image or video that was with it will still work\n\n and that it can also archive 18+ tweets \n\n(btw I know fxtwitter is not an archiver  and it's just embed the tweets it as just an example)", "author_fullname": "t2_nnj9ml6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is there any way to archive a tweets with discord?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_128yei2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680382196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;something similar to fxtwitter, except that if the original tweet is deleted, the image or video that was with it will still work&lt;/p&gt;\n\n&lt;p&gt;and that it can also archive 18+ tweets &lt;/p&gt;\n\n&lt;p&gt;(btw I know fxtwitter is not an archiver  and it&amp;#39;s just embed the tweets it as just an example)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128yei2", "is_robot_indexable": true, "report_reasons": null, "author": "SaiaExitt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128yei2/is_there_any_way_to_archive_a_tweets_with_discord/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128yei2/is_there_any_way_to_archive_a_tweets_with_discord/", "subreddit_subscribers": 676330, "created_utc": 1680382196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used it for under 700 hours just to back up another drive, other than that I didn't even plug it in, so actual usage time would probably be about 70 hours.\n\nWould the manufacturer care if I told them about this or do they only care that it's no longer under warranty?\n\nThank you!", "author_fullname": "t2_76pk34pc0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've had this drive for just over 2 years, have barely used it but it shows me a warning.. Is there anything I can do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_128y3nf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uGDpDwnwDM1xXRGgqNVWadP7WZBLIhbtZdgGAkih7fs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680381530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used it for under 700 hours just to back up another drive, other than that I didn&amp;#39;t even plug it in, so actual usage time would probably be about 70 hours.&lt;/p&gt;\n\n&lt;p&gt;Would the manufacturer care if I told them about this or do they only care that it&amp;#39;s no longer under warranty?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/mbpt0fw0ldra1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/mbpt0fw0ldra1.png?auto=webp&amp;v=enabled&amp;s=556ffdbbfc29740fdad1e118545688b8d1edb061", "width": 2022, "height": 2066}, "resolutions": [{"url": "https://preview.redd.it/mbpt0fw0ldra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c5b9bce99d7f74fb5066c735e52305d335e8789", "width": 108, "height": 110}, {"url": "https://preview.redd.it/mbpt0fw0ldra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de8680c56cd3c22a401ca046fae4e479a6c5d441", "width": 216, "height": 220}, {"url": "https://preview.redd.it/mbpt0fw0ldra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f9673b85e9211c202078446967cf7e0ffa0650f", "width": 320, "height": 326}, {"url": "https://preview.redd.it/mbpt0fw0ldra1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45ef71893d3a8487bfa313e3e63c76a8d7cdc4fc", "width": 640, "height": 653}, {"url": "https://preview.redd.it/mbpt0fw0ldra1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f7d56a96f6faf1920d03b2df625af68c98ffe76", "width": 960, "height": 980}, {"url": "https://preview.redd.it/mbpt0fw0ldra1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10ee7816d6459cbf69c4bb9249e94a731ef30fc9", "width": 1080, "height": 1103}], "variants": {}, "id": "4s9Iva2uC6y1JjjHOFxTTs1TtHQj3JlGZ-_ALKcnSVw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128y3nf", "is_robot_indexable": true, "report_reasons": null, "author": "brightdelicategenius", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128y3nf/ive_had_this_drive_for_just_over_2_years_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/mbpt0fw0ldra1.png", "subreddit_subscribers": 676330, "created_utc": 1680381530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm honestly surprised there are no other posts like this on this sub, maybe there's a better sub?...\n\nI'll get to the real world implications and problems which is what I want advice on but first I want to demonstrate/sanity check that this is indeed technically possible and how it COULD be done.\n\nI have a few spare terabytes on my computer now + can record for days on my phone if need be with its storage + an sd card reader that reads at about 88Mb/s.\n\n#Current basic plan:# (This sounds very simple but it also causes a lot of weekly labour hours and problems...)\n\n&gt;80Megabytes per minute recording usage on my phone (could be less in dark or unchanging envoironments etc. - sd cards can be 1tb now meaning I can at minimum record for - 960gb/(0.08gb*60minutes) = 200 hours?!? That's a straight week or two weeks of waking hours! I there are phones that are able to use &lt;10mbps video please do tell me that's the least my phone will record on. And I would like to stretch the terabyte sd card and then my hard drive storage the furthest I possibly can so transfer can also not take hours each week while being less frequent than each week? Perhaps custom phone apps or phone operating systems allow this or there are developer options? I'm not too into phones, only PCs so I wouldn't be aware of anything that might be obvious to you.\n\n&gt;Get sd card to 512gb or 1tb for \u00a330-100 for storage that is FAST enough but not nessesarily DURABLE enough, I cannot seem to find good reveiws on these drives whatsoever let alone durability?\n\n&gt;Transfer whenever I do my laundry or something. Hit record again when I am done with laundry.\n\n##Problems:## Listed going from more technical to more social\n\n&gt;Micro sd write endurace and power on time - even western digitals purple and sandisks \"maxendurance sd cards only exist in 512gb and 256gb respectively and at least twice as expensive and 256gb for 120,000 hours of write endurance could allow the card to go for a decade but if you have to plug it in every 50 hours to transfer the data back it wouldn't last long if it was always writing to max capacity. Additionally even survalence drives probably aren't supposed to be in such high quality.\nEdit I found a western digital purple at 1tb for an eye watering \u00a3220 - I could get a 4TB NVME for that price -- https://www.amazon.co.uk/gp/product/B08VH3RX3Y/ref=ox_sc_act_title_1?smid=A30DC7701CXIBH&amp;psc=1\n\n&gt;Storage/quality - yes I can record a week with h.264 at around 10mbps but I would need to compress it when it got to my laptop, I image that's something I could automate, perhaps i could program a batch file or something to do it whenever something arrive in x folder to compress it using y, no idea how to do the y side of thing though. But even if I could compress it in such a way where it doesn't cause hours of labour for me to check its going smoothly every week and very little downtime, once compressed am I what gonna put it to 1mbps from 10? 1080p for everything in your feild of vision is already not good enough, if I didn't realise I was being robbed etc. at the time or some situation like there where very small details matter then is the persons face even going to be close enough at 10mbps to recognise them? Then what I compress to 1mpbs and later realise I need to edit them export then compress again and its even worse. Even then, let's run the numbers. I currently have 2 8tb HDDs in raid 0 planning to upgrade to 24tb raid 5 with 8tb redundancy so 8tb/(0.00125GB*3600*24*365) = 0.203 max quality, &lt;1 = I cannot store that even ignoring all my backups and other data. Divide by 10 for min quality, multiply by 3 for max storage. If I manage to compress by ten times footage for a year would take up 1/2.03 of the 8tb I have, no backups. Half of everything for 1 year of shittiest quality footage, definately gonna need to be able to stop recording at night. Good news is the hard drives only have to be written to once a week and will be filled up at most ten times by whats being written to them. However, I have to account for 3 things in reality. 1) Allll my backups for computing use. 2) Allll the ARCHIVED video compressed for storage ~4tb per year. 3) All the video PRE archiving that is just transfered right to my computer each week. So even if I do this every 4 weeks after a year I will be filling it up every 4 weeks when the last week of footage goes in IF I compress everything that often. Realistically I will soon need more than 4TB just for backups so this isn't acceptable. Do I compress each week then? Even there its always using the 1TB transfer + 10% If such a large file can even be compressed using video software this MUST be automated such that when I unplug my phone after transfer it will compress what I stored and ONLY what I stored and then delete it but I must ALSO be able to trust the compressed footage is good enough on it's own... I guess I could find some way to record in lower quality in the first place which would also allow me to record more overall? But then  what if I do need to catch a small detail in frame?\n\n&gt;Editing - I will enevitably accidentally record at night or something or I eventually want to get around to deleting all my old footage how the heck do I a) KNOW what footage I need and b) how do I save ONLY what I need? Do I take my 5-6tb of footage put a little guassian blur here, cut this out and wait until the heat death of the universe for it to try and export then crash for me to realise the software isn't compatable. Or so I take all that footage I've gotten by that point and take a glance at each day to try and remember if there was anything significant, I know for sure there are gonna be significant days I look at and dismiss not just because I won't make the connection from what I see sometimes but because, do I carry a diary with me to write down notable moments? Not only will I forget to even if I realise how significant something is at the time  after going through a hundred days the signicance of any individual one won't hit me... and oh yeah it'd be weeks at least since they'd actually happened. But also half the point of this is that I don't realise how significant this is most of the time. It'd be after the fact I'd think OH SHOOT, that person was lying to me or ripped me off, mislead me or that info was important if only I could remember etc.\n\n&gt;Quality - Where's the phone gonna point? Is the audio quality going to be acceptable\n\n&gt;Reliability - Your sd breaks - you lose your week if you were depending on a recording you're screwed + the storage requirement is so high backups are unlikely to happen + break downs are more likely with such high useage + Potentially wear on phone camera and storage that is not designed for constant usage. Even if the phone camera is sold state which I assume it is the sd card would surely heat up quite a lot after a full weeks running at full pelt. Additionally burn in even from an indicator in the corner that recording is happenig is not great if not the camera screen would be burning in constantly on OLED, are there settings or phones that allow me to change this? Some micro sd cards CLAIM to be waterproof, western digital claims all their sandisk micro sd cards \"are NOW\" of 13/09/22 waterproof for 72hours under 1m of water and other sd cards seem to give the same values for things like temperature protection but I don't know any reveiw sites looking to verify this? ( https://support-en.wd.com/app/answers/detailweb/a_id/37516/~/sd-%26-microsd-card%3A-environmental-tolerance-%28waterproof%2C-temperature%2C-magnetic )\n\n&gt;How the heck do I use my phone for other things when doing this? So far as I'm aware most phones will stay on recording but even then it uses resources and there is usually an onscreen indicator that it is recording.\n\n&gt;If the phone runs out of power or otherwise fails how do I make sure I don't just lose a full days recording? I'm a bit embarressed to admit I don't actually know if the video file is saved until it's been stopped and saved? I know I can open some that aren't fully downloaded etc. but I'd need to test this I guess?\n\n&gt;Errr, how do I carry it around? Like when I'm in exercise clothes or take my clothes off I don't care to film - those circumstances don't require it but if I am at a store or a bouncer is getting aggressive or I'm blackout drunk, how do I make sure it's idiot proof enough so that I won't break it\n\n&gt;How do I transfer a terrabyte on the weekly? Yes I can record it but it'd also take about the same amount of time to transfer back, I have actually transfered large files with these things and was surprisd to learn they could transfer 88Mb/s consistently with large video files... but that's still over 3 hours of data transfer.\n\n&gt;Even in places I want to record for safety I don't always want it to be known that I am recording and android phones I know of either have to be on the camera screen or will fade out the screen with a timer, which means it would always be known if I even accidentally pull my phone out... will I have to constantly explain all the reasons why I do this, like do I explain to a sales person at a store, oh I'm recording you because I don't trust you or to a person at work I am holding them to account and the law TECHNICALLY states if there is no expectation of privacy as other poeople are around... etc. or that well i have been burnt in the past by not being able to domonstrate the truth with proof so... blah blah blah.\n\n&gt;Recording in situations where others expect privacy - I can't go back and erase each and every moment I cross into private property? Or is there a way I can quickly edit MASSIVE files - so far as I'm aware most video editing software requires export of full files to edit them even with tiny changes? Is that an inherent limitation of how the files work? And even then having recorded in the first place is problematic, which will enevitably happen I just won't realise the transition all the time and am I what supposed to realise I've crosssed an imaginary boundary mid deep conversation and whip my phone out and not tell them why? Or interupt the conversation to explain, oh I was respecting the covnersation by stopping recording, drop that bombshell on them and expect them to trust I'm not continuing to record.\n\n###Solutions:###\n\n&gt;1) Find a phone that can either a) transfer directly via a usb 3.0 port from it's sd card but since that'd take atleast 3 hours with ZERO interuption, preferably b) when I eventually get my next phone it would have 2 micro sd card slots, if there are even any phones I'd use that could do that, take one sd card out, continue recording on the other - only problem is I'd need to be able to tell the phone specifically which sd card to record to.\n\n1.5) (1) causes a problem where I am choosing between simply just plugging in my phone and whipping out a sim key or pin to dig out an sd card - either a) my phone is stuck at my computer for hours or b) I am fiddling with an sd card and have to likely keep remembering to reselect which card it is, check I didn't record to internal storage etc. etc. a) could be automated very nicely and is just plugging something in eventually but limits what I physically can do and b) is very involved but means once I've manually done all the work things should be fine.\n\n&gt;2) Every say month or two I will compress the video from the files I am putting on my computer each week.\n\n?3?) If there is some kind of phone based \"RAID 0\" or RAID 5 type thing where my phone could consistently offload some data to a different drive to make sure when I record something I can definately rely on it being recorded and me being able to use it in a court of law or to remember that fact that thing I was told its paramount I remember or that thing in a lecture I forgot about, as long as I can remember roughly when  I learnt it or experienced its there - only problem is putting such strain on my phone and storage devices cannot be good. Even if I somehow get a \"RAID 0\" or RAID 5 system its likely to fail on both my storage systems because the stress on them both would be so high.\n\n4) Survalence micro sd?/hard drives?!? The hard drives are probably not as much of a worry because they're just in my server and it hasn't failed just from power on time? And the writes will be infrequent. But microsd cards to my knowledge are made from the bad flash left over after nvme and SATA ssds have taken the cream of the crop? And hence you're mostly getting the worst perfoming overheating and also probably a tonne of qlc drives which aren't supposed to be written to more than 50 times always getting down to 10% capacity and god forbid 0%/a few megabytes or a gigabyte if I do my laundry a day late or wake up earlier that week than I accounted for or the videos take up more stoage because of more detail envoironements that week for instance.\n\n___\nWhat are the problems I've missed/ways I could streamline this?\n\n5 questions I have for anyone whos read thus far (don't answer all you poor thing):\n\n1) Do I need to look at something other than a phone to be doing this? I would REALLY prefer this be a phone so it's not a big deal I just have it on me - it records things.\n\n2) Are there any sites that give details or test things like durability and throughput on SD cards? Maybe a dp-review type site as I guess this is in that vein?\n\n3) Are there any phones that are ideal for this? E.g. large sensor area, reliable, could fit in a sewn pocket, 2 micro sd slots, usb 3? That's a lot of specific things so I'm not sure.\n\n4) What settings, operating systems or software for the phone might I consider if I decide to pull the trigger on this.\n\n5) What automation steps and setup/devices do you reccommend having to make this realistic.\n\nThanks a million in advance for your input! If the plan starts having fewer holes so I decide I could reasonably do this I will\n\n## ##Conclusion:## ##\n\nThis is probably not worth it for all the extra work and even the backups one could use the files for but maybe in future when the tech is a bit better I can look back at this post and unshelve it if we get say 4tb extremely durable drives I'd only have to half fill up that can offload all the footage quickly and reliably with some camera/smartphone hybrid that I can actuall sew into my pocket qutie discretely.\n\n### ## My Homework before proceeding:\n\n&gt; 1) Test process with current hardware to a) check for issues and b) improve process and c) find what I actually need to buy.\n\n&gt; 2) Test failures on purpose e.g. power downs, perhaps water damage on my current sd card? It's only an older sandisk though. idk how to test power down without just draining the battery or clicking the power button though? This gives the phone a chance to account for it, even if I wasted an old phone that still works it'd still be a different device but I guess I could know the SD card is CAPABLE of holding the video after the phone abruptly stops but that'd be a big waste of a perfectly good device, however it might be the only way to test these scenarios, if I am vulnerable to someone just taking my phone and breaking it and I can't save the last few seconds I'm in trouble, those situations are disproportionately likely to need recording e.g. litterally showing what happened when my phone broke or how I fell, seeing on video myself collapsing for instance could be quite valuable.\n\n&gt; 3) Sew into my clothes a subtle pocket or way of keeping my phone say zipper or velcrowed in a good position for both filming and recording audio (might do this to only 1 peice of clothing and wait to do the rest for a new phone to get the specifics right - don't want a GAPING hole for the camera and mic).\n\n&gt; 4) Find a phone that'd not only fit but has good enough hardware to do this if I eventually decide this is worth upgrading for.", "author_fullname": "t2_4s2j111y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Efficient way to offload video recorded on your phone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_128xwf8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680381673.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680381091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m honestly surprised there are no other posts like this on this sub, maybe there&amp;#39;s a better sub?...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll get to the real world implications and problems which is what I want advice on but first I want to demonstrate/sanity check that this is indeed technically possible and how it COULD be done.&lt;/p&gt;\n\n&lt;p&gt;I have a few spare terabytes on my computer now + can record for days on my phone if need be with its storage + an sd card reader that reads at about 88Mb/s.&lt;/p&gt;\n\n&lt;h1&gt;Current basic plan:# (This sounds very simple but it also causes a lot of weekly labour hours and problems...)&lt;/h1&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;80Megabytes per minute recording usage on my phone (could be less in dark or unchanging envoironments etc. - sd cards can be 1tb now meaning I can at minimum record for - 960gb/(0.08gb*60minutes) = 200 hours?!? That&amp;#39;s a straight week or two weeks of waking hours! I there are phones that are able to use &amp;lt;10mbps video please do tell me that&amp;#39;s the least my phone will record on. And I would like to stretch the terabyte sd card and then my hard drive storage the furthest I possibly can so transfer can also not take hours each week while being less frequent than each week? Perhaps custom phone apps or phone operating systems allow this or there are developer options? I&amp;#39;m not too into phones, only PCs so I wouldn&amp;#39;t be aware of anything that might be obvious to you.&lt;/p&gt;\n\n&lt;p&gt;Get sd card to 512gb or 1tb for \u00a330-100 for storage that is FAST enough but not nessesarily DURABLE enough, I cannot seem to find good reveiws on these drives whatsoever let alone durability?&lt;/p&gt;\n\n&lt;p&gt;Transfer whenever I do my laundry or something. Hit record again when I am done with laundry.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;h2&gt;Problems:## Listed going from more technical to more social&lt;/h2&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Micro sd write endurace and power on time - even western digitals purple and sandisks &amp;quot;maxendurance sd cards only exist in 512gb and 256gb respectively and at least twice as expensive and 256gb for 120,000 hours of write endurance could allow the card to go for a decade but if you have to plug it in every 50 hours to transfer the data back it wouldn&amp;#39;t last long if it was always writing to max capacity. Additionally even survalence drives probably aren&amp;#39;t supposed to be in such high quality.\nEdit I found a western digital purple at 1tb for an eye watering \u00a3220 - I could get a 4TB NVME for that price -- &lt;a href=\"https://www.amazon.co.uk/gp/product/B08VH3RX3Y/ref=ox_sc_act_title_1?smid=A30DC7701CXIBH&amp;amp;psc=1\"&gt;https://www.amazon.co.uk/gp/product/B08VH3RX3Y/ref=ox_sc_act_title_1?smid=A30DC7701CXIBH&amp;amp;psc=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Storage/quality - yes I can record a week with h.264 at around 10mbps but I would need to compress it when it got to my laptop, I image that&amp;#39;s something I could automate, perhaps i could program a batch file or something to do it whenever something arrive in x folder to compress it using y, no idea how to do the y side of thing though. But even if I could compress it in such a way where it doesn&amp;#39;t cause hours of labour for me to check its going smoothly every week and very little downtime, once compressed am I what gonna put it to 1mbps from 10? 1080p for everything in your feild of vision is already not good enough, if I didn&amp;#39;t realise I was being robbed etc. at the time or some situation like there where very small details matter then is the persons face even going to be close enough at 10mbps to recognise them? Then what I compress to 1mpbs and later realise I need to edit them export then compress again and its even worse. Even then, let&amp;#39;s run the numbers. I currently have 2 8tb HDDs in raid 0 planning to upgrade to 24tb raid 5 with 8tb redundancy so 8tb/(0.00125GB&lt;em&gt;3600&lt;/em&gt;24*365) = 0.203 max quality, &amp;lt;1 = I cannot store that even ignoring all my backups and other data. Divide by 10 for min quality, multiply by 3 for max storage. If I manage to compress by ten times footage for a year would take up 1/2.03 of the 8tb I have, no backups. Half of everything for 1 year of shittiest quality footage, definately gonna need to be able to stop recording at night. Good news is the hard drives only have to be written to once a week and will be filled up at most ten times by whats being written to them. However, I have to account for 3 things in reality. 1) Allll my backups for computing use. 2) Allll the ARCHIVED video compressed for storage ~4tb per year. 3) All the video PRE archiving that is just transfered right to my computer each week. So even if I do this every 4 weeks after a year I will be filling it up every 4 weeks when the last week of footage goes in IF I compress everything that often. Realistically I will soon need more than 4TB just for backups so this isn&amp;#39;t acceptable. Do I compress each week then? Even there its always using the 1TB transfer + 10% If such a large file can even be compressed using video software this MUST be automated such that when I unplug my phone after transfer it will compress what I stored and ONLY what I stored and then delete it but I must ALSO be able to trust the compressed footage is good enough on it&amp;#39;s own... I guess I could find some way to record in lower quality in the first place which would also allow me to record more overall? But then  what if I do need to catch a small detail in frame?&lt;/p&gt;\n\n&lt;p&gt;Editing - I will enevitably accidentally record at night or something or I eventually want to get around to deleting all my old footage how the heck do I a) KNOW what footage I need and b) how do I save ONLY what I need? Do I take my 5-6tb of footage put a little guassian blur here, cut this out and wait until the heat death of the universe for it to try and export then crash for me to realise the software isn&amp;#39;t compatable. Or so I take all that footage I&amp;#39;ve gotten by that point and take a glance at each day to try and remember if there was anything significant, I know for sure there are gonna be significant days I look at and dismiss not just because I won&amp;#39;t make the connection from what I see sometimes but because, do I carry a diary with me to write down notable moments? Not only will I forget to even if I realise how significant something is at the time  after going through a hundred days the signicance of any individual one won&amp;#39;t hit me... and oh yeah it&amp;#39;d be weeks at least since they&amp;#39;d actually happened. But also half the point of this is that I don&amp;#39;t realise how significant this is most of the time. It&amp;#39;d be after the fact I&amp;#39;d think OH SHOOT, that person was lying to me or ripped me off, mislead me or that info was important if only I could remember etc.&lt;/p&gt;\n\n&lt;p&gt;Quality - Where&amp;#39;s the phone gonna point? Is the audio quality going to be acceptable&lt;/p&gt;\n\n&lt;p&gt;Reliability - Your sd breaks - you lose your week if you were depending on a recording you&amp;#39;re screwed + the storage requirement is so high backups are unlikely to happen + break downs are more likely with such high useage + Potentially wear on phone camera and storage that is not designed for constant usage. Even if the phone camera is sold state which I assume it is the sd card would surely heat up quite a lot after a full weeks running at full pelt. Additionally burn in even from an indicator in the corner that recording is happenig is not great if not the camera screen would be burning in constantly on OLED, are there settings or phones that allow me to change this? Some micro sd cards CLAIM to be waterproof, western digital claims all their sandisk micro sd cards &amp;quot;are NOW&amp;quot; of 13/09/22 waterproof for 72hours under 1m of water and other sd cards seem to give the same values for things like temperature protection but I don&amp;#39;t know any reveiw sites looking to verify this? ( &lt;a href=\"https://support-en.wd.com/app/answers/detailweb/a_id/37516/%7E/sd-%26-microsd-card%3A-environmental-tolerance-%28waterproof%2C-temperature%2C-magnetic\"&gt;https://support-en.wd.com/app/answers/detailweb/a_id/37516/~/sd-%26-microsd-card%3A-environmental-tolerance-%28waterproof%2C-temperature%2C-magnetic&lt;/a&gt; )&lt;/p&gt;\n\n&lt;p&gt;How the heck do I use my phone for other things when doing this? So far as I&amp;#39;m aware most phones will stay on recording but even then it uses resources and there is usually an onscreen indicator that it is recording.&lt;/p&gt;\n\n&lt;p&gt;If the phone runs out of power or otherwise fails how do I make sure I don&amp;#39;t just lose a full days recording? I&amp;#39;m a bit embarressed to admit I don&amp;#39;t actually know if the video file is saved until it&amp;#39;s been stopped and saved? I know I can open some that aren&amp;#39;t fully downloaded etc. but I&amp;#39;d need to test this I guess?&lt;/p&gt;\n\n&lt;p&gt;Errr, how do I carry it around? Like when I&amp;#39;m in exercise clothes or take my clothes off I don&amp;#39;t care to film - those circumstances don&amp;#39;t require it but if I am at a store or a bouncer is getting aggressive or I&amp;#39;m blackout drunk, how do I make sure it&amp;#39;s idiot proof enough so that I won&amp;#39;t break it&lt;/p&gt;\n\n&lt;p&gt;How do I transfer a terrabyte on the weekly? Yes I can record it but it&amp;#39;d also take about the same amount of time to transfer back, I have actually transfered large files with these things and was surprisd to learn they could transfer 88Mb/s consistently with large video files... but that&amp;#39;s still over 3 hours of data transfer.&lt;/p&gt;\n\n&lt;p&gt;Even in places I want to record for safety I don&amp;#39;t always want it to be known that I am recording and android phones I know of either have to be on the camera screen or will fade out the screen with a timer, which means it would always be known if I even accidentally pull my phone out... will I have to constantly explain all the reasons why I do this, like do I explain to a sales person at a store, oh I&amp;#39;m recording you because I don&amp;#39;t trust you or to a person at work I am holding them to account and the law TECHNICALLY states if there is no expectation of privacy as other poeople are around... etc. or that well i have been burnt in the past by not being able to domonstrate the truth with proof so... blah blah blah.&lt;/p&gt;\n\n&lt;p&gt;Recording in situations where others expect privacy - I can&amp;#39;t go back and erase each and every moment I cross into private property? Or is there a way I can quickly edit MASSIVE files - so far as I&amp;#39;m aware most video editing software requires export of full files to edit them even with tiny changes? Is that an inherent limitation of how the files work? And even then having recorded in the first place is problematic, which will enevitably happen I just won&amp;#39;t realise the transition all the time and am I what supposed to realise I&amp;#39;ve crosssed an imaginary boundary mid deep conversation and whip my phone out and not tell them why? Or interupt the conversation to explain, oh I was respecting the covnersation by stopping recording, drop that bombshell on them and expect them to trust I&amp;#39;m not continuing to record.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;h3&gt;Solutions:&lt;/h3&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;1) Find a phone that can either a) transfer directly via a usb 3.0 port from it&amp;#39;s sd card but since that&amp;#39;d take atleast 3 hours with ZERO interuption, preferably b) when I eventually get my next phone it would have 2 micro sd card slots, if there are even any phones I&amp;#39;d use that could do that, take one sd card out, continue recording on the other - only problem is I&amp;#39;d need to be able to tell the phone specifically which sd card to record to.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;1.5) (1) causes a problem where I am choosing between simply just plugging in my phone and whipping out a sim key or pin to dig out an sd card - either a) my phone is stuck at my computer for hours or b) I am fiddling with an sd card and have to likely keep remembering to reselect which card it is, check I didn&amp;#39;t record to internal storage etc. etc. a) could be automated very nicely and is just plugging something in eventually but limits what I physically can do and b) is very involved but means once I&amp;#39;ve manually done all the work things should be fine.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;2) Every say month or two I will compress the video from the files I am putting on my computer each week.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;?3?) If there is some kind of phone based &amp;quot;RAID 0&amp;quot; or RAID 5 type thing where my phone could consistently offload some data to a different drive to make sure when I record something I can definately rely on it being recorded and me being able to use it in a court of law or to remember that fact that thing I was told its paramount I remember or that thing in a lecture I forgot about, as long as I can remember roughly when  I learnt it or experienced its there - only problem is putting such strain on my phone and storage devices cannot be good. Even if I somehow get a &amp;quot;RAID 0&amp;quot; or RAID 5 system its likely to fail on both my storage systems because the stress on them both would be so high.&lt;/p&gt;\n\n&lt;p&gt;4) Survalence micro sd?/hard drives?!? The hard drives are probably not as much of a worry because they&amp;#39;re just in my server and it hasn&amp;#39;t failed just from power on time? And the writes will be infrequent. But microsd cards to my knowledge are made from the bad flash left over after nvme and SATA ssds have taken the cream of the crop? And hence you&amp;#39;re mostly getting the worst perfoming overheating and also probably a tonne of qlc drives which aren&amp;#39;t supposed to be written to more than 50 times always getting down to 10% capacity and god forbid 0%/a few megabytes or a gigabyte if I do my laundry a day late or wake up earlier that week than I accounted for or the videos take up more stoage because of more detail envoironements that week for instance.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;What are the problems I&amp;#39;ve missed/ways I could streamline this?&lt;/p&gt;\n\n&lt;p&gt;5 questions I have for anyone whos read thus far (don&amp;#39;t answer all you poor thing):&lt;/p&gt;\n\n&lt;p&gt;1) Do I need to look at something other than a phone to be doing this? I would REALLY prefer this be a phone so it&amp;#39;s not a big deal I just have it on me - it records things.&lt;/p&gt;\n\n&lt;p&gt;2) Are there any sites that give details or test things like durability and throughput on SD cards? Maybe a dp-review type site as I guess this is in that vein?&lt;/p&gt;\n\n&lt;p&gt;3) Are there any phones that are ideal for this? E.g. large sensor area, reliable, could fit in a sewn pocket, 2 micro sd slots, usb 3? That&amp;#39;s a lot of specific things so I&amp;#39;m not sure.&lt;/p&gt;\n\n&lt;p&gt;4) What settings, operating systems or software for the phone might I consider if I decide to pull the trigger on this.&lt;/p&gt;\n\n&lt;p&gt;5) What automation steps and setup/devices do you reccommend having to make this realistic.&lt;/p&gt;\n\n&lt;p&gt;Thanks a million in advance for your input! If the plan starts having fewer holes so I decide I could reasonably do this I will&lt;/p&gt;\n\n&lt;h2&gt;##Conclusion:##&lt;/h2&gt;\n\n&lt;p&gt;This is probably not worth it for all the extra work and even the backups one could use the files for but maybe in future when the tech is a bit better I can look back at this post and unshelve it if we get say 4tb extremely durable drives I&amp;#39;d only have to half fill up that can offload all the footage quickly and reliably with some camera/smartphone hybrid that I can actuall sew into my pocket qutie discretely.&lt;/p&gt;\n\n&lt;h3&gt;## My Homework before proceeding:&lt;/h3&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;1) Test process with current hardware to a) check for issues and b) improve process and c) find what I actually need to buy.&lt;/p&gt;\n\n&lt;p&gt;2) Test failures on purpose e.g. power downs, perhaps water damage on my current sd card? It&amp;#39;s only an older sandisk though. idk how to test power down without just draining the battery or clicking the power button though? This gives the phone a chance to account for it, even if I wasted an old phone that still works it&amp;#39;d still be a different device but I guess I could know the SD card is CAPABLE of holding the video after the phone abruptly stops but that&amp;#39;d be a big waste of a perfectly good device, however it might be the only way to test these scenarios, if I am vulnerable to someone just taking my phone and breaking it and I can&amp;#39;t save the last few seconds I&amp;#39;m in trouble, those situations are disproportionately likely to need recording e.g. litterally showing what happened when my phone broke or how I fell, seeing on video myself collapsing for instance could be quite valuable.&lt;/p&gt;\n\n&lt;p&gt;3) Sew into my clothes a subtle pocket or way of keeping my phone say zipper or velcrowed in a good position for both filming and recording audio (might do this to only 1 peice of clothing and wait to do the rest for a new phone to get the specifics right - don&amp;#39;t want a GAPING hole for the camera and mic).&lt;/p&gt;\n\n&lt;p&gt;4) Find a phone that&amp;#39;d not only fit but has good enough hardware to do this if I eventually decide this is worth upgrading for.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128xwf8", "is_robot_indexable": true, "report_reasons": null, "author": "VoiceofRedditMkI", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128xwf8/efficient_way_to_offload_video_recorded_on_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128xwf8/efficient_way_to_offload_video_recorded_on_your/", "subreddit_subscribers": 676330, "created_utc": 1680381091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "for HUH721212AL4200, can't find on WD or HGST on google. things like speed, power usage, format, CMR or SMR etc\n\nit seems much more difficult to find them compare to seagate's drive, pls help.", "author_fullname": "t2_1esnlvr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need some help finding info on a specific model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128tpn5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680372009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;for HUH721212AL4200, can&amp;#39;t find on WD or HGST on google. things like speed, power usage, format, CMR or SMR etc&lt;/p&gt;\n\n&lt;p&gt;it seems much more difficult to find them compare to seagate&amp;#39;s drive, pls help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128tpn5", "is_robot_indexable": true, "report_reasons": null, "author": "tallguyyo", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128tpn5/need_some_help_finding_info_on_a_specific_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128tpn5/need_some_help_finding_info_on_a_specific_model/", "subreddit_subscribers": 676330, "created_utc": 1680372009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've currently got just shy of 20TB on my google drive but I want to move, or at least copy, most of it to my seedbox so I can access content, especially videos, more easily. Unfortunately I'm struggling to transfer the data over due to Google's daily 750gb limit.\n\nHow can I transfer it over(remotely)?", "author_fullname": "t2_16l87w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "google drive to seedbox", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128quom", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680365870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680365676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve currently got just shy of 20TB on my google drive but I want to move, or at least copy, most of it to my seedbox so I can access content, especially videos, more easily. Unfortunately I&amp;#39;m struggling to transfer the data over due to Google&amp;#39;s daily 750gb limit.&lt;/p&gt;\n\n&lt;p&gt;How can I transfer it over(remotely)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "128quom", "is_robot_indexable": true, "report_reasons": null, "author": "KingRollos", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/128quom/google_drive_to_seedbox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/128quom/google_drive_to_seedbox/", "subreddit_subscribers": 676330, "created_utc": 1680365676.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}