{"kind": "Listing", "data": {"after": "t3_124ira5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m05ys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "State of Data Engineering 2022", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_124d6qi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 211, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 211, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XmlxxjlEhCNUJSpfPjnRzbG1hzK9Z09cc4VEVqVrHyo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679978995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/3rc8hxffueqa1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?auto=webp&amp;v=enabled&amp;s=5bfc9a5f597301e66c74afd35b5ca66828c9dd16", "width": 2560, "height": 1440}, "resolutions": [{"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a13140d19c377599589380edb419b3a63b68b8f2", "width": 108, "height": 60}, {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43bad1618623404a3bf95da32678a69518d1fe54", "width": 216, "height": 121}, {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d786150cd0e697484cf1dd7b5ae2bdd2888cbe76", "width": 320, "height": 180}, {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=defadaa89bef2c15d0d3a5a57771d3e5a295b248", "width": 640, "height": 360}, {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7231f337000a18a5fdbedadfbb539d3a07b9cdaa", "width": 960, "height": 540}, {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8208f54f0c1783e220f30b7f8cb69071fe240b39", "width": 1080, "height": 607}], "variants": {}, "id": "it_5DM1yYpIndVcCpr5Y7cS912omKACEHGt4msvndo0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "124d6qi", "is_robot_indexable": true, "report_reasons": null, "author": "SyntheticBlood", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124d6qi/state_of_data_engineering_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/3rc8hxffueqa1.jpg", "subreddit_subscribers": 94698, "created_utc": 1679978995.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Join us in the Rockies for an unforgettable conference featuring thought-provoking talks, workshops, craft beer tasting, hiking, and immersive experiences that will change the way you think about software development. Grab your Early Bird Ticket: [https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687](https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687)", "author_fullname": "t2_qhroetn8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big news! LambdaConf returns Sept 16-19th and is better than ever! \ud83d\udd25", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124ozln", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 92, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 92, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680010541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Join us in the Rockies for an unforgettable conference featuring thought-provoking talks, workshops, craft beer tasting, hiking, and immersive experiences that will change the way you think about software development. Grab your Early Bird Ticket: &lt;a href=\"https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687\"&gt;https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?auto=webp&amp;v=enabled&amp;s=6813a1ea4b5d401e335caa02583865546ee29898", "width": 1000, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b30c45d7350f71944e7ed6d27d03555b62a91684", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0379a777f803e71a1b906de4a5bcb32220be1005", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa1fdba3bf291c3b14470d653f8ebd575fd72c25", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4436df3f9c64439cf26ced83657393f9c08df83", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f84ac18aab2e94b4f7e4951bca30d20fc0fb70c8", "width": 960, "height": 480}], "variants": {}, "id": "DaWPZMpGJTzhhl3d5dsciFzXnZ4QqNEDthE7iz5CkU0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "124ozln", "is_robot_indexable": true, "report_reasons": null, "author": "Agataziverge", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124ozln/big_news_lambdaconf_returns_sept_1619th_and_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124ozln/big_news_lambdaconf_returns_sept_1619th_and_is/", "subreddit_subscribers": 94698, "created_utc": 1680010541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_92dedrzo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SMBC-comics.com \"now squeeze your points together to make your results look big\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_124mi0z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680004763.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/cz9tbicqygqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "preview": {"images": [{"source": {"url": "https://preview.redd.it/cz9tbicqygqa1.png?auto=webp&amp;v=enabled&amp;s=90836b30830e55386ac9b7c92ee4ab49d5b8c5ef", "width": 684, "height": 710}, "resolutions": [{"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b7b39b47ef18f4c425929d90b90df902a33a95e", "width": 108, "height": 112}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b047fb2fa23d8baa5445531cf278c8b4c1f9f87", "width": 216, "height": 224}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31c15c84067af3ef3911763420b5fb26c12c1733", "width": 320, "height": 332}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6aec00547714de581430e536c536f33ac4a5d2d", "width": 640, "height": 664}], "variants": {"obfuscated": {"source": {"url": "https://preview.redd.it/cz9tbicqygqa1.png?blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6519edc62e55abdf79ca69bce3f2da3a4214ad1c", "width": 684, "height": 710}, "resolutions": [{"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3ff45f8a205382c9be541f2f1ed00b65d08d001d", "width": 108, "height": 112}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=bb493ae483cf64ca6319edf4804d970b4a9fc724", "width": 216, "height": 224}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6730631a29389d92806643438810c7fad87eb076", "width": 320, "height": 332}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=96deb0aba516333cefd018f89758acbd383c1131", "width": 640, "height": 664}]}, "nsfw": {"source": {"url": "https://preview.redd.it/cz9tbicqygqa1.png?blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6519edc62e55abdf79ca69bce3f2da3a4214ad1c", "width": 684, "height": 710}, "resolutions": [{"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3ff45f8a205382c9be541f2f1ed00b65d08d001d", "width": 108, "height": 112}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=bb493ae483cf64ca6319edf4804d970b4a9fc724", "width": 216, "height": 224}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6730631a29389d92806643438810c7fad87eb076", "width": 320, "height": 332}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=96deb0aba516333cefd018f89758acbd383c1131", "width": 640, "height": 664}]}}, "id": "o_ZfZakw9vonrfxEDxzx3f9eg682BMvwf-Xnnf7ApQ0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "124mi0z", "is_robot_indexable": true, "report_reasons": null, "author": "rackhamlerouge9", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124mi0z/smbccomicscom_now_squeeze_your_points_together_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/cz9tbicqygqa1.png", "subreddit_subscribers": 94698, "created_utc": 1680004763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This has been bugging me for a while and I wanted to discuss it with you all. I have been working with spark for around 3 years\n\nWhenever I'm working on long spark jobs or running multiple iterations of shorter ones, I get super unproductive just waiting around for them to finish. I understand its not like app development where we get instant feedback. \n\nBut switching to another story/task sometimes feels like an unnecessary context switch, plus if I start browsing or learning something new, it's tough to snap back into the job at hand. \n\nHave you been on similar boat? What do you all do to stay productive or what would you do during these slow moments?", "author_fullname": "t2_3xh5j7zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Productivity advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124385k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679958443.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679956362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This has been bugging me for a while and I wanted to discuss it with you all. I have been working with spark for around 3 years&lt;/p&gt;\n\n&lt;p&gt;Whenever I&amp;#39;m working on long spark jobs or running multiple iterations of shorter ones, I get super unproductive just waiting around for them to finish. I understand its not like app development where we get instant feedback. &lt;/p&gt;\n\n&lt;p&gt;But switching to another story/task sometimes feels like an unnecessary context switch, plus if I start browsing or learning something new, it&amp;#39;s tough to snap back into the job at hand. &lt;/p&gt;\n\n&lt;p&gt;Have you been on similar boat? What do you all do to stay productive or what would you do during these slow moments?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "124385k", "is_robot_indexable": true, "report_reasons": null, "author": "thedatumgirl", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124385k/productivity_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124385k/productivity_advice/", "subreddit_subscribers": 94698, "created_utc": 1679956362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the pros and cons of both? Actually I am facing so many issues in deploying Airflow that I give up and I am thinking that I should try step functions because it might be easy to deploy and maintain.  \n\n\nPS : If you can give any good deployment guide for someone who is new to cloud that would be helpful too", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow vs Step Functions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123x94t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679944719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the pros and cons of both? Actually I am facing so many issues in deploying Airflow that I give up and I am thinking that I should try step functions because it might be easy to deploy and maintain.  &lt;/p&gt;\n\n&lt;p&gt;PS : If you can give any good deployment guide for someone who is new to cloud that would be helpful too&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "123x94t", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/123x94t/airflow_vs_step_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/123x94t/airflow_vs_step_functions/", "subreddit_subscribers": 94698, "created_utc": 1679944719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/nrphxx8szdqa1.png?width=1487&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0582491b373e38549a189d8c406f6327e9fd1c12\n\n In this article, we will walk through the process of setting up a real-time data processing and analytics environment for vehicle plate recognition. We will use Docker to manage our services, MySQL for data storage, Redpanda as a streaming platform, MinIO as an object storage server, and Apache Spark for data processing and analysis. We will also integrate the Twilio API to send SMS notifications in real-time based on the processed data. \n\n [https://medium.com/@stefentaime\\_10958/real-time-data-processing-and-analytics-with-docker-mysql-redpanda-minio-and-apache-spark-eca83f210ef6](https://medium.com/@stefentaime_10958/real-time-data-processing-and-analytics-with-docker-mysql-redpanda-minio-and-apache-spark-eca83f210ef6)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-Time Data Processing and Analytics with Docker, MySQL, Redpanda, MinIO, and Apache Spark Using Delta Lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nrphxx8szdqa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/nrphxx8szdqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5ad116c34e71f9dddc755296b06b5fbfca17612"}, {"y": 110, "x": 216, "u": "https://preview.redd.it/nrphxx8szdqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2a3766df5dc7f7bed8f725018b33c3b5d98d668"}, {"y": 163, "x": 320, "u": "https://preview.redd.it/nrphxx8szdqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7d19325c5e912c3313afb88cb0c3e62dfa4854c"}, {"y": 326, "x": 640, "u": "https://preview.redd.it/nrphxx8szdqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=997910e0a155aff7db6589537f7fa19814a36d84"}, {"y": 489, "x": 960, "u": "https://preview.redd.it/nrphxx8szdqa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fb64d1ebb85065c96c92fe6aa424ca2499053f7"}, {"y": 550, "x": 1080, "u": "https://preview.redd.it/nrphxx8szdqa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2793f52fb54e2a7d99c413858cab1151399c655e"}], "s": {"y": 758, "x": 1487, "u": "https://preview.redd.it/nrphxx8szdqa1.png?width=1487&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0582491b373e38549a189d8c406f6327e9fd1c12"}, "id": "nrphxx8szdqa1"}}, "name": "t3_1248ypa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ygQa3znhbllPikDzZ5Ad85cOuEeLID0dBfu1J4qxE_w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1679968719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nrphxx8szdqa1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0582491b373e38549a189d8c406f6327e9fd1c12\"&gt;https://preview.redd.it/nrphxx8szdqa1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0582491b373e38549a189d8c406f6327e9fd1c12&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this article, we will walk through the process of setting up a real-time data processing and analytics environment for vehicle plate recognition. We will use Docker to manage our services, MySQL for data storage, Redpanda as a streaming platform, MinIO as an object storage server, and Apache Spark for data processing and analysis. We will also integrate the Twilio API to send SMS notifications in real-time based on the processed data. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/real-time-data-processing-and-analytics-with-docker-mysql-redpanda-minio-and-apache-spark-eca83f210ef6\"&gt;https://medium.com/@stefentaime_10958/real-time-data-processing-and-analytics-with-docker-mysql-redpanda-minio-and-apache-spark-eca83f210ef6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IW1GXFCD2SMjmgzvyCi8QIHZXoCbz_6nbPAyzypvG5w.jpg?auto=webp&amp;v=enabled&amp;s=d066d16326714a53d5f2235ccf69b8330488159e", "width": 1200, "height": 612}, "resolutions": [{"url": "https://external-preview.redd.it/IW1GXFCD2SMjmgzvyCi8QIHZXoCbz_6nbPAyzypvG5w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=831af4d9535c6d553e7ad13d321f3d41cf4fa042", "width": 108, "height": 55}, {"url": "https://external-preview.redd.it/IW1GXFCD2SMjmgzvyCi8QIHZXoCbz_6nbPAyzypvG5w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0049e4bce7a56d3702088e375a3bf1dbc39f44c4", "width": 216, "height": 110}, {"url": "https://external-preview.redd.it/IW1GXFCD2SMjmgzvyCi8QIHZXoCbz_6nbPAyzypvG5w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0d6921d5e93d70550a256263b7c5c84b0e001a2", "width": 320, "height": 163}, {"url": "https://external-preview.redd.it/IW1GXFCD2SMjmgzvyCi8QIHZXoCbz_6nbPAyzypvG5w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d2fcfae67450d7709ae9bf049d2bc150ddb83ab", "width": 640, "height": 326}, {"url": "https://external-preview.redd.it/IW1GXFCD2SMjmgzvyCi8QIHZXoCbz_6nbPAyzypvG5w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db987f4d6848bdbccdca3fd69301316d23bd41e9", "width": 960, "height": 489}, {"url": "https://external-preview.redd.it/IW1GXFCD2SMjmgzvyCi8QIHZXoCbz_6nbPAyzypvG5w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=694b9e88b7cd101394b4d181d51a3506e76f5ab1", "width": 1080, "height": 550}], "variants": {}, "id": "E6PYkGbAH391Mdwd0tNLuKTj6PQyMltA3JaiUv53pU4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1248ypa", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1248ypa/realtime_data_processing_and_analytics_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1248ypa/realtime_data_processing_and_analytics_with/", "subreddit_subscribers": 94698, "created_utc": 1679968719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I made it to the second round for a data engineering interview. Contrary to the first interview being a 1 hour video call, the 2nd interview is a 1 hour in-person interview. It's a remote job, so they are aware I am a 2.5 hour train trip away from the offices where the interview will be held. \n\nI've asked what format the interview will take and the answer was vague: \"they just want to ask more technical questions to gauge your experience and how they can support you in the role\".\n\nI like to prepare, and I like to prepare well. So my question to this sub-reddit is: what could they possibly want from me in-person that requires a 5 hour round trip commute that they cannot get in a video call?\n\nAre you thinking tests and live coding?", "author_fullname": "t2_r7bmwiee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What happens at an in-person 2nd round interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123vraz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679941725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made it to the second round for a data engineering interview. Contrary to the first interview being a 1 hour video call, the 2nd interview is a 1 hour in-person interview. It&amp;#39;s a remote job, so they are aware I am a 2.5 hour train trip away from the offices where the interview will be held. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve asked what format the interview will take and the answer was vague: &amp;quot;they just want to ask more technical questions to gauge your experience and how they can support you in the role&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I like to prepare, and I like to prepare well. So my question to this sub-reddit is: what could they possibly want from me in-person that requires a 5 hour round trip commute that they cannot get in a video call?&lt;/p&gt;\n\n&lt;p&gt;Are you thinking tests and live coding?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "123vraz", "is_robot_indexable": true, "report_reasons": null, "author": "va1kyrja-kara", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/123vraz/what_happens_at_an_inperson_2nd_round_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/123vraz/what_happens_at_an_inperson_2nd_round_interview/", "subreddit_subscribers": 94698, "created_utc": 1679941725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey /r/dataengineering! \n\nI\u2019m Toby and over the last few months, I\u2019ve been working with a team of engineers from Airbnb, Apple, Google, and Netflix, to simplify developing data pipelines with [SQLMesh](https://github.com/TobikoData/sqlmesh). \n\nWe\u2019re tired of fragile pipelines, untested SQL queries, and expensive staging environments for data. Software engineers have reaped the benefits of DevOps through unit tests, continuous integration, and continuous deployment for years. We felt like it was time for data teams to have the same confidence and efficiency in development as their peers. It\u2019s time for DataOps!\n\nSQLMesh can be used through a CLI/notebook or in our open source web based IDE (in preview). SQLMesh builds efficient dev / staging environments through \u201cVirtual Data Marts\u201d using views, which allows you to seamlessly rollback or roll forward your changes! With a simple pointer swap you can promote your \u201cstaging\u201d data into production. This means you get unlimited copy-on-write environments that make data exploration and preview of changes cheap, easy, safe. Some other key features are:\n\n* Automatic DAG generation by semantically parsing and understanding SQL or Python scripts\n* CI-Runnable Unit and Integration tests with optional conversion to DuckDB\n* Change detection and reconciliation through column level lineage \n* Native Airflow Integration\n* Import an existing DBT project and run it on SQLMesh\u2019s runtime (in preview)\n\nWe\u2019re just getting started on our journey to change the way data pipelines are built and deployed. We\u2019re huge proponents of open source and hope that we can grow together with your feedback and contributions. Try out SQLMesh by following the [quick start guide](https://sqlmesh.readthedocs.io/en/stable/quick_start/). We\u2019d love to chat and hear about your experiences and ideas in our [Slack community](https://join.slack.com/t/tobiko-data/shared_invite/zt-1ma66d79v-a4dbf4DUpLAQJ8ptQrJygg).", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQLMesh: The future of DataOps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_124tspm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680019793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m Toby and over the last few months, I\u2019ve been working with a team of engineers from Airbnb, Apple, Google, and Netflix, to simplify developing data pipelines with &lt;a href=\"https://github.com/TobikoData/sqlmesh\"&gt;SQLMesh&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;We\u2019re tired of fragile pipelines, untested SQL queries, and expensive staging environments for data. Software engineers have reaped the benefits of DevOps through unit tests, continuous integration, and continuous deployment for years. We felt like it was time for data teams to have the same confidence and efficiency in development as their peers. It\u2019s time for DataOps!&lt;/p&gt;\n\n&lt;p&gt;SQLMesh can be used through a CLI/notebook or in our open source web based IDE (in preview). SQLMesh builds efficient dev / staging environments through \u201cVirtual Data Marts\u201d using views, which allows you to seamlessly rollback or roll forward your changes! With a simple pointer swap you can promote your \u201cstaging\u201d data into production. This means you get unlimited copy-on-write environments that make data exploration and preview of changes cheap, easy, safe. Some other key features are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Automatic DAG generation by semantically parsing and understanding SQL or Python scripts&lt;/li&gt;\n&lt;li&gt;CI-Runnable Unit and Integration tests with optional conversion to DuckDB&lt;/li&gt;\n&lt;li&gt;Change detection and reconciliation through column level lineage &lt;/li&gt;\n&lt;li&gt;Native Airflow Integration&lt;/li&gt;\n&lt;li&gt;Import an existing DBT project and run it on SQLMesh\u2019s runtime (in preview)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We\u2019re just getting started on our journey to change the way data pipelines are built and deployed. We\u2019re huge proponents of open source and hope that we can grow together with your feedback and contributions. Try out SQLMesh by following the &lt;a href=\"https://sqlmesh.readthedocs.io/en/stable/quick_start/\"&gt;quick start guide&lt;/a&gt;. We\u2019d love to chat and hear about your experiences and ideas in our &lt;a href=\"https://join.slack.com/t/tobiko-data/shared_invite/zt-1ma66d79v-a4dbf4DUpLAQJ8ptQrJygg\"&gt;Slack community&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "124tspm", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124tspm/sqlmesh_the_future_of_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124tspm/sqlmesh_the_future_of_dataops/", "subreddit_subscribers": 94698, "created_utc": 1680019793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey I'm wondering if it's possible to automatically check if a) all columns and b) all models in my dbt project have docstrings, so I can include this check in my cicd tests to ensure that my dwh is fully documented for downstream teams. Any tips would be appreciated. Google results didn't yield what I was looking for, and neither did looking at the dbt docs.", "author_fullname": "t2_68y0hb6s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt - is it possible to test it all docstrings are present?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124ampx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679972755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey I&amp;#39;m wondering if it&amp;#39;s possible to automatically check if a) all columns and b) all models in my dbt project have docstrings, so I can include this check in my cicd tests to ensure that my dwh is fully documented for downstream teams. Any tips would be appreciated. Google results didn&amp;#39;t yield what I was looking for, and neither did looking at the dbt docs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124ampx", "is_robot_indexable": true, "report_reasons": null, "author": "the_Wallie", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124ampx/dbt_is_it_possible_to_test_it_all_docstrings_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124ampx/dbt_is_it_possible_to_test_it_all_docstrings_are/", "subreddit_subscribers": 94698, "created_utc": 1679972755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\nI'm data engineer with about 2 yrs experience. Working with SQL and Python on different projects in a SaaS startup. \nDo you recommend getting an online master or cloud certificates like AWS data engineer for career development? Aiming +140k CAD in next 2-3 years. \n\nThanks :)", "author_fullname": "t2_bfz4zq58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Online Master Vs Cloud Certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1243iwk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679956939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m data engineer with about 2 yrs experience. Working with SQL and Python on different projects in a SaaS startup. \nDo you recommend getting an online master or cloud certificates like AWS data engineer for career development? Aiming +140k CAD in next 2-3 years. &lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1243iwk", "is_robot_indexable": true, "report_reasons": null, "author": "Sharp_Ad_8085", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1243iwk/online_master_vs_cloud_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1243iwk/online_master_vs_cloud_certification/", "subreddit_subscribers": 94698, "created_utc": 1679956939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel like frontend developers get a better developer experience as they can see impact from their code instantly on the browser as they update the code locally. Have you experienced something similar, where you can see the impact of your code changes on the data, using an iso-prod environment and that within seconds or minutes?", "author_fullname": "t2_975og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best developer experience you had as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124nms9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680007509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like frontend developers get a better developer experience as they can see impact from their code instantly on the browser as they update the code locally. Have you experienced something similar, where you can see the impact of your code changes on the data, using an iso-prod environment and that within seconds or minutes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "124nms9", "is_robot_indexable": true, "report_reasons": null, "author": "arimbr", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124nms9/what_is_the_best_developer_experience_you_had_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124nms9/what_is_the_best_developer_experience_you_had_as/", "subreddit_subscribers": 94698, "created_utc": 1680007509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At the moment I'm loading to a temporary table, and if that succeeds I'm deleting that date range from my final table, then copying from temp -&gt; live. I'd like to avoid the extra copy step if possible. How can I do this?\n\nI see there's an option for [preactions](https://docs.snowflake.com/en/user-guide/spark-connector-use#setting-configuration-options-for-the-connector) with the Snowflake Spark connector but after testing it doesn't look like these commands roll back if the load fails.\n\nEDIT: Solved! Preactions/postactions can be rolled into a transaction with the data transfer by combining options as per [this article](https://community.snowflake.com/s/article/How-To-Execute-the-queries-under-a-transaction-in-Spark-connector).", "author_fullname": "t2_22ksp1z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark to Snowflake: How to delete from table prior to load, but only if load is successful", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1245jyh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680018328.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679961088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At the moment I&amp;#39;m loading to a temporary table, and if that succeeds I&amp;#39;m deleting that date range from my final table, then copying from temp -&amp;gt; live. I&amp;#39;d like to avoid the extra copy step if possible. How can I do this?&lt;/p&gt;\n\n&lt;p&gt;I see there&amp;#39;s an option for &lt;a href=\"https://docs.snowflake.com/en/user-guide/spark-connector-use#setting-configuration-options-for-the-connector\"&gt;preactions&lt;/a&gt; with the Snowflake Spark connector but after testing it doesn&amp;#39;t look like these commands roll back if the load fails.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Solved! Preactions/postactions can be rolled into a transaction with the data transfer by combining options as per &lt;a href=\"https://community.snowflake.com/s/article/How-To-Execute-the-queries-under-a-transaction-in-Spark-connector\"&gt;this article&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1245jyh", "is_robot_indexable": true, "report_reasons": null, "author": "x1084", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1245jyh/pyspark_to_snowflake_how_to_delete_from_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1245jyh/pyspark_to_snowflake_how_to_delete_from_table/", "subreddit_subscribers": 94698, "created_utc": 1679961088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI've recently built a POC for Synapse Dedicated Pool, I used Synapse Pipelines to pull data from on premise, landed it in to the Data Lake and then in to an import table in a Dedicated Pool, then ran transformations with SPROC's (every 15 minutes)\n\nI landed the data in the data lake for two reasons:\n\n1) Polybase load in to Dedicated Pool\n2) To have a year/month/day/.. record of data loaded\n\nFor my main fact I have to join around 10-12 tables together, any of which could change, ranging from an order date of today back to 2008, so I couldn't see any way around having staging tables (other than using Spark &amp; Delta Lake, but I didn't want to spend even more on compute!)\n\nForgetting Dedicated Pool for a minute, the Synapse Pipelines alone for 50 or so tables is too expensive (running every 15 minutes)\n\nMy questions are:\n\n1) Does this architecture make sense?\n2) Is the cloud just expensive?  Or am I missing something?\n\nAny help or advice would be appreciated as I'm pulling my hair out with all the different options out there!\n\nFor context we're probably generating around 15GB of data a day, our key tables are a couple of hundred million rows.  We want to incrementally refresh every 15 minutes.", "author_fullname": "t2_9h6gf9pp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern Stack Help/Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123tztx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679938095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently built a POC for Synapse Dedicated Pool, I used Synapse Pipelines to pull data from on premise, landed it in to the Data Lake and then in to an import table in a Dedicated Pool, then ran transformations with SPROC&amp;#39;s (every 15 minutes)&lt;/p&gt;\n\n&lt;p&gt;I landed the data in the data lake for two reasons:&lt;/p&gt;\n\n&lt;p&gt;1) Polybase load in to Dedicated Pool\n2) To have a year/month/day/.. record of data loaded&lt;/p&gt;\n\n&lt;p&gt;For my main fact I have to join around 10-12 tables together, any of which could change, ranging from an order date of today back to 2008, so I couldn&amp;#39;t see any way around having staging tables (other than using Spark &amp;amp; Delta Lake, but I didn&amp;#39;t want to spend even more on compute!)&lt;/p&gt;\n\n&lt;p&gt;Forgetting Dedicated Pool for a minute, the Synapse Pipelines alone for 50 or so tables is too expensive (running every 15 minutes)&lt;/p&gt;\n\n&lt;p&gt;My questions are:&lt;/p&gt;\n\n&lt;p&gt;1) Does this architecture make sense?\n2) Is the cloud just expensive?  Or am I missing something?&lt;/p&gt;\n\n&lt;p&gt;Any help or advice would be appreciated as I&amp;#39;m pulling my hair out with all the different options out there!&lt;/p&gt;\n\n&lt;p&gt;For context we&amp;#39;re probably generating around 15GB of data a day, our key tables are a couple of hundred million rows.  We want to incrementally refresh every 15 minutes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "123tztx", "is_robot_indexable": true, "report_reasons": null, "author": "V10Matt", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/123tztx/modern_stack_helpadvice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/123tztx/modern_stack_helpadvice/", "subreddit_subscribers": 94698, "created_utc": 1679938095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently passed my DP-203 certification exam with the lowest score possible. I was a little sad bc I put in about 3 weeks of studying for it but at the end of the day... a pass is a pass.\n\n**Background:** I recently moved into a Cloud Data Eng role within my company a month ago. I had little to no experience with data eng. All I had was just course on the concept of ETL Data Warehousing. Previous experience role: Developer Analyst in my companies logistics domain working with our Global WMS solution.\n\n**Conclusion:** Test was a lot harder than expected. The exam consisted of curveballs and trick questions which made consistently second guess myself.  I read a couple of other ppls post on this sub that passed the exam and they made it sound like the exam was piece of cake. I think I may have struggled more because I had little to no real life hands on experience in regards to Data Eng. Also, exam went into more detail in regards to syntax as well as security and access. \n\n**Prep:** I prepared for about a month. Below is what I did\n\n1)My company payed for me to take the instructor led course offered on the Microsoft website and it was a complete nightmare/waste of time. The course was outsourced to a company named Koenig and they were struggling with the fact that microsoft had updated course content earlier this year in February. The instructor felt scripted and the labs did not match the course content. The instructor also had no real life experience and struggled with scenario based questions. For example, \"in which case would this distribution be best'?'... The only good thing about the course is that they later gave me a workaround lab environment with the updated labs which provided me with some simple generic hands on experience. I think besides the lab environments, I would suggest to take the self paced version and go through it in a manner where you understand the content with detail. The level of detail that they covered did not suffice the level of detail that exam questions covered.\n\n2)Practice test offered on Microsoft test: this were good but again did not match the level of detail that the questions of the exams consisted off. I would \\*\\*suggest\\*\\* getting the questions wrong and studying the corresponding links of the knowledge articles that they provide.\n\n3)Knowledge check questions on the course. Similar to the practice test\n\n4)Labs on the course . \\*\\*Recommend\\*\\*: reading knowledge articles corresponding to each lab\n\n5)[www.examtopics.com/exams/microsoft/dp-203](https://www.examtopics.com/exams/microsoft/dp-203) : I didn't hit this that hard but I would recommend doing so. These are the type of questions that are on the exams.\n\nPlease feel free to reach out if you guys have any questions.", "author_fullname": "t2_hi3uk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recently passed DP-203 Certification Exam", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_124t0qi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680018157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently passed my DP-203 certification exam with the lowest score possible. I was a little sad bc I put in about 3 weeks of studying for it but at the end of the day... a pass is a pass.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; I recently moved into a Cloud Data Eng role within my company a month ago. I had little to no experience with data eng. All I had was just course on the concept of ETL Data Warehousing. Previous experience role: Developer Analyst in my companies logistics domain working with our Global WMS solution.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Test was a lot harder than expected. The exam consisted of curveballs and trick questions which made consistently second guess myself.  I read a couple of other ppls post on this sub that passed the exam and they made it sound like the exam was piece of cake. I think I may have struggled more because I had little to no real life hands on experience in regards to Data Eng. Also, exam went into more detail in regards to syntax as well as security and access. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Prep:&lt;/strong&gt; I prepared for about a month. Below is what I did&lt;/p&gt;\n\n&lt;p&gt;1)My company payed for me to take the instructor led course offered on the Microsoft website and it was a complete nightmare/waste of time. The course was outsourced to a company named Koenig and they were struggling with the fact that microsoft had updated course content earlier this year in February. The instructor felt scripted and the labs did not match the course content. The instructor also had no real life experience and struggled with scenario based questions. For example, &amp;quot;in which case would this distribution be best&amp;#39;?&amp;#39;... The only good thing about the course is that they later gave me a workaround lab environment with the updated labs which provided me with some simple generic hands on experience. I think besides the lab environments, I would suggest to take the self paced version and go through it in a manner where you understand the content with detail. The level of detail that they covered did not suffice the level of detail that exam questions covered.&lt;/p&gt;\n\n&lt;p&gt;2)Practice test offered on Microsoft test: this were good but again did not match the level of detail that the questions of the exams consisted off. I would **suggest** getting the questions wrong and studying the corresponding links of the knowledge articles that they provide.&lt;/p&gt;\n\n&lt;p&gt;3)Knowledge check questions on the course. Similar to the practice test&lt;/p&gt;\n\n&lt;p&gt;4)Labs on the course . **Recommend**: reading knowledge articles corresponding to each lab&lt;/p&gt;\n\n&lt;p&gt;5)&lt;a href=\"https://www.examtopics.com/exams/microsoft/dp-203\"&gt;www.examtopics.com/exams/microsoft/dp-203&lt;/a&gt; : I didn&amp;#39;t hit this that hard but I would recommend doing so. These are the type of questions that are on the exams.&lt;/p&gt;\n\n&lt;p&gt;Please feel free to reach out if you guys have any questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "124t0qi", "is_robot_indexable": true, "report_reasons": null, "author": "Jpvilla5454", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124t0qi/recently_passed_dp203_certification_exam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124t0qi/recently_passed_dp203_certification_exam/", "subreddit_subscribers": 94698, "created_utc": 1680018157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just got the greenlight to implement airflow. What do you find the most scalable way to use it for ETL?\nDont feel pressured to answer everything. Any advice could help .\n\n Here's some background info:\n\n1. Handling Confidential data, not allowed to be on main ETL with rest of business. \n\n2. We just built a server with a fresh install\n\n3. I am the only data engineer outside ETL team. \n\n4. Using postgres data warehouse for this project.\n\n5. 6 different source systems, with API or SFTP drops.\n\n6. I usually  Use psycopg2 execute values because it's super fast. Pandas to SQL always seems too slow, but I love pandas for transformations before loading. I convert df to tuple value for speed.\n\nNow, here are my questions on extract and load:\n\nIn airflow, using python operator:\n\nHow do you structure the tasks within your DAG? Do you build the ETL all in a single task? Or do you split the extract, transform, load into different  tasks and make them a task group? If you don't need to transform (outside of selecting which fields you want from the API json format) is it cool to just put it all in one task? \n\nIf you need data from 10 different methods from a rest API , and are just doing a raw data pull, I am trying to determine if that should be 10,20 or 30 tasks. \n\nFor the loads in your DAG, are you passing the same connection to each of your functions, or are your tasks opening and closing the connections as they complete? ( Committing along the way)\n\nDo you use bash operator at all? Bash operator would be nice because we have a ton of stuff on cron and all of the scripts are built to run pull and load raw data. We could just swap to a different scheduler. At the same time, I have freedom and time to make this an ideal implementation. \n\n\n\n\nTransformations:\n\nNot allowed to use dbt, so the team before me made a bunch of view on view relationships to do transforms. They are BI analysts with no real tech support until now. I plan on using dbt cli in dev to re create the views as tables (or ephemeral if not used in any downstream queries and tables) and then stealing the create table scripts out of the target folder as a starting place and tweaking, adding some referential integrity, etc. \n\nDbt was really nice with scheduling transformations and building DAGs for them as you go. Airflow seems more manual (but way better than cron or view on view). How do you manage tons of different SQL based transformations? Are you using python operator to werap your SQL in ? or are you executing SQL files via bash operator?\n\nAnd I guess I'll leave it there.", "author_fullname": "t2_7mo0tj9f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most ideal Airflow task structure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124mxjc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680005842.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just got the greenlight to implement airflow. What do you find the most scalable way to use it for ETL?\nDont feel pressured to answer everything. Any advice could help .&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s some background info:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Handling Confidential data, not allowed to be on main ETL with rest of business. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We just built a server with a fresh install&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I am the only data engineer outside ETL team. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using postgres data warehouse for this project.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;6 different source systems, with API or SFTP drops.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I usually  Use psycopg2 execute values because it&amp;#39;s super fast. Pandas to SQL always seems too slow, but I love pandas for transformations before loading. I convert df to tuple value for speed.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, here are my questions on extract and load:&lt;/p&gt;\n\n&lt;p&gt;In airflow, using python operator:&lt;/p&gt;\n\n&lt;p&gt;How do you structure the tasks within your DAG? Do you build the ETL all in a single task? Or do you split the extract, transform, load into different  tasks and make them a task group? If you don&amp;#39;t need to transform (outside of selecting which fields you want from the API json format) is it cool to just put it all in one task? &lt;/p&gt;\n\n&lt;p&gt;If you need data from 10 different methods from a rest API , and are just doing a raw data pull, I am trying to determine if that should be 10,20 or 30 tasks. &lt;/p&gt;\n\n&lt;p&gt;For the loads in your DAG, are you passing the same connection to each of your functions, or are your tasks opening and closing the connections as they complete? ( Committing along the way)&lt;/p&gt;\n\n&lt;p&gt;Do you use bash operator at all? Bash operator would be nice because we have a ton of stuff on cron and all of the scripts are built to run pull and load raw data. We could just swap to a different scheduler. At the same time, I have freedom and time to make this an ideal implementation. &lt;/p&gt;\n\n&lt;p&gt;Transformations:&lt;/p&gt;\n\n&lt;p&gt;Not allowed to use dbt, so the team before me made a bunch of view on view relationships to do transforms. They are BI analysts with no real tech support until now. I plan on using dbt cli in dev to re create the views as tables (or ephemeral if not used in any downstream queries and tables) and then stealing the create table scripts out of the target folder as a starting place and tweaking, adding some referential integrity, etc. &lt;/p&gt;\n\n&lt;p&gt;Dbt was really nice with scheduling transformations and building DAGs for them as you go. Airflow seems more manual (but way better than cron or view on view). How do you manage tons of different SQL based transformations? Are you using python operator to werap your SQL in ? or are you executing SQL files via bash operator?&lt;/p&gt;\n\n&lt;p&gt;And I guess I&amp;#39;ll leave it there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124mxjc", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial_Ranger_5", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124mxjc/most_ideal_airflow_task_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124mxjc/most_ideal_airflow_task_structure/", "subreddit_subscribers": 94698, "created_utc": 1680005842.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to get some tips or guidance regarding tools, pipeline or resources on building a simple datapipeline that runs daily and fetches the data from an sql database.\n\nWhere I have the most questions is after pulling the data from the sql database, and transform it and load it into s3, how can I make sure that the next day the new records added will be pulled and inserted into its respective tables? I don't think well built pipelines fetch all the tables on a daily basis, but the new information. (How to keep track of new records and old ones that were edited) \n\nThank you so much for your time", "author_fullname": "t2_5x9e117l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to insert new updates to dwh from transactional database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124ebk0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679981134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to get some tips or guidance regarding tools, pipeline or resources on building a simple datapipeline that runs daily and fetches the data from an sql database.&lt;/p&gt;\n\n&lt;p&gt;Where I have the most questions is after pulling the data from the sql database, and transform it and load it into s3, how can I make sure that the next day the new records added will be pulled and inserted into its respective tables? I don&amp;#39;t think well built pipelines fetch all the tables on a daily basis, but the new information. (How to keep track of new records and old ones that were edited) &lt;/p&gt;\n\n&lt;p&gt;Thank you so much for your time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124ebk0", "is_robot_indexable": true, "report_reasons": null, "author": "readoyniando", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124ebk0/how_to_insert_new_updates_to_dwh_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124ebk0/how_to_insert_new_updates_to_dwh_from/", "subreddit_subscribers": 94698, "created_utc": 1679981134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a new initiative, and I need a data visualization tool that can be syndicated out to internal teams as well as external 3rd parties (business partners) to share common dashboards.\n\nI have not used Looker in a loooooong time (Pre-Google acquisition), so I am taking a fresh look at the new [https://datastudio.withgoogle.com/](https://datastudio.withgoogle.com/) thingy.  \nAre people using it in this kind of set-up? \n\nIt would be a high latency (\\~ twice daily refresh) setup with just a handful of KPIs on the dash.", "author_fullname": "t2_pl4q8ng7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What happened to Looker/Google Data Studio?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123tojm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679937452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a new initiative, and I need a data visualization tool that can be syndicated out to internal teams as well as external 3rd parties (business partners) to share common dashboards.&lt;/p&gt;\n\n&lt;p&gt;I have not used Looker in a loooooong time (Pre-Google acquisition), so I am taking a fresh look at the new &lt;a href=\"https://datastudio.withgoogle.com/\"&gt;https://datastudio.withgoogle.com/&lt;/a&gt; thingy.&lt;br/&gt;\nAre people using it in this kind of set-up? &lt;/p&gt;\n\n&lt;p&gt;It would be a high latency (~ twice daily refresh) setup with just a handful of KPIs on the dash.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "123tojm", "is_robot_indexable": true, "report_reasons": null, "author": "droppedorphan", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/123tojm/what_happened_to_lookergoogle_data_studio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/123tojm/what_happened_to_lookergoogle_data_studio/", "subreddit_subscribers": 94698, "created_utc": 1679937452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have sql code that turns a column into a list of values. \n\nSelect array_agg(distinct resortcode) as list_of_resorts \nfrom res_restrict \nWhere list name in (\u2018je\u2019, \u2018ps\u2019);\n\nI want to set a variable in Snowflake to reference this list in later queries in my worksheet.  Example -&gt; \u201cWhen resort_id not in $list_of_resorts\u201d instead of hard coding the the list of resorts \n\nThanks", "author_fullname": "t2_7xrbvyn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake SQL - variable to store a list of values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123x96w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679944722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have sql code that turns a column into a list of values. &lt;/p&gt;\n\n&lt;p&gt;Select array_agg(distinct resortcode) as list_of_resorts \nfrom res_restrict \nWhere list name in (\u2018je\u2019, \u2018ps\u2019);&lt;/p&gt;\n\n&lt;p&gt;I want to set a variable in Snowflake to reference this list in later queries in my worksheet.  Example -&amp;gt; \u201cWhen resort_id not in $list_of_resorts\u201d instead of hard coding the the list of resorts &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "123x96w", "is_robot_indexable": true, "report_reasons": null, "author": "KMG3IU", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/123x96w/snowflake_sql_variable_to_store_a_list_of_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/123x96w/snowflake_sql_variable_to_store_a_list_of_values/", "subreddit_subscribers": 94698, "created_utc": 1679944722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_20a1cwjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting Data from Wikidata Using SPARQL and Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_123vw7d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/b9i_Vv35fsfz7a5uQgZekF1l8Pd2gbSchgq7_NgN2M8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679941992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@jelle.vankerkvoorde/extracting-data-from-wikidata-using-sparql-and-python-59e0037996f", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/W18wIqajhVaQ4QS07umrXEIWHxgQPmFJ9ne3JZatAKc.jpg?auto=webp&amp;v=enabled&amp;s=694ec3da828a3bbbc14a0f430f98694efd741d66", "width": 960, "height": 540}, "resolutions": [{"url": "https://external-preview.redd.it/W18wIqajhVaQ4QS07umrXEIWHxgQPmFJ9ne3JZatAKc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c30b51cf895b75d2ca0924e5012462104163f3f", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/W18wIqajhVaQ4QS07umrXEIWHxgQPmFJ9ne3JZatAKc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=200ed081790522c6a86dbcef8aec6f091d784813", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/W18wIqajhVaQ4QS07umrXEIWHxgQPmFJ9ne3JZatAKc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e4ef087c1aaffea665b395faecd5a12f7275dce", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/W18wIqajhVaQ4QS07umrXEIWHxgQPmFJ9ne3JZatAKc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6960866b9cfe547403cdd4112a5790001ca4641e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/W18wIqajhVaQ4QS07umrXEIWHxgQPmFJ9ne3JZatAKc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f454652383797a23f4cc9493376712b9da85d6f0", "width": 960, "height": 540}], "variants": {}, "id": "1tu1M-UZ6LVJB9xkVokCrmIdNeD935qI6tYyQkiRgRw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "123vw7d", "is_robot_indexable": true, "report_reasons": null, "author": "jkspiderdog", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/123vw7d/extracting_data_from_wikidata_using_sparql_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@jelle.vankerkvoorde/extracting-data-from-wikidata-using-sparql-and-python-59e0037996f", "subreddit_subscribers": 94698, "created_utc": 1679941992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI like to do a personal project and welcome any advice, feedback, recommendations. First, an intro so you know my starting position.\n\n\\- I work in Data Analytics, Management, Governance, including the quality control, process improvement, stakeholder management, and creating internal tools.  \n\\- I've always been interested in the tech/IT side and truly excited and fascinated about all the available stuff these days, which is one of two reasons for this project  \n\\- I did research on 100 data-related manager/lead/director job openings (in my region) and one of the things that was mentioned in 78% of them was \"... knowledge and experiences of data pipelines, how they work, what every part does (on high level), and able to see opportunities to make it more effective, better quality checks, better security, and maximize the value\"; I fully agree with this requirement because even though I might not be the one who will actually do all this stuff, I find it important to understand what the department does, and what my IT colleagues are telling me.\n\n**So what am I thinking off (ballpark)**\n\n\\- not a giant spaghetti that will likely become an untamable beast to complete, but not a super small one just to guarantee success, I like a challenge and it needs to be valuable and relevant to today's business challenges  \n\\- something modular, so I can start with one part and at the end, it works, then move to a second part to expand the pipeline, not necessarily linear, but perhaps a side-chain  \n\\- a bit with automation and a bit with automated SQL transformation  \n\\- a bit with manual input, maybe to validate data which triggers different flows  \n\\- creating custom datasets specifically for this project (but I don't need a million datapoints please)  \n\\- a pipeline that is intentionally not 1000% optimized, I want it to reflect the common situation of companies growing their pipelines along the way to the point that it's just barely/acceptable to manage lol (you know what I mean) \n\n**What do I have available?**\n\n\\- MS 365 + Power Platform  \n\\- PostgreSQL and MySQL in cloud, plus dbeaver  \n\\- Retool\n\n**What Have I been looking at?**\n\n\\- Fivetran for both orchestration and transformation\n\n**What I want to exclude**\n\n\\- anything that requires me to be a programmer, I'm definitely not and will likely drown in the project due to the scripting part\n\n**What I am asking you**\n\nI've been trying to come up with the project content for two months and just drawing blank, coming up with something from scratch has never been my strong suit. I like to ask you for ideas.\n\nYou can throw anything to me: words, stories, bullet-points, even (or especially) sketches of pipelines that you ~~recommend~~ challenge me to do.\n\nThanks for reading and taking the effort to help.\n\nCheers.", "author_fullname": "t2_kh3ubtce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Doing a personal project as a non-DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_124v675", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680022693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I like to do a personal project and welcome any advice, feedback, recommendations. First, an intro so you know my starting position.&lt;/p&gt;\n\n&lt;p&gt;- I work in Data Analytics, Management, Governance, including the quality control, process improvement, stakeholder management, and creating internal tools.&lt;br/&gt;\n- I&amp;#39;ve always been interested in the tech/IT side and truly excited and fascinated about all the available stuff these days, which is one of two reasons for this project&lt;br/&gt;\n- I did research on 100 data-related manager/lead/director job openings (in my region) and one of the things that was mentioned in 78% of them was &amp;quot;... knowledge and experiences of data pipelines, how they work, what every part does (on high level), and able to see opportunities to make it more effective, better quality checks, better security, and maximize the value&amp;quot;; I fully agree with this requirement because even though I might not be the one who will actually do all this stuff, I find it important to understand what the department does, and what my IT colleagues are telling me.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So what am I thinking off (ballpark)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- not a giant spaghetti that will likely become an untamable beast to complete, but not a super small one just to guarantee success, I like a challenge and it needs to be valuable and relevant to today&amp;#39;s business challenges&lt;br/&gt;\n- something modular, so I can start with one part and at the end, it works, then move to a second part to expand the pipeline, not necessarily linear, but perhaps a side-chain&lt;br/&gt;\n- a bit with automation and a bit with automated SQL transformation&lt;br/&gt;\n- a bit with manual input, maybe to validate data which triggers different flows&lt;br/&gt;\n- creating custom datasets specifically for this project (but I don&amp;#39;t need a million datapoints please)&lt;br/&gt;\n- a pipeline that is intentionally not 1000% optimized, I want it to reflect the common situation of companies growing their pipelines along the way to the point that it&amp;#39;s just barely/acceptable to manage lol (you know what I mean) &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What do I have available?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- MS 365 + Power Platform&lt;br/&gt;\n- PostgreSQL and MySQL in cloud, plus dbeaver&lt;br/&gt;\n- Retool&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What Have I been looking at?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Fivetran for both orchestration and transformation&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I want to exclude&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- anything that requires me to be a programmer, I&amp;#39;m definitely not and will likely drown in the project due to the scripting part&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I am asking you&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to come up with the project content for two months and just drawing blank, coming up with something from scratch has never been my strong suit. I like to ask you for ideas.&lt;/p&gt;\n\n&lt;p&gt;You can throw anything to me: words, stories, bullet-points, even (or especially) sketches of pipelines that you &lt;del&gt;recommend&lt;/del&gt; challenge me to do.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading and taking the effort to help.&lt;/p&gt;\n\n&lt;p&gt;Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "124v675", "is_robot_indexable": true, "report_reasons": null, "author": "SquidsAndMartians", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124v675/doing_a_personal_project_as_a_nonde/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124v675/doing_a_personal_project_as_a_nonde/", "subreddit_subscribers": 94698, "created_utc": 1680022693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modeling, Visualizing, and Navigating a Transportation Network with Memgraph", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_124nrnn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PIL_MqQBQnNitwsKOx28oRm-wvsdrXPtNVHxFxIuF3o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680007823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memgraph.com/blog/modeling-visualizing-navigating-a-transportation-network-with-memgraph", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?auto=webp&amp;v=enabled&amp;s=0c9f1b23bba282af67a247becd39519c0edbe17c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14ff056a80c39be180153fce59b2081bdeba8410", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8077d07db4e4c707984c73abab7eaee45f9f7c0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0d59358e6ad575e0a5713660b78fd51b3b02f84", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68774c1f2e0e7d990b9f90069676d51b0d9f9cd1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4e0dc784f7bdb785a87fd524d6b846b89bf524f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47361b6a6eb55fc89a8d1c50c67dbc1b5c29eab5", "width": 1080, "height": 540}], "variants": {}, "id": "N-DWmh8WkOxwsiQVxILtoW-lg1gVclggDSKUbODSu2M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "124nrnn", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124nrnn/modeling_visualizing_and_navigating_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memgraph.com/blog/modeling-visualizing-navigating-a-transportation-network-with-memgraph", "subreddit_subscribers": 94698, "created_utc": 1680007823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'll get to the point. I am taking as a premise that Sketch Engine and AntConc, which are commonly used for concordance searches, lie within the realm of data engineering inasmuch as they facilitate data. My understanding is that both accept data in XML, meaning that you can bring in a linguistic corpus with a predefined scheme for the tags and the software knows how to handle this. Am I correct about this?   \nEven if it's not the two examples I've noted, is it true that there is software which allows for data analysis (of varying sophistication) where the input is an XML document with its schema?  \n\n\nMany thanks in advance!", "author_fullname": "t2_graxcz2q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Quick Question] How do software such as Sketch Engine and AntConc work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124ndnm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680006917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll get to the point. I am taking as a premise that Sketch Engine and AntConc, which are commonly used for concordance searches, lie within the realm of data engineering inasmuch as they facilitate data. My understanding is that both accept data in XML, meaning that you can bring in a linguistic corpus with a predefined scheme for the tags and the software knows how to handle this. Am I correct about this?&lt;br/&gt;\nEven if it&amp;#39;s not the two examples I&amp;#39;ve noted, is it true that there is software which allows for data analysis (of varying sophistication) where the input is an XML document with its schema?  &lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124ndnm", "is_robot_indexable": true, "report_reasons": null, "author": "alpolvovolvere", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124ndnm/quick_question_how_do_software_such_as_sketch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124ndnm/quick_question_how_do_software_such_as_sketch/", "subreddit_subscribers": 94698, "created_utc": 1680006917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My expectations:\n\n&amp;#x200B;\n\n1. Open Source\n2. Relatively simple\n3. Support for SparQL", "author_fullname": "t2_8w2gvpfk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please recommend a graph DB like RedisGraph but with SparQL support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124kfrg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679999665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My expectations:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Open Source&lt;/li&gt;\n&lt;li&gt;Relatively simple&lt;/li&gt;\n&lt;li&gt;Support for SparQL&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124kfrg", "is_robot_indexable": true, "report_reasons": null, "author": "Vitaly_v_ch", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124kfrg/please_recommend_a_graph_db_like_redisgraph_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124kfrg/please_recommend_a_graph_db_like_redisgraph_but/", "subreddit_subscribers": 94698, "created_utc": 1679999665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi.  Over the last few months we have been developing an open source CI/CD platform for dbt.\n\nThough it's possible to do CI/CD and orchestration with Gitlab, Jenkins, Airflow etc, I felt there was an opportunity to do something specific for dbt.  This would reduce the need to build a custom pipeline and mean that we could build a GUI that is tailored to Data Engineers.  \n\nThe bigger aim is also to help Data Engineers adopt a strict development workflow, with pull requests, environments, source control, CI/CD etc as sometimes this is lacking.  \n\nI would be really interested in any constructive feedback and thoughts on the idea and the more general problem of CI/CD with dbt and data transformation code.\n\nDemo video here - [https://www.youtube.com/watch?v=-s-bULKecvY&amp;t=64s](https://www.youtube.com/watch?v=-s-bULKecvY&amp;t=64s)\n\nIntroductory blog post here - [https://timeflow.systems/blog/introducing-timeflow](https://timeflow.systems/blog/introducing-timeflow)", "author_fullname": "t2_599jquwy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interested in thoughts on a CI/CD platform for dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124jhu3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679996823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.  Over the last few months we have been developing an open source CI/CD platform for dbt.&lt;/p&gt;\n\n&lt;p&gt;Though it&amp;#39;s possible to do CI/CD and orchestration with Gitlab, Jenkins, Airflow etc, I felt there was an opportunity to do something specific for dbt.  This would reduce the need to build a custom pipeline and mean that we could build a GUI that is tailored to Data Engineers.  &lt;/p&gt;\n\n&lt;p&gt;The bigger aim is also to help Data Engineers adopt a strict development workflow, with pull requests, environments, source control, CI/CD etc as sometimes this is lacking.  &lt;/p&gt;\n\n&lt;p&gt;I would be really interested in any constructive feedback and thoughts on the idea and the more general problem of CI/CD with dbt and data transformation code.&lt;/p&gt;\n\n&lt;p&gt;Demo video here - &lt;a href=\"https://www.youtube.com/watch?v=-s-bULKecvY&amp;amp;t=64s\"&gt;https://www.youtube.com/watch?v=-s-bULKecvY&amp;amp;t=64s&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Introductory blog post here - &lt;a href=\"https://timeflow.systems/blog/introducing-timeflow\"&gt;https://timeflow.systems/blog/introducing-timeflow&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wUvpuFvCbiBrKmgzAE51FmYH7I8w9Z8Oy7ssZvxNmMw.jpg?auto=webp&amp;v=enabled&amp;s=f3f3b026ff445ae93f720d470dbdbbd81bcd6d89", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/wUvpuFvCbiBrKmgzAE51FmYH7I8w9Z8Oy7ssZvxNmMw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf8f7ed84a73d257380e708f1bca6ab553436dfb", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/wUvpuFvCbiBrKmgzAE51FmYH7I8w9Z8Oy7ssZvxNmMw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8a148459d5a3f1fd8bb082cd8e79c00fa55e844", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/wUvpuFvCbiBrKmgzAE51FmYH7I8w9Z8Oy7ssZvxNmMw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81794c66641227265b59927a713c6b8eed4dcdf4", "width": 320, "height": 240}], "variants": {}, "id": "Bk_76OB7iZZqNzV4J6Xz67nCfxKHvNH4IGGQd2J6RVo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "124jhu3", "is_robot_indexable": true, "report_reasons": null, "author": "benjaminwootton888", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124jhu3/interested_in_thoughts_on_a_cicd_platform_for_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124jhu3/interested_in_thoughts_on_a_cicd_platform_for_dbt/", "subreddit_subscribers": 94698, "created_utc": 1679996823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,.\nWhat could be the best way to built Zoho CRM or any other crm streaming data pipeline inside gcp?\nI am thinking to use bigquery as a data warehouse. But confused about which tool to use or which will be better and cost efficient in this case. Any suggestions will be appreciated.", "author_fullname": "t2_vkmvzdm7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline for Zoho/other CRM in gcp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124ira5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679994579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,.\nWhat could be the best way to built Zoho CRM or any other crm streaming data pipeline inside gcp?\nI am thinking to use bigquery as a data warehouse. But confused about which tool to use or which will be better and cost efficient in this case. Any suggestions will be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124ira5", "is_robot_indexable": true, "report_reasons": null, "author": "shaikh21", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124ira5/data_pipeline_for_zohoother_crm_in_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124ira5/data_pipeline_for_zohoother_crm_in_gcp/", "subreddit_subscribers": 94698, "created_utc": 1679994579.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}