{"kind": "Listing", "data": {"after": "t3_123l2xs", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1sg4r0ip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data hoarding is older than we thought! MAD Magazine 215 from 1980", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 58, "top_awarded_type": null, "hide_score": false, "name": "t3_123w808", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 1344, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 1344, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DyWr-OUsjo-LrfIC3NbmVrq_4GrjuCEyIO7Qwki-2pY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679942659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/pgh70h2dubqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/pgh70h2dubqa1.png?auto=webp&amp;v=enabled&amp;s=d3c24d6005948561a54e232c18a44c894e698e61", "width": 979, "height": 407}, "resolutions": [{"url": "https://preview.redd.it/pgh70h2dubqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6228b30804ff7a35cc7f7fbc4a0bb032bef8537", "width": 108, "height": 44}, {"url": "https://preview.redd.it/pgh70h2dubqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8ecc378bb39266037437737a214d2764ad11fd8", "width": 216, "height": 89}, {"url": "https://preview.redd.it/pgh70h2dubqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a404abd8dce6591fcd9acfc33e8e307a186bb2c", "width": 320, "height": 133}, {"url": "https://preview.redd.it/pgh70h2dubqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3edad980b0b337de7d0b6081147155e3dd60d719", "width": 640, "height": 266}, {"url": "https://preview.redd.it/pgh70h2dubqa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f609192eeef539443b05919a7cd1c41a0d0b105", "width": 960, "height": 399}], "variants": {}, "id": "4aZpRgFzCZQYsqoLMMYIkU25nRUwKGJQW7vwOeeqhwQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123w808", "is_robot_indexable": true, "report_reasons": null, "author": "Hong-Hong-Hang-Hang", "discussion_type": null, "num_comments": 71, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123w808/data_hoarding_is_older_than_we_thought_mad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/pgh70h2dubqa1.png", "subreddit_subscribers": 675683, "created_utc": 1679942659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_d5sfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smr hard drive at 100% active time, copying small files, but still at 100% when file copy is paused. It stays like this until I reset the machine and causes other programs to hang. Is this normal for smr?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 101, "top_awarded_type": null, "hide_score": false, "name": "t3_123g0jl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 142, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 142, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DY6JK2XWJG2bzsLiU-DqFgu_V3jUD-U_8b7yq20u0Rc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679904482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8j8jl62yo8qa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?auto=webp&amp;v=enabled&amp;s=3722830cb77cbeea00cd9571efbaf1bd505548fb", "width": 1610, "height": 1163}, "resolutions": [{"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=452a1042b19fb2d9600aef249e07a006f1d8433a", "width": 108, "height": 78}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe7e15d8aa133955b2f4c3239be38f2885ac5417", "width": 216, "height": 156}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efad0366954076f53ecce0fe01fa12530583dd39", "width": 320, "height": 231}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19bb55f8503cb77b9c676745911b7f1b168072fd", "width": 640, "height": 462}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91e470f5180ff6beaf0571157ded6b1cfe8a8309", "width": 960, "height": 693}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e364e68cbe7403aff77612aea4ac37c68f8ac590", "width": 1080, "height": 780}], "variants": {}, "id": "DB_ZhI18qR7656g_EQRuq-TN-_nrrehZX9OTOGY7RDc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123g0jl", "is_robot_indexable": true, "report_reasons": null, "author": "bluejeans90210", "discussion_type": null, "num_comments": 99, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123g0jl/smr_hard_drive_at_100_active_time_copying_small/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8j8jl62yo8qa1.png", "subreddit_subscribers": 675683, "created_utc": 1679904482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I heard backblaze is a very good option for a backing up one\u2019s PC, however, I also noticed that a lot of people complain about the restore process once their drive fails or whatever mishaps happen. Are there any other alternatives that offer a better restore process? I\u2019m fairly new to backing up stuff (lost precious memories before and had to learn it the hard way\u2026) but I always keep a copy of my PC files/data on an external SSD. I definitely want to have more copies of my data, not just in an external drive as everything is prone to failure/degradation/etc. I was thinking of using backblaze and sync.com (one for pure backup and the other for syncing) but I\u2019m undecided on backblaze.", "author_fullname": "t2_4wwjb46d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on backblaze for backing up a PC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1239123", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": 1679886355.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679885669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I heard backblaze is a very good option for a backing up one\u2019s PC, however, I also noticed that a lot of people complain about the restore process once their drive fails or whatever mishaps happen. Are there any other alternatives that offer a better restore process? I\u2019m fairly new to backing up stuff (lost precious memories before and had to learn it the hard way\u2026) but I always keep a copy of my PC files/data on an external SSD. I definitely want to have more copies of my data, not just in an external drive as everything is prone to failure/degradation/etc. I was thinking of using backblaze and sync.com (one for pure backup and the other for syncing) but I\u2019m undecided on backblaze.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1239123", "is_robot_indexable": true, "report_reasons": null, "author": "calpthemcheeks", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1239123/thoughts_on_backblaze_for_backing_up_a_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1239123/thoughts_on_backblaze_for_backing_up_a_pc/", "subreddit_subscribers": 675683, "created_utc": 1679885669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\"Counter-Strike 2 will replace CS:GO once the game officially launches, a departure from previous updates to the Counter-Strike franchise which saw multiple versions of the game \u2014 and esports scenes \u2014 run simultaneously.\"\n\nhttps://esportsinsider.com/2023/03/counter-strike-2\n\nValve is calling this more of an \"upgrade\" than anything, but the fact that the original CS:GO will no longer be available doesn't sit right with me... \n\nFor those of you that don't know, CS:GO is a Source 1 game which is now free to play. Source is the engine Half Life 2 was built on. \n\nCS2 is based on Source 2, which is a newer engine released by Valve in 2015. \n\nHope some hero out there gets CS:GO working independently of Steam so that it can be preserved in it's current state. \n\nAlso: RIP to all the poor gamers out there who can barely run CS:GO as is, let alone CS2.", "author_fullname": "t2_cqz2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Counter Strike 2 is a replacement for Global Offensive, and CS:GO will no longer be available after CS2 drops.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123v9iw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679940706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Counter-Strike 2 will replace CS:GO once the game officially launches, a departure from previous updates to the Counter-Strike franchise which saw multiple versions of the game \u2014 and esports scenes \u2014 run simultaneously.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://esportsinsider.com/2023/03/counter-strike-2\"&gt;https://esportsinsider.com/2023/03/counter-strike-2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Valve is calling this more of an &amp;quot;upgrade&amp;quot; than anything, but the fact that the original CS:GO will no longer be available doesn&amp;#39;t sit right with me... &lt;/p&gt;\n\n&lt;p&gt;For those of you that don&amp;#39;t know, CS:GO is a Source 1 game which is now free to play. Source is the engine Half Life 2 was built on. &lt;/p&gt;\n\n&lt;p&gt;CS2 is based on Source 2, which is a newer engine released by Valve in 2015. &lt;/p&gt;\n\n&lt;p&gt;Hope some hero out there gets CS:GO working independently of Steam so that it can be preserved in it&amp;#39;s current state. &lt;/p&gt;\n\n&lt;p&gt;Also: RIP to all the poor gamers out there who can barely run CS:GO as is, let alone CS2.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?auto=webp&amp;v=enabled&amp;s=52f8e57846df3d9d3adc539d3629cf2028afb797", "width": 1920, "height": 949}, "resolutions": [{"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f19d470bbb7e8f395e8238e8d56a24ec47b71913", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c73209fe0cb0da492e73b1ef77114aa67e9ba442", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3b18ae3ee5cc11acdee1ba635a6dee92e81e025", "width": 320, "height": 158}, {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5550f65eedfc990833c8148d537b3818f48988ec", "width": 640, "height": 316}, {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc0a55a67c606d5fe64a579a4a07b58b926c8ae0", "width": 960, "height": 474}, {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1eab0a37d6733ff85e95311b7d49adf8308a5f3", "width": 1080, "height": 533}], "variants": {}, "id": "TYX9lXDOlTmcK5BrzHpz7T7XsXL1CaeKuHxzopEmf_g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123v9iw", "is_robot_indexable": true, "report_reasons": null, "author": "blackletum", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123v9iw/counter_strike_2_is_a_replacement_for_global/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123v9iw/counter_strike_2_is_a_replacement_for_global/", "subreddit_subscribers": 675683, "created_utc": 1679940706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been writing some scripts for my own personal use recently due to an interesting use-case. I have data that I want to store on one machine in one format, and multiple target machines that I want to sync the data to in slightly varying formats. If the source files change, I want to resync and re-transform the files into the proper formats. The result of weeks of polishing these scripts up is this budding new GitHub project of mine, that I am calling (for now, anyway) \"Dumb-Sync\".\n\nMain page and README: [https://github.com/sloshy/dumb-sync](https://github.com/sloshy/dumb-sync)\n\nLicense: LGPL v2.1 (subject to change)\n\nBasically the idea is this: you create a JSON file that describes one or more remote directories you wish to sync to your local machine. From there, you can optionally wire in multiple transformation steps that just exist as bash scripts. Each of these scripts can take any config arguments you want, specified in each JSON config or dynamically from current runtime values (i.e. the current file name, output directory, list of all known remote files). You can set these scripts to be ran once per-file, or once per-directory,  and you can even include your own comparison logic scripts for determining if a transformed file should be synced, kept in case it's removed on a remote system, and so on.\n\nIn the [example transformations directory](https://github.com/sloshy/dumb-sync/tree/main/example_transformations) there is an example config that illustrates the following scenario:\n\n* You have a remote directory full of ZIP files.\n* Inside each ZIP file are BIN/CUE files representing lossless disc images.\n* You want to sync only the ZIP files from that directory to your local machine.\n* Once per-file, you want to extract all of its contents to the current output directory using 7Zip.\n* After they are extracted, you want to re-compress them in the CHD format, which is from the MAME project for creating real-time playable disc images that take up less space.\n* If any files exist afterward that are not meant to be there, delete them on each re-sync.\n\nWith the example config, after each transformation, the local files are only ever re-synced if the remote they are derived from changes. A time offset is stored for your config that displays the latest time a sync was attempted. This might get options to be more granular in the future, but for now it is just a single timestamp for all configs.\n\nThe scripts are still being regularly updated, as I just released this publicly over the weekend, and it will likely gain several more features as time goes on. Just recently I added support for specifying minimum and maximum file size thresholds for syncing. The general idea is that eventually this will encompass most of the meaningful featureset of \"rsync\" which this uses internally, and grow to be even more flexible in ways that people find useful. For example, I would love a way for people to share template configs that other people can plug into their own in order to import scripts and even sync configurations automatically or using parameter options. For now, this mostly just covers my own immediate use cases though.\n\nLet me know what you think and please feel free to report any bugs or feature requests!", "author_fullname": "t2_cb3vo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dumb-Sync: A bash/rsync/jq-based microframework for syncing transformed files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123d8q7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679896800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been writing some scripts for my own personal use recently due to an interesting use-case. I have data that I want to store on one machine in one format, and multiple target machines that I want to sync the data to in slightly varying formats. If the source files change, I want to resync and re-transform the files into the proper formats. The result of weeks of polishing these scripts up is this budding new GitHub project of mine, that I am calling (for now, anyway) &amp;quot;Dumb-Sync&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Main page and README: &lt;a href=\"https://github.com/sloshy/dumb-sync\"&gt;https://github.com/sloshy/dumb-sync&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;License: LGPL v2.1 (subject to change)&lt;/p&gt;\n\n&lt;p&gt;Basically the idea is this: you create a JSON file that describes one or more remote directories you wish to sync to your local machine. From there, you can optionally wire in multiple transformation steps that just exist as bash scripts. Each of these scripts can take any config arguments you want, specified in each JSON config or dynamically from current runtime values (i.e. the current file name, output directory, list of all known remote files). You can set these scripts to be ran once per-file, or once per-directory,  and you can even include your own comparison logic scripts for determining if a transformed file should be synced, kept in case it&amp;#39;s removed on a remote system, and so on.&lt;/p&gt;\n\n&lt;p&gt;In the &lt;a href=\"https://github.com/sloshy/dumb-sync/tree/main/example_transformations\"&gt;example transformations directory&lt;/a&gt; there is an example config that illustrates the following scenario:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You have a remote directory full of ZIP files.&lt;/li&gt;\n&lt;li&gt;Inside each ZIP file are BIN/CUE files representing lossless disc images.&lt;/li&gt;\n&lt;li&gt;You want to sync only the ZIP files from that directory to your local machine.&lt;/li&gt;\n&lt;li&gt;Once per-file, you want to extract all of its contents to the current output directory using 7Zip.&lt;/li&gt;\n&lt;li&gt;After they are extracted, you want to re-compress them in the CHD format, which is from the MAME project for creating real-time playable disc images that take up less space.&lt;/li&gt;\n&lt;li&gt;If any files exist afterward that are not meant to be there, delete them on each re-sync.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;With the example config, after each transformation, the local files are only ever re-synced if the remote they are derived from changes. A time offset is stored for your config that displays the latest time a sync was attempted. This might get options to be more granular in the future, but for now it is just a single timestamp for all configs.&lt;/p&gt;\n\n&lt;p&gt;The scripts are still being regularly updated, as I just released this publicly over the weekend, and it will likely gain several more features as time goes on. Just recently I added support for specifying minimum and maximum file size thresholds for syncing. The general idea is that eventually this will encompass most of the meaningful featureset of &amp;quot;rsync&amp;quot; which this uses internally, and grow to be even more flexible in ways that people find useful. For example, I would love a way for people to share template configs that other people can plug into their own in order to import scripts and even sync configurations automatically or using parameter options. For now, this mostly just covers my own immediate use cases though.&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think and please feel free to report any bugs or feature requests!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?auto=webp&amp;v=enabled&amp;s=1b05a6503cf7321bcabd6fffc6de3e782c98f09b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=781490f75b82b874929b7369ddfe5d7ccba2ed31", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a87e4648dd1ff81e6507801666027f14ee2a4857", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b179abb9c1b8ebacf4a5d02554c50fc8aea793b7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90c0f920565833f809ed411db25ec817ccb187e8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14c75efb92af99897a5b65a689691238eb7de2a4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f71a5e12e386d900f540bf882b8cda4539d8cc9", "width": 1080, "height": 540}], "variants": {}, "id": "YmfaUbVP1XUFLOtPgkgC3giOyD9givyh-uQPhGgTUPI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123d8q7", "is_robot_indexable": true, "report_reasons": null, "author": "Sloshy42", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123d8q7/dumbsync_a_bashrsyncjqbased_microframework_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123d8q7/dumbsync_a_bashrsyncjqbased_microframework_for/", "subreddit_subscribers": 675683, "created_utc": 1679896800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_169tozec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have about 20k files like this. How do I mass rename to .mp3? I'm on Windows, thank you!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": false, "name": "t3_123utbe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/l3_beH1XxRwgqozLpli8wgsXzLM0DDSrPUScKrjad48.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679939801.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/4gdgtgmtlbqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?auto=webp&amp;v=enabled&amp;s=1e3253bbaed71a2b1ba6bf7be95a9eb8eca7af0c", "width": 651, "height": 101}, "resolutions": [{"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6300da0768860dd6a90e89a2ef4322728f7cc53a", "width": 108, "height": 16}, {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f64122edf79fbe09a74027e590f24484541f614", "width": 216, "height": 33}, {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44c0f63d601b12cb9ea2c51468c1ca798ba239ff", "width": 320, "height": 49}, {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0352d6ec78931b63f442f1a61aed1b510451d074", "width": 640, "height": 99}], "variants": {}, "id": "WCFNRn7Ih8SraNX3YYZNWYsv6-sBSavYoJby_KhdbvA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123utbe", "is_robot_indexable": true, "report_reasons": null, "author": "AutomaticInitiative", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123utbe/i_have_about_20k_files_like_this_how_do_i_mass/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/4gdgtgmtlbqa1.png", "subreddit_subscribers": 675683, "created_utc": 1679939801.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So got my new workstation Thursday.- Part of this setup is a 6 x 18tb set of disks connected to the motherboard's SATA connections.- I am not using the motherboard's RAID\n\nI initially set up the RAID using linux mdadm.  About 80gb into copy there was a array failure, but upon reboot it was fine again.  Resumed copying and after a hour of copying again it failed once more.\n\nI then ran SMART checks on the disks, everything came back reading 100% fine.\n\nThinking this could be a software issue I wiped the array, deleted all the mdadm configs and installed ZFS and set up a pool with the same disks.  This got me a little further, about two terrabytes in the pool went into a offline state but after a reboot it was fine again.\n\nI have checked in dmesg and found several angry comments about failures on sda and sdb, so I then checked against the zpool when I resumed copying expecting it to fail.  The pool agreed stating that sda and sdb had hundreds of write failures.\n\nI have been running badblocks on the two disks since Saturday night, it's already finishing the second pass on the two disks and so far nothing is showing up as bad, zero blocks failed.\n\nThis is my conundrum, all of the disk checks swear that the disks are perfectly fine, but using them makes the OS/array say otherwise.  So far the only thing I could assume is the cables could be defective?  \n\n\nUPDATE: So have been running a zfs cluster without sda and sdb which were erroring.  So far it's been running steady all morning/afternoon, nearly 10tb copied without complaint in dmesg.  Thoughts: if this is a voltage/cable issue, not running the two could be tested by creating a new cluster with sda-sdd, using the two it complained about, but still just a four disk cluster.  If it passes then it's more likely a power issue not a physical cable issue.  Alternately I was thinking of making a two disk array with them and copying from my NVME stick, seeing if it could cause it to still error out.", "author_fullname": "t2_d3vpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mixed messages on a new drive array", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123m7o5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679951191.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679921854.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So got my new workstation Thursday.- Part of this setup is a 6 x 18tb set of disks connected to the motherboard&amp;#39;s SATA connections.- I am not using the motherboard&amp;#39;s RAID&lt;/p&gt;\n\n&lt;p&gt;I initially set up the RAID using linux mdadm.  About 80gb into copy there was a array failure, but upon reboot it was fine again.  Resumed copying and after a hour of copying again it failed once more.&lt;/p&gt;\n\n&lt;p&gt;I then ran SMART checks on the disks, everything came back reading 100% fine.&lt;/p&gt;\n\n&lt;p&gt;Thinking this could be a software issue I wiped the array, deleted all the mdadm configs and installed ZFS and set up a pool with the same disks.  This got me a little further, about two terrabytes in the pool went into a offline state but after a reboot it was fine again.&lt;/p&gt;\n\n&lt;p&gt;I have checked in dmesg and found several angry comments about failures on sda and sdb, so I then checked against the zpool when I resumed copying expecting it to fail.  The pool agreed stating that sda and sdb had hundreds of write failures.&lt;/p&gt;\n\n&lt;p&gt;I have been running badblocks on the two disks since Saturday night, it&amp;#39;s already finishing the second pass on the two disks and so far nothing is showing up as bad, zero blocks failed.&lt;/p&gt;\n\n&lt;p&gt;This is my conundrum, all of the disk checks swear that the disks are perfectly fine, but using them makes the OS/array say otherwise.  So far the only thing I could assume is the cables could be defective?  &lt;/p&gt;\n\n&lt;p&gt;UPDATE: So have been running a zfs cluster without sda and sdb which were erroring.  So far it&amp;#39;s been running steady all morning/afternoon, nearly 10tb copied without complaint in dmesg.  Thoughts: if this is a voltage/cable issue, not running the two could be tested by creating a new cluster with sda-sdd, using the two it complained about, but still just a four disk cluster.  If it passes then it&amp;#39;s more likely a power issue not a physical cable issue.  Alternately I was thinking of making a two disk array with them and copying from my NVME stick, seeing if it could cause it to still error out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123m7o5", "is_robot_indexable": true, "report_reasons": null, "author": "TheIllusioneer", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123m7o5/mixed_messages_on_a_new_drive_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123m7o5/mixed_messages_on_a_new_drive_array/", "subreddit_subscribers": 675683, "created_utc": 1679921854.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone successfully ripped video files (movies, TV shows, etc) onto a BD-R disc and played it on a Blu-Ray player or console?\n\nAny recommendations on best software for ripping/menu creation/misc?\n\nDoes DRM prevent this from working?\n\nI imagine that most people in this sub are doing the opposite. Ripping the Blu's onto PC. I don't know where else to go for this information.", "author_fullname": "t2_8zpyfcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping to Blu-Ray discs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123apwr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679889739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone successfully ripped video files (movies, TV shows, etc) onto a BD-R disc and played it on a Blu-Ray player or console?&lt;/p&gt;\n\n&lt;p&gt;Any recommendations on best software for ripping/menu creation/misc?&lt;/p&gt;\n\n&lt;p&gt;Does DRM prevent this from working?&lt;/p&gt;\n\n&lt;p&gt;I imagine that most people in this sub are doing the opposite. Ripping the Blu&amp;#39;s onto PC. I don&amp;#39;t know where else to go for this information.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123apwr", "is_robot_indexable": true, "report_reasons": null, "author": "HunteHorseman", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123apwr/ripping_to_bluray_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123apwr/ripping_to_bluray_discs/", "subreddit_subscribers": 675683, "created_utc": 1679889739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I may end up getting a bunch of HDDs, possibly 50+, not sure of the size. The last ones were 500 GB I am sure these ones are going to be larger , possibly 1 tb plus.  The PCs are ones that I am planning to give away to local charity but may strip down a few of them to personal use.  Or repurpose. This is PCs from local college.\n\nHere is my dilema.\n\nI currently have 20 HDDs 111,599 GB various sizes. External, internal, Enclosure etc. I currently have room for 3 more HDDS in an enclosure.  And have an  option for quick HDD swaps with one external device that I have.\n\nAll these hard drives are used for backup, movies, pictures, picture archives etc.\n\nAny suggestions on expanding for room for multiple HDD additions.", "author_fullname": "t2_f1bbee6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding New HDDs to my 20 HDD collection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123829e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679883446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I may end up getting a bunch of HDDs, possibly 50+, not sure of the size. The last ones were 500 GB I am sure these ones are going to be larger , possibly 1 tb plus.  The PCs are ones that I am planning to give away to local charity but may strip down a few of them to personal use.  Or repurpose. This is PCs from local college.&lt;/p&gt;\n\n&lt;p&gt;Here is my dilema.&lt;/p&gt;\n\n&lt;p&gt;I currently have 20 HDDs 111,599 GB various sizes. External, internal, Enclosure etc. I currently have room for 3 more HDDS in an enclosure.  And have an  option for quick HDD swaps with one external device that I have.&lt;/p&gt;\n\n&lt;p&gt;All these hard drives are used for backup, movies, pictures, picture archives etc.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions on expanding for room for multiple HDD additions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123829e", "is_robot_indexable": true, "report_reasons": null, "author": "Monkeydu2", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123829e/adding_new_hdds_to_my_20_hdd_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123829e/adding_new_hdds_to_my_20_hdd_collection/", "subreddit_subscribers": 675683, "created_utc": 1679883446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New, store-bought SSD has \u2248800GB worth of instrument software on it", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1244jln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_7k499ptf", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/SXnhf7a8BZctOkPbYBtV2GgaYGU1YsA52lORTTw4_NY.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "pcmasterrace", "selftext": "", "author_fullname": "t2_i5z1jbg1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New, store-bought SSD has \u2248800GB worth of instrument software on it", "link_flair_richtext": [{"e": "text", "t": "Hardware"}], "subreddit_name_prefixed": "r/pcmasterrace", "hidden": false, "pwls": 6, "link_flair_css_class": "blue", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_123lugm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 7819, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hardware", "can_mod_post": false, "score": 7819, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/SXnhf7a8BZctOkPbYBtV2GgaYGU1YsA52lORTTw4_NY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679920871.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/kdma41wp1aqa1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?auto=webp&amp;v=enabled&amp;s=6f7c0e45f9603b7a16f55abd9a55a6469c392995", "width": 992, "height": 998}, "resolutions": [{"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39ba2d87428726d7bb42cbc74c9b8f2c990a5c7c", "width": 108, "height": 108}, {"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a9c8665248cf3aaa2b0b4d5a6caafd716722034", "width": 216, "height": 217}, {"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc6bb4c2172637b9015c3018e0275b3b0340a84c", "width": 320, "height": 321}, {"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5aeee34512c224d1e52e37d4988e57ba231f5e31", "width": 640, "height": 643}, {"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aabb6f6ad0aed52c6b6f8d7d8e8bc607a9bad5b6", "width": 960, "height": 965}], "variants": {}, "id": "59NpmpZ7KqeSnNYe2FaCHVmK9_ehWsRyDKb8XUunyW4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "e35708a2-c51a-11e3-b550-12313b0d38eb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sgp1", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0000ff", "id": "123lugm", "is_robot_indexable": true, "report_reasons": null, "author": "All-Seeing_Hands", "discussion_type": null, "num_comments": 559, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/pcmasterrace/comments/123lugm/new_storebought_ssd_has_800gb_worth_of_instrument/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/kdma41wp1aqa1.jpg", "subreddit_subscribers": 7381121, "created_utc": 1679920871.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1679958937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/kdma41wp1aqa1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?auto=webp&amp;v=enabled&amp;s=6f7c0e45f9603b7a16f55abd9a55a6469c392995", "width": 992, "height": 998}, "resolutions": [{"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39ba2d87428726d7bb42cbc74c9b8f2c990a5c7c", "width": 108, "height": 108}, {"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a9c8665248cf3aaa2b0b4d5a6caafd716722034", "width": 216, "height": 217}, {"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc6bb4c2172637b9015c3018e0275b3b0340a84c", "width": 320, "height": 321}, {"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5aeee34512c224d1e52e37d4988e57ba231f5e31", "width": 640, "height": 643}, {"url": "https://preview.redd.it/kdma41wp1aqa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aabb6f6ad0aed52c6b6f8d7d8e8bc607a9bad5b6", "width": 960, "height": 965}], "variants": {}, "id": "59NpmpZ7KqeSnNYe2FaCHVmK9_ehWsRyDKb8XUunyW4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Collector", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1244jln", "is_robot_indexable": true, "report_reasons": null, "author": "NXGZ", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_123lugm", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1244jln/new_storebought_ssd_has_800gb_worth_of_instrument/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/kdma41wp1aqa1.jpg", "subreddit_subscribers": 675683, "created_utc": 1679958937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a license for it but it's starting to get on my nerves. The backups are excruciatingly slow, like it's taking a whole afternoon to incorporate a 10 GB incremental backup image into the main one. It doesn't need to be free but I don't want any subscription based services. Thank you for your help.", "author_fullname": "t2_8muaq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any alternatives to Macrium Reflect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123t7el", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679936485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a license for it but it&amp;#39;s starting to get on my nerves. The backups are excruciatingly slow, like it&amp;#39;s taking a whole afternoon to incorporate a 10 GB incremental backup image into the main one. It doesn&amp;#39;t need to be free but I don&amp;#39;t want any subscription based services. Thank you for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123t7el", "is_robot_indexable": true, "report_reasons": null, "author": "nicktheone", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123t7el/any_alternatives_to_macrium_reflect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123t7el/any_alternatives_to_macrium_reflect/", "subreddit_subscribers": 675683, "created_utc": 1679936485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Welcome to my post. \n\nIt is so bad now that I have spent over \u20ac10k on a server I don't need. And have no clue what to use it for.\n\nI'm trying to save up some money so I can go travel europe. But everytime I got something saved up I spend it on hard drives or new server hardware...\n\nThis is the server I'm talking about. \n\n/-----------------------------------------------------------/\n\n13600K....\n\n128gb ram....\n\nMSI z690-A....\n\n4x 1tb nvme....\n\n4x 1,92tb ssd....\n\n22x 18tb....\n\n4x 3tb....\n\n3x LSI 8i hba....\n\nFractal design define 7xl....\n\nCorsair hx1000i....\n\nDark Rock pro 4....\n\n/------------------------------------------------------/\n\nI already got another setup with 100+ tb of storage where I host my Plex. \n\n\nI absolutely love building servers like these. \n\nThat what's made me the hardware guy. I don't really enjoy the software side of things.\n\nSo anyone an idea of what is an easy thing to setup that maybe benefits me a little from building this server ?\n\n\n[Edit: mobile Layout]", "author_fullname": "t2_4re1q231", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Well, I got a problem.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1244pl4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679959508.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679959264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome to my post. &lt;/p&gt;\n\n&lt;p&gt;It is so bad now that I have spent over \u20ac10k on a server I don&amp;#39;t need. And have no clue what to use it for.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to save up some money so I can go travel europe. But everytime I got something saved up I spend it on hard drives or new server hardware...&lt;/p&gt;\n\n&lt;p&gt;This is the server I&amp;#39;m talking about. &lt;/p&gt;\n\n&lt;p&gt;/-----------------------------------------------------------/&lt;/p&gt;\n\n&lt;p&gt;13600K....&lt;/p&gt;\n\n&lt;p&gt;128gb ram....&lt;/p&gt;\n\n&lt;p&gt;MSI z690-A....&lt;/p&gt;\n\n&lt;p&gt;4x 1tb nvme....&lt;/p&gt;\n\n&lt;p&gt;4x 1,92tb ssd....&lt;/p&gt;\n\n&lt;p&gt;22x 18tb....&lt;/p&gt;\n\n&lt;p&gt;4x 3tb....&lt;/p&gt;\n\n&lt;p&gt;3x LSI 8i hba....&lt;/p&gt;\n\n&lt;p&gt;Fractal design define 7xl....&lt;/p&gt;\n\n&lt;p&gt;Corsair hx1000i....&lt;/p&gt;\n\n&lt;p&gt;Dark Rock pro 4....&lt;/p&gt;\n\n&lt;p&gt;/------------------------------------------------------/&lt;/p&gt;\n\n&lt;p&gt;I already got another setup with 100+ tb of storage where I host my Plex. &lt;/p&gt;\n\n&lt;p&gt;I absolutely love building servers like these. &lt;/p&gt;\n\n&lt;p&gt;That what&amp;#39;s made me the hardware guy. I don&amp;#39;t really enjoy the software side of things.&lt;/p&gt;\n\n&lt;p&gt;So anyone an idea of what is an easy thing to setup that maybe benefits me a little from building this server ?&lt;/p&gt;\n\n&lt;p&gt;[Edit: mobile Layout]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "252TB RAW", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1244pl4", "is_robot_indexable": true, "report_reasons": null, "author": "henk1313", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1244pl4/well_i_got_a_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1244pl4/well_i_got_a_problem/", "subreddit_subscribers": 675683, "created_utc": 1679959264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nJust wanted to give you all an update on the Dim Project - we're still actively working on it and making progress toward our goal of creating an incredible self-hosted media experience.\n\nWe would like to invite you to fill out our survey to help us better understand your thoughts and opinions on self-hosted media and what can be improved. Your input will help us make better decisions and develop strategies to create an innovative experience.\n\nSo, if you have a few minutes to spare, please take the time to fill out our survey. We appreciate your support and look forward to sharing our progress with you all.\n\nSurvey link: [https://forms.gle/uuFvuUb6FHQenY2Y9](https://forms.gle/uuFvuUb6FHQenY2Y9)\n\nGithub: [https://github.com/Dusk-Labs/dim](https://github.com/Dusk-Labs/dim)\n\nThank you from the team at [Dusk Labs](https://dusklabs.io/)!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4220hzdprbqa1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=128bd58d43159c98942fc318c6eb28986139fbaa", "author_fullname": "t2_2dwfny8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Dim Media Manager is still active - Help us by filling out our survey!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4220hzdprbqa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a7247e8caebf2731905effba8913c5e707b0f1d"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d228ae62967ec38e5205f7f31d89d3f4d139af4"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4dc44fee647f1f90969f00bf01828c9a2f9ba8b"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e324cb7cc5b65847a135ca81217f48967e98744"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d62b7d2321819fff35eb962fea9bc471fb56c9f6"}, {"y": 675, "x": 1080, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9c2c115f0d0885a0289ae4bf68af85d9d1da836"}], "s": {"y": 900, "x": 1440, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=128bd58d43159c98942fc318c6eb28986139fbaa"}, "id": "4220hzdprbqa1"}}, "name": "t3_123vrk8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DW6LK6_DZMfFMM1as4beqfY-F6KtygzEK1LY1tBXbQU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1679941739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Just wanted to give you all an update on the Dim Project - we&amp;#39;re still actively working on it and making progress toward our goal of creating an incredible self-hosted media experience.&lt;/p&gt;\n\n&lt;p&gt;We would like to invite you to fill out our survey to help us better understand your thoughts and opinions on self-hosted media and what can be improved. Your input will help us make better decisions and develop strategies to create an innovative experience.&lt;/p&gt;\n\n&lt;p&gt;So, if you have a few minutes to spare, please take the time to fill out our survey. We appreciate your support and look forward to sharing our progress with you all.&lt;/p&gt;\n\n&lt;p&gt;Survey link: &lt;a href=\"https://forms.gle/uuFvuUb6FHQenY2Y9\"&gt;https://forms.gle/uuFvuUb6FHQenY2Y9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/Dusk-Labs/dim\"&gt;https://github.com/Dusk-Labs/dim&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you from the team at &lt;a href=\"https://dusklabs.io/\"&gt;Dusk Labs&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4220hzdprbqa1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=128bd58d43159c98942fc318c6eb28986139fbaa\"&gt;https://preview.redd.it/4220hzdprbqa1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=128bd58d43159c98942fc318c6eb28986139fbaa&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?auto=webp&amp;v=enabled&amp;s=21620f0372507a6acc0f7982c5182a89cd6b1b7b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83ab4e94efe55fe1a599012612b3424e37cfb5b5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=387cac8ea6ecdd5403e898e1ef2c470867d4bb30", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68a83a5874284ed7aff41c11299c83c1df25a24f", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba7a406398df7441f0f347a9c323bc134b7d70f1", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=127394252ea2e44854b7e7d33f838cd2395b846f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33679000502df4016a4e7b01628b5a88040a819e", "width": 1080, "height": 567}], "variants": {}, "id": "jOfA63H5Nm6IY4ygjSZkFNqIYJzt-g4hTEZ3ut1zAgI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123vrk8", "is_robot_indexable": true, "report_reasons": null, "author": "HinaCh4n", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123vrk8/the_dim_media_manager_is_still_active_help_us_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123vrk8/the_dim_media_manager_is_still_active_help_us_by/", "subreddit_subscribers": 675683, "created_utc": 1679941739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried using fx or vxtwitter but if a tweet is deleted the image/video that was in it becomes invalid, I wanted to know a way to leave it archived without these problems", "author_fullname": "t2_nnj9ml6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is there any way to archive a tweet with discord?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123uw7w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679939972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried using fx or vxtwitter but if a tweet is deleted the image/video that was in it becomes invalid, I wanted to know a way to leave it archived without these problems&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123uw7w", "is_robot_indexable": true, "report_reasons": null, "author": "SaiaExitt", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123uw7w/is_there_any_way_to_archive_a_tweet_with_discord/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123uw7w/is_there_any_way_to_archive_a_tweet_with_discord/", "subreddit_subscribers": 675683, "created_utc": 1679939972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone!!!!!  \nI KNOW - i messed up. I should have backed up (in plaintext) but hear me out.   \n\n\nI'm in a bit of a bind and could use your expertise in decrypting a .json file that contains **all** of my bookmarks from a private bookmark extension. Unfortunately, the password to access the extension is no longer valid due to a bug in the extension, which has locked me out of accessing it. while opening/unlocking the extension theres a bug if firefox closes it can \"corrupt\" the decryption of the file causing your password to never work again. Weird... i know.\n\nThankfully, I can still open the .json in notepad and view the encrypted text, but I'm not sure how to reverse engineer it to get a password that will work. I'm hoping someone here might be able to help me out with this.  **I should also mention that the bookmark extension I used is** ***open-source*****,  which means the code is available for anyone to see and audit. If that  helps with finding a solution to my problem, please let me know. (**[**https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/**](https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/)**) according to the FAQ - it used** [**https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto**](https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto) \n\nIf that doesn't work, I'm also curious if it's possible to \"rollback\" my PC/Firefox so that I can revert the files to their state from a few hours ago. If anyone has experience with this, any advice would be appreciated.  \n\n\nor heck, maybe even re-engineer the addon to show the password/ allow me to click the \"export as plain text\" option in the backups.\n\nThank you in advance for any help you can provide.", "author_fullname": "t2_maupq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help decrypting a .json file containing my private bookmarks. Huge list of tons of things saved over the years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123sy9k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679935958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!!!!!&lt;br/&gt;\nI KNOW - i messed up. I should have backed up (in plaintext) but hear me out.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in a bit of a bind and could use your expertise in decrypting a .json file that contains &lt;strong&gt;all&lt;/strong&gt; of my bookmarks from a private bookmark extension. Unfortunately, the password to access the extension is no longer valid due to a bug in the extension, which has locked me out of accessing it. while opening/unlocking the extension theres a bug if firefox closes it can &amp;quot;corrupt&amp;quot; the decryption of the file causing your password to never work again. Weird... i know.&lt;/p&gt;\n\n&lt;p&gt;Thankfully, I can still open the .json in notepad and view the encrypted text, but I&amp;#39;m not sure how to reverse engineer it to get a password that will work. I&amp;#39;m hoping someone here might be able to help me out with this.  &lt;strong&gt;I should also mention that the bookmark extension I used is&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;open-source&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;,  which means the code is available for anyone to see and audit. If that  helps with finding a solution to my problem, please let me know. (&lt;/strong&gt;&lt;a href=\"https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/\"&gt;&lt;strong&gt;https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;) according to the FAQ - it used&lt;/strong&gt; &lt;a href=\"https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto\"&gt;&lt;strong&gt;https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;If that doesn&amp;#39;t work, I&amp;#39;m also curious if it&amp;#39;s possible to &amp;quot;rollback&amp;quot; my PC/Firefox so that I can revert the files to their state from a few hours ago. If anyone has experience with this, any advice would be appreciated.  &lt;/p&gt;\n\n&lt;p&gt;or heck, maybe even re-engineer the addon to show the password/ allow me to click the &amp;quot;export as plain text&amp;quot; option in the backups.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for any help you can provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?auto=webp&amp;v=enabled&amp;s=857980c7cbe0664da13e89a9f9d2077b6a36034b", "width": 350, "height": 525}, "resolutions": [{"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8df7dfdb53b40222072c38663ff3a7c5ba814e8", "width": 108, "height": 162}, {"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea7cc113736f27bb02b0184181dbffef1b0d4a62", "width": 216, "height": 324}, {"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb4eda9c727d02b73ab314a521f44f0231e911dd", "width": 320, "height": 480}], "variants": {}, "id": "1pf_pu1xk_lkxMMadhSke0hAPmCHs5IrzQRU0RoxoFc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123sy9k", "is_robot_indexable": true, "report_reasons": null, "author": "InhaleMC", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123sy9k/need_help_decrypting_a_json_file_containing_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123sy9k/need_help_decrypting_a_json_file_containing_my/", "subreddit_subscribers": 675683, "created_utc": 1679935958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I feel like NVMe and SSD are gradually replacing other types of storage media, with HDDs reserved for large sizes and cold storage in data centers. \n\nWhat do you think of a good strategy for long-term storage of important data (tax documents, photos etc)?\n\nStored properly, M-disks promise storage for several decades. Thoughts on using optical media for archival of limited data? \n\nExperience with durability of DVDs and Blue Rays? \n\nOther concerns are availability of a reader, and that presumably some of the well known brands such as verbatim have sold business to other companies that may not maintain the same quality.\n\nReading comments on DVDs on   r/datahoarder, I see mixed opinions. But most of the posts I have read are old.", "author_fullname": "t2_l1vjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optical media such as M-Disk for archival storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123gvpm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679908486.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679907204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like NVMe and SSD are gradually replacing other types of storage media, with HDDs reserved for large sizes and cold storage in data centers. &lt;/p&gt;\n\n&lt;p&gt;What do you think of a good strategy for long-term storage of important data (tax documents, photos etc)?&lt;/p&gt;\n\n&lt;p&gt;Stored properly, M-disks promise storage for several decades. Thoughts on using optical media for archival of limited data? &lt;/p&gt;\n\n&lt;p&gt;Experience with durability of DVDs and Blue Rays? &lt;/p&gt;\n\n&lt;p&gt;Other concerns are availability of a reader, and that presumably some of the well known brands such as verbatim have sold business to other companies that may not maintain the same quality.&lt;/p&gt;\n\n&lt;p&gt;Reading comments on DVDs on   &lt;a href=\"/r/datahoarder\"&gt;r/datahoarder&lt;/a&gt;, I see mixed opinions. But most of the posts I have read are old.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123gvpm", "is_robot_indexable": true, "report_reasons": null, "author": "chaplin2", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123gvpm/optical_media_such_as_mdisk_for_archival_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123gvpm/optical_media_such_as_mdisk_for_archival_storage/", "subreddit_subscribers": 675683, "created_utc": 1679907204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All,\n\n&amp;#x200B;\n\nI have one of the logic cases SC-4324 not sure if its going bad but it seems the top and bottom row don't work properly (it kills the drives!) so down 8 possible disk and limited to the three middle rows. Looking to replace it but in the UK i cant find any smilar stock.\n\nI got that one from [Servercases.co.uk](https://Servercases.co.uk) in \\~2016 but they don't seem to have any stock for similar cases now.\n\n&amp;#x200B;\n\nI came across this [https://www.xcase.co.uk/products/xpc-424-hs-24-bay-hotswap-storage-chassis-12gb-backplane](https://www.xcase.co.uk/products/xpc-424-hs-24-bay-hotswap-storage-chassis-12gb-backplane)  \n\nBut googling in here and on serve the home seems have never heard of the brand. At least when i searched way back when logic did have some history. Just wanted to check if anyone had one or if anyone had any ideas about the 2 rows on my current case not working :(! \n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_9vjn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "24 Bay Server Case", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123zry7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679949891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have one of the logic cases SC-4324 not sure if its going bad but it seems the top and bottom row don&amp;#39;t work properly (it kills the drives!) so down 8 possible disk and limited to the three middle rows. Looking to replace it but in the UK i cant find any smilar stock.&lt;/p&gt;\n\n&lt;p&gt;I got that one from &lt;a href=\"https://Servercases.co.uk\"&gt;Servercases.co.uk&lt;/a&gt; in ~2016 but they don&amp;#39;t seem to have any stock for similar cases now.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I came across this &lt;a href=\"https://www.xcase.co.uk/products/xpc-424-hs-24-bay-hotswap-storage-chassis-12gb-backplane\"&gt;https://www.xcase.co.uk/products/xpc-424-hs-24-bay-hotswap-storage-chassis-12gb-backplane&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;But googling in here and on serve the home seems have never heard of the brand. At least when i searched way back when logic did have some history. Just wanted to check if anyone had one or if anyone had any ideas about the 2 rows on my current case not working :(! &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "120TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123zry7", "is_robot_indexable": true, "report_reasons": null, "author": "dan897", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/123zry7/24_bay_server_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123zry7/24_bay_server_case/", "subreddit_subscribers": 675683, "created_utc": 1679949891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title, broadly. I'll be crossposting.\n\nI understand that this is only tangentially related to data hoarding, but it is related to my \"Spent $405 for a single show\" flair.\n\nEssentially, the BD for Avatar: The Last Airbender is HUGE. Like, \u2248 420GB (including DVD extras)^(1)\n\nIf it were up to me, I'd encode using AV1, but I'd like to also maybe \"release\" this (if you catch my drift) and apparently nobody uses that. So I'm left with HEVC. But holy SHIET there's a lot of discourse on this. [Scenerules](https://scenerules.org/t.html?id=2020_X265.nfo) has their, well, rules, [redditors](https://www.reddit.com/r/x265/comments/e08tfu/ultimate_encoding_test_results_for_animation/) have opinions, doom9 has discussions from [2015](https://forum.doom9.org/showthread.php?t=172458) and [2017](https://forum.doom9.org/showthread.php?t=174679) for 4k, there's a pretty nice Azumanga Daioh BDrip using [these](https://bin.theindex.moe/?177c7f606a37cd6a#9Es7QtMRBoBbsihRgLYuv8cqKRuB2xZN7XSs28QGyVzr) settings [(alt)](https://pastebin.com/hPbNmhns).\n\nSo many choices. Does anybody have any tips? Any guides you usually follow? I'm going for something like \"more or less visually lossless\" and \"not immediately obvious it's been compressed\" at the very least, though ideally I'd love to apply some kind of sharpening(?)/de-ionizing(?) filter to make it look better (the BD has a lot of film grain, and it's obviously a 480p source)\n\nThank you *so* much in advance.\n\n&amp;#x200B;\n\n^(1) I started this journey because I couldn't find a release online with quality I was happy with (I have since found releases that are probably good enough, but I digress). I'd like to make my own encode, not only for myself, but for everyone else unhappy with the quality on public trackers.", "author_fullname": "t2_1hutcmww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Transcoding] Overwhelmed with options, how can I best encode 1080p with x265?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123vp5h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679942289.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679941603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title, broadly. I&amp;#39;ll be crossposting.&lt;/p&gt;\n\n&lt;p&gt;I understand that this is only tangentially related to data hoarding, but it is related to my &amp;quot;Spent $405 for a single show&amp;quot; flair.&lt;/p&gt;\n\n&lt;p&gt;Essentially, the BD for Avatar: The Last Airbender is HUGE. Like, \u2248 420GB (including DVD extras)&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;\n\n&lt;p&gt;If it were up to me, I&amp;#39;d encode using AV1, but I&amp;#39;d like to also maybe &amp;quot;release&amp;quot; this (if you catch my drift) and apparently nobody uses that. So I&amp;#39;m left with HEVC. But holy SHIET there&amp;#39;s a lot of discourse on this. &lt;a href=\"https://scenerules.org/t.html?id=2020_X265.nfo\"&gt;Scenerules&lt;/a&gt; has their, well, rules, &lt;a href=\"https://www.reddit.com/r/x265/comments/e08tfu/ultimate_encoding_test_results_for_animation/\"&gt;redditors&lt;/a&gt; have opinions, doom9 has discussions from &lt;a href=\"https://forum.doom9.org/showthread.php?t=172458\"&gt;2015&lt;/a&gt; and &lt;a href=\"https://forum.doom9.org/showthread.php?t=174679\"&gt;2017&lt;/a&gt; for 4k, there&amp;#39;s a pretty nice Azumanga Daioh BDrip using &lt;a href=\"https://bin.theindex.moe/?177c7f606a37cd6a#9Es7QtMRBoBbsihRgLYuv8cqKRuB2xZN7XSs28QGyVzr\"&gt;these&lt;/a&gt; settings &lt;a href=\"https://pastebin.com/hPbNmhns\"&gt;(alt)&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;So many choices. Does anybody have any tips? Any guides you usually follow? I&amp;#39;m going for something like &amp;quot;more or less visually lossless&amp;quot; and &amp;quot;not immediately obvious it&amp;#39;s been compressed&amp;quot; at the very least, though ideally I&amp;#39;d love to apply some kind of sharpening(?)/de-ionizing(?) filter to make it look better (the BD has a lot of film grain, and it&amp;#39;s obviously a 480p source)&lt;/p&gt;\n\n&lt;p&gt;Thank you &lt;em&gt;so&lt;/em&gt; much in advance.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; I started this journey because I couldn&amp;#39;t find a release online with quality I was happy with (I have since found releases that are probably good enough, but I digress). I&amp;#39;d like to make my own encode, not only for myself, but for everyone else unhappy with the quality on public trackers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Spent $405 for a single show", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "123vp5h", "is_robot_indexable": true, "report_reasons": null, "author": "General-Stryker", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/123vp5h/transcoding_overwhelmed_with_options_how_can_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123vp5h/transcoding_overwhelmed_with_options_how_can_i/", "subreddit_subscribers": 675683, "created_utc": 1679941603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone! I'm new to being a hoarder of data but my job enabled me to get a dl380 HP server and enclosure which has the following:\n\n1. 2 x 300GB drives in a RAID 1 array holding my windows server 2012 os\n2. 10 x 300GB drives in RAID 6 as a local cloud\n3. 12 x 2TB drives IN RAID 6 holding my library\n\nI am looking to create a 3-2-1 backup plan for the 20tb array as I've seen preached here! I am interested in setting up a physical backup at my job and a cloud backup. My thoughts are as follows:\n\nSetup a RAID 1 using 2 x 22TB wd gold drives in a server at my job OR use more smaller drives to create another RAID 6 array for my physical backup, I'm interested in opinions on this as I plan on keeping my library ideally for the rest of my life and don't know what the best long-term route would be here?\n\nFor cloud.. is backblaze really reliable? I was looking into 20tb using Google drive, onedrive or Dropbox but those are very expensive. I saw backblaze which appealed to me because of the unlimited storage but I've seen lots of hit or miss reviews. I don't have a full 20tb worth of data, just 2-5tb but plan to add more. \n\nI'm just looking for insight on my plan, better alternatives and opinions on my setup are welcome! I just don't want to lose everything", "author_fullname": "t2_jfddw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backups for new library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123rlg9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679933242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! I&amp;#39;m new to being a hoarder of data but my job enabled me to get a dl380 HP server and enclosure which has the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;2 x 300GB drives in a RAID 1 array holding my windows server 2012 os&lt;/li&gt;\n&lt;li&gt;10 x 300GB drives in RAID 6 as a local cloud&lt;/li&gt;\n&lt;li&gt;12 x 2TB drives IN RAID 6 holding my library&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I am looking to create a 3-2-1 backup plan for the 20tb array as I&amp;#39;ve seen preached here! I am interested in setting up a physical backup at my job and a cloud backup. My thoughts are as follows:&lt;/p&gt;\n\n&lt;p&gt;Setup a RAID 1 using 2 x 22TB wd gold drives in a server at my job OR use more smaller drives to create another RAID 6 array for my physical backup, I&amp;#39;m interested in opinions on this as I plan on keeping my library ideally for the rest of my life and don&amp;#39;t know what the best long-term route would be here?&lt;/p&gt;\n\n&lt;p&gt;For cloud.. is backblaze really reliable? I was looking into 20tb using Google drive, onedrive or Dropbox but those are very expensive. I saw backblaze which appealed to me because of the unlimited storage but I&amp;#39;ve seen lots of hit or miss reviews. I don&amp;#39;t have a full 20tb worth of data, just 2-5tb but plan to add more. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just looking for insight on my plan, better alternatives and opinions on my setup are welcome! I just don&amp;#39;t want to lose everything&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123rlg9", "is_robot_indexable": true, "report_reasons": null, "author": "Th3_L1Nx", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123rlg9/backups_for_new_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123rlg9/backups_for_new_library/", "subreddit_subscribers": 675683, "created_utc": 1679933242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there. I'm travelling without access to a computer, so I intend to use my Android phone to upload my photos and videos from sd cards/external drive via OTG to the cloud.\n\nThe problem I have is that I can't seem to find a good cloud provider that wouldn't cause issues with managing duplicates. What I tried so far:\n\n- Google Drive, OneDrive, IceDrive, pCloud - when I perform a dump upload of my sd card and it fails (for example because of bad Internet connection), retrying it doesn't detect which files have already been uploaded. The issue with those providers is that when you try to upload a file that's already on the cloud, they don't ask whether you'd want to replace it or rename it, they upload it without a warning and give the file a new name. This is a very undesirable behaviour as I'd end up with multiple duplicates if I just try to select all files and upload them in bulk. What's ridiculous is that the behaviour in the Web apps of most of those providers is different the there's an option to choose the desired behaviour. Not in the app, though.\n\n- iDrive - the app sees which files have already been uploaded, but I'm not able to upload from other places than the phone internal storage...\n\n- MEGA - has duplicate detection working, but whenever I try to upload a bulk of files, the app would crash. It seems to be that the app is caching the files on phone memory before uploading them, and for bulk uploads, this simply cannot work. I get a warning from the system that it's running out of memory, and the app keeps on crashing until I wipe its memory.\n\nIs there any cloud that would have a mechanism to prevent the creation of duplicates working in an Android app?", "author_fullname": "t2_3e6lr021", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud storage for bulk uploads on Android", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123r91a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679934627.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679932610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there. I&amp;#39;m travelling without access to a computer, so I intend to use my Android phone to upload my photos and videos from sd cards/external drive via OTG to the cloud.&lt;/p&gt;\n\n&lt;p&gt;The problem I have is that I can&amp;#39;t seem to find a good cloud provider that wouldn&amp;#39;t cause issues with managing duplicates. What I tried so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Google Drive, OneDrive, IceDrive, pCloud - when I perform a dump upload of my sd card and it fails (for example because of bad Internet connection), retrying it doesn&amp;#39;t detect which files have already been uploaded. The issue with those providers is that when you try to upload a file that&amp;#39;s already on the cloud, they don&amp;#39;t ask whether you&amp;#39;d want to replace it or rename it, they upload it without a warning and give the file a new name. This is a very undesirable behaviour as I&amp;#39;d end up with multiple duplicates if I just try to select all files and upload them in bulk. What&amp;#39;s ridiculous is that the behaviour in the Web apps of most of those providers is different the there&amp;#39;s an option to choose the desired behaviour. Not in the app, though.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;iDrive - the app sees which files have already been uploaded, but I&amp;#39;m not able to upload from other places than the phone internal storage...&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;MEGA - has duplicate detection working, but whenever I try to upload a bulk of files, the app would crash. It seems to be that the app is caching the files on phone memory before uploading them, and for bulk uploads, this simply cannot work. I get a warning from the system that it&amp;#39;s running out of memory, and the app keeps on crashing until I wipe its memory.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is there any cloud that would have a mechanism to prevent the creation of duplicates working in an Android app?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123r91a", "is_robot_indexable": true, "report_reasons": null, "author": "Pramus", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123r91a/cloud_storage_for_bulk_uploads_on_android/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123r91a/cloud_storage_for_bulk_uploads_on_android/", "subreddit_subscribers": 675683, "created_utc": 1679932610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was wondering if this even exists. I'd like to connect a few 3.5in HDD to my mini server via a couple sata ports. But I'm powering it with a PicoPSU which doesn't have enough power to power the additional HDDs. What I do have is a bunch of 12v 6/8 pin connectors via a server PSU.\n\nAre there any sata backplanes that take 12v 6/8 pin PCIE power connectors?", "author_fullname": "t2_ef5xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a 12v powered sata backplane", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123eyqc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679901734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if this even exists. I&amp;#39;d like to connect a few 3.5in HDD to my mini server via a couple sata ports. But I&amp;#39;m powering it with a PicoPSU which doesn&amp;#39;t have enough power to power the additional HDDs. What I do have is a bunch of 12v 6/8 pin connectors via a server PSU.&lt;/p&gt;\n\n&lt;p&gt;Are there any sata backplanes that take 12v 6/8 pin PCIE power connectors?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123eyqc", "is_robot_indexable": true, "report_reasons": null, "author": "FallingSnowStar", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123eyqc/looking_for_a_12v_powered_sata_backplane/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123eyqc/looking_for_a_12v_powered_sata_backplane/", "subreddit_subscribers": 675683, "created_utc": 1679901734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was encoding some videos I don't care super much about with HEVC and using a CRF of 16 and vbv-maxrate=15000.  I ended up with a 12GB video of my 4k HDR10 78GB video.\n\nI then ran the source thru ab-av1.  It came back with\n\nCRF 20.2 VMAF 95.04 predicted video stream size 122.43 MiB (32%) taking 57 seconds\n\nOK i'm not going to believe the size or the time to encode so not sure I should trust the CRF value but I tried it with CRF 20.2 and took out the maxrate value from the encode.  Came back with an 18GB file.\n\nSo is a smaller constrained file with a \"better\" CRF value better to go with or is a \"worse\" quality CRF value unconstrained better?\n\nFor reference running a VMAF check with ffmpeg the CRF 16 value constrained against the source reteurns 91.1 and the CRF 20.2 unconstrained returned oddly 83.24 VMAF score even though overall 20.2 CRM seemed like there were higher bitrates during encoding.", "author_fullname": "t2_wsyij", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open CRF vs constrained?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12460ua", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679962121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was encoding some videos I don&amp;#39;t care super much about with HEVC and using a CRF of 16 and vbv-maxrate=15000.  I ended up with a 12GB video of my 4k HDR10 78GB video.&lt;/p&gt;\n\n&lt;p&gt;I then ran the source thru ab-av1.  It came back with&lt;/p&gt;\n\n&lt;p&gt;CRF 20.2 VMAF 95.04 predicted video stream size 122.43 MiB (32%) taking 57 seconds&lt;/p&gt;\n\n&lt;p&gt;OK i&amp;#39;m not going to believe the size or the time to encode so not sure I should trust the CRF value but I tried it with CRF 20.2 and took out the maxrate value from the encode.  Came back with an 18GB file.&lt;/p&gt;\n\n&lt;p&gt;So is a smaller constrained file with a &amp;quot;better&amp;quot; CRF value better to go with or is a &amp;quot;worse&amp;quot; quality CRF value unconstrained better?&lt;/p&gt;\n\n&lt;p&gt;For reference running a VMAF check with ffmpeg the CRF 16 value constrained against the source reteurns 91.1 and the CRF 20.2 unconstrained returned oddly 83.24 VMAF score even though overall 20.2 CRM seemed like there were higher bitrates during encoding.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12460ua", "is_robot_indexable": true, "report_reasons": null, "author": "jriker1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12460ua/open_crf_vs_constrained/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12460ua/open_crf_vs_constrained/", "subreddit_subscribers": 675683, "created_utc": 1679962121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Whats up fam, I got a crazy deal on a 22TB WD purple pro for $180....I understand that these are made for surveillance but my goal is to use it to house my media and strictly use it for my plex server....I dont have experience with these particular drives and have read mixed reviews where some say its just as capable as the WD Red Pro and others that say to stick w the Red's for NAS purposes, just such a good deal on the space....should I resell and buy a Red or run that sucker till it burns? Cheers", "author_fullname": "t2_a0u6ksr5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Purple Pro for strict Plex use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123vyvn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679942141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whats up fam, I got a crazy deal on a 22TB WD purple pro for $180....I understand that these are made for surveillance but my goal is to use it to house my media and strictly use it for my plex server....I dont have experience with these particular drives and have read mixed reviews where some say its just as capable as the WD Red Pro and others that say to stick w the Red&amp;#39;s for NAS purposes, just such a good deal on the space....should I resell and buy a Red or run that sucker till it burns? Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123vyvn", "is_robot_indexable": true, "report_reasons": null, "author": "Pissssgang", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123vyvn/wd_purple_pro_for_strict_plex_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123vyvn/wd_purple_pro_for_strict_plex_use/", "subreddit_subscribers": 675683, "created_utc": 1679942141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI'm looking for a list or references of old negative scanners (I mean cheap second hand).  \nThat allow at least 4 35mm films strip to be scanned at the same time.\n\nCan any of you help me?\n\nThanks", "author_fullname": "t2_exg3en8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanner negative that scan at least 4 35 mm film strip", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123nmcz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679925346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a list or references of old negative scanners (I mean cheap second hand).&lt;br/&gt;\nThat allow at least 4 35mm films strip to be scanned at the same time.&lt;/p&gt;\n\n&lt;p&gt;Can any of you help me?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123nmcz", "is_robot_indexable": true, "report_reasons": null, "author": "mnemonickus", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123nmcz/scanner_negative_that_scan_at_least_4_35_mm_film/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123nmcz/scanner_negative_that_scan_at_least_4_35_mm_film/", "subreddit_subscribers": 675683, "created_utc": 1679925346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Team. Starting to get to the higher levels of media storage and instead of expanding my server, I thought I would archive some of my older stuff on tape. Now, I'm new to tape. As in, I have zero experience with it to-date.\n\nFound a single internal IBM LTO7 drive and bought a LTo6 tape for testing (as I understand it, drives can write to their own generation + the one immediately before it i.e. a 7 drive can write to 7 and 6 tape. and they can read their own + two preceding gen tapes i.e. 7, 6, and 5.)\n\nSo got all setup. Drive is all installed, systems (windows) see it etc. I am able to assign it a drive letter using the IBM config tool. \n\nBut then I stick in the media (brand new LTO6 tape cartridge) and it spins up as expected, but then the little diskette icon turns into an icon with a yellow triangle warning. I double-click it to try and browse the drive and I get the error that Z: is not accessible\" Tape partition information could not be found when loading a tape.\" No associated coded or anything, just a windows error box with that code. The net tells me that's a \"1107\", but can't see anything about how to troubleshoot.\n\nCan confirm all the drivers/necessaries are installed and up-to-date. Can also confirm that the media is not locked/write-protected.\n\nAny help would be appreciated.", "author_fullname": "t2_hx1zn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IBM LTO 7 Tape drive problems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123l2xs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679918828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Team. Starting to get to the higher levels of media storage and instead of expanding my server, I thought I would archive some of my older stuff on tape. Now, I&amp;#39;m new to tape. As in, I have zero experience with it to-date.&lt;/p&gt;\n\n&lt;p&gt;Found a single internal IBM LTO7 drive and bought a LTo6 tape for testing (as I understand it, drives can write to their own generation + the one immediately before it i.e. a 7 drive can write to 7 and 6 tape. and they can read their own + two preceding gen tapes i.e. 7, 6, and 5.)&lt;/p&gt;\n\n&lt;p&gt;So got all setup. Drive is all installed, systems (windows) see it etc. I am able to assign it a drive letter using the IBM config tool. &lt;/p&gt;\n\n&lt;p&gt;But then I stick in the media (brand new LTO6 tape cartridge) and it spins up as expected, but then the little diskette icon turns into an icon with a yellow triangle warning. I double-click it to try and browse the drive and I get the error that Z: is not accessible&amp;quot; Tape partition information could not be found when loading a tape.&amp;quot; No associated coded or anything, just a windows error box with that code. The net tells me that&amp;#39;s a &amp;quot;1107&amp;quot;, but can&amp;#39;t see anything about how to troubleshoot.&lt;/p&gt;\n\n&lt;p&gt;Can confirm all the drivers/necessaries are installed and up-to-date. Can also confirm that the media is not locked/write-protected.&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123l2xs", "is_robot_indexable": true, "report_reasons": null, "author": "ZimmerFrameThief", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123l2xs/ibm_lto_7_tape_drive_problems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123l2xs/ibm_lto_7_tape_drive_problems/", "subreddit_subscribers": 675683, "created_utc": 1679918828.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}