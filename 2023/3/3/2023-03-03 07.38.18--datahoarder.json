{"kind": "Listing", "data": {"after": "t3_11gdoaa", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a plex server with a couple of ssd's and 6 spinning hard drives, 3 of which are shucked wd easystore drives. I decided to finally label my drive caddies to make a failed drive easier to identify in case of a failure or replacement. After labelling the caddies I slid them back into my case, attached cabling and booted the computer.\n\nI launched file explorer and was shocked to see that 3 of my spinning hard drives were not there at all. This is where my moronic tale begins. I shut the server down and checked all of the cable connections and rebooted. Still missing 3 drives! Being a logical moron, I decided to pull a couple of the missing drives, since they were handily labelled now, and install them in an external enclosure to see if they somehow had failed. They tested fine.\n\nI reinstalled the drives and booted the computer. Still 3 missing drives! I decided that I must have damaged either the sata or power cable (shared) for all 3 drives simultaneously, so I swapped out cabling for all 3 drives and rebooted. Still missing 3 drives! I replaced the original cabling and it was about this time, 2 hours into my moronic adventure, that I noticed that all 3 of the invisible drives were white label shucked drives. DUHHH, I wondered if the capstan tape that I applied to the 3rd power connector pin had been damaged on all 3 drives. What are the odds that could happen at the same time on all 3 drives?\n\nI pulled all 3 drives for the 47th time and what do you know? The tape was indeed damaged on all 3 drives. I carefully removed and then replaced the tape and put the drives back in and booted the server. Still missing 3 drives! You have got to be fu@#ing kidding!!! I pulled all 3 drives for the 48th time and inspected them. Being the moron that I am, I had applied the tape to the 3rd pin on the DATA connector, not the power connector. I removed the tape and applied it to the correct 3rd pins, reinstalled the drives and booted. The 3 previously missing drives were now present and functioning normally.\n\nAs a pathetic defense, it had been a while since I had installed these 3 shucked drives and the 3rd pin tape requirement was a bit of a distant memory. However I am still a moron, dimwit, bonehead and idiot for spending half a day on a 30 minute project. Just thought I would share...", "author_fullname": "t2_56nvy1n8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My astounding stupidity, tale of a moron", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g9nu4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 231, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 231, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677780899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a plex server with a couple of ssd&amp;#39;s and 6 spinning hard drives, 3 of which are shucked wd easystore drives. I decided to finally label my drive caddies to make a failed drive easier to identify in case of a failure or replacement. After labelling the caddies I slid them back into my case, attached cabling and booted the computer.&lt;/p&gt;\n\n&lt;p&gt;I launched file explorer and was shocked to see that 3 of my spinning hard drives were not there at all. This is where my moronic tale begins. I shut the server down and checked all of the cable connections and rebooted. Still missing 3 drives! Being a logical moron, I decided to pull a couple of the missing drives, since they were handily labelled now, and install them in an external enclosure to see if they somehow had failed. They tested fine.&lt;/p&gt;\n\n&lt;p&gt;I reinstalled the drives and booted the computer. Still 3 missing drives! I decided that I must have damaged either the sata or power cable (shared) for all 3 drives simultaneously, so I swapped out cabling for all 3 drives and rebooted. Still missing 3 drives! I replaced the original cabling and it was about this time, 2 hours into my moronic adventure, that I noticed that all 3 of the invisible drives were white label shucked drives. DUHHH, I wondered if the capstan tape that I applied to the 3rd power connector pin had been damaged on all 3 drives. What are the odds that could happen at the same time on all 3 drives?&lt;/p&gt;\n\n&lt;p&gt;I pulled all 3 drives for the 47th time and what do you know? The tape was indeed damaged on all 3 drives. I carefully removed and then replaced the tape and put the drives back in and booted the server. Still missing 3 drives! You have got to be fu@#ing kidding!!! I pulled all 3 drives for the 48th time and inspected them. Being the moron that I am, I had applied the tape to the 3rd pin on the DATA connector, not the power connector. I removed the tape and applied it to the correct 3rd pins, reinstalled the drives and booted. The 3 previously missing drives were now present and functioning normally.&lt;/p&gt;\n\n&lt;p&gt;As a pathetic defense, it had been a while since I had installed these 3 shucked drives and the 3rd pin tape requirement was a bit of a distant memory. However I am still a moron, dimwit, bonehead and idiot for spending half a day on a 30 minute project. Just thought I would share...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11g9nu4", "is_robot_indexable": true, "report_reasons": null, "author": "bctf1", "discussion_type": null, "num_comments": 84, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g9nu4/my_astounding_stupidity_tale_of_a_moron/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g9nu4/my_astounding_stupidity_tale_of_a_moron/", "subreddit_subscribers": 671875, "created_utc": 1677780899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SimpleX File Transfer Protocol (aka XFTP) \u2013 a new open-source protocol for sending large files efficiently, privately and securely \u2013 beta versions of XFTP relays and CLI are released!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fxcfx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 123, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_7k499ptf", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 123, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "PrivacyGuides", "selftext": "**XFTP** is a new file transfer protocol focussed on meta-data protection - it is based on the same principles as SimpleX Messaging Protocol used in [SimpleX Chat](https://simplex.chat) messenger:\n\n- asynchronous file delivery - the sender does not need to be online for file to be received, it is stored on XFTP relays for a limited time (currently, it is 48 hours) or until deleted by the sender.\n- padded e2e encryption of file content.\n- content padding and fixed size chunks sent via different XFTP relays, assembled back into the original file by the receiving client.\n- efficient sending to multiple recipients (the file needs to be uploaded only once).\n- no identifiers or ciphertext in common between sent and received relay traffic, same as for messages delivered by SMP relays.\n- protection of sender IP address from the recipients.\n\nYou can download XFTP CLI (Linux) to send files via the command line [here](https://github.com/simplex-chat/simplexmq/releases/tag/v5.0.0-beta.3) - you need the file named `xftp-ubuntu-20_04-x86-64`, rename it to `xftp`.\n\n**Send the file in 3 steps**:\n\n1. to send: `xftp send filename.ext`\n2. to share: pass the generated file description(s) to the recipient(s) via any secure channel, e.g. via SimpleX Chat.\n3. to receive: `xftp recv rcvN.xftp`\n\n**Please let us know what you think**, what downsides you see to this approach, and any ideas you have about how it can be improved.\n\nWe are currently integrating the support of XFTP protocol into SimpleX Chat that will allow sending videos and large files seamlessly and without the sender being online - it is coming soon!\n\nRead more details in this blog post: https://simplex.chat/blog/20230301-simplex-file-transfer-protocol.html\n\nThe source code: https://github.com/simplex-chat/simplexmq/tree/xftp", "author_fullname": "t2_13t18x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SimpleX File Transfer Protocol (aka XFTP) \u2013 a new open-source protocol for sending large files efficiently, privately and securely \u2013 beta versions of XFTP relays and CLI are released!", "link_flair_richtext": [{"e": "text", "t": "News"}], "subreddit_name_prefixed": "r/PrivacyGuides", "hidden": false, "pwls": 7, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fg9fa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": "", "subreddit_type": "public", "ups": 219, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 219, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"e": "text", "t": "Simplex founder"}], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677701359.0, "link_flair_type": "richtext", "wls": 7, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "self.PrivacyGuides", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;XFTP&lt;/strong&gt; is a new file transfer protocol focussed on meta-data protection - it is based on the same principles as SimpleX Messaging Protocol used in &lt;a href=\"https://simplex.chat\"&gt;SimpleX Chat&lt;/a&gt; messenger:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;asynchronous file delivery - the sender does not need to be online for file to be received, it is stored on XFTP relays for a limited time (currently, it is 48 hours) or until deleted by the sender.&lt;/li&gt;\n&lt;li&gt;padded e2e encryption of file content.&lt;/li&gt;\n&lt;li&gt;content padding and fixed size chunks sent via different XFTP relays, assembled back into the original file by the receiving client.&lt;/li&gt;\n&lt;li&gt;efficient sending to multiple recipients (the file needs to be uploaded only once).&lt;/li&gt;\n&lt;li&gt;no identifiers or ciphertext in common between sent and received relay traffic, same as for messages delivered by SMP relays.&lt;/li&gt;\n&lt;li&gt;protection of sender IP address from the recipients.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can download XFTP CLI (Linux) to send files via the command line &lt;a href=\"https://github.com/simplex-chat/simplexmq/releases/tag/v5.0.0-beta.3\"&gt;here&lt;/a&gt; - you need the file named &lt;code&gt;xftp-ubuntu-20_04-x86-64&lt;/code&gt;, rename it to &lt;code&gt;xftp&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Send the file in 3 steps&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;to send: &lt;code&gt;xftp send filename.ext&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;to share: pass the generated file description(s) to the recipient(s) via any secure channel, e.g. via SimpleX Chat.&lt;/li&gt;\n&lt;li&gt;to receive: &lt;code&gt;xftp recv rcvN.xftp&lt;/code&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Please let us know what you think&lt;/strong&gt;, what downsides you see to this approach, and any ideas you have about how it can be improved.&lt;/p&gt;\n\n&lt;p&gt;We are currently integrating the support of XFTP protocol into SimpleX Chat that will allow sending videos and large files seamlessly and without the sender being online - it is coming soon!&lt;/p&gt;\n\n&lt;p&gt;Read more details in this blog post: &lt;a href=\"https://simplex.chat/blog/20230301-simplex-file-transfer-protocol.html\"&gt;https://simplex.chat/blog/20230301-simplex-file-transfer-protocol.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The source code: &lt;a href=\"https://github.com/simplex-chat/simplexmq/tree/xftp\"&gt;https://github.com/simplex-chat/simplexmq/tree/xftp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PAfHS3IZ634_axqDop1LTgM5m7ZHZ3bNhL4kDYjWxxA.jpg?auto=webp&amp;v=enabled&amp;s=889388775ee1c3fbd7bb2ea12a30fc6ebb7bcc52", "width": 320, "height": 320}, "resolutions": [{"url": "https://external-preview.redd.it/PAfHS3IZ634_axqDop1LTgM5m7ZHZ3bNhL4kDYjWxxA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=afff9106dad27a5f8a2442b818e24b9ea06cafbc", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/PAfHS3IZ634_axqDop1LTgM5m7ZHZ3bNhL4kDYjWxxA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf9070c44fdda0fb8b9dd4c76cad6417a3450ded", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/PAfHS3IZ634_axqDop1LTgM5m7ZHZ3bNhL4kDYjWxxA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bec8ec72f16644ac73f507f40e9601a208de806", "width": 320, "height": 320}], "variants": {}, "id": "U3iCf8GjcquT1qPFC8OR00SlD9Hnw9Acft2_3PjJN9Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b4cdd4f0-2171-11ec-b47c-9a667d49ea89", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Simplex founder", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2p35dk", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11fg9fa", "is_robot_indexable": true, "report_reasons": null, "author": "epoberezkin", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "some_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/PrivacyGuides/comments/11fg9fa/simplex_file_transfer_protocol_aka_xftp_a_new/", "parent_whitelist_status": "some_ads", "stickied": false, "url": "https://old.reddit.com/r/PrivacyGuides/comments/11fg9fa/simplex_file_transfer_protocol_aka_xftp_a_new/", "subreddit_subscribers": 53742, "created_utc": 1677701359.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1677745270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.PrivacyGuides", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/PrivacyGuides/comments/11fg9fa/simplex_file_transfer_protocol_aka_xftp_a_new/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PAfHS3IZ634_axqDop1LTgM5m7ZHZ3bNhL4kDYjWxxA.jpg?auto=webp&amp;v=enabled&amp;s=889388775ee1c3fbd7bb2ea12a30fc6ebb7bcc52", "width": 320, "height": 320}, "resolutions": [{"url": "https://external-preview.redd.it/PAfHS3IZ634_axqDop1LTgM5m7ZHZ3bNhL4kDYjWxxA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=afff9106dad27a5f8a2442b818e24b9ea06cafbc", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/PAfHS3IZ634_axqDop1LTgM5m7ZHZ3bNhL4kDYjWxxA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf9070c44fdda0fb8b9dd4c76cad6417a3450ded", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/PAfHS3IZ634_axqDop1LTgM5m7ZHZ3bNhL4kDYjWxxA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bec8ec72f16644ac73f507f40e9601a208de806", "width": 320, "height": 320}], "variants": {}, "id": "U3iCf8GjcquT1qPFC8OR00SlD9Hnw9Acft2_3PjJN9Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Collector", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11fxcfx", "is_robot_indexable": true, "report_reasons": null, "author": "NXGZ", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_11fg9fa", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11fxcfx/simplex_file_transfer_protocol_aka_xftp_a_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/PrivacyGuides/comments/11fg9fa/simplex_file_transfer_protocol_aka_xftp_a_new/", "subreddit_subscribers": 671875, "created_utc": 1677745270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_16k9lu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Did eBay sell me a knock off Seagate Exos 16TB HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 100, "top_awarded_type": null, "hide_score": false, "media_metadata": {"due343t2dfla1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 77, "x": 108, "u": "https://preview.redd.it/due343t2dfla1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8daa04f73512e7d88092a529f7a0a2c6705bb26"}, {"y": 154, "x": 216, "u": "https://preview.redd.it/due343t2dfla1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=793bd1f1c1d1707ce10f90df9eb52bc7bfc0e5a2"}, {"y": 229, "x": 320, "u": "https://preview.redd.it/due343t2dfla1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29c4c78ec092f28b9ddeb5aed814b8c9cbd5588c"}, {"y": 458, "x": 640, "u": "https://preview.redd.it/due343t2dfla1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4009b296b6deaf683834e55e39b3349d6d784b6"}], "s": {"y": 570, "x": 796, "u": "https://preview.redd.it/due343t2dfla1.png?width=796&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=03f6a0f2b80236b4d3283d68f01340bf02f1b99c"}, "id": "due343t2dfla1"}, "ao9lbl13dfla1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e425027ebb39046891a0a93f1aa77d07339ea92"}, {"y": 134, "x": 216, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd986bc87053c98b88c73dd15ca0557abe843e6e"}, {"y": 199, "x": 320, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13e4e03745b95d617d8462878f1714fd810d7628"}, {"y": 398, "x": 640, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a67be6983eaae12e8b4fa2696fc0ec47e875a24"}], "s": {"y": 474, "x": 761, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=761&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aee2bad26ece0736b01fb82c001c956d55edbcd5"}, "id": "ao9lbl13dfla1"}}, "name": "t3_11gmykf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 65, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "This is one of the drives that arrived from eBay", "media_id": "due343t2dfla1", "id": 246640328}, {"caption": "This is what the online images of the 16TH Exos drives look like", "media_id": "ao9lbl13dfla1", "id": 246640329}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/o-9GYlzLMjLgkQDYH_eA9WX4LxFqnakRnGrsy0koYs8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677806043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/11gmykf", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gmykf", "is_robot_indexable": true, "report_reasons": null, "author": "AwefulUsername", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gmykf/did_ebay_sell_me_a_knock_off_seagate_exos_16tb_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/11gmykf", "subreddit_subscribers": 671875, "created_utc": 1677806043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I live in an area where symmetrical internet is either expensive or unavailable. My offsite backup is in my brother's house and just rsync'ing it didn't seem reasonable with my upload speed / bandwidth limitations. I decided to go with the tried and true sneakernet method. I decided to take advantage of ZFS' snapshotting functionality. The first script automatically creates incremental snapshots holding all the changes between the latest update on the backup machine and the current date as of running the script. These snapshots can be ferried between the original and backup machines via a portable hard drive. The second script applies runs the backup machine and will apply the snapshots from the portable hard drive to the backup system.  \n\nI thought I'd share the scripts in case their useful for anyone else sneakernetting their backups as well as some feedback from the community.  \n\nThe first script (ideally set to run periodically via cron/systemd.timers)\n\n    #!/bin/bash\n    \n     # This is part one of my 'patented' sneakernet backup system. The inte-\n     # nded usecase is to use a portable hard drive to shuttle incremental \n     # zfs snapshots from the original server (where this script will reside\n     # and run) to the backup server. There are a few requirements:\n     # 1) Both the original and backup servers have matching datasets\n     # 2) The portable hard drive is formatted with ZFS with a pool named\n     #    Sync\n     # 3) When the portable hard drive makes its way back to the original \n     #    server, you zpool import it and rsync the contents of the \n     #    ZFS_incremental folder onto it\n     \n    # Honestly this probably isn't needed in this script but I put it in all\n    # my bash scripts\n    set -e\n    \n    # Determine current date\n    currentdate=$(date +\"%Y-%m-%d\")\n    \n    # Determine the last time we synced with the backupserver. The file is\n    # generated by the backup server and either rsynced via SSH or pulled\n    # from the portable hard drive\n    lwsync=$(tail -n -1 /root/last_backup_sync.txt)\n    \n    # Using the current date to create snapshot names. Endpoints for the\n    # incremental snapshot\n    curISOssnapshot=\"Pool1/ISOs@${currentdate}\"\n    curlinuxsnapshot=\"Pool1/linux@${currentdate}\"\n    curpubdomainsnapshot=\"Pool1/pubdomain@${currentdate}\"\n    \n    # Using the last date we to create snapshot names. Start-points for the\n    # incremental snapshot\n    lwsyncISOssnapshot=\"Pool1/ISOs@${lwsync}\"\n    lwsynclinuxsnapshot=\"Pool1/linux@${lwsync}\"\n    lwsyncpubdomainsnapshot=\"Pool1/pubdomain@${lwsync}\"\n    \n    \n    # use zfs send to save incremental snapshots of each pool\n    echo $curISOssnapshot\n    echo $curlinuxsnapshot\n    echo $curpubdomainsnapshot\n    \n    \n    # Create snapshots of with current date\n    zfs snapshot $curISOssnapshot\n    zfs snapshot $curlinuxsnapshot\n    zfs snapshot $curpubdomainsnapshot\n    echo \"Snapshots made ${currentdate}\" &gt;&gt; /root/backupscript.log\n    \n    # Create Incremental Snapshots of ISOs, linux and pubdomain\n    zfs send -Ri $lwsyncISOssnapshot $curISOssnapshot &gt; /mnt/Pool1/ZFS_incremental/ISOs_latest\n    zfs send -Ri $lwsynclinuxsnapshot $curlinuxsnapshot &gt; /mnt/Pool1/ZFS_incremental/linux_latest\n    zfs send -Ri $lwsyncpubdomainsnapshot $curpubdomainsnapshot &gt; /mnt/Pool1/ZFS_incremental/pubdomain_latest\n    echo \"Incremental snapshots saved to /mnt/Pool1/ZFS_incremental/ \" &gt;&gt; /root/backupscript.log\n    \n\nThe second script (ideally run automatically via cron/systemd.timers. should be setup to run every day for when the backup machine is located at a relative with low technical abilities):\n\n    #! /bin/bash\n    \n    # This script is the second part of my 'patented' sneakernet update \n    # system. The script is intended to be run on the backup server once the \n    # portable hard drive is plugged into the machine. This script does have \n    # a few dependencies: \n    # 1) the backup server must have the same zfs datasets that are going to \n    #    be transferred from the portable hard drive. \n    # 2) the backup server must have automatic snapshotting turned OFF. \n    # 3) this script assumes that you can ssh into your primary server. \n    \n    # Basically stops the script when something goes wrong. Mostly for when\n    # the script is set to run automatically (via systemd.timers or crontab)\n    # and the \n    set -e\n    \n    # set variables for the latest snapshots of the backup system (useful\n    # for later\n    last_ISOs_snapshot=$(zfs list -H -o name -t snapshot BackupPool/ISOs | tail -n -1)\n    last_linux_snapshot=$(zfs list -H -o name -t snapshot BackupPool/linux | tail -n -1)\n    last_pubdomain_snapshot=$(zfs list -H -o name -t snapshot BackupPool/pubdomain | tail -n -1)\n    \n    # Honestly, just here to provide some output for logging to see where\n    # it might have failed\n    echo $last_ISOs_snapshot\n    echo $last_linux_snapshot\n    echo $last_pubdomain_snapshot\n    \n    echo $(date +\"%Y-%m-%d\") &gt;&gt; /root/sync.log\n    echo $(date +\"%Y-%m-%d\")\n    \n    # Imports the pool located on the portable hard drive\n    zpool import -o altroot=/mnt Sync\n    echo \"Import Done\" &gt;&gt; /root/sync.log\n    echo \"Import Done\"\n    \n    # Rollback to last snapshot. This is necessary in case the person using\n    # the backupserver happened to accidentaly ignore your instructions and\n    # alters any of the pools that are going to be synced.\n    zfs rollback $last_ISOs_snapshot\n    zfs rollback $last_linux_snapshot\n    zfs rollback $last_pubdomain_snapshot\n    echo \"Snapshot rollback done\" &gt;&gt; /root/sync.log\n    echo \"Snapshot rollback done\"\n    \n    # Sync Incremental Snapshots of ISOs, linux and pubdomain\n    zfs recv -F BackupPool/ISOs &lt; /mnt/Sync/ISOs_latest\n    echo \"ISOs updated\" &gt;&gt; /root/sync.log\n    echo \"ISOs updated\"\n    \n    zfs recv -F BackupPool/linux &lt; /mnt/Sync/linux_latest\n    echo \"linux updated\" &gt;&gt; /root/sync.log\n    echo \"linux updated\"\n    \n    zfs recv -F BackupPool/pubdomain &lt; /mnt/Sync/pubdomain_latest\n    echo \"pubdomain updated\" &gt;&gt; /root/sync.log\n    echo \"pubdomain updated\"\n    \n    echo \"Sync Complete\" &gt;&gt; /root/sync.log\n    echo \"Sync Complete\"\n    \n    # Create the file that will be referenced by the first script when det-\n    # ermining what the start point is for the incremental snapshot. Saves \n    # it to the portable hard drive in case ssh'ing into the progenitor \n    # server is not possible\n    zfs list -H -o name -t snapshot BackupPool/ISOs &gt; tempa.txt\n    sed 's/BackupPool\\/ISOs@//' tempa.txt &gt; /root/last_backup_sync.txt\n    rm tempa.txt\n    cp /root/last_backup_sync.txt /mnt/Sync/last_backup_sync.txt\n    echo \"Update last_backup_sync.txt done\" &gt;&gt; /root/sync.log\n    echo \"Update last_backup_sync.txt done\"\n    \n    # Sends the file to the original server. Means that the originator ser-\n    # ver can start making the correct incremental snapshots. This, in turn,\n    # means that when the original server receives the portable hard drive,\n    # it need only copy over the pregenerated incremental snapshots.\n    rsync -e \"ssh\" -avz /root/last_backup_sync.txt root@###.###.###.###:/root/\n    echo \"Send last_backup_sync.txt to Original server\" &gt;&gt; /root/sync.log\n    echo \"Send last_backup_sync.txt to Original server\"\n    \n    # Basically ejects the portable hard drive.\n    zpool export Sync\n    echo \"Disconnect Sync complete\" &gt;&gt; /root/sync.log\n    echo \"---------------------------------------------------\" &gt;&gt; /root/sync.log\n    echo \"Disconnect Sync complete\"\n\nCheers,  \nBasilisk\\_hunters", "author_fullname": "t2_2xnvildp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created some scripts to backup my data via sneakernet. thought I'd share", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gpedp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677812574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I live in an area where symmetrical internet is either expensive or unavailable. My offsite backup is in my brother&amp;#39;s house and just rsync&amp;#39;ing it didn&amp;#39;t seem reasonable with my upload speed / bandwidth limitations. I decided to go with the tried and true sneakernet method. I decided to take advantage of ZFS&amp;#39; snapshotting functionality. The first script automatically creates incremental snapshots holding all the changes between the latest update on the backup machine and the current date as of running the script. These snapshots can be ferried between the original and backup machines via a portable hard drive. The second script applies runs the backup machine and will apply the snapshots from the portable hard drive to the backup system.  &lt;/p&gt;\n\n&lt;p&gt;I thought I&amp;#39;d share the scripts in case their useful for anyone else sneakernetting their backups as well as some feedback from the community.  &lt;/p&gt;\n\n&lt;p&gt;The first script (ideally set to run periodically via cron/systemd.timers)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#!/bin/bash\n\n # This is part one of my &amp;#39;patented&amp;#39; sneakernet backup system. The inte-\n # nded usecase is to use a portable hard drive to shuttle incremental \n # zfs snapshots from the original server (where this script will reside\n # and run) to the backup server. There are a few requirements:\n # 1) Both the original and backup servers have matching datasets\n # 2) The portable hard drive is formatted with ZFS with a pool named\n #    Sync\n # 3) When the portable hard drive makes its way back to the original \n #    server, you zpool import it and rsync the contents of the \n #    ZFS_incremental folder onto it\n\n# Honestly this probably isn&amp;#39;t needed in this script but I put it in all\n# my bash scripts\nset -e\n\n# Determine current date\ncurrentdate=$(date +&amp;quot;%Y-%m-%d&amp;quot;)\n\n# Determine the last time we synced with the backupserver. The file is\n# generated by the backup server and either rsynced via SSH or pulled\n# from the portable hard drive\nlwsync=$(tail -n -1 /root/last_backup_sync.txt)\n\n# Using the current date to create snapshot names. Endpoints for the\n# incremental snapshot\ncurISOssnapshot=&amp;quot;Pool1/ISOs@${currentdate}&amp;quot;\ncurlinuxsnapshot=&amp;quot;Pool1/linux@${currentdate}&amp;quot;\ncurpubdomainsnapshot=&amp;quot;Pool1/pubdomain@${currentdate}&amp;quot;\n\n# Using the last date we to create snapshot names. Start-points for the\n# incremental snapshot\nlwsyncISOssnapshot=&amp;quot;Pool1/ISOs@${lwsync}&amp;quot;\nlwsynclinuxsnapshot=&amp;quot;Pool1/linux@${lwsync}&amp;quot;\nlwsyncpubdomainsnapshot=&amp;quot;Pool1/pubdomain@${lwsync}&amp;quot;\n\n\n# use zfs send to save incremental snapshots of each pool\necho $curISOssnapshot\necho $curlinuxsnapshot\necho $curpubdomainsnapshot\n\n\n# Create snapshots of with current date\nzfs snapshot $curISOssnapshot\nzfs snapshot $curlinuxsnapshot\nzfs snapshot $curpubdomainsnapshot\necho &amp;quot;Snapshots made ${currentdate}&amp;quot; &amp;gt;&amp;gt; /root/backupscript.log\n\n# Create Incremental Snapshots of ISOs, linux and pubdomain\nzfs send -Ri $lwsyncISOssnapshot $curISOssnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/ISOs_latest\nzfs send -Ri $lwsynclinuxsnapshot $curlinuxsnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/linux_latest\nzfs send -Ri $lwsyncpubdomainsnapshot $curpubdomainsnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/pubdomain_latest\necho &amp;quot;Incremental snapshots saved to /mnt/Pool1/ZFS_incremental/ &amp;quot; &amp;gt;&amp;gt; /root/backupscript.log\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The second script (ideally run automatically via cron/systemd.timers. should be setup to run every day for when the backup machine is located at a relative with low technical abilities):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#! /bin/bash\n\n# This script is the second part of my &amp;#39;patented&amp;#39; sneakernet update \n# system. The script is intended to be run on the backup server once the \n# portable hard drive is plugged into the machine. This script does have \n# a few dependencies: \n# 1) the backup server must have the same zfs datasets that are going to \n#    be transferred from the portable hard drive. \n# 2) the backup server must have automatic snapshotting turned OFF. \n# 3) this script assumes that you can ssh into your primary server. \n\n# Basically stops the script when something goes wrong. Mostly for when\n# the script is set to run automatically (via systemd.timers or crontab)\n# and the \nset -e\n\n# set variables for the latest snapshots of the backup system (useful\n# for later\nlast_ISOs_snapshot=$(zfs list -H -o name -t snapshot BackupPool/ISOs | tail -n -1)\nlast_linux_snapshot=$(zfs list -H -o name -t snapshot BackupPool/linux | tail -n -1)\nlast_pubdomain_snapshot=$(zfs list -H -o name -t snapshot BackupPool/pubdomain | tail -n -1)\n\n# Honestly, just here to provide some output for logging to see where\n# it might have failed\necho $last_ISOs_snapshot\necho $last_linux_snapshot\necho $last_pubdomain_snapshot\n\necho $(date +&amp;quot;%Y-%m-%d&amp;quot;) &amp;gt;&amp;gt; /root/sync.log\necho $(date +&amp;quot;%Y-%m-%d&amp;quot;)\n\n# Imports the pool located on the portable hard drive\nzpool import -o altroot=/mnt Sync\necho &amp;quot;Import Done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Import Done&amp;quot;\n\n# Rollback to last snapshot. This is necessary in case the person using\n# the backupserver happened to accidentaly ignore your instructions and\n# alters any of the pools that are going to be synced.\nzfs rollback $last_ISOs_snapshot\nzfs rollback $last_linux_snapshot\nzfs rollback $last_pubdomain_snapshot\necho &amp;quot;Snapshot rollback done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Snapshot rollback done&amp;quot;\n\n# Sync Incremental Snapshots of ISOs, linux and pubdomain\nzfs recv -F BackupPool/ISOs &amp;lt; /mnt/Sync/ISOs_latest\necho &amp;quot;ISOs updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;ISOs updated&amp;quot;\n\nzfs recv -F BackupPool/linux &amp;lt; /mnt/Sync/linux_latest\necho &amp;quot;linux updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;linux updated&amp;quot;\n\nzfs recv -F BackupPool/pubdomain &amp;lt; /mnt/Sync/pubdomain_latest\necho &amp;quot;pubdomain updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;pubdomain updated&amp;quot;\n\necho &amp;quot;Sync Complete&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Sync Complete&amp;quot;\n\n# Create the file that will be referenced by the first script when det-\n# ermining what the start point is for the incremental snapshot. Saves \n# it to the portable hard drive in case ssh&amp;#39;ing into the progenitor \n# server is not possible\nzfs list -H -o name -t snapshot BackupPool/ISOs &amp;gt; tempa.txt\nsed &amp;#39;s/BackupPool\\/ISOs@//&amp;#39; tempa.txt &amp;gt; /root/last_backup_sync.txt\nrm tempa.txt\ncp /root/last_backup_sync.txt /mnt/Sync/last_backup_sync.txt\necho &amp;quot;Update last_backup_sync.txt done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Update last_backup_sync.txt done&amp;quot;\n\n# Sends the file to the original server. Means that the originator ser-\n# ver can start making the correct incremental snapshots. This, in turn,\n# means that when the original server receives the portable hard drive,\n# it need only copy over the pregenerated incremental snapshots.\nrsync -e &amp;quot;ssh&amp;quot; -avz /root/last_backup_sync.txt root@###.###.###.###:/root/\necho &amp;quot;Send last_backup_sync.txt to Original server&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Send last_backup_sync.txt to Original server&amp;quot;\n\n# Basically ejects the portable hard drive.\nzpool export Sync\necho &amp;quot;Disconnect Sync complete&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;---------------------------------------------------&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Disconnect Sync complete&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Cheers,&lt;br/&gt;\nBasilisk_hunters&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gpedp", "is_robot_indexable": true, "report_reasons": null, "author": "Basilisk_hunters", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gpedp/i_created_some_scripts_to_backup_my_data_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gpedp/i_created_some_scripts_to_backup_my_data_via/", "subreddit_subscribers": 671875, "created_utc": 1677812574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " My drives have around 100k files on them, totalling 450GB.\n\nChecking the properties of folders on the old HDD is almost instant. Connected via USB3 to SATA adapter. On my SSD it takes forever. It's been about 10minuts so far and it's still going. I'm comparing two very similar folders and checking their file count and size. I even changed the USB-C cable to a shorter one that came with the SSD but its still so slow. Why could this be? Spec's below\n\nSeagate HDD\n\n* 5900rpm\n* 2TB\n* USB 3-SATA adapter\n\nSandisk SSD Sandisk Extreme Portable (Gen1)\n\n* 550MB/\n* 1TB\n* USB-C-USB-C", "author_fullname": "t2_4d5ehhv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does SSD (USB-C/USB-C) take so long to read file properties vs old HDD (USB3/SATA)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ge2gq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677791252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My drives have around 100k files on them, totalling 450GB.&lt;/p&gt;\n\n&lt;p&gt;Checking the properties of folders on the old HDD is almost instant. Connected via USB3 to SATA adapter. On my SSD it takes forever. It&amp;#39;s been about 10minuts so far and it&amp;#39;s still going. I&amp;#39;m comparing two very similar folders and checking their file count and size. I even changed the USB-C cable to a shorter one that came with the SSD but its still so slow. Why could this be? Spec&amp;#39;s below&lt;/p&gt;\n\n&lt;p&gt;Seagate HDD&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;5900rpm&lt;/li&gt;\n&lt;li&gt;2TB&lt;/li&gt;\n&lt;li&gt;USB 3-SATA adapter&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sandisk SSD Sandisk Extreme Portable (Gen1)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;550MB/&lt;/li&gt;\n&lt;li&gt;1TB&lt;/li&gt;\n&lt;li&gt;USB-C-USB-C&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ge2gq", "is_robot_indexable": true, "report_reasons": null, "author": "i-dm", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ge2gq/why_does_ssd_usbcusbc_take_so_long_to_read_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ge2gq/why_does_ssd_usbcusbc_take_so_long_to_read_file/", "subreddit_subscribers": 671875, "created_utc": 1677791252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had an Unraid server for 6 years now, and have decided on building another system - \\*which will definitely be ZFS\\* (speed considerations + I already have all the drives).  \nI'm really conflicted between buying another Unraid license and using the ZFS Plugin until 6.12 comes out with official ZFS support, but on the other hand there's TrueNAS Scale which is free and has a lot of users (and also TrueCharts so a lot of available apps).\n\nWhat are the cons of TrueNAS Scale? Is Kubernetes better than the Unraid Docker?And how about the community and support for the apps? (Like TrueCharts)\n\nThanks in advance for any help!", "author_fullname": "t2_143yl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS System - TrueNAS Scale or ZFS Plugin+Unraid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11gsrmp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677822496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had an Unraid server for 6 years now, and have decided on building another system - *which will definitely be ZFS* (speed considerations + I already have all the drives).&lt;br/&gt;\nI&amp;#39;m really conflicted between buying another Unraid license and using the ZFS Plugin until 6.12 comes out with official ZFS support, but on the other hand there&amp;#39;s TrueNAS Scale which is free and has a lot of users (and also TrueCharts so a lot of available apps).&lt;/p&gt;\n\n&lt;p&gt;What are the cons of TrueNAS Scale? Is Kubernetes better than the Unraid Docker?And how about the community and support for the apps? (Like TrueCharts)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "59TB RAID6 | 43TB Usable", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gsrmp", "is_robot_indexable": true, "report_reasons": null, "author": "n0llbyte", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11gsrmp/zfs_system_truenas_scale_or_zfs_pluginunraid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gsrmp/zfs_system_truenas_scale_or_zfs_pluginunraid/", "subreddit_subscribers": 671875, "created_utc": 1677822496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_a1w685cb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are these any good? $130 at Costco. or is there anything better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11glm1t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vnyjYihYE3KsC85uwlTmVfRhZw5K5Kr0Jc45ADlgkHM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677802563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yk25kz1fkgla1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?auto=webp&amp;v=enabled&amp;s=a19a23fd3def1c7d4d07f1b5fa808af7b1625cf2", "width": 6936, "height": 9248}, "resolutions": [{"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33121898d9c7fa8a349aba5be2c2cf92a0698dd3", "width": 108, "height": 144}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d391560c65fa468587dd99187454e7f96da555c0", "width": 216, "height": 288}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29a9a067b00a99664ed0163d9e00051f8fd575ea", "width": 320, "height": 426}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=701bd21366e9494dfbc92111f8fed777d052440a", "width": 640, "height": 853}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d184ebbd2c889f99300850138c380889ca9cc225", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2694bccbf32a1b8add40481652641e1e7d6ceba7", "width": 1080, "height": 1440}], "variants": {}, "id": "y2qsInJSywm4T4RRIBKBd3j79xE18CpKUMDVCd2t1pM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11glm1t", "is_robot_indexable": true, "report_reasons": null, "author": "goofgroot", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11glm1t/are_these_any_good_130_at_costco_or_is_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yk25kz1fkgla1.jpg", "subreddit_subscribers": 671875, "created_utc": 1677802563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys! I am running a small 100TB home NAS (windows based) and I am wondering what software I should buy which automatically does drive surface tests etc on schedule to keep abreast of any impending drive failures! \n\nCorrect me if I am wrong but from what I've heard I think that the gold standard is hard disk sentinel? or there are better softwares out there than that ?", "author_fullname": "t2_723w1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What software is good for drive management?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gci5k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677787659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys! I am running a small 100TB home NAS (windows based) and I am wondering what software I should buy which automatically does drive surface tests etc on schedule to keep abreast of any impending drive failures! &lt;/p&gt;\n\n&lt;p&gt;Correct me if I am wrong but from what I&amp;#39;ve heard I think that the gold standard is hard disk sentinel? or there are better softwares out there than that ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gci5k", "is_robot_indexable": true, "report_reasons": null, "author": "sephiroth_vg", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gci5k/what_software_is_good_for_drive_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gci5k/what_software_is_good_for_drive_management/", "subreddit_subscribers": 671875, "created_utc": 1677787659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, thanks in advance for your help!  \n\n\nMy BF and I have a 40Tb NAS file server in a 4U, 20 drive rackmount server chassis. For a bunch of reasons, not least of which is that some of the drives are quite old, we're planning to condense the content from the various 3 and 6Tb drives onto newer, higher density HDDs. Ideally, we'd like to yes, get more storage space, but also mount them in a smaller chassis. We'd repurpose the healthiest drives we already have by replacing the HDDs in our NAS backup server with ones retired from the file server.   \n\n\nWe like the WD Red Pro drives, and will probably get several of those. Clearly, transferring the data will take some \\*serious\\* time. If possible, we'd like to speed up that process and avoid burdening our machines with the data transfer overhead.   \n\n\nAre harddrive duplicators worth it? If so, any thoughts on which one(s) are better at copying the data quickly \\*and\\* accurately?  \n\n\nI'd also welcome any advice on the least painful way to do this. Please keep in mind this is a home lab setup with 2 Linux NASes (file server and backup server), and while we're both tech savvy, we are intermediate 'Nix users, not pros.   \n\n\nThanks!", "author_fullname": "t2_jxrwr82a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying data between multiple HDDs. Advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g8ukc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677778940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, thanks in advance for your help!  &lt;/p&gt;\n\n&lt;p&gt;My BF and I have a 40Tb NAS file server in a 4U, 20 drive rackmount server chassis. For a bunch of reasons, not least of which is that some of the drives are quite old, we&amp;#39;re planning to condense the content from the various 3 and 6Tb drives onto newer, higher density HDDs. Ideally, we&amp;#39;d like to yes, get more storage space, but also mount them in a smaller chassis. We&amp;#39;d repurpose the healthiest drives we already have by replacing the HDDs in our NAS backup server with ones retired from the file server.   &lt;/p&gt;\n\n&lt;p&gt;We like the WD Red Pro drives, and will probably get several of those. Clearly, transferring the data will take some *serious* time. If possible, we&amp;#39;d like to speed up that process and avoid burdening our machines with the data transfer overhead.   &lt;/p&gt;\n\n&lt;p&gt;Are harddrive duplicators worth it? If so, any thoughts on which one(s) are better at copying the data quickly *and* accurately?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d also welcome any advice on the least painful way to do this. Please keep in mind this is a home lab setup with 2 Linux NASes (file server and backup server), and while we&amp;#39;re both tech savvy, we are intermediate &amp;#39;Nix users, not pros.   &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11g8ukc", "is_robot_indexable": true, "report_reasons": null, "author": "renarde33", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g8ukc/copying_data_between_multiple_hdds_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g8ukc/copying_data_between_multiple_hdds_advice/", "subreddit_subscribers": 671875, "created_utc": 1677778940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a whole bunch of PDFs (a few thousand pages over a few hundred files), and I want to search through them but none of them have any kind of OCR/highlighting/searchability. What's a good tool that would let me scan all of these in bulk and produce something easily searched?\n\nIdeally:\n\n--Runs on Windows\n\n--Is fairly simply to use (I can cope with command lines but my willpower falters at 'compiling and tweaking')\n\n--Is free (unfortunately I can't access Adobe Acrobat)", "author_fullname": "t2_t84b2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting OCR on lots of PDFs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ge41a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677791356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a whole bunch of PDFs (a few thousand pages over a few hundred files), and I want to search through them but none of them have any kind of OCR/highlighting/searchability. What&amp;#39;s a good tool that would let me scan all of these in bulk and produce something easily searched?&lt;/p&gt;\n\n&lt;p&gt;Ideally:&lt;/p&gt;\n\n&lt;p&gt;--Runs on Windows&lt;/p&gt;\n\n&lt;p&gt;--Is fairly simply to use (I can cope with command lines but my willpower falters at &amp;#39;compiling and tweaking&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;--Is free (unfortunately I can&amp;#39;t access Adobe Acrobat)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ge41a", "is_robot_indexable": true, "report_reasons": null, "author": "Wyverncraft", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ge41a/getting_ocr_on_lots_of_pdfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ge41a/getting_ocr_on_lots_of_pdfs/", "subreddit_subscribers": 671875, "created_utc": 1677791356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm sending in some old miniDV cassettes to get them digitalized. Now I asked them if they can add the date of the recording onto the video, but since they'll do it afterwards. it won't be like the old school VHS tapes (he actually let me know they use Arial for that lol). So I asked them if I can have the metadata to do it myself in Premiere Pro so I can make it look better.  \nHe told me he can send me the files as AVI files (besides the normal mp4 they usually send) and I can take all the metadata from there.  \n\n\n**Now my question is**, is it easy to find all the single date and time info for all the clips within 1 file? I never done this, so I don't know how to get the individual recording date for each clip (since there are multiple events recorded on each cassette). He said they'll be set up like chapters...  \n(I'm a mac user)", "author_fullname": "t2_12wt346g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best way to extract Metadata (Date) from AVI Files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g4put", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677768896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sending in some old miniDV cassettes to get them digitalized. Now I asked them if they can add the date of the recording onto the video, but since they&amp;#39;ll do it afterwards. it won&amp;#39;t be like the old school VHS tapes (he actually let me know they use Arial for that lol). So I asked them if I can have the metadata to do it myself in Premiere Pro so I can make it look better.&lt;br/&gt;\nHe told me he can send me the files as AVI files (besides the normal mp4 they usually send) and I can take all the metadata from there.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Now my question is&lt;/strong&gt;, is it easy to find all the single date and time info for all the clips within 1 file? I never done this, so I don&amp;#39;t know how to get the individual recording date for each clip (since there are multiple events recorded on each cassette). He said they&amp;#39;ll be set up like chapters...&lt;br/&gt;\n(I&amp;#39;m a mac user)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11g4put", "is_robot_indexable": true, "report_reasons": null, "author": "haebollago", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g4put/best_way_to_extract_metadata_date_from_avi_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g4put/best_way_to_extract_metadata_date_from_avi_files/", "subreddit_subscribers": 671875, "created_utc": 1677768896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for software that will scan every photo in my library and add tags to them to allow searching. Ideally, this will function similarly to the photo search from iOS where you can search \"shoe\" and shoes will appear.", "author_fullname": "t2_50n4upyn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo Library Scan &amp; Organization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11go6ou", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677809271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for software that will scan every photo in my library and add tags to them to allow searching. Ideally, this will function similarly to the photo search from iOS where you can search &amp;quot;shoe&amp;quot; and shoes will appear.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11go6ou", "is_robot_indexable": true, "report_reasons": null, "author": "c00pdwg", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11go6ou/photo_library_scan_organization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11go6ou/photo_library_scan_organization/", "subreddit_subscribers": 671875, "created_utc": 1677809271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Until now I've relied on various clouds for my important  data. I don't like  it but  it's easy &amp; ensures that I have all of this  data in 2 locations. Makes it easy to access on multiple machines or when I switch  to a new laptop as I  just did. But in the past I've had lots of large photos,  raw/jpg/etc that I've lost track of simply because  I moved to a new desktop and didn't secure the old data before wiping  out the disk. In those cases I  typically kept the jpgs &amp; ditched  the raw thinking I'd never need it. Now  I wish I had it. Then  I have data from  my company (1 person company, software programmer, accounting software) that I need to keep until  I wind  up the company  in a year or two. So it's all  of the source code, sample data etc. I will want this going forward because I have written software for myself to track  my investments so that needs to  be secure.\n\nI've decided to get a Synology box to start. I was thinking about the DS220+ but I see the DS223 just came out and  expect a DS223+ shortly so I'm waiting on that. It seems that the DS220+ is still preferable to the  DS223 but comparisons are hard  to come by. Ideally I'd like to get to a DS923+ but I'm thinking I'll start with a 22\\* and then gain some experience &amp; get a 923 in  the future and use the 22\\* to mirror data from the 923. There are very cheap WD Red Pro disks on sale in Canada right now ($399 for 18Tb) so  I  ordered 2 of those before the sale ends. I'll  start with those in whatever box I  get and do a RAID 1. It's way more space than  I  need but cheap enough  to  just bite  the bullet to start.\n\nI'd love  any suggestions you  may have.", "author_fullname": "t2_4mhep53l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting out focusing on securing my data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g71ie", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677774614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Until now I&amp;#39;ve relied on various clouds for my important  data. I don&amp;#39;t like  it but  it&amp;#39;s easy &amp;amp; ensures that I have all of this  data in 2 locations. Makes it easy to access on multiple machines or when I switch  to a new laptop as I  just did. But in the past I&amp;#39;ve had lots of large photos,  raw/jpg/etc that I&amp;#39;ve lost track of simply because  I moved to a new desktop and didn&amp;#39;t secure the old data before wiping  out the disk. In those cases I  typically kept the jpgs &amp;amp; ditched  the raw thinking I&amp;#39;d never need it. Now  I wish I had it. Then  I have data from  my company (1 person company, software programmer, accounting software) that I need to keep until  I wind  up the company  in a year or two. So it&amp;#39;s all  of the source code, sample data etc. I will want this going forward because I have written software for myself to track  my investments so that needs to  be secure.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve decided to get a Synology box to start. I was thinking about the DS220+ but I see the DS223 just came out and  expect a DS223+ shortly so I&amp;#39;m waiting on that. It seems that the DS220+ is still preferable to the  DS223 but comparisons are hard  to come by. Ideally I&amp;#39;d like to get to a DS923+ but I&amp;#39;m thinking I&amp;#39;ll start with a 22* and then gain some experience &amp;amp; get a 923 in  the future and use the 22* to mirror data from the 923. There are very cheap WD Red Pro disks on sale in Canada right now ($399 for 18Tb) so  I  ordered 2 of those before the sale ends. I&amp;#39;ll  start with those in whatever box I  get and do a RAID 1. It&amp;#39;s way more space than  I  need but cheap enough  to  just bite  the bullet to start.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love  any suggestions you  may have.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11g71ie", "is_robot_indexable": true, "report_reasons": null, "author": "DaveWpgC", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g71ie/starting_out_focusing_on_securing_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g71ie/starting_out_focusing_on_securing_my_data/", "subreddit_subscribers": 671875, "created_utc": 1677774614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! I'm running Windows. I have an external hard drive with my data on it using the ExFAT file system. I know this is a terrible idea due to lack of journaling, so I want to reformat it to NTFS.\n\nI do backups on the free version of VEEAM to a second external drive. \n\nIf I reformat my first drive to NTFS, can I then restore through VEEAM, and all my data will just be ready and waiting? Do I need to anything special to prepare for this (like, should I do a full backup in VEEAM as opposed to an incremental one before I do this?)\n\nThanks!", "author_fullname": "t2_2gu72213", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Veeam Restoring after Reformatting source disk to change file format?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g6msm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677773612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m running Windows. I have an external hard drive with my data on it using the ExFAT file system. I know this is a terrible idea due to lack of journaling, so I want to reformat it to NTFS.&lt;/p&gt;\n\n&lt;p&gt;I do backups on the free version of VEEAM to a second external drive. &lt;/p&gt;\n\n&lt;p&gt;If I reformat my first drive to NTFS, can I then restore through VEEAM, and all my data will just be ready and waiting? Do I need to anything special to prepare for this (like, should I do a full backup in VEEAM as opposed to an incremental one before I do this?)&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11g6msm", "is_robot_indexable": true, "report_reasons": null, "author": "exixius", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g6msm/veeam_restoring_after_reformatting_source_disk_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g6msm/veeam_restoring_after_reformatting_source_disk_to/", "subreddit_subscribers": 671875, "created_utc": 1677773612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone! I'm trying to download all the songs that are in a few of my playlists on Soundcloud. Is there a tool available that would allow me to download an entire playlist on Soundcloud?  \n\nI wanted to point out I found a tool ([https://www.ahmd.world/blog/soundcloud-likes-to-playlist-v2](https://www.ahmd.world/blog/soundcloud-likes-to-playlist-v2)) that allowed me to move all my likes into Playlists. You might want to consider doing this as your likes are public, whereas you can make playlists private.\n\nAny help would be much appreciated. Thanks", "author_fullname": "t2_w13bg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a tool to download an entire Soundcloud playlist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g00xp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677754941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I&amp;#39;m trying to download all the songs that are in a few of my playlists on Soundcloud. Is there a tool available that would allow me to download an entire playlist on Soundcloud?  &lt;/p&gt;\n\n&lt;p&gt;I wanted to point out I found a tool (&lt;a href=\"https://www.ahmd.world/blog/soundcloud-likes-to-playlist-v2\"&gt;https://www.ahmd.world/blog/soundcloud-likes-to-playlist-v2&lt;/a&gt;) that allowed me to move all my likes into Playlists. You might want to consider doing this as your likes are public, whereas you can make playlists private.&lt;/p&gt;\n\n&lt;p&gt;Any help would be much appreciated. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11g00xp", "is_robot_indexable": true, "report_reasons": null, "author": "risingmiles", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g00xp/is_there_a_tool_to_download_an_entire_soundcloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g00xp/is_there_a_tool_to_download_an_entire_soundcloud/", "subreddit_subscribers": 671875, "created_utc": 1677754941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there\n\nI have around 4  terabytes of data on my current desktop but my internal hard drives are failing so i'm planning to transfer that data to new hdd and there is two options i have in mind internal again and an external hdd so am asking what is the best choice here regarding durability for only long term data backup cause i will not use the new hdd or plug it into my pc\n\nIn my local market there is many internal hdd brands but for externals am torn between wd my passport and my book\n\nThanks in advance", "author_fullname": "t2_i93yi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best choices for longterm data backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fx3uz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677744442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;I have around 4  terabytes of data on my current desktop but my internal hard drives are failing so i&amp;#39;m planning to transfer that data to new hdd and there is two options i have in mind internal again and an external hdd so am asking what is the best choice here regarding durability for only long term data backup cause i will not use the new hdd or plug it into my pc&lt;/p&gt;\n\n&lt;p&gt;In my local market there is many internal hdd brands but for externals am torn between wd my passport and my book&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11fx3uz", "is_robot_indexable": true, "report_reasons": null, "author": "JackLancer", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11fx3uz/best_choices_for_longterm_data_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11fx3uz/best_choices_for_longterm_data_backup/", "subreddit_subscribers": 671875, "created_utc": 1677744442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I just bought a Samsung PM9A3 for my NAS. But the part number on it is different from what Samsung has on their website.   \nWhat I have in hand is MZQL23T8HCLS, and what Samsung have on their website is MZQL23T8HCJS.   \nI can still find MZQL23T8HCLS's product page through google but cannot find it on the Samsung product list. \n\nMy question is does anyone know why there is a difference in part number between two exact same product. \n\nAnd could anyone please tell me if there is a decoder for Samsung ssd? I found decoder for WD and Kinsgton, but nothing for Samsung.", "author_fullname": "t2_jzmpf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Same Samsung SSD model with different part numbers, how do I decode it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11gtz6d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677826488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I just bought a Samsung PM9A3 for my NAS. But the part number on it is different from what Samsung has on their website.&lt;br/&gt;\nWhat I have in hand is MZQL23T8HCLS, and what Samsung have on their website is MZQL23T8HCJS.&lt;br/&gt;\nI can still find MZQL23T8HCLS&amp;#39;s product page through google but cannot find it on the Samsung product list. &lt;/p&gt;\n\n&lt;p&gt;My question is does anyone know why there is a difference in part number between two exact same product. &lt;/p&gt;\n\n&lt;p&gt;And could anyone please tell me if there is a decoder for Samsung ssd? I found decoder for WD and Kinsgton, but nothing for Samsung.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gtz6d", "is_robot_indexable": true, "report_reasons": null, "author": "kdsl001", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gtz6d/same_samsung_ssd_model_with_different_part/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gtz6d/same_samsung_ssd_model_with_different_part/", "subreddit_subscribers": 671875, "created_utc": 1677826488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_vtl8mp58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pure claims 300TB flash drives coming 2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 134, "top_awarded_type": null, "hide_score": false, "name": "t3_11grde5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/e_yYYge0eH4A2tqfsuTthxLL_SojId3KD7c04WR_kus.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677818199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blocksandfiles.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blocksandfiles.com/2023/03/01/300tb-flash-drives-coming-from-pure-storage/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/T0gdlp0cgZkeLfcR1W-n51c4KxxrbM_3St1HxKdrbxw.jpg?auto=webp&amp;v=enabled&amp;s=9359f9eb3090c3fb2935d7a5752ceed7a6366766", "width": 950, "height": 911}, "resolutions": [{"url": "https://external-preview.redd.it/T0gdlp0cgZkeLfcR1W-n51c4KxxrbM_3St1HxKdrbxw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59d8a67a5e86ebf9cca8b0c0678c225b5fc7bd83", "width": 108, "height": 103}, {"url": "https://external-preview.redd.it/T0gdlp0cgZkeLfcR1W-n51c4KxxrbM_3St1HxKdrbxw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffa789aeba8cfd904c1c2c1cb899a1628973dc7e", "width": 216, "height": 207}, {"url": "https://external-preview.redd.it/T0gdlp0cgZkeLfcR1W-n51c4KxxrbM_3St1HxKdrbxw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ee2b490339e22c4320a8d9ae31759890e1fbfcf", "width": 320, "height": 306}, {"url": "https://external-preview.redd.it/T0gdlp0cgZkeLfcR1W-n51c4KxxrbM_3St1HxKdrbxw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=177647b53f9b21f05283225b480955b879646a74", "width": 640, "height": 613}], "variants": {}, "id": "9-pQp1Q0n4znPXCYr_MgiZMyUOQwRlZsb02bw2eeWEY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11grde5", "is_robot_indexable": true, "report_reasons": null, "author": "Phantom_Poops", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11grde5/pure_claims_300tb_flash_drives_coming_2026/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blocksandfiles.com/2023/03/01/300tb-flash-drives-coming-from-pure-storage/", "subreddit_subscribers": 671875, "created_utc": 1677818199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using MultCloud right now. Here's my issue,.. I have 4 dropbox accounts using different emails... Can I can add them even though the email I used to create the MultCloud account is assigned to only one of the 4 dropbox accounts? \n\nSide note... I'd like to keep it organized if that's possible. Does this mean I need several MultCloud accounts too? \n\nThanks in advance.", "author_fullname": "t2_jb6dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "several email addresses using same clouds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gql0c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677815912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using MultCloud right now. Here&amp;#39;s my issue,.. I have 4 dropbox accounts using different emails... Can I can add them even though the email I used to create the MultCloud account is assigned to only one of the 4 dropbox accounts? &lt;/p&gt;\n\n&lt;p&gt;Side note... I&amp;#39;d like to keep it organized if that&amp;#39;s possible. Does this mean I need several MultCloud accounts too? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gql0c", "is_robot_indexable": true, "report_reasons": null, "author": "badcat130", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gql0c/several_email_addresses_using_same_clouds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gql0c/several_email_addresses_using_same_clouds/", "subreddit_subscribers": 671875, "created_utc": 1677815912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a Seagate (edit) Barracuda 8tb which I bought because they are quiet. It is an SMR drive so to move a lot of data onto the drive I am doing it in chunks of a few 100 GB at a time. And then I give the SMR drive some time to reorganize the data before I transfer another chunk.\n\nIs there a command or utility in Windows or a third party tool that I can use to find out if the data reorganization is done?", "author_fullname": "t2_lx4rfjoi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SMR drive - how to tell it has finished reorganizing the drive - without touching it", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11got0o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677820475.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677810946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Seagate (edit) Barracuda 8tb which I bought because they are quiet. It is an SMR drive so to move a lot of data onto the drive I am doing it in chunks of a few 100 GB at a time. And then I give the SMR drive some time to reorganize the data before I transfer another chunk.&lt;/p&gt;\n\n&lt;p&gt;Is there a command or utility in Windows or a third party tool that I can use to find out if the data reorganization is done?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11got0o", "is_robot_indexable": true, "report_reasons": null, "author": "PeterCraig65", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11got0o/smr_drive_how_to_tell_it_has_finished/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11got0o/smr_drive_how_to_tell_it_has_finished/", "subreddit_subscribers": 671875, "created_utc": 1677810946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello guys i need help,just downloaded a profile through instaloader on Android using termux but can't find where the pictures are saved", "author_fullname": "t2_aiq1l3mb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Instaloader help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11go9b3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677809468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys i need help,just downloaded a profile through instaloader on Android using termux but can&amp;#39;t find where the pictures are saved&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11go9b3", "is_robot_indexable": true, "report_reasons": null, "author": "ArousedMtherfaker", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11go9b3/instaloader_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11go9b3/instaloader_help/", "subreddit_subscribers": 671875, "created_utc": 1677809468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What I really want is a program that overlays with explorer (Windows), so when I'm searching for a file to upload or send, I can see the ranking and tags that I've added to the file, without slowing things down too much. \n\n&amp;#x200B;\n\nDoes TagSpaces work like that, or is it only some browser interface where you can see your tags and ratings?", "author_fullname": "t2_fl91vcm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for software that lets me rank (and maybe tag) files? Is Tagspaces a good and safe option?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gkl0g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677800220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I really want is a program that overlays with explorer (Windows), so when I&amp;#39;m searching for a file to upload or send, I can see the ranking and tags that I&amp;#39;ve added to the file, without slowing things down too much. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does TagSpaces work like that, or is it only some browser interface where you can see your tags and ratings?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gkl0g", "is_robot_indexable": true, "report_reasons": null, "author": "Atlantic0ne", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gkl0g/looking_for_software_that_lets_me_rank_and_maybe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gkl0g/looking_for_software_that_lets_me_rank_and_maybe/", "subreddit_subscribers": 671875, "created_utc": 1677800220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After some consideration, I\u2019ve decided to backup my mediaserver to BackBlaze B2.  Just doing an rclone sync for the time being, as I continue to clean up my local directories. \n\nI\u2019m not using encryption on thr client side, so I imagine that backblaze has the capability to see everything I\u2019ve backed up. \n\nMy question is, does anyone know if they scan private B2 buckets for copyrighted data? It would be might inconvienent if they did, and I\u2019d probably have to come up with a new strategy to encrypt and remove obfuscate before uploading\n\nThx!", "author_fullname": "t2_r6bjole0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Backup q?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gj5go", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677798281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After some consideration, I\u2019ve decided to backup my mediaserver to BackBlaze B2.  Just doing an rclone sync for the time being, as I continue to clean up my local directories. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m not using encryption on thr client side, so I imagine that backblaze has the capability to see everything I\u2019ve backed up. &lt;/p&gt;\n\n&lt;p&gt;My question is, does anyone know if they scan private B2 buckets for copyrighted data? It would be might inconvienent if they did, and I\u2019d probably have to come up with a new strategy to encrypt and remove obfuscate before uploading&lt;/p&gt;\n\n&lt;p&gt;Thx!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gj5go", "is_robot_indexable": true, "report_reasons": null, "author": "AuthenticImposter", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gj5go/cloud_backup_q/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gj5go/cloud_backup_q/", "subreddit_subscribers": 671875, "created_utc": 1677798281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently working on devising a plan for a way to store a good amount of media from across the different orgs and other groups on our university campus to go into a time capsule to be opened on our 50th reunion.\n\nI already did base research in figuring out roughly what storage types and methods I should be going for but wanted to get external opinions/questions answered before I commit to it.\n\nIdeally the plan is to go with optical disks such as BR or M-Disc (only concern with the latter is finding hardware for burning) and immediately after place it back in its factory case. I'm also toying with the idea of adding a vacuum sealed bag + silica packs since this will be buried underground along with other items in the capsule. \n\nMy main concerns are budget, both monetary and timewise, is this feasible with a lower $100-200 budget and a timeframe of about 1 month? also if there are any further precautions I should look into taking. \n\nIf there's anything I missed please lmk thank you", "author_fullname": "t2_3d2xhhyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time Capsule options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ghpb8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677796679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working on devising a plan for a way to store a good amount of media from across the different orgs and other groups on our university campus to go into a time capsule to be opened on our 50th reunion.&lt;/p&gt;\n\n&lt;p&gt;I already did base research in figuring out roughly what storage types and methods I should be going for but wanted to get external opinions/questions answered before I commit to it.&lt;/p&gt;\n\n&lt;p&gt;Ideally the plan is to go with optical disks such as BR or M-Disc (only concern with the latter is finding hardware for burning) and immediately after place it back in its factory case. I&amp;#39;m also toying with the idea of adding a vacuum sealed bag + silica packs since this will be buried underground along with other items in the capsule. &lt;/p&gt;\n\n&lt;p&gt;My main concerns are budget, both monetary and timewise, is this feasible with a lower $100-200 budget and a timeframe of about 1 month? also if there are any further precautions I should look into taking. &lt;/p&gt;\n\n&lt;p&gt;If there&amp;#39;s anything I missed please lmk thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ghpb8", "is_robot_indexable": true, "report_reasons": null, "author": "Mango_yoshi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ghpb8/time_capsule_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ghpb8/time_capsule_options/", "subreddit_subscribers": 671875, "created_utc": 1677796679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a problem with my segate fire cuda hdd, during transfers it doesn't go over 5 mbs, the hard disk is attached to a usb hub ( RSHTECH Active USB 3.0 Hub with 36W(12V/3A) Power Supply Aluminum 10 USB 3.0 Ports Hub with Separate LED Switches and Power Adapter for Linux, Windows, Mac OS and More (RSH-A10) https://amzn.eu/d/9K9lqCE ), which in turn is connected to the pc via a usb 3.2 port gen 2 ( red ) can you advise me something ?  Is there anything that might annoy him during the transfer?", "author_fullname": "t2_3w77x29v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "slow hdd transfer 5 mbps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gdoaa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677790357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a problem with my segate fire cuda hdd, during transfers it doesn&amp;#39;t go over 5 mbs, the hard disk is attached to a usb hub ( RSHTECH Active USB 3.0 Hub with 36W(12V/3A) Power Supply Aluminum 10 USB 3.0 Ports Hub with Separate LED Switches and Power Adapter for Linux, Windows, Mac OS and More (RSH-A10) &lt;a href=\"https://amzn.eu/d/9K9lqCE\"&gt;https://amzn.eu/d/9K9lqCE&lt;/a&gt; ), which in turn is connected to the pc via a usb 3.2 port gen 2 ( red ) can you advise me something ?  Is there anything that might annoy him during the transfer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11gdoaa", "is_robot_indexable": true, "report_reasons": null, "author": "KodoKunaz", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gdoaa/slow_hdd_transfer_5_mbps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gdoaa/slow_hdd_transfer_5_mbps/", "subreddit_subscribers": 671875, "created_utc": 1677790357.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}