{"kind": "Listing", "data": {"after": "t3_11h1lxq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1zqb2jib", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download and store 900 3 hour videos in a month?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 111, "top_awarded_type": null, "hide_score": false, "name": "t3_11gwl82", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 452, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 452, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ITlbvE7gVbTwP1MPZEhXENu7FL-95hz01lI-R_hOGHk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677836341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/nf5076evcjla1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/nf5076evcjla1.jpg?auto=webp&amp;v=enabled&amp;s=4a59b90fd926ebf281834b8dcc2cf3e9ecc44355", "width": 1080, "height": 858}, "resolutions": [{"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cb79c537f5c0f47df6a981645cd56eed81eb09f", "width": 108, "height": 85}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3acaf7a28a7cb2f37b176dfe9ee4aeb44cd298d", "width": 216, "height": 171}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78c3ceec035fdd400d90f227f277264ef87eee8c", "width": 320, "height": 254}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=521f2912f298ead72ecf3b1d62b98d4e20283ab0", "width": 640, "height": 508}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=083fd0ea4ac390e888934284aa27460f4c7a02f4", "width": 960, "height": 762}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0347fa8686184d59b36d40475af30e82270f1f9", "width": 1080, "height": 858}], "variants": {}, "id": "6XkkTsTw7RgZSjNidbwi8XNuNWRlJf8vURQIoBd1DAY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gwl82", "is_robot_indexable": true, "report_reasons": null, "author": "Phixiately", "discussion_type": null, "num_comments": 132, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gwl82/how_to_download_and_store_900_3_hour_videos_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/nf5076evcjla1.jpg", "subreddit_subscribers": 671929, "created_utc": 1677836341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_16k9lu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Did eBay sell me a knock off Seagate Exos 16TB HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 100, "top_awarded_type": null, "hide_score": false, "media_metadata": {"due343t2dfla1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 77, "x": 108, "u": "https://preview.redd.it/due343t2dfla1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8daa04f73512e7d88092a529f7a0a2c6705bb26"}, {"y": 154, "x": 216, "u": "https://preview.redd.it/due343t2dfla1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=793bd1f1c1d1707ce10f90df9eb52bc7bfc0e5a2"}, {"y": 229, "x": 320, "u": "https://preview.redd.it/due343t2dfla1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29c4c78ec092f28b9ddeb5aed814b8c9cbd5588c"}, {"y": 458, "x": 640, "u": "https://preview.redd.it/due343t2dfla1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4009b296b6deaf683834e55e39b3349d6d784b6"}], "s": {"y": 570, "x": 796, "u": "https://preview.redd.it/due343t2dfla1.png?width=796&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=03f6a0f2b80236b4d3283d68f01340bf02f1b99c"}, "id": "due343t2dfla1"}, "ao9lbl13dfla1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e425027ebb39046891a0a93f1aa77d07339ea92"}, {"y": 134, "x": 216, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd986bc87053c98b88c73dd15ca0557abe843e6e"}, {"y": 199, "x": 320, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13e4e03745b95d617d8462878f1714fd810d7628"}, {"y": 398, "x": 640, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a67be6983eaae12e8b4fa2696fc0ec47e875a24"}], "s": {"y": 474, "x": 761, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=761&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aee2bad26ece0736b01fb82c001c956d55edbcd5"}, "id": "ao9lbl13dfla1"}}, "name": "t3_11gmykf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 206, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "This is one of the drives that arrived from eBay", "media_id": "due343t2dfla1", "id": 246640328}, {"caption": "This is what the online images of the 16TH Exos drives look like", "media_id": "ao9lbl13dfla1", "id": 246640329}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 206, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/o-9GYlzLMjLgkQDYH_eA9WX4LxFqnakRnGrsy0koYs8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677806043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/11gmykf", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gmykf", "is_robot_indexable": true, "report_reasons": null, "author": "AwefulUsername", "discussion_type": null, "num_comments": 69, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gmykf/did_ebay_sell_me_a_knock_off_seagate_exos_16tb_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/11gmykf", "subreddit_subscribers": 671929, "created_utc": 1677806043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Been posting in this sub for a few weeks now and realised its one of the most helpful subs I've used on Reddit. The replies to my often dumb questions have been met with fantastic knowledge and some really smart folks (special ty to  u/HTWingNut  \\+ so many others).\n\nThe average IQ of a DataHoarder is evidently much higher than that of other subs and full of people take pride in sharing knowledge and not being a smartass or ridiculing dumb folks like me. When everyone's left for the day, and you spend an extra 30mins sorting out your data, know that you're doing it because you're SMART, and not because you're ill \ud83d\udc40\n\nAnyway, it's Friday, and I thought I'd say thanks to the regular guys and gals who make data interesting and exciting to understand for all. \ud83c\udf7a", "author_fullname": "t2_4d5ehhv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Random thanks to this sub for helpful replies rather than smartass ones", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11h1w2q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677853709.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677853076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been posting in this sub for a few weeks now and realised its one of the most helpful subs I&amp;#39;ve used on Reddit. The replies to my often dumb questions have been met with fantastic knowledge and some really smart folks (special ty to  &lt;a href=\"/u/HTWingNut\"&gt;u/HTWingNut&lt;/a&gt;  + so many others).&lt;/p&gt;\n\n&lt;p&gt;The average IQ of a DataHoarder is evidently much higher than that of other subs and full of people take pride in sharing knowledge and not being a smartass or ridiculing dumb folks like me. When everyone&amp;#39;s left for the day, and you spend an extra 30mins sorting out your data, know that you&amp;#39;re doing it because you&amp;#39;re SMART, and not because you&amp;#39;re ill \ud83d\udc40&lt;/p&gt;\n\n&lt;p&gt;Anyway, it&amp;#39;s Friday, and I thought I&amp;#39;d say thanks to the regular guys and gals who make data interesting and exciting to understand for all. \ud83c\udf7a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11h1w2q", "is_robot_indexable": true, "report_reasons": null, "author": "i-dm", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11h1w2q/random_thanks_to_this_sub_for_helpful_replies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11h1w2q/random_thanks_to_this_sub_for_helpful_replies/", "subreddit_subscribers": 671929, "created_utc": 1677853076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I live in an area where symmetrical internet is either expensive or unavailable. My offsite backup is in my brother's house and just rsync'ing it didn't seem reasonable with my upload speed / bandwidth limitations. I decided to go with the tried and true sneakernet method. I decided to take advantage of ZFS' snapshotting functionality. The first script automatically creates incremental snapshots holding all the changes between the latest update on the backup machine and the current date as of running the script. These snapshots can be ferried between the original and backup machines via a portable hard drive. The second script applies runs the backup machine and will apply the snapshots from the portable hard drive to the backup system.  \n\nI thought I'd share the scripts in case their useful for anyone else sneakernetting their backups as well as some feedback from the community.  \n\nThe first script (ideally set to run periodically via cron/systemd.timers)\n\n    #!/bin/bash\n    \n     # This is part one of my 'patented' sneakernet backup system. The inte-\n     # nded usecase is to use a portable hard drive to shuttle incremental \n     # zfs snapshots from the original server (where this script will reside\n     # and run) to the backup server. There are a few requirements:\n     # 1) Both the original and backup servers have matching datasets\n     # 2) The portable hard drive is formatted with ZFS with a pool named\n     #    Sync\n     # 3) When the portable hard drive makes its way back to the original \n     #    server, you zpool import it and rsync the contents of the \n     #    ZFS_incremental folder onto it\n     \n    # Honestly this probably isn't needed in this script but I put it in all\n    # my bash scripts\n    set -e\n    \n    # Determine current date\n    currentdate=$(date +\"%Y-%m-%d\")\n    \n    # Determine the last time we synced with the backupserver. The file is\n    # generated by the backup server and either rsynced via SSH or pulled\n    # from the portable hard drive\n    lwsync=$(tail -n -1 /root/last_backup_sync.txt)\n    \n    # Using the current date to create snapshot names. Endpoints for the\n    # incremental snapshot\n    curISOssnapshot=\"Pool1/ISOs@${currentdate}\"\n    curlinuxsnapshot=\"Pool1/linux@${currentdate}\"\n    curpubdomainsnapshot=\"Pool1/pubdomain@${currentdate}\"\n    \n    # Using the last date we to create snapshot names. Start-points for the\n    # incremental snapshot\n    lwsyncISOssnapshot=\"Pool1/ISOs@${lwsync}\"\n    lwsynclinuxsnapshot=\"Pool1/linux@${lwsync}\"\n    lwsyncpubdomainsnapshot=\"Pool1/pubdomain@${lwsync}\"\n    \n    \n    # use zfs send to save incremental snapshots of each pool\n    echo $curISOssnapshot\n    echo $curlinuxsnapshot\n    echo $curpubdomainsnapshot\n    \n    \n    # Create snapshots of with current date\n    zfs snapshot $curISOssnapshot\n    zfs snapshot $curlinuxsnapshot\n    zfs snapshot $curpubdomainsnapshot\n    echo \"Snapshots made ${currentdate}\" &gt;&gt; /root/backupscript.log\n    \n    # Create Incremental Snapshots of ISOs, linux and pubdomain\n    zfs send -Ri $lwsyncISOssnapshot $curISOssnapshot &gt; /mnt/Pool1/ZFS_incremental/ISOs_latest\n    zfs send -Ri $lwsynclinuxsnapshot $curlinuxsnapshot &gt; /mnt/Pool1/ZFS_incremental/linux_latest\n    zfs send -Ri $lwsyncpubdomainsnapshot $curpubdomainsnapshot &gt; /mnt/Pool1/ZFS_incremental/pubdomain_latest\n    echo \"Incremental snapshots saved to /mnt/Pool1/ZFS_incremental/ \" &gt;&gt; /root/backupscript.log\n    \n\nThe second script (ideally run automatically via cron/systemd.timers. should be setup to run every day for when the backup machine is located at a relative with low technical abilities):\n\n    #! /bin/bash\n    \n    # This script is the second part of my 'patented' sneakernet update \n    # system. The script is intended to be run on the backup server once the \n    # portable hard drive is plugged into the machine. This script does have \n    # a few dependencies: \n    # 1) the backup server must have the same zfs datasets that are going to \n    #    be transferred from the portable hard drive. \n    # 2) the backup server must have automatic snapshotting turned OFF. \n    # 3) this script assumes that you can ssh into your primary server. \n    \n    # Basically stops the script when something goes wrong. Mostly for when\n    # the script is set to run automatically (via systemd.timers or crontab)\n    # and the \n    set -e\n    \n    # set variables for the latest snapshots of the backup system (useful\n    # for later\n    last_ISOs_snapshot=$(zfs list -H -o name -t snapshot BackupPool/ISOs | tail -n -1)\n    last_linux_snapshot=$(zfs list -H -o name -t snapshot BackupPool/linux | tail -n -1)\n    last_pubdomain_snapshot=$(zfs list -H -o name -t snapshot BackupPool/pubdomain | tail -n -1)\n    \n    # Honestly, just here to provide some output for logging to see where\n    # it might have failed\n    echo $last_ISOs_snapshot\n    echo $last_linux_snapshot\n    echo $last_pubdomain_snapshot\n    \n    echo $(date +\"%Y-%m-%d\") &gt;&gt; /root/sync.log\n    echo $(date +\"%Y-%m-%d\")\n    \n    # Imports the pool located on the portable hard drive\n    zpool import -o altroot=/mnt Sync\n    echo \"Import Done\" &gt;&gt; /root/sync.log\n    echo \"Import Done\"\n    \n    # Rollback to last snapshot. This is necessary in case the person using\n    # the backupserver happened to accidentaly ignore your instructions and\n    # alters any of the pools that are going to be synced.\n    zfs rollback $last_ISOs_snapshot\n    zfs rollback $last_linux_snapshot\n    zfs rollback $last_pubdomain_snapshot\n    echo \"Snapshot rollback done\" &gt;&gt; /root/sync.log\n    echo \"Snapshot rollback done\"\n    \n    # Sync Incremental Snapshots of ISOs, linux and pubdomain\n    zfs recv -F BackupPool/ISOs &lt; /mnt/Sync/ISOs_latest\n    echo \"ISOs updated\" &gt;&gt; /root/sync.log\n    echo \"ISOs updated\"\n    \n    zfs recv -F BackupPool/linux &lt; /mnt/Sync/linux_latest\n    echo \"linux updated\" &gt;&gt; /root/sync.log\n    echo \"linux updated\"\n    \n    zfs recv -F BackupPool/pubdomain &lt; /mnt/Sync/pubdomain_latest\n    echo \"pubdomain updated\" &gt;&gt; /root/sync.log\n    echo \"pubdomain updated\"\n    \n    echo \"Sync Complete\" &gt;&gt; /root/sync.log\n    echo \"Sync Complete\"\n    \n    # Create the file that will be referenced by the first script when det-\n    # ermining what the start point is for the incremental snapshot. Saves \n    # it to the portable hard drive in case ssh'ing into the progenitor \n    # server is not possible\n    zfs list -H -o name -t snapshot BackupPool/ISOs &gt; tempa.txt\n    sed 's/BackupPool\\/ISOs@//' tempa.txt &gt; /root/last_backup_sync.txt\n    rm tempa.txt\n    cp /root/last_backup_sync.txt /mnt/Sync/last_backup_sync.txt\n    echo \"Update last_backup_sync.txt done\" &gt;&gt; /root/sync.log\n    echo \"Update last_backup_sync.txt done\"\n    \n    # Sends the file to the original server. Means that the originator ser-\n    # ver can start making the correct incremental snapshots. This, in turn,\n    # means that when the original server receives the portable hard drive,\n    # it need only copy over the pregenerated incremental snapshots.\n    rsync -e \"ssh\" -avz /root/last_backup_sync.txt root@###.###.###.###:/root/\n    echo \"Send last_backup_sync.txt to Original server\" &gt;&gt; /root/sync.log\n    echo \"Send last_backup_sync.txt to Original server\"\n    \n    # Basically ejects the portable hard drive.\n    zpool export Sync\n    echo \"Disconnect Sync complete\" &gt;&gt; /root/sync.log\n    echo \"---------------------------------------------------\" &gt;&gt; /root/sync.log\n    echo \"Disconnect Sync complete\"\n\nCheers,  \nBasilisk\\_hunters", "author_fullname": "t2_2xnvildp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created some scripts to backup my data via sneakernet. thought I'd share", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gpedp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677812574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I live in an area where symmetrical internet is either expensive or unavailable. My offsite backup is in my brother&amp;#39;s house and just rsync&amp;#39;ing it didn&amp;#39;t seem reasonable with my upload speed / bandwidth limitations. I decided to go with the tried and true sneakernet method. I decided to take advantage of ZFS&amp;#39; snapshotting functionality. The first script automatically creates incremental snapshots holding all the changes between the latest update on the backup machine and the current date as of running the script. These snapshots can be ferried between the original and backup machines via a portable hard drive. The second script applies runs the backup machine and will apply the snapshots from the portable hard drive to the backup system.  &lt;/p&gt;\n\n&lt;p&gt;I thought I&amp;#39;d share the scripts in case their useful for anyone else sneakernetting their backups as well as some feedback from the community.  &lt;/p&gt;\n\n&lt;p&gt;The first script (ideally set to run periodically via cron/systemd.timers)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#!/bin/bash\n\n # This is part one of my &amp;#39;patented&amp;#39; sneakernet backup system. The inte-\n # nded usecase is to use a portable hard drive to shuttle incremental \n # zfs snapshots from the original server (where this script will reside\n # and run) to the backup server. There are a few requirements:\n # 1) Both the original and backup servers have matching datasets\n # 2) The portable hard drive is formatted with ZFS with a pool named\n #    Sync\n # 3) When the portable hard drive makes its way back to the original \n #    server, you zpool import it and rsync the contents of the \n #    ZFS_incremental folder onto it\n\n# Honestly this probably isn&amp;#39;t needed in this script but I put it in all\n# my bash scripts\nset -e\n\n# Determine current date\ncurrentdate=$(date +&amp;quot;%Y-%m-%d&amp;quot;)\n\n# Determine the last time we synced with the backupserver. The file is\n# generated by the backup server and either rsynced via SSH or pulled\n# from the portable hard drive\nlwsync=$(tail -n -1 /root/last_backup_sync.txt)\n\n# Using the current date to create snapshot names. Endpoints for the\n# incremental snapshot\ncurISOssnapshot=&amp;quot;Pool1/ISOs@${currentdate}&amp;quot;\ncurlinuxsnapshot=&amp;quot;Pool1/linux@${currentdate}&amp;quot;\ncurpubdomainsnapshot=&amp;quot;Pool1/pubdomain@${currentdate}&amp;quot;\n\n# Using the last date we to create snapshot names. Start-points for the\n# incremental snapshot\nlwsyncISOssnapshot=&amp;quot;Pool1/ISOs@${lwsync}&amp;quot;\nlwsynclinuxsnapshot=&amp;quot;Pool1/linux@${lwsync}&amp;quot;\nlwsyncpubdomainsnapshot=&amp;quot;Pool1/pubdomain@${lwsync}&amp;quot;\n\n\n# use zfs send to save incremental snapshots of each pool\necho $curISOssnapshot\necho $curlinuxsnapshot\necho $curpubdomainsnapshot\n\n\n# Create snapshots of with current date\nzfs snapshot $curISOssnapshot\nzfs snapshot $curlinuxsnapshot\nzfs snapshot $curpubdomainsnapshot\necho &amp;quot;Snapshots made ${currentdate}&amp;quot; &amp;gt;&amp;gt; /root/backupscript.log\n\n# Create Incremental Snapshots of ISOs, linux and pubdomain\nzfs send -Ri $lwsyncISOssnapshot $curISOssnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/ISOs_latest\nzfs send -Ri $lwsynclinuxsnapshot $curlinuxsnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/linux_latest\nzfs send -Ri $lwsyncpubdomainsnapshot $curpubdomainsnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/pubdomain_latest\necho &amp;quot;Incremental snapshots saved to /mnt/Pool1/ZFS_incremental/ &amp;quot; &amp;gt;&amp;gt; /root/backupscript.log\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The second script (ideally run automatically via cron/systemd.timers. should be setup to run every day for when the backup machine is located at a relative with low technical abilities):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#! /bin/bash\n\n# This script is the second part of my &amp;#39;patented&amp;#39; sneakernet update \n# system. The script is intended to be run on the backup server once the \n# portable hard drive is plugged into the machine. This script does have \n# a few dependencies: \n# 1) the backup server must have the same zfs datasets that are going to \n#    be transferred from the portable hard drive. \n# 2) the backup server must have automatic snapshotting turned OFF. \n# 3) this script assumes that you can ssh into your primary server. \n\n# Basically stops the script when something goes wrong. Mostly for when\n# the script is set to run automatically (via systemd.timers or crontab)\n# and the \nset -e\n\n# set variables for the latest snapshots of the backup system (useful\n# for later\nlast_ISOs_snapshot=$(zfs list -H -o name -t snapshot BackupPool/ISOs | tail -n -1)\nlast_linux_snapshot=$(zfs list -H -o name -t snapshot BackupPool/linux | tail -n -1)\nlast_pubdomain_snapshot=$(zfs list -H -o name -t snapshot BackupPool/pubdomain | tail -n -1)\n\n# Honestly, just here to provide some output for logging to see where\n# it might have failed\necho $last_ISOs_snapshot\necho $last_linux_snapshot\necho $last_pubdomain_snapshot\n\necho $(date +&amp;quot;%Y-%m-%d&amp;quot;) &amp;gt;&amp;gt; /root/sync.log\necho $(date +&amp;quot;%Y-%m-%d&amp;quot;)\n\n# Imports the pool located on the portable hard drive\nzpool import -o altroot=/mnt Sync\necho &amp;quot;Import Done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Import Done&amp;quot;\n\n# Rollback to last snapshot. This is necessary in case the person using\n# the backupserver happened to accidentaly ignore your instructions and\n# alters any of the pools that are going to be synced.\nzfs rollback $last_ISOs_snapshot\nzfs rollback $last_linux_snapshot\nzfs rollback $last_pubdomain_snapshot\necho &amp;quot;Snapshot rollback done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Snapshot rollback done&amp;quot;\n\n# Sync Incremental Snapshots of ISOs, linux and pubdomain\nzfs recv -F BackupPool/ISOs &amp;lt; /mnt/Sync/ISOs_latest\necho &amp;quot;ISOs updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;ISOs updated&amp;quot;\n\nzfs recv -F BackupPool/linux &amp;lt; /mnt/Sync/linux_latest\necho &amp;quot;linux updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;linux updated&amp;quot;\n\nzfs recv -F BackupPool/pubdomain &amp;lt; /mnt/Sync/pubdomain_latest\necho &amp;quot;pubdomain updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;pubdomain updated&amp;quot;\n\necho &amp;quot;Sync Complete&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Sync Complete&amp;quot;\n\n# Create the file that will be referenced by the first script when det-\n# ermining what the start point is for the incremental snapshot. Saves \n# it to the portable hard drive in case ssh&amp;#39;ing into the progenitor \n# server is not possible\nzfs list -H -o name -t snapshot BackupPool/ISOs &amp;gt; tempa.txt\nsed &amp;#39;s/BackupPool\\/ISOs@//&amp;#39; tempa.txt &amp;gt; /root/last_backup_sync.txt\nrm tempa.txt\ncp /root/last_backup_sync.txt /mnt/Sync/last_backup_sync.txt\necho &amp;quot;Update last_backup_sync.txt done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Update last_backup_sync.txt done&amp;quot;\n\n# Sends the file to the original server. Means that the originator ser-\n# ver can start making the correct incremental snapshots. This, in turn,\n# means that when the original server receives the portable hard drive,\n# it need only copy over the pregenerated incremental snapshots.\nrsync -e &amp;quot;ssh&amp;quot; -avz /root/last_backup_sync.txt root@###.###.###.###:/root/\necho &amp;quot;Send last_backup_sync.txt to Original server&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Send last_backup_sync.txt to Original server&amp;quot;\n\n# Basically ejects the portable hard drive.\nzpool export Sync\necho &amp;quot;Disconnect Sync complete&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;---------------------------------------------------&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Disconnect Sync complete&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Cheers,&lt;br/&gt;\nBasilisk_hunters&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gpedp", "is_robot_indexable": true, "report_reasons": null, "author": "Basilisk_hunters", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gpedp/i_created_some_scripts_to_backup_my_data_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gpedp/i_created_some_scripts_to_backup_my_data_via/", "subreddit_subscribers": 671929, "created_utc": 1677812574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_a1w685cb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are these any good? $130 at Costco. or is there anything better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11glm1t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vnyjYihYE3KsC85uwlTmVfRhZw5K5Kr0Jc45ADlgkHM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677802563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yk25kz1fkgla1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?auto=webp&amp;v=enabled&amp;s=a19a23fd3def1c7d4d07f1b5fa808af7b1625cf2", "width": 6936, "height": 9248}, "resolutions": [{"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33121898d9c7fa8a349aba5be2c2cf92a0698dd3", "width": 108, "height": 144}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d391560c65fa468587dd99187454e7f96da555c0", "width": 216, "height": 288}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29a9a067b00a99664ed0163d9e00051f8fd575ea", "width": 320, "height": 426}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=701bd21366e9494dfbc92111f8fed777d052440a", "width": 640, "height": 853}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d184ebbd2c889f99300850138c380889ca9cc225", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2694bccbf32a1b8add40481652641e1e7d6ceba7", "width": 1080, "height": 1440}], "variants": {}, "id": "y2qsInJSywm4T4RRIBKBd3j79xE18CpKUMDVCd2t1pM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11glm1t", "is_robot_indexable": true, "report_reasons": null, "author": "goofgroot", "discussion_type": null, "num_comments": 62, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11glm1t/are_these_any_good_130_at_costco_or_is_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yk25kz1fkgla1.jpg", "subreddit_subscribers": 671929, "created_utc": 1677802563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had an Unraid server for 6 years now, and have decided on building another system - \\*which will definitely be ZFS\\* (speed considerations + I already have all the drives).  \nI'm really conflicted between buying another Unraid license and using the ZFS Plugin until 6.12 comes out with official ZFS support, but on the other hand there's TrueNAS Scale which is free and has a lot of users (and also TrueCharts so a lot of available apps).\n\nWhat are the cons of TrueNAS Scale? Is Kubernetes better than the Unraid Docker?And how about the community and support for the apps? (Like TrueCharts)\n\nThanks in advance for any help!", "author_fullname": "t2_143yl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS System - TrueNAS Scale or ZFS Plugin+Unraid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gsrmp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677822496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had an Unraid server for 6 years now, and have decided on building another system - *which will definitely be ZFS* (speed considerations + I already have all the drives).&lt;br/&gt;\nI&amp;#39;m really conflicted between buying another Unraid license and using the ZFS Plugin until 6.12 comes out with official ZFS support, but on the other hand there&amp;#39;s TrueNAS Scale which is free and has a lot of users (and also TrueCharts so a lot of available apps).&lt;/p&gt;\n\n&lt;p&gt;What are the cons of TrueNAS Scale? Is Kubernetes better than the Unraid Docker?And how about the community and support for the apps? (Like TrueCharts)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "59TB RAID6 | 43TB Usable", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gsrmp", "is_robot_indexable": true, "report_reasons": null, "author": "n0llbyte", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11gsrmp/zfs_system_truenas_scale_or_zfs_pluginunraid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gsrmp/zfs_system_truenas_scale_or_zfs_pluginunraid/", "subreddit_subscribers": 671929, "created_utc": 1677822496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " My drives have around 100k files on them, totalling 450GB.\n\nChecking the properties of folders on the old HDD is almost instant. Connected via USB3 to SATA adapter. On my SSD it takes forever. It's been about 10minuts so far and it's still going. I'm comparing two very similar folders and checking their file count and size. I even changed the USB-C cable to a shorter one that came with the SSD but its still so slow. Why could this be? Spec's below\n\nSeagate HDD\n\n* 5900rpm\n* 2TB\n* USB 3-SATA adapter\n\nSandisk SSD Sandisk Extreme Portable (Gen1)\n\n* 550MB/\n* 1TB\n* USB-C-USB-C", "author_fullname": "t2_4d5ehhv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does SSD (USB-C/USB-C) take so long to read file properties vs old HDD (USB3/SATA)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ge2gq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677791252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My drives have around 100k files on them, totalling 450GB.&lt;/p&gt;\n\n&lt;p&gt;Checking the properties of folders on the old HDD is almost instant. Connected via USB3 to SATA adapter. On my SSD it takes forever. It&amp;#39;s been about 10minuts so far and it&amp;#39;s still going. I&amp;#39;m comparing two very similar folders and checking their file count and size. I even changed the USB-C cable to a shorter one that came with the SSD but its still so slow. Why could this be? Spec&amp;#39;s below&lt;/p&gt;\n\n&lt;p&gt;Seagate HDD&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;5900rpm&lt;/li&gt;\n&lt;li&gt;2TB&lt;/li&gt;\n&lt;li&gt;USB 3-SATA adapter&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sandisk SSD Sandisk Extreme Portable (Gen1)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;550MB/&lt;/li&gt;\n&lt;li&gt;1TB&lt;/li&gt;\n&lt;li&gt;USB-C-USB-C&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ge2gq", "is_robot_indexable": true, "report_reasons": null, "author": "i-dm", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ge2gq/why_does_ssd_usbcusbc_take_so_long_to_read_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ge2gq/why_does_ssd_usbcusbc_take_so_long_to_read_file/", "subreddit_subscribers": 671929, "created_utc": 1677791252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Would prefer a Python method...to for example, sort it all by facial recognition or body-type/attributes, hair color, anything at all? Or are we still all stuck the stone-age when it comes to organizing this stuff? I'm using Lightroom and the nested key-wording is nice, but it's laborious with a large collection.", "author_fullname": "t2_ygrrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using AI (through Python scripts or otherwise) to organize their massively large porn images (or video?) through any auto-keyword/tagging method?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11hdgfx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677874257.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would prefer a Python method...to for example, sort it all by facial recognition or body-type/attributes, hair color, anything at all? Or are we still all stuck the stone-age when it comes to organizing this stuff? I&amp;#39;m using Lightroom and the nested key-wording is nice, but it&amp;#39;s laborious with a large collection.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hdgfx", "is_robot_indexable": true, "report_reasons": null, "author": "clevnumb", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hdgfx/is_anyone_using_ai_through_python_scripts_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hdgfx/is_anyone_using_ai_through_python_scripts_or/", "subreddit_subscribers": 671929, "created_utc": 1677874257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys, been passively lurking here for years and I've finally decided to take the leap now that I can actually afford to buy things.\n\nI created a Master Folder which contains everything that I ever want to possess (software, media, games, courses, etc) or come across as I go through life. That folder will be the main folder with all the sub folders.\n\nJust bought a 2TB (small start) external hard disk and I will be using it as a backup for the Main Folder. I was wondering if there's any way to manage this data since there would be a gap of time (1-4 weeks roughly) before I update the HDD to sync with my laptop.\n\nI tried googling/searching but couldn't find anything (probably didn't use the right words)\n\nIt would be great to have a tool that can help me with this so that I can eventually get a back-up for the back-up and slowly archive everything under the sun!", "author_fullname": "t2_5csbxqyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manage backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hbq9e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677870285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, been passively lurking here for years and I&amp;#39;ve finally decided to take the leap now that I can actually afford to buy things.&lt;/p&gt;\n\n&lt;p&gt;I created a Master Folder which contains everything that I ever want to possess (software, media, games, courses, etc) or come across as I go through life. That folder will be the main folder with all the sub folders.&lt;/p&gt;\n\n&lt;p&gt;Just bought a 2TB (small start) external hard disk and I will be using it as a backup for the Main Folder. I was wondering if there&amp;#39;s any way to manage this data since there would be a gap of time (1-4 weeks roughly) before I update the HDD to sync with my laptop.&lt;/p&gt;\n\n&lt;p&gt;I tried googling/searching but couldn&amp;#39;t find anything (probably didn&amp;#39;t use the right words)&lt;/p&gt;\n\n&lt;p&gt;It would be great to have a tool that can help me with this so that I can eventually get a back-up for the back-up and slowly archive everything under the sun!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hbq9e", "is_robot_indexable": true, "report_reasons": null, "author": "Russell_fer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hbq9e/how_to_manage_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hbq9e/how_to_manage_backups/", "subreddit_subscribers": 671929, "created_utc": 1677870285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been thinking about using Amazon Photos as another redudant storage for my pictures, but it seems this sub doesn't have much love for Amazon Photos and it surprises me a bit, specially after Google limited Photos's storage, as Amazon Photos seems to maintain an unlimited policy for pictures, for now at least.\n\nIs there a reason for that? Maybe its due to privacy concerns, being an Amazon product? Or is it just a bad service (app, web, windows client...)?", "author_fullname": "t2_ekn6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon Photos vs Google Photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11h5dqh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677861845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been thinking about using Amazon Photos as another redudant storage for my pictures, but it seems this sub doesn&amp;#39;t have much love for Amazon Photos and it surprises me a bit, specially after Google limited Photos&amp;#39;s storage, as Amazon Photos seems to maintain an unlimited policy for pictures, for now at least.&lt;/p&gt;\n\n&lt;p&gt;Is there a reason for that? Maybe its due to privacy concerns, being an Amazon product? Or is it just a bad service (app, web, windows client...)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11h5dqh", "is_robot_indexable": true, "report_reasons": null, "author": "pelluchan", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11h5dqh/amazon_photos_vs_google_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11h5dqh/amazon_photos_vs_google_photos/", "subreddit_subscribers": 671929, "created_utc": 1677861845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have several disks of diffent sizes in my snapraid/mergerfs configuration. The latest addition is a 20TB drive which is wastly bigger than the smallest (4 TB). I did a mergerfs.balance operation and all drives are now filled equally (\\~45%). I'd like to keep it that way.\n\nHowever, my understanding of mfs is, that it will fill the disk based on \\*actual\\* most free space - not percentual. So my 20TB drive would be almost completely filled before any further writes will be made to the 4TB (about 45% filled right now).\n\nI check the documentation and couldn't find a policy that allows for that. Is there any setting/policy that enables me to fill the disks equally on a percentual basis - similar to how mergerfs.balance levels out the disks?\n\nThank you in advance for any ideas/guidance on that.", "author_fullname": "t2_khebnfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mergerfs - most free space (percentage basis)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gwrm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677837061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several disks of diffent sizes in my snapraid/mergerfs configuration. The latest addition is a 20TB drive which is wastly bigger than the smallest (4 TB). I did a mergerfs.balance operation and all drives are now filled equally (~45%). I&amp;#39;d like to keep it that way.&lt;/p&gt;\n\n&lt;p&gt;However, my understanding of mfs is, that it will fill the disk based on *actual* most free space - not percentual. So my 20TB drive would be almost completely filled before any further writes will be made to the 4TB (about 45% filled right now).&lt;/p&gt;\n\n&lt;p&gt;I check the documentation and couldn&amp;#39;t find a policy that allows for that. Is there any setting/policy that enables me to fill the disks equally on a percentual basis - similar to how mergerfs.balance levels out the disks?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for any ideas/guidance on that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gwrm0", "is_robot_indexable": true, "report_reasons": null, "author": "SillyMochi", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gwrm0/mergerfs_most_free_space_percentage_basis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gwrm0/mergerfs_most_free_space_percentage_basis/", "subreddit_subscribers": 671929, "created_utc": 1677837061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have had no luck with [archive.org](https://archive.org) at this point hope might be lost I've tried using a few url to mp4 converters still no luck. This is the video [https://www.youtube.com/watch?v=QCHfSfO2neU](https://www.youtube.com/watch?v=QCHfSfO2neU)", "author_fullname": "t2_vynj9t7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any sites besides archive.org that can play removed YouTube videos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11h4w46", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677860729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have had no luck with &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt; at this point hope might be lost I&amp;#39;ve tried using a few url to mp4 converters still no luck. This is the video &lt;a href=\"https://www.youtube.com/watch?v=QCHfSfO2neU\"&gt;https://www.youtube.com/watch?v=QCHfSfO2neU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11h4w46", "is_robot_indexable": true, "report_reasons": null, "author": "Independent-Hold-329", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11h4w46/are_there_any_sites_besides_archiveorg_that_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11h4w46/are_there_any_sites_besides_archiveorg_that_can/", "subreddit_subscribers": 671929, "created_utc": 1677860729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a whole bunch of PDFs (a few thousand pages over a few hundred files), and I want to search through them but none of them have any kind of OCR/highlighting/searchability. What's a good tool that would let me scan all of these in bulk and produce something easily searched?\n\nIdeally:\n\n--Runs on Windows\n\n--Is fairly simply to use (I can cope with command lines but my willpower falters at 'compiling and tweaking')\n\n--Is free (unfortunately I can't access Adobe Acrobat)", "author_fullname": "t2_t84b2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting OCR on lots of PDFs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ge41a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677791356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a whole bunch of PDFs (a few thousand pages over a few hundred files), and I want to search through them but none of them have any kind of OCR/highlighting/searchability. What&amp;#39;s a good tool that would let me scan all of these in bulk and produce something easily searched?&lt;/p&gt;\n\n&lt;p&gt;Ideally:&lt;/p&gt;\n\n&lt;p&gt;--Runs on Windows&lt;/p&gt;\n\n&lt;p&gt;--Is fairly simply to use (I can cope with command lines but my willpower falters at &amp;#39;compiling and tweaking&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;--Is free (unfortunately I can&amp;#39;t access Adobe Acrobat)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ge41a", "is_robot_indexable": true, "report_reasons": null, "author": "Wyverncraft", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ge41a/getting_ocr_on_lots_of_pdfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ge41a/getting_ocr_on_lots_of_pdfs/", "subreddit_subscribers": 671929, "created_utc": 1677791356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm tinkering with some cloud storage and thought it might be interesting to combine a few of them into a \"RAID\" for benefits similar to local arrays. If one of the providers goes down, the account gets banned or policy changes or whatever, then I won't lose everything immediately like on a RAID 0 equivalent. And I'll be able to use the aggregated storage more effectively since I have don't have to worry about how full each individual drive is.\n\nI figure it's going to be annoying as hell to combine all the free 2 or 5GB of storage here and there so I mainly want to do this with large 3 disks; Oracle Cloud (150/200GB, using SFTP), Storj (150GB) and SFTP / local.\n\nOur other lord and savior (the primary one being zfs of course lol) rclone doesn't seem to have a trick for this one. The \"combine\" policies I found seem to be oriented towards where to place the data (lowest free space used, fewest objects stored etc) and doesn't have a function for RAID. Does rclone have a feature for this?\n\nIf not, I think I can try splitting the files up and semi manully adding 33% parity data. I don't know if that guarantees single drive redundancy though, since a data chunk and parity of that chunk could end up on the same disk. I don't suppose there's an rclone solution for this... Lol.", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is 1 drive redundancy possible with cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gziv0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677846528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m tinkering with some cloud storage and thought it might be interesting to combine a few of them into a &amp;quot;RAID&amp;quot; for benefits similar to local arrays. If one of the providers goes down, the account gets banned or policy changes or whatever, then I won&amp;#39;t lose everything immediately like on a RAID 0 equivalent. And I&amp;#39;ll be able to use the aggregated storage more effectively since I have don&amp;#39;t have to worry about how full each individual drive is.&lt;/p&gt;\n\n&lt;p&gt;I figure it&amp;#39;s going to be annoying as hell to combine all the free 2 or 5GB of storage here and there so I mainly want to do this with large 3 disks; Oracle Cloud (150/200GB, using SFTP), Storj (150GB) and SFTP / local.&lt;/p&gt;\n\n&lt;p&gt;Our other lord and savior (the primary one being zfs of course lol) rclone doesn&amp;#39;t seem to have a trick for this one. The &amp;quot;combine&amp;quot; policies I found seem to be oriented towards where to place the data (lowest free space used, fewest objects stored etc) and doesn&amp;#39;t have a function for RAID. Does rclone have a feature for this?&lt;/p&gt;\n\n&lt;p&gt;If not, I think I can try splitting the files up and semi manully adding 33% parity data. I don&amp;#39;t know if that guarantees single drive redundancy though, since a data chunk and parity of that chunk could end up on the same disk. I don&amp;#39;t suppose there&amp;#39;s an rclone solution for this... Lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "vTrueNAS 72TB / Hyper-V", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gziv0", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11gziv0/is_1_drive_redundancy_possible_with_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gziv0/is_1_drive_redundancy_possible_with_cloud/", "subreddit_subscribers": 671929, "created_utc": 1677846528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for software that will scan every photo in my library and add tags to them to allow searching. Ideally, this will function similarly to the photo search from iOS where you can search \"shoe\" and shoes will appear.", "author_fullname": "t2_50n4upyn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo Library Scan &amp; Organization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11go6ou", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677809271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for software that will scan every photo in my library and add tags to them to allow searching. Ideally, this will function similarly to the photo search from iOS where you can search &amp;quot;shoe&amp;quot; and shoes will appear.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11go6ou", "is_robot_indexable": true, "report_reasons": null, "author": "c00pdwg", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11go6ou/photo_library_scan_organization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11go6ou/photo_library_scan_organization/", "subreddit_subscribers": 671929, "created_utc": 1677809271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What I really want is a program that overlays with explorer (Windows), so when I'm searching for a file to upload or send, I can see the ranking and tags that I've added to the file, without slowing things down too much. \n\n&amp;#x200B;\n\nDoes TagSpaces work like that, or is it only some browser interface where you can see your tags and ratings?", "author_fullname": "t2_fl91vcm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for software that lets me rank (and maybe tag) files? Is Tagspaces a good and safe option?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gkl0g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677800220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I really want is a program that overlays with explorer (Windows), so when I&amp;#39;m searching for a file to upload or send, I can see the ranking and tags that I&amp;#39;ve added to the file, without slowing things down too much. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does TagSpaces work like that, or is it only some browser interface where you can see your tags and ratings?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gkl0g", "is_robot_indexable": true, "report_reasons": null, "author": "Atlantic0ne", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gkl0g/looking_for_software_that_lets_me_rank_and_maybe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gkl0g/looking_for_software_that_lets_me_rank_and_maybe/", "subreddit_subscribers": 671929, "created_utc": 1677800220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently working on devising a plan for a way to store a good amount of media from across the different orgs and other groups on our university campus to go into a time capsule to be opened on our 50th reunion.\n\nI already did base research in figuring out roughly what storage types and methods I should be going for but wanted to get external opinions/questions answered before I commit to it.\n\nIdeally the plan is to go with optical disks such as BR or M-Disc (only concern with the latter is finding hardware for burning) and immediately after place it back in its factory case. I'm also toying with the idea of adding a vacuum sealed bag + silica packs since this will be buried underground along with other items in the capsule. \n\nMy main concerns are budget, both monetary and timewise, is this feasible with a lower $100-200 budget and a timeframe of about 1 month? also if there are any further precautions I should look into taking. \n\nIf there's anything I missed please lmk thank you", "author_fullname": "t2_3d2xhhyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time Capsule options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ghpb8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677796679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working on devising a plan for a way to store a good amount of media from across the different orgs and other groups on our university campus to go into a time capsule to be opened on our 50th reunion.&lt;/p&gt;\n\n&lt;p&gt;I already did base research in figuring out roughly what storage types and methods I should be going for but wanted to get external opinions/questions answered before I commit to it.&lt;/p&gt;\n\n&lt;p&gt;Ideally the plan is to go with optical disks such as BR or M-Disc (only concern with the latter is finding hardware for burning) and immediately after place it back in its factory case. I&amp;#39;m also toying with the idea of adding a vacuum sealed bag + silica packs since this will be buried underground along with other items in the capsule. &lt;/p&gt;\n\n&lt;p&gt;My main concerns are budget, both monetary and timewise, is this feasible with a lower $100-200 budget and a timeframe of about 1 month? also if there are any further precautions I should look into taking. &lt;/p&gt;\n\n&lt;p&gt;If there&amp;#39;s anything I missed please lmk thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ghpb8", "is_robot_indexable": true, "report_reasons": null, "author": "Mango_yoshi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ghpb8/time_capsule_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ghpb8/time_capsule_options/", "subreddit_subscribers": 671929, "created_utc": 1677796679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Countless backups, photos, videos, movies, torrents, Windows bootables, Linux bootables and what not hoarded on these over time.... And some are peripheral driver/sw disks.\n\nEven had a excel database with all of them marked by a number and the table listing exactly what's in them\n\nI think I still remember how cool it felt to be able to \"Write\" something to a DVD (was scared of what \"burning\" a DVD meant initially haha, the good old Nero !)\n\nI think it's an end for this good old pile of DVDs and CDs, with all important stuff moved to my recently built DIY NAS. Felt nostalgic getting rid of them, so I just ordered a slimline sata cable to use with a old laptop dvd drive, and gave each of them a run once , sort of like, a goodbye, before disposing them off :)  (I can't let go of things easily)\n\nAlso a friendly reminder that DVDs &amp; CDs do not have a very long shelf life, Igot data errors from like 50% of the old drives.\n\nSo long buddies....\n\n&amp;#x200B;\n\nhttps://preview.redd.it/lyde37ry6lla1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5fef573713d5aa7482ab3b2547782c649e37c633", "author_fullname": "t2_6fdo6pa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm letting go !", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"lyde37ry6lla1": {"status": "valid", "e": "Image", "m": "image/jpg", "o": [{"y": 4032, "x": 3024, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=1080&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=abbb316f70cc4f21cdd810faf2bcfebd8ec9693e"}], "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=266d5a008ba0a73665fb6c5149d35aa70238b854"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d73daf216f46bec21d7e8c113241e1151596195"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d9ebe6cc478b8461032a533e40993578c4bb457"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50be12f661a31fb2e7cbe928d3d95d4f30968075"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4517ea0ac78dc2b70fa7d9bdaf33292254ec56a7"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c458305109f569e1b76698eed54bb33e7d72b44e"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5fef573713d5aa7482ab3b2547782c649e37c633"}, "id": "lyde37ry6lla1"}}, "name": "t3_11heuto", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677877393.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Countless backups, photos, videos, movies, torrents, Windows bootables, Linux bootables and what not hoarded on these over time.... And some are peripheral driver/sw disks.&lt;/p&gt;\n\n&lt;p&gt;Even had a excel database with all of them marked by a number and the table listing exactly what&amp;#39;s in them&lt;/p&gt;\n\n&lt;p&gt;I think I still remember how cool it felt to be able to &amp;quot;Write&amp;quot; something to a DVD (was scared of what &amp;quot;burning&amp;quot; a DVD meant initially haha, the good old Nero !)&lt;/p&gt;\n\n&lt;p&gt;I think it&amp;#39;s an end for this good old pile of DVDs and CDs, with all important stuff moved to my recently built DIY NAS. Felt nostalgic getting rid of them, so I just ordered a slimline sata cable to use with a old laptop dvd drive, and gave each of them a run once , sort of like, a goodbye, before disposing them off :)  (I can&amp;#39;t let go of things easily)&lt;/p&gt;\n\n&lt;p&gt;Also a friendly reminder that DVDs &amp;amp; CDs do not have a very long shelf life, Igot data errors from like 50% of the old drives.&lt;/p&gt;\n\n&lt;p&gt;So long buddies....&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lyde37ry6lla1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5fef573713d5aa7482ab3b2547782c649e37c633\"&gt;https://preview.redd.it/lyde37ry6lla1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5fef573713d5aa7482ab3b2547782c649e37c633&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11heuto", "is_robot_indexable": true, "report_reasons": null, "author": "abhishekr700", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11heuto/im_letting_go/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11heuto/im_letting_go/", "subreddit_subscribers": 671929, "created_utc": 1677877393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, thanks in advance for your help!\n\nI'm building a new Arch Linux file serving NAS to replace one with old hardware and drives, and of course, get more storage space :-) Will go to larger capacity drives and a shorter chassis (4U to 2U) in my home office rack. Plan to keep using mergerFS and SnapRAID (for parity). A lot of the data is irreplaceable and we access it several times per day (so long downtimes aren't great). I've been using RAID0 (not great, I know) but have been thinking about switching to the increased safety of RAID5, 6 or 10 instead.\n\nSince I'm going to have to migrate all of my data anyway and this is the perfect time to change RAID types, which type of RAID do you recommend for my situation?\n\n[View Poll](https://www.reddit.com/poll/11hebk2)", "author_fullname": "t2_jxrwr82a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick question - best type of RAID array for irreplaceable data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11hebk2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677876214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, thanks in advance for your help!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building a new Arch Linux file serving NAS to replace one with old hardware and drives, and of course, get more storage space :-) Will go to larger capacity drives and a shorter chassis (4U to 2U) in my home office rack. Plan to keep using mergerFS and SnapRAID (for parity). A lot of the data is irreplaceable and we access it several times per day (so long downtimes aren&amp;#39;t great). I&amp;#39;ve been using RAID0 (not great, I know) but have been thinking about switching to the increased safety of RAID5, 6 or 10 instead.&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;m going to have to migrate all of my data anyway and this is the perfect time to change RAID types, which type of RAID do you recommend for my situation?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/11hebk2\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hebk2", "is_robot_indexable": true, "report_reasons": null, "author": "renarde33", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1678481014449, "options": [{"text": "RAID1 (1:1 Mirroring)", "id": "21883957"}, {"text": "RAID5 (Striping with Parity)", "id": "21883958"}, {"text": "RAID6 (Striping with Double Parity)", "id": "21883959"}, {"text": "RAID 10 (1+0, Mirroring with Striping)", "id": "21883960"}, {"text": "A hybrid allowed by SnapRAID", "id": "21883961"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 43, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hebk2/quick_question_best_type_of_raid_array_for/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/11hebk2/quick_question_best_type_of_raid_array_for/", "subreddit_subscribers": 671929, "created_utc": 1677876214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[http://www.liveleak.com/view?i=3ad\\_1356876614](http://www.liveleak.com/view?i=3ad_1356876614) work for a syrian american NGO that tracks warcrimes in the Syrian Conflict  \n\n\nWTF is up with all the censorship on the internet? Takdown requests from the 2000s with US  govt etc...these are prosecutable offenses. [archive.com](https://archive.com) loads the webpage but cnanot load the videos. is all that footage gone for good?", "author_fullname": "t2_lww73a7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "i can't load any liveleak videos from archive.com ....i am trying to find war criminals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11h4gcl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677859713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"http://www.liveleak.com/view?i=3ad_1356876614\"&gt;http://www.liveleak.com/view?i=3ad_1356876614&lt;/a&gt; work for a syrian american NGO that tracks warcrimes in the Syrian Conflict  &lt;/p&gt;\n\n&lt;p&gt;WTF is up with all the censorship on the internet? Takdown requests from the 2000s with US  govt etc...these are prosecutable offenses. &lt;a href=\"https://archive.com\"&gt;archive.com&lt;/a&gt; loads the webpage but cnanot load the videos. is all that footage gone for good?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IRMwLVcCmMSPC-e0XYKJjD9O0GFrV_xJ2W30oy0c-o8.jpg?auto=webp&amp;v=enabled&amp;s=dedee15f39fb46836812cb9fb256b0396539e058", "width": 138, "height": 33}, "resolutions": [{"url": "https://external-preview.redd.it/IRMwLVcCmMSPC-e0XYKJjD9O0GFrV_xJ2W30oy0c-o8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0872c80d8c6abc2bc4ce74535900ea95b869df74", "width": 108, "height": 25}], "variants": {}, "id": "W3WFk2B6T_I4K-vzuU_x5eiocMGdOOQKy1t4-7FEHMQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11h4gcl", "is_robot_indexable": true, "report_reasons": null, "author": "fartuni4", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11h4gcl/i_cant_load_any_liveleak_videos_from_archivecom_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11h4gcl/i_cant_load_any_liveleak_videos_from_archivecom_i/", "subreddit_subscribers": 671929, "created_utc": 1677859713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a cheap backup for 1 Mac with a NAS, between 1TB to 2TB\n\nAny opinions good or bad on PolarBackup? I've been using them for the last year at a price of 64 USD per year for 650 GB (probably up to 1TB). I must admit when the renewal came up, I wondered if there was anything better/cheaper.\n\nAlso their advertising is not clear about costs.\n\niCloud presently 8USD for a year then about 80 USD ...\n\nAnything else?\n\nBackblaze still the favourite for a small home setup?\n\nThanks", "author_fullname": "t2_xzqlc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a cheap backup for 1 Mac with a NAS, between 1TB to 2TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gwxpw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677837721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a cheap backup for 1 Mac with a NAS, between 1TB to 2TB&lt;/p&gt;\n\n&lt;p&gt;Any opinions good or bad on PolarBackup? I&amp;#39;ve been using them for the last year at a price of 64 USD per year for 650 GB (probably up to 1TB). I must admit when the renewal came up, I wondered if there was anything better/cheaper.&lt;/p&gt;\n\n&lt;p&gt;Also their advertising is not clear about costs.&lt;/p&gt;\n\n&lt;p&gt;iCloud presently 8USD for a year then about 80 USD ...&lt;/p&gt;\n\n&lt;p&gt;Anything else?&lt;/p&gt;\n\n&lt;p&gt;Backblaze still the favourite for a small home setup?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gwxpw", "is_robot_indexable": true, "report_reasons": null, "author": "crabbiesgreenginger", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gwxpw/looking_for_a_cheap_backup_for_1_mac_with_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gwxpw/looking_for_a_cheap_backup_for_1_mac_with_a_nas/", "subreddit_subscribers": 671929, "created_utc": 1677837721.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just purchased a 12TB Seagate Ironwolf drive from Amazon and it wouldn't spin up. It beeped when powered on and wasn't recognized by Windows. I figured that the drive was DOA, so I got a replacement from Amazon. \n\nThe replacement drive behaved the same way. I'm powering it using an external enclosure as a temporary solution until I get a better setup. The enclosure works perfectly for my 4TB Barracuda drives, and it's rated for drives up to 18TB.\nI also tried both drives in a desktop environment and had the same result.\n\nI'm curious if I'm just unlucky or if I'm missing something. I know that sometimes 3.3v can cause issues if the drive supports PWDIS, but I haven't found any documentation that states that this drive has PWDIS, so I haven't tried the tape method yet.", "author_fullname": "t2_tn5jk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My new Ironwolf drives beep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gu903", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677827433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just purchased a 12TB Seagate Ironwolf drive from Amazon and it wouldn&amp;#39;t spin up. It beeped when powered on and wasn&amp;#39;t recognized by Windows. I figured that the drive was DOA, so I got a replacement from Amazon. &lt;/p&gt;\n\n&lt;p&gt;The replacement drive behaved the same way. I&amp;#39;m powering it using an external enclosure as a temporary solution until I get a better setup. The enclosure works perfectly for my 4TB Barracuda drives, and it&amp;#39;s rated for drives up to 18TB.\nI also tried both drives in a desktop environment and had the same result.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious if I&amp;#39;m just unlucky or if I&amp;#39;m missing something. I know that sometimes 3.3v can cause issues if the drive supports PWDIS, but I haven&amp;#39;t found any documentation that states that this drive has PWDIS, so I haven&amp;#39;t tried the tape method yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gu903", "is_robot_indexable": true, "report_reasons": null, "author": "Scrambles225", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gu903/my_new_ironwolf_drives_beep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gu903/my_new_ironwolf_drives_beep/", "subreddit_subscribers": 671929, "created_utc": 1677827433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using MultCloud right now. Here's my issue,.. I have 4 dropbox accounts using different emails... Can I can add them even though the email I used to create the MultCloud account is assigned to only one of the 4 dropbox accounts? \n\nSide note... I'd like to keep it organized if that's possible. Does this mean I need several MultCloud accounts too? \n\nThanks in advance.", "author_fullname": "t2_jb6dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "several email addresses using same clouds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gql0c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677815912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using MultCloud right now. Here&amp;#39;s my issue,.. I have 4 dropbox accounts using different emails... Can I can add them even though the email I used to create the MultCloud account is assigned to only one of the 4 dropbox accounts? &lt;/p&gt;\n\n&lt;p&gt;Side note... I&amp;#39;d like to keep it organized if that&amp;#39;s possible. Does this mean I need several MultCloud accounts too? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gql0c", "is_robot_indexable": true, "report_reasons": null, "author": "badcat130", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gql0c/several_email_addresses_using_same_clouds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gql0c/several_email_addresses_using_same_clouds/", "subreddit_subscribers": 671929, "created_utc": 1677815912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After some consideration, I\u2019ve decided to backup my mediaserver to BackBlaze B2.  Just doing an rclone sync for the time being, as I continue to clean up my local directories. \n\nI\u2019m not using encryption on thr client side, so I imagine that backblaze has the capability to see everything I\u2019ve backed up. \n\nMy question is, does anyone know if they scan private B2 buckets for copyrighted data? It would be might inconvienent if they did, and I\u2019d probably have to come up with a new strategy to encrypt and remove obfuscate before uploading\n\nThx!", "author_fullname": "t2_r6bjole0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Backup q?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gj5go", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677798281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After some consideration, I\u2019ve decided to backup my mediaserver to BackBlaze B2.  Just doing an rclone sync for the time being, as I continue to clean up my local directories. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m not using encryption on thr client side, so I imagine that backblaze has the capability to see everything I\u2019ve backed up. &lt;/p&gt;\n\n&lt;p&gt;My question is, does anyone know if they scan private B2 buckets for copyrighted data? It would be might inconvienent if they did, and I\u2019d probably have to come up with a new strategy to encrypt and remove obfuscate before uploading&lt;/p&gt;\n\n&lt;p&gt;Thx!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gj5go", "is_robot_indexable": true, "report_reasons": null, "author": "AuthenticImposter", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gj5go/cloud_backup_q/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gj5go/cloud_backup_q/", "subreddit_subscribers": 671929, "created_utc": 1677798281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "To use Google workspace you need to verify that you own the domain. To do this they ask you to place a HTML file, 56 characters long, containing a unique code on your website.\nSince this is such a minute file how/where is the cheapest way to do this?", "author_fullname": "t2_16l87w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google workspace domain host?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11h1lxq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677852317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To use Google workspace you need to verify that you own the domain. To do this they ask you to place a HTML file, 56 characters long, containing a unique code on your website.\nSince this is such a minute file how/where is the cheapest way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11h1lxq", "is_robot_indexable": true, "report_reasons": null, "author": "KingRollos", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11h1lxq/google_workspace_domain_host/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11h1lxq/google_workspace_domain_host/", "subreddit_subscribers": 671929, "created_utc": 1677852317.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}