{"kind": "Listing", "data": {"after": "t3_11ghpb8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a plex server with a couple of ssd's and 6 spinning hard drives, 3 of which are shucked wd easystore drives. I decided to finally label my drive caddies to make a failed drive easier to identify in case of a failure or replacement. After labelling the caddies I slid them back into my case, attached cabling and booted the computer.\n\nI launched file explorer and was shocked to see that 3 of my spinning hard drives were not there at all. This is where my moronic tale begins. I shut the server down and checked all of the cable connections and rebooted. Still missing 3 drives! Being a logical moron, I decided to pull a couple of the missing drives, since they were handily labelled now, and install them in an external enclosure to see if they somehow had failed. They tested fine.\n\nI reinstalled the drives and booted the computer. Still 3 missing drives! I decided that I must have damaged either the sata or power cable (shared) for all 3 drives simultaneously, so I swapped out cabling for all 3 drives and rebooted. Still missing 3 drives! I replaced the original cabling and it was about this time, 2 hours into my moronic adventure, that I noticed that all 3 of the invisible drives were white label shucked drives. DUHHH, I wondered if the capstan tape that I applied to the 3rd power connector pin had been damaged on all 3 drives. What are the odds that could happen at the same time on all 3 drives?\n\nI pulled all 3 drives for the 47th time and what do you know? The tape was indeed damaged on all 3 drives. I carefully removed and then replaced the tape and put the drives back in and booted the server. Still missing 3 drives! You have got to be fu@#ing kidding!!! I pulled all 3 drives for the 48th time and inspected them. Being the moron that I am, I had applied the tape to the 3rd pin on the DATA connector, not the power connector. I removed the tape and applied it to the correct 3rd pins, reinstalled the drives and booted. The 3 previously missing drives were now present and functioning normally.\n\nAs a pathetic defense, it had been a while since I had installed these 3 shucked drives and the 3rd pin tape requirement was a bit of a distant memory. However I am still a moron, dimwit, bonehead and idiot for spending half a day on a 30 minute project. Just thought I would share...", "author_fullname": "t2_56nvy1n8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My astounding stupidity, tale of a moron", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g9nu4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 249, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 249, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677780899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a plex server with a couple of ssd&amp;#39;s and 6 spinning hard drives, 3 of which are shucked wd easystore drives. I decided to finally label my drive caddies to make a failed drive easier to identify in case of a failure or replacement. After labelling the caddies I slid them back into my case, attached cabling and booted the computer.&lt;/p&gt;\n\n&lt;p&gt;I launched file explorer and was shocked to see that 3 of my spinning hard drives were not there at all. This is where my moronic tale begins. I shut the server down and checked all of the cable connections and rebooted. Still missing 3 drives! Being a logical moron, I decided to pull a couple of the missing drives, since they were handily labelled now, and install them in an external enclosure to see if they somehow had failed. They tested fine.&lt;/p&gt;\n\n&lt;p&gt;I reinstalled the drives and booted the computer. Still 3 missing drives! I decided that I must have damaged either the sata or power cable (shared) for all 3 drives simultaneously, so I swapped out cabling for all 3 drives and rebooted. Still missing 3 drives! I replaced the original cabling and it was about this time, 2 hours into my moronic adventure, that I noticed that all 3 of the invisible drives were white label shucked drives. DUHHH, I wondered if the capstan tape that I applied to the 3rd power connector pin had been damaged on all 3 drives. What are the odds that could happen at the same time on all 3 drives?&lt;/p&gt;\n\n&lt;p&gt;I pulled all 3 drives for the 47th time and what do you know? The tape was indeed damaged on all 3 drives. I carefully removed and then replaced the tape and put the drives back in and booted the server. Still missing 3 drives! You have got to be fu@#ing kidding!!! I pulled all 3 drives for the 48th time and inspected them. Being the moron that I am, I had applied the tape to the 3rd pin on the DATA connector, not the power connector. I removed the tape and applied it to the correct 3rd pins, reinstalled the drives and booted. The 3 previously missing drives were now present and functioning normally.&lt;/p&gt;\n\n&lt;p&gt;As a pathetic defense, it had been a while since I had installed these 3 shucked drives and the 3rd pin tape requirement was a bit of a distant memory. However I am still a moron, dimwit, bonehead and idiot for spending half a day on a 30 minute project. Just thought I would share...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11g9nu4", "is_robot_indexable": true, "report_reasons": null, "author": "bctf1", "discussion_type": null, "num_comments": 88, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g9nu4/my_astounding_stupidity_tale_of_a_moron/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g9nu4/my_astounding_stupidity_tale_of_a_moron/", "subreddit_subscribers": 671891, "created_utc": 1677780899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_16k9lu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Did eBay sell me a knock off Seagate Exos 16TB HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 100, "top_awarded_type": null, "hide_score": false, "media_metadata": {"due343t2dfla1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 77, "x": 108, "u": "https://preview.redd.it/due343t2dfla1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8daa04f73512e7d88092a529f7a0a2c6705bb26"}, {"y": 154, "x": 216, "u": "https://preview.redd.it/due343t2dfla1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=793bd1f1c1d1707ce10f90df9eb52bc7bfc0e5a2"}, {"y": 229, "x": 320, "u": "https://preview.redd.it/due343t2dfla1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29c4c78ec092f28b9ddeb5aed814b8c9cbd5588c"}, {"y": 458, "x": 640, "u": "https://preview.redd.it/due343t2dfla1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4009b296b6deaf683834e55e39b3349d6d784b6"}], "s": {"y": 570, "x": 796, "u": "https://preview.redd.it/due343t2dfla1.png?width=796&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=03f6a0f2b80236b4d3283d68f01340bf02f1b99c"}, "id": "due343t2dfla1"}, "ao9lbl13dfla1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e425027ebb39046891a0a93f1aa77d07339ea92"}, {"y": 134, "x": 216, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd986bc87053c98b88c73dd15ca0557abe843e6e"}, {"y": 199, "x": 320, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13e4e03745b95d617d8462878f1714fd810d7628"}, {"y": 398, "x": 640, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a67be6983eaae12e8b4fa2696fc0ec47e875a24"}], "s": {"y": 474, "x": 761, "u": "https://preview.redd.it/ao9lbl13dfla1.png?width=761&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aee2bad26ece0736b01fb82c001c956d55edbcd5"}, "id": "ao9lbl13dfla1"}}, "name": "t3_11gmykf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 166, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "This is one of the drives that arrived from eBay", "media_id": "due343t2dfla1", "id": 246640328}, {"caption": "This is what the online images of the 16TH Exos drives look like", "media_id": "ao9lbl13dfla1", "id": 246640329}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 166, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/o-9GYlzLMjLgkQDYH_eA9WX4LxFqnakRnGrsy0koYs8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677806043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/11gmykf", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gmykf", "is_robot_indexable": true, "report_reasons": null, "author": "AwefulUsername", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gmykf/did_ebay_sell_me_a_knock_off_seagate_exos_16tb_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/11gmykf", "subreddit_subscribers": 671891, "created_utc": 1677806043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1zqb2jib", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download and store 900 3 hour videos in a month?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 111, "top_awarded_type": null, "hide_score": false, "name": "t3_11gwl82", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 117, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 117, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ITlbvE7gVbTwP1MPZEhXENu7FL-95hz01lI-R_hOGHk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677836341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/nf5076evcjla1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/nf5076evcjla1.jpg?auto=webp&amp;v=enabled&amp;s=4a59b90fd926ebf281834b8dcc2cf3e9ecc44355", "width": 1080, "height": 858}, "resolutions": [{"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cb79c537f5c0f47df6a981645cd56eed81eb09f", "width": 108, "height": 85}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3acaf7a28a7cb2f37b176dfe9ee4aeb44cd298d", "width": 216, "height": 171}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78c3ceec035fdd400d90f227f277264ef87eee8c", "width": 320, "height": 254}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=521f2912f298ead72ecf3b1d62b98d4e20283ab0", "width": 640, "height": 508}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=083fd0ea4ac390e888934284aa27460f4c7a02f4", "width": 960, "height": 762}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0347fa8686184d59b36d40475af30e82270f1f9", "width": 1080, "height": 858}], "variants": {}, "id": "6XkkTsTw7RgZSjNidbwi8XNuNWRlJf8vURQIoBd1DAY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gwl82", "is_robot_indexable": true, "report_reasons": null, "author": "Phixiately", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gwl82/how_to_download_and_store_900_3_hour_videos_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/nf5076evcjla1.jpg", "subreddit_subscribers": 671891, "created_utc": 1677836341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I live in an area where symmetrical internet is either expensive or unavailable. My offsite backup is in my brother's house and just rsync'ing it didn't seem reasonable with my upload speed / bandwidth limitations. I decided to go with the tried and true sneakernet method. I decided to take advantage of ZFS' snapshotting functionality. The first script automatically creates incremental snapshots holding all the changes between the latest update on the backup machine and the current date as of running the script. These snapshots can be ferried between the original and backup machines via a portable hard drive. The second script applies runs the backup machine and will apply the snapshots from the portable hard drive to the backup system.  \n\nI thought I'd share the scripts in case their useful for anyone else sneakernetting their backups as well as some feedback from the community.  \n\nThe first script (ideally set to run periodically via cron/systemd.timers)\n\n    #!/bin/bash\n    \n     # This is part one of my 'patented' sneakernet backup system. The inte-\n     # nded usecase is to use a portable hard drive to shuttle incremental \n     # zfs snapshots from the original server (where this script will reside\n     # and run) to the backup server. There are a few requirements:\n     # 1) Both the original and backup servers have matching datasets\n     # 2) The portable hard drive is formatted with ZFS with a pool named\n     #    Sync\n     # 3) When the portable hard drive makes its way back to the original \n     #    server, you zpool import it and rsync the contents of the \n     #    ZFS_incremental folder onto it\n     \n    # Honestly this probably isn't needed in this script but I put it in all\n    # my bash scripts\n    set -e\n    \n    # Determine current date\n    currentdate=$(date +\"%Y-%m-%d\")\n    \n    # Determine the last time we synced with the backupserver. The file is\n    # generated by the backup server and either rsynced via SSH or pulled\n    # from the portable hard drive\n    lwsync=$(tail -n -1 /root/last_backup_sync.txt)\n    \n    # Using the current date to create snapshot names. Endpoints for the\n    # incremental snapshot\n    curISOssnapshot=\"Pool1/ISOs@${currentdate}\"\n    curlinuxsnapshot=\"Pool1/linux@${currentdate}\"\n    curpubdomainsnapshot=\"Pool1/pubdomain@${currentdate}\"\n    \n    # Using the last date we to create snapshot names. Start-points for the\n    # incremental snapshot\n    lwsyncISOssnapshot=\"Pool1/ISOs@${lwsync}\"\n    lwsynclinuxsnapshot=\"Pool1/linux@${lwsync}\"\n    lwsyncpubdomainsnapshot=\"Pool1/pubdomain@${lwsync}\"\n    \n    \n    # use zfs send to save incremental snapshots of each pool\n    echo $curISOssnapshot\n    echo $curlinuxsnapshot\n    echo $curpubdomainsnapshot\n    \n    \n    # Create snapshots of with current date\n    zfs snapshot $curISOssnapshot\n    zfs snapshot $curlinuxsnapshot\n    zfs snapshot $curpubdomainsnapshot\n    echo \"Snapshots made ${currentdate}\" &gt;&gt; /root/backupscript.log\n    \n    # Create Incremental Snapshots of ISOs, linux and pubdomain\n    zfs send -Ri $lwsyncISOssnapshot $curISOssnapshot &gt; /mnt/Pool1/ZFS_incremental/ISOs_latest\n    zfs send -Ri $lwsynclinuxsnapshot $curlinuxsnapshot &gt; /mnt/Pool1/ZFS_incremental/linux_latest\n    zfs send -Ri $lwsyncpubdomainsnapshot $curpubdomainsnapshot &gt; /mnt/Pool1/ZFS_incremental/pubdomain_latest\n    echo \"Incremental snapshots saved to /mnt/Pool1/ZFS_incremental/ \" &gt;&gt; /root/backupscript.log\n    \n\nThe second script (ideally run automatically via cron/systemd.timers. should be setup to run every day for when the backup machine is located at a relative with low technical abilities):\n\n    #! /bin/bash\n    \n    # This script is the second part of my 'patented' sneakernet update \n    # system. The script is intended to be run on the backup server once the \n    # portable hard drive is plugged into the machine. This script does have \n    # a few dependencies: \n    # 1) the backup server must have the same zfs datasets that are going to \n    #    be transferred from the portable hard drive. \n    # 2) the backup server must have automatic snapshotting turned OFF. \n    # 3) this script assumes that you can ssh into your primary server. \n    \n    # Basically stops the script when something goes wrong. Mostly for when\n    # the script is set to run automatically (via systemd.timers or crontab)\n    # and the \n    set -e\n    \n    # set variables for the latest snapshots of the backup system (useful\n    # for later\n    last_ISOs_snapshot=$(zfs list -H -o name -t snapshot BackupPool/ISOs | tail -n -1)\n    last_linux_snapshot=$(zfs list -H -o name -t snapshot BackupPool/linux | tail -n -1)\n    last_pubdomain_snapshot=$(zfs list -H -o name -t snapshot BackupPool/pubdomain | tail -n -1)\n    \n    # Honestly, just here to provide some output for logging to see where\n    # it might have failed\n    echo $last_ISOs_snapshot\n    echo $last_linux_snapshot\n    echo $last_pubdomain_snapshot\n    \n    echo $(date +\"%Y-%m-%d\") &gt;&gt; /root/sync.log\n    echo $(date +\"%Y-%m-%d\")\n    \n    # Imports the pool located on the portable hard drive\n    zpool import -o altroot=/mnt Sync\n    echo \"Import Done\" &gt;&gt; /root/sync.log\n    echo \"Import Done\"\n    \n    # Rollback to last snapshot. This is necessary in case the person using\n    # the backupserver happened to accidentaly ignore your instructions and\n    # alters any of the pools that are going to be synced.\n    zfs rollback $last_ISOs_snapshot\n    zfs rollback $last_linux_snapshot\n    zfs rollback $last_pubdomain_snapshot\n    echo \"Snapshot rollback done\" &gt;&gt; /root/sync.log\n    echo \"Snapshot rollback done\"\n    \n    # Sync Incremental Snapshots of ISOs, linux and pubdomain\n    zfs recv -F BackupPool/ISOs &lt; /mnt/Sync/ISOs_latest\n    echo \"ISOs updated\" &gt;&gt; /root/sync.log\n    echo \"ISOs updated\"\n    \n    zfs recv -F BackupPool/linux &lt; /mnt/Sync/linux_latest\n    echo \"linux updated\" &gt;&gt; /root/sync.log\n    echo \"linux updated\"\n    \n    zfs recv -F BackupPool/pubdomain &lt; /mnt/Sync/pubdomain_latest\n    echo \"pubdomain updated\" &gt;&gt; /root/sync.log\n    echo \"pubdomain updated\"\n    \n    echo \"Sync Complete\" &gt;&gt; /root/sync.log\n    echo \"Sync Complete\"\n    \n    # Create the file that will be referenced by the first script when det-\n    # ermining what the start point is for the incremental snapshot. Saves \n    # it to the portable hard drive in case ssh'ing into the progenitor \n    # server is not possible\n    zfs list -H -o name -t snapshot BackupPool/ISOs &gt; tempa.txt\n    sed 's/BackupPool\\/ISOs@//' tempa.txt &gt; /root/last_backup_sync.txt\n    rm tempa.txt\n    cp /root/last_backup_sync.txt /mnt/Sync/last_backup_sync.txt\n    echo \"Update last_backup_sync.txt done\" &gt;&gt; /root/sync.log\n    echo \"Update last_backup_sync.txt done\"\n    \n    # Sends the file to the original server. Means that the originator ser-\n    # ver can start making the correct incremental snapshots. This, in turn,\n    # means that when the original server receives the portable hard drive,\n    # it need only copy over the pregenerated incremental snapshots.\n    rsync -e \"ssh\" -avz /root/last_backup_sync.txt root@###.###.###.###:/root/\n    echo \"Send last_backup_sync.txt to Original server\" &gt;&gt; /root/sync.log\n    echo \"Send last_backup_sync.txt to Original server\"\n    \n    # Basically ejects the portable hard drive.\n    zpool export Sync\n    echo \"Disconnect Sync complete\" &gt;&gt; /root/sync.log\n    echo \"---------------------------------------------------\" &gt;&gt; /root/sync.log\n    echo \"Disconnect Sync complete\"\n\nCheers,  \nBasilisk\\_hunters", "author_fullname": "t2_2xnvildp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created some scripts to backup my data via sneakernet. thought I'd share", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gpedp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677812574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I live in an area where symmetrical internet is either expensive or unavailable. My offsite backup is in my brother&amp;#39;s house and just rsync&amp;#39;ing it didn&amp;#39;t seem reasonable with my upload speed / bandwidth limitations. I decided to go with the tried and true sneakernet method. I decided to take advantage of ZFS&amp;#39; snapshotting functionality. The first script automatically creates incremental snapshots holding all the changes between the latest update on the backup machine and the current date as of running the script. These snapshots can be ferried between the original and backup machines via a portable hard drive. The second script applies runs the backup machine and will apply the snapshots from the portable hard drive to the backup system.  &lt;/p&gt;\n\n&lt;p&gt;I thought I&amp;#39;d share the scripts in case their useful for anyone else sneakernetting their backups as well as some feedback from the community.  &lt;/p&gt;\n\n&lt;p&gt;The first script (ideally set to run periodically via cron/systemd.timers)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#!/bin/bash\n\n # This is part one of my &amp;#39;patented&amp;#39; sneakernet backup system. The inte-\n # nded usecase is to use a portable hard drive to shuttle incremental \n # zfs snapshots from the original server (where this script will reside\n # and run) to the backup server. There are a few requirements:\n # 1) Both the original and backup servers have matching datasets\n # 2) The portable hard drive is formatted with ZFS with a pool named\n #    Sync\n # 3) When the portable hard drive makes its way back to the original \n #    server, you zpool import it and rsync the contents of the \n #    ZFS_incremental folder onto it\n\n# Honestly this probably isn&amp;#39;t needed in this script but I put it in all\n# my bash scripts\nset -e\n\n# Determine current date\ncurrentdate=$(date +&amp;quot;%Y-%m-%d&amp;quot;)\n\n# Determine the last time we synced with the backupserver. The file is\n# generated by the backup server and either rsynced via SSH or pulled\n# from the portable hard drive\nlwsync=$(tail -n -1 /root/last_backup_sync.txt)\n\n# Using the current date to create snapshot names. Endpoints for the\n# incremental snapshot\ncurISOssnapshot=&amp;quot;Pool1/ISOs@${currentdate}&amp;quot;\ncurlinuxsnapshot=&amp;quot;Pool1/linux@${currentdate}&amp;quot;\ncurpubdomainsnapshot=&amp;quot;Pool1/pubdomain@${currentdate}&amp;quot;\n\n# Using the last date we to create snapshot names. Start-points for the\n# incremental snapshot\nlwsyncISOssnapshot=&amp;quot;Pool1/ISOs@${lwsync}&amp;quot;\nlwsynclinuxsnapshot=&amp;quot;Pool1/linux@${lwsync}&amp;quot;\nlwsyncpubdomainsnapshot=&amp;quot;Pool1/pubdomain@${lwsync}&amp;quot;\n\n\n# use zfs send to save incremental snapshots of each pool\necho $curISOssnapshot\necho $curlinuxsnapshot\necho $curpubdomainsnapshot\n\n\n# Create snapshots of with current date\nzfs snapshot $curISOssnapshot\nzfs snapshot $curlinuxsnapshot\nzfs snapshot $curpubdomainsnapshot\necho &amp;quot;Snapshots made ${currentdate}&amp;quot; &amp;gt;&amp;gt; /root/backupscript.log\n\n# Create Incremental Snapshots of ISOs, linux and pubdomain\nzfs send -Ri $lwsyncISOssnapshot $curISOssnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/ISOs_latest\nzfs send -Ri $lwsynclinuxsnapshot $curlinuxsnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/linux_latest\nzfs send -Ri $lwsyncpubdomainsnapshot $curpubdomainsnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/pubdomain_latest\necho &amp;quot;Incremental snapshots saved to /mnt/Pool1/ZFS_incremental/ &amp;quot; &amp;gt;&amp;gt; /root/backupscript.log\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The second script (ideally run automatically via cron/systemd.timers. should be setup to run every day for when the backup machine is located at a relative with low technical abilities):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#! /bin/bash\n\n# This script is the second part of my &amp;#39;patented&amp;#39; sneakernet update \n# system. The script is intended to be run on the backup server once the \n# portable hard drive is plugged into the machine. This script does have \n# a few dependencies: \n# 1) the backup server must have the same zfs datasets that are going to \n#    be transferred from the portable hard drive. \n# 2) the backup server must have automatic snapshotting turned OFF. \n# 3) this script assumes that you can ssh into your primary server. \n\n# Basically stops the script when something goes wrong. Mostly for when\n# the script is set to run automatically (via systemd.timers or crontab)\n# and the \nset -e\n\n# set variables for the latest snapshots of the backup system (useful\n# for later\nlast_ISOs_snapshot=$(zfs list -H -o name -t snapshot BackupPool/ISOs | tail -n -1)\nlast_linux_snapshot=$(zfs list -H -o name -t snapshot BackupPool/linux | tail -n -1)\nlast_pubdomain_snapshot=$(zfs list -H -o name -t snapshot BackupPool/pubdomain | tail -n -1)\n\n# Honestly, just here to provide some output for logging to see where\n# it might have failed\necho $last_ISOs_snapshot\necho $last_linux_snapshot\necho $last_pubdomain_snapshot\n\necho $(date +&amp;quot;%Y-%m-%d&amp;quot;) &amp;gt;&amp;gt; /root/sync.log\necho $(date +&amp;quot;%Y-%m-%d&amp;quot;)\n\n# Imports the pool located on the portable hard drive\nzpool import -o altroot=/mnt Sync\necho &amp;quot;Import Done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Import Done&amp;quot;\n\n# Rollback to last snapshot. This is necessary in case the person using\n# the backupserver happened to accidentaly ignore your instructions and\n# alters any of the pools that are going to be synced.\nzfs rollback $last_ISOs_snapshot\nzfs rollback $last_linux_snapshot\nzfs rollback $last_pubdomain_snapshot\necho &amp;quot;Snapshot rollback done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Snapshot rollback done&amp;quot;\n\n# Sync Incremental Snapshots of ISOs, linux and pubdomain\nzfs recv -F BackupPool/ISOs &amp;lt; /mnt/Sync/ISOs_latest\necho &amp;quot;ISOs updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;ISOs updated&amp;quot;\n\nzfs recv -F BackupPool/linux &amp;lt; /mnt/Sync/linux_latest\necho &amp;quot;linux updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;linux updated&amp;quot;\n\nzfs recv -F BackupPool/pubdomain &amp;lt; /mnt/Sync/pubdomain_latest\necho &amp;quot;pubdomain updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;pubdomain updated&amp;quot;\n\necho &amp;quot;Sync Complete&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Sync Complete&amp;quot;\n\n# Create the file that will be referenced by the first script when det-\n# ermining what the start point is for the incremental snapshot. Saves \n# it to the portable hard drive in case ssh&amp;#39;ing into the progenitor \n# server is not possible\nzfs list -H -o name -t snapshot BackupPool/ISOs &amp;gt; tempa.txt\nsed &amp;#39;s/BackupPool\\/ISOs@//&amp;#39; tempa.txt &amp;gt; /root/last_backup_sync.txt\nrm tempa.txt\ncp /root/last_backup_sync.txt /mnt/Sync/last_backup_sync.txt\necho &amp;quot;Update last_backup_sync.txt done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Update last_backup_sync.txt done&amp;quot;\n\n# Sends the file to the original server. Means that the originator ser-\n# ver can start making the correct incremental snapshots. This, in turn,\n# means that when the original server receives the portable hard drive,\n# it need only copy over the pregenerated incremental snapshots.\nrsync -e &amp;quot;ssh&amp;quot; -avz /root/last_backup_sync.txt root@###.###.###.###:/root/\necho &amp;quot;Send last_backup_sync.txt to Original server&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Send last_backup_sync.txt to Original server&amp;quot;\n\n# Basically ejects the portable hard drive.\nzpool export Sync\necho &amp;quot;Disconnect Sync complete&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;---------------------------------------------------&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Disconnect Sync complete&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Cheers,&lt;br/&gt;\nBasilisk_hunters&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gpedp", "is_robot_indexable": true, "report_reasons": null, "author": "Basilisk_hunters", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gpedp/i_created_some_scripts_to_backup_my_data_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gpedp/i_created_some_scripts_to_backup_my_data_via/", "subreddit_subscribers": 671891, "created_utc": 1677812574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had an Unraid server for 6 years now, and have decided on building another system - \\*which will definitely be ZFS\\* (speed considerations + I already have all the drives).  \nI'm really conflicted between buying another Unraid license and using the ZFS Plugin until 6.12 comes out with official ZFS support, but on the other hand there's TrueNAS Scale which is free and has a lot of users (and also TrueCharts so a lot of available apps).\n\nWhat are the cons of TrueNAS Scale? Is Kubernetes better than the Unraid Docker?And how about the community and support for the apps? (Like TrueCharts)\n\nThanks in advance for any help!", "author_fullname": "t2_143yl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS System - TrueNAS Scale or ZFS Plugin+Unraid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gsrmp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677822496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had an Unraid server for 6 years now, and have decided on building another system - *which will definitely be ZFS* (speed considerations + I already have all the drives).&lt;br/&gt;\nI&amp;#39;m really conflicted between buying another Unraid license and using the ZFS Plugin until 6.12 comes out with official ZFS support, but on the other hand there&amp;#39;s TrueNAS Scale which is free and has a lot of users (and also TrueCharts so a lot of available apps).&lt;/p&gt;\n\n&lt;p&gt;What are the cons of TrueNAS Scale? Is Kubernetes better than the Unraid Docker?And how about the community and support for the apps? (Like TrueCharts)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "59TB RAID6 | 43TB Usable", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gsrmp", "is_robot_indexable": true, "report_reasons": null, "author": "n0llbyte", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11gsrmp/zfs_system_truenas_scale_or_zfs_pluginunraid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gsrmp/zfs_system_truenas_scale_or_zfs_pluginunraid/", "subreddit_subscribers": 671891, "created_utc": 1677822496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_a1w685cb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are these any good? $130 at Costco. or is there anything better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11glm1t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vnyjYihYE3KsC85uwlTmVfRhZw5K5Kr0Jc45ADlgkHM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677802563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yk25kz1fkgla1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?auto=webp&amp;v=enabled&amp;s=a19a23fd3def1c7d4d07f1b5fa808af7b1625cf2", "width": 6936, "height": 9248}, "resolutions": [{"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33121898d9c7fa8a349aba5be2c2cf92a0698dd3", "width": 108, "height": 144}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d391560c65fa468587dd99187454e7f96da555c0", "width": 216, "height": 288}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29a9a067b00a99664ed0163d9e00051f8fd575ea", "width": 320, "height": 426}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=701bd21366e9494dfbc92111f8fed777d052440a", "width": 640, "height": 853}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d184ebbd2c889f99300850138c380889ca9cc225", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/yk25kz1fkgla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2694bccbf32a1b8add40481652641e1e7d6ceba7", "width": 1080, "height": 1440}], "variants": {}, "id": "y2qsInJSywm4T4RRIBKBd3j79xE18CpKUMDVCd2t1pM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11glm1t", "is_robot_indexable": true, "report_reasons": null, "author": "goofgroot", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11glm1t/are_these_any_good_130_at_costco_or_is_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yk25kz1fkgla1.jpg", "subreddit_subscribers": 671891, "created_utc": 1677802563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " My drives have around 100k files on them, totalling 450GB.\n\nChecking the properties of folders on the old HDD is almost instant. Connected via USB3 to SATA adapter. On my SSD it takes forever. It's been about 10minuts so far and it's still going. I'm comparing two very similar folders and checking their file count and size. I even changed the USB-C cable to a shorter one that came with the SSD but its still so slow. Why could this be? Spec's below\n\nSeagate HDD\n\n* 5900rpm\n* 2TB\n* USB 3-SATA adapter\n\nSandisk SSD Sandisk Extreme Portable (Gen1)\n\n* 550MB/\n* 1TB\n* USB-C-USB-C", "author_fullname": "t2_4d5ehhv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does SSD (USB-C/USB-C) take so long to read file properties vs old HDD (USB3/SATA)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ge2gq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677791252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My drives have around 100k files on them, totalling 450GB.&lt;/p&gt;\n\n&lt;p&gt;Checking the properties of folders on the old HDD is almost instant. Connected via USB3 to SATA adapter. On my SSD it takes forever. It&amp;#39;s been about 10minuts so far and it&amp;#39;s still going. I&amp;#39;m comparing two very similar folders and checking their file count and size. I even changed the USB-C cable to a shorter one that came with the SSD but its still so slow. Why could this be? Spec&amp;#39;s below&lt;/p&gt;\n\n&lt;p&gt;Seagate HDD&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;5900rpm&lt;/li&gt;\n&lt;li&gt;2TB&lt;/li&gt;\n&lt;li&gt;USB 3-SATA adapter&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sandisk SSD Sandisk Extreme Portable (Gen1)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;550MB/&lt;/li&gt;\n&lt;li&gt;1TB&lt;/li&gt;\n&lt;li&gt;USB-C-USB-C&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ge2gq", "is_robot_indexable": true, "report_reasons": null, "author": "i-dm", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ge2gq/why_does_ssd_usbcusbc_take_so_long_to_read_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ge2gq/why_does_ssd_usbcusbc_take_so_long_to_read_file/", "subreddit_subscribers": 671891, "created_utc": 1677791252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys! I am running a small 100TB home NAS (windows based) and I am wondering what software I should buy which automatically does drive surface tests etc on schedule to keep abreast of any impending drive failures! \n\nCorrect me if I am wrong but from what I've heard I think that the gold standard is hard disk sentinel? or there are better softwares out there than that ?", "author_fullname": "t2_723w1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What software is good for drive management?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gci5k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677787659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys! I am running a small 100TB home NAS (windows based) and I am wondering what software I should buy which automatically does drive surface tests etc on schedule to keep abreast of any impending drive failures! &lt;/p&gt;\n\n&lt;p&gt;Correct me if I am wrong but from what I&amp;#39;ve heard I think that the gold standard is hard disk sentinel? or there are better softwares out there than that ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gci5k", "is_robot_indexable": true, "report_reasons": null, "author": "sephiroth_vg", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gci5k/what_software_is_good_for_drive_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gci5k/what_software_is_good_for_drive_management/", "subreddit_subscribers": 671891, "created_utc": 1677787659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, thanks in advance for your help!  \n\n\nMy BF and I have a 40Tb NAS file server in a 4U, 20 drive rackmount server chassis. For a bunch of reasons, not least of which is that some of the drives are quite old, we're planning to condense the content from the various 3 and 6Tb drives onto newer, higher density HDDs. Ideally, we'd like to yes, get more storage space, but also mount them in a smaller chassis. We'd repurpose the healthiest drives we already have by replacing the HDDs in our NAS backup server with ones retired from the file server.   \n\n\nWe like the WD Red Pro drives, and will probably get several of those. Clearly, transferring the data will take some \\*serious\\* time. If possible, we'd like to speed up that process and avoid burdening our machines with the data transfer overhead.   \n\n\nAre harddrive duplicators worth it? If so, any thoughts on which one(s) are better at copying the data quickly \\*and\\* accurately?  \n\n\nI'd also welcome any advice on the least painful way to do this. Please keep in mind this is a home lab setup with 2 Linux NASes (file server and backup server), and while we're both tech savvy, we are intermediate 'Nix users, not pros.   \n\n\nThanks!", "author_fullname": "t2_jxrwr82a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying data between multiple HDDs. Advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g8ukc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677778940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, thanks in advance for your help!  &lt;/p&gt;\n\n&lt;p&gt;My BF and I have a 40Tb NAS file server in a 4U, 20 drive rackmount server chassis. For a bunch of reasons, not least of which is that some of the drives are quite old, we&amp;#39;re planning to condense the content from the various 3 and 6Tb drives onto newer, higher density HDDs. Ideally, we&amp;#39;d like to yes, get more storage space, but also mount them in a smaller chassis. We&amp;#39;d repurpose the healthiest drives we already have by replacing the HDDs in our NAS backup server with ones retired from the file server.   &lt;/p&gt;\n\n&lt;p&gt;We like the WD Red Pro drives, and will probably get several of those. Clearly, transferring the data will take some *serious* time. If possible, we&amp;#39;d like to speed up that process and avoid burdening our machines with the data transfer overhead.   &lt;/p&gt;\n\n&lt;p&gt;Are harddrive duplicators worth it? If so, any thoughts on which one(s) are better at copying the data quickly *and* accurately?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d also welcome any advice on the least painful way to do this. Please keep in mind this is a home lab setup with 2 Linux NASes (file server and backup server), and while we&amp;#39;re both tech savvy, we are intermediate &amp;#39;Nix users, not pros.   &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11g8ukc", "is_robot_indexable": true, "report_reasons": null, "author": "renarde33", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g8ukc/copying_data_between_multiple_hdds_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g8ukc/copying_data_between_multiple_hdds_advice/", "subreddit_subscribers": 671891, "created_utc": 1677778940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a cheap backup for 1 Mac with a NAS, between 1TB to 2TB\n\nAny opinions good or bad on PolarBackup? I've been using them for the last year at a price of 64 USD per year for 650 GB (probably up to 1TB). I must admit when the renewal came up, I wondered if there was anything better/cheaper.\n\nAlso their advertising is not clear about costs.\n\niCloud presently 8USD for a year then about 80 USD ...\n\nAnything else?\n\nBackblaze still the favourite for a small home setup?\n\nThanks", "author_fullname": "t2_xzqlc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a cheap backup for 1 Mac with a NAS, between 1TB to 2TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gwxpw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677837721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a cheap backup for 1 Mac with a NAS, between 1TB to 2TB&lt;/p&gt;\n\n&lt;p&gt;Any opinions good or bad on PolarBackup? I&amp;#39;ve been using them for the last year at a price of 64 USD per year for 650 GB (probably up to 1TB). I must admit when the renewal came up, I wondered if there was anything better/cheaper.&lt;/p&gt;\n\n&lt;p&gt;Also their advertising is not clear about costs.&lt;/p&gt;\n\n&lt;p&gt;iCloud presently 8USD for a year then about 80 USD ...&lt;/p&gt;\n\n&lt;p&gt;Anything else?&lt;/p&gt;\n\n&lt;p&gt;Backblaze still the favourite for a small home setup?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gwxpw", "is_robot_indexable": true, "report_reasons": null, "author": "crabbiesgreenginger", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gwxpw/looking_for_a_cheap_backup_for_1_mac_with_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gwxpw/looking_for_a_cheap_backup_for_1_mac_with_a_nas/", "subreddit_subscribers": 671891, "created_utc": 1677837721.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a whole bunch of PDFs (a few thousand pages over a few hundred files), and I want to search through them but none of them have any kind of OCR/highlighting/searchability. What's a good tool that would let me scan all of these in bulk and produce something easily searched?\n\nIdeally:\n\n--Runs on Windows\n\n--Is fairly simply to use (I can cope with command lines but my willpower falters at 'compiling and tweaking')\n\n--Is free (unfortunately I can't access Adobe Acrobat)", "author_fullname": "t2_t84b2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting OCR on lots of PDFs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ge41a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677791356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a whole bunch of PDFs (a few thousand pages over a few hundred files), and I want to search through them but none of them have any kind of OCR/highlighting/searchability. What&amp;#39;s a good tool that would let me scan all of these in bulk and produce something easily searched?&lt;/p&gt;\n\n&lt;p&gt;Ideally:&lt;/p&gt;\n\n&lt;p&gt;--Runs on Windows&lt;/p&gt;\n\n&lt;p&gt;--Is fairly simply to use (I can cope with command lines but my willpower falters at &amp;#39;compiling and tweaking&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;--Is free (unfortunately I can&amp;#39;t access Adobe Acrobat)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ge41a", "is_robot_indexable": true, "report_reasons": null, "author": "Wyverncraft", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ge41a/getting_ocr_on_lots_of_pdfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ge41a/getting_ocr_on_lots_of_pdfs/", "subreddit_subscribers": 671891, "created_utc": 1677791356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm sending in some old miniDV cassettes to get them digitalized. Now I asked them if they can add the date of the recording onto the video, but since they'll do it afterwards. it won't be like the old school VHS tapes (he actually let me know they use Arial for that lol). So I asked them if I can have the metadata to do it myself in Premiere Pro so I can make it look better.  \nHe told me he can send me the files as AVI files (besides the normal mp4 they usually send) and I can take all the metadata from there.  \n\n\n**Now my question is**, is it easy to find all the single date and time info for all the clips within 1 file? I never done this, so I don't know how to get the individual recording date for each clip (since there are multiple events recorded on each cassette). He said they'll be set up like chapters...  \n(I'm a mac user)", "author_fullname": "t2_12wt346g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best way to extract Metadata (Date) from AVI Files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g4put", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677768896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sending in some old miniDV cassettes to get them digitalized. Now I asked them if they can add the date of the recording onto the video, but since they&amp;#39;ll do it afterwards. it won&amp;#39;t be like the old school VHS tapes (he actually let me know they use Arial for that lol). So I asked them if I can have the metadata to do it myself in Premiere Pro so I can make it look better.&lt;br/&gt;\nHe told me he can send me the files as AVI files (besides the normal mp4 they usually send) and I can take all the metadata from there.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Now my question is&lt;/strong&gt;, is it easy to find all the single date and time info for all the clips within 1 file? I never done this, so I don&amp;#39;t know how to get the individual recording date for each clip (since there are multiple events recorded on each cassette). He said they&amp;#39;ll be set up like chapters...&lt;br/&gt;\n(I&amp;#39;m a mac user)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11g4put", "is_robot_indexable": true, "report_reasons": null, "author": "haebollago", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g4put/best_way_to_extract_metadata_date_from_avi_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g4put/best_way_to_extract_metadata_date_from_avi_files/", "subreddit_subscribers": 671891, "created_utc": 1677768896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have four unused SATA ports on my motherboard. Looking to add below NAS HDDs to only store movies, videos (i.e. no gaming, editing etc)  \nPC is only used for web browsing and downloading/watching movies/sports streams. Not kept on 24/7.\n\n[Seagate IronWolf 12TB NAS HDD ST12000VN0008](https://www.amazon.com.au/Seagate-IronWolf-12TB-Internal-Drive/dp/B084ZTSMWF/ref=sr_1_1?crid=2MZXV0IK9T4U2&amp;keywords=ST12000VN0008&amp;qid=1677842461&amp;sprefix=st12000vn0008%2Caps%2C256&amp;sr=8-1&amp;th=1)\n\nWhat software, if any, should I use to setup my drives, and why?  \nI will backup important files in 2TB/4TB portable hard drives. But I don't need everything backed up.   \n\n\nPC specs:\n\nRyzen 3 3100 + Arctic Freezer 7 X   \nASUS TUF GAMING B550M PLUS  \n2x8GB Vulcan Z 3200MHz DDR4  \nKingston 500GB A2000 M.2 2280 NVMe  \nGeForce GTX 1660 Super  \nCORSAIR RM850  \nbe quiet! Shadow Wings 2 140mm (x3)", "author_fullname": "t2_ag6gszl8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding 4 x 12TB HDD to my PC - setup required?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11gyqse", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677844194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have four unused SATA ports on my motherboard. Looking to add below NAS HDDs to only store movies, videos (i.e. no gaming, editing etc)&lt;br/&gt;\nPC is only used for web browsing and downloading/watching movies/sports streams. Not kept on 24/7.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com.au/Seagate-IronWolf-12TB-Internal-Drive/dp/B084ZTSMWF/ref=sr_1_1?crid=2MZXV0IK9T4U2&amp;amp;keywords=ST12000VN0008&amp;amp;qid=1677842461&amp;amp;sprefix=st12000vn0008%2Caps%2C256&amp;amp;sr=8-1&amp;amp;th=1\"&gt;Seagate IronWolf 12TB NAS HDD ST12000VN0008&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What software, if any, should I use to setup my drives, and why?&lt;br/&gt;\nI will backup important files in 2TB/4TB portable hard drives. But I don&amp;#39;t need everything backed up.   &lt;/p&gt;\n\n&lt;p&gt;PC specs:&lt;/p&gt;\n\n&lt;p&gt;Ryzen 3 3100 + Arctic Freezer 7 X&lt;br/&gt;\nASUS TUF GAMING B550M PLUS&lt;br/&gt;\n2x8GB Vulcan Z 3200MHz DDR4&lt;br/&gt;\nKingston 500GB A2000 M.2 2280 NVMe&lt;br/&gt;\nGeForce GTX 1660 Super&lt;br/&gt;\nCORSAIR RM850&lt;br/&gt;\nbe quiet! Shadow Wings 2 140mm (x3)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gyqse", "is_robot_indexable": true, "report_reasons": null, "author": "hhaahhahahahhah", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gyqse/adding_4_x_12tb_hdd_to_my_pc_setup_required/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gyqse/adding_4_x_12tb_hdd_to_my_pc_setup_required/", "subreddit_subscribers": 671891, "created_utc": 1677844194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "so as the title says above I wish to download this thread but all the unroll / reader app says the thread its old. What can I do . \n\np.s can someone do it for me pls", "author_fullname": "t2_bhrfh3cf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "help downloading a large twitter thread", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gxedc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677839478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so as the title says above I wish to download this thread but all the unroll / reader app says the thread its old. What can I do . &lt;/p&gt;\n\n&lt;p&gt;p.s can someone do it for me pls&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gxedc", "is_robot_indexable": true, "report_reasons": null, "author": "Muted_Amphibian_9325", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gxedc/help_downloading_a_large_twitter_thread/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gxedc/help_downloading_a_large_twitter_thread/", "subreddit_subscribers": 671891, "created_utc": 1677839478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have several disks of diffent sizes in my snapraid/mergerfs configuration. The latest addition is a 20TB drive which is wastly bigger than the smallest (4 TB). I did a mergerfs.balance operation and all drives are now filled equally (\\~45%). I'd like to keep it that way.\n\nHowever, my understanding of mfs is, that it will fill the disk based on \\*actual\\* most free space - not percentual. So my 20TB drive would be almost completely filled before any further writes will be made to the 4TB (about 45% filled right now).\n\nI check the documentation and couldn't find a policy that allows for that. Is there any setting/policy that enables me to fill the disks equally on a percentual basis - similar to how mergerfs.balance levels out the disks?\n\nThank you in advance for any ideas/guidance on that.", "author_fullname": "t2_khebnfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mergerfs - most free space (percentage basis)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gwrm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677837061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several disks of diffent sizes in my snapraid/mergerfs configuration. The latest addition is a 20TB drive which is wastly bigger than the smallest (4 TB). I did a mergerfs.balance operation and all drives are now filled equally (~45%). I&amp;#39;d like to keep it that way.&lt;/p&gt;\n\n&lt;p&gt;However, my understanding of mfs is, that it will fill the disk based on *actual* most free space - not percentual. So my 20TB drive would be almost completely filled before any further writes will be made to the 4TB (about 45% filled right now).&lt;/p&gt;\n\n&lt;p&gt;I check the documentation and couldn&amp;#39;t find a policy that allows for that. Is there any setting/policy that enables me to fill the disks equally on a percentual basis - similar to how mergerfs.balance levels out the disks?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for any ideas/guidance on that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gwrm0", "is_robot_indexable": true, "report_reasons": null, "author": "SillyMochi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gwrm0/mergerfs_most_free_space_percentage_basis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gwrm0/mergerfs_most_free_space_percentage_basis/", "subreddit_subscribers": 671891, "created_utc": 1677837061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys, I've went through many posts and google searches, but I didn't find an answer yet, I want to buy few HDD i prefer nas editions not just an external drive. most users I see them complaining from amazon shipping.\n\nare there no specialized companies for such tasks?  I live in Bahrain, shipment first goes to my US inbox which it will be shipped again to my country, so double horray more shipping time more chance of destruction xD \n\nif anyone can help I'll be grateful, I have 4TB data server atm and I'm trying to expand it starting with 2x 16TB drives. there is no point in buy them if they will arrive destroyed or working for few hours before they dead... any tips is appreciated, I don't mind spending extra for protection.", "author_fullname": "t2_e4a5t65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What best shipping method/ company for HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gwgaq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677835793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I&amp;#39;ve went through many posts and google searches, but I didn&amp;#39;t find an answer yet, I want to buy few HDD i prefer nas editions not just an external drive. most users I see them complaining from amazon shipping.&lt;/p&gt;\n\n&lt;p&gt;are there no specialized companies for such tasks?  I live in Bahrain, shipment first goes to my US inbox which it will be shipped again to my country, so double horray more shipping time more chance of destruction xD &lt;/p&gt;\n\n&lt;p&gt;if anyone can help I&amp;#39;ll be grateful, I have 4TB data server atm and I&amp;#39;m trying to expand it starting with 2x 16TB drives. there is no point in buy them if they will arrive destroyed or working for few hours before they dead... any tips is appreciated, I don&amp;#39;t mind spending extra for protection.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gwgaq", "is_robot_indexable": true, "report_reasons": null, "author": "Unkindled_x", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gwgaq/what_best_shipping_method_company_for_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gwgaq/what_best_shipping_method_company_for_hdd/", "subreddit_subscribers": 671891, "created_utc": 1677835793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just purchased a 12TB Seagate Ironwolf drive from Amazon and it wouldn't spin up. It beeped when powered on and wasn't recognized by Windows. I figured that the drive was DOA, so I got a replacement from Amazon. \n\nThe replacement drive behaved the same way. I'm powering it using an external enclosure as a temporary solution until I get a better setup. The enclosure works perfectly for my 4TB Barracuda drives, and it's rated for drives up to 18TB.\nI also tried both drives in a desktop environment and had the same result.\n\nI'm curious if I'm just unlucky or if I'm missing something. I know that sometimes 3.3v can cause issues if the drive supports PWDIS, but I haven't found any documentation that states that this drive has PWDIS, so I haven't tried the tape method yet.", "author_fullname": "t2_tn5jk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My new Ironwolf drives beep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gu903", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677827433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just purchased a 12TB Seagate Ironwolf drive from Amazon and it wouldn&amp;#39;t spin up. It beeped when powered on and wasn&amp;#39;t recognized by Windows. I figured that the drive was DOA, so I got a replacement from Amazon. &lt;/p&gt;\n\n&lt;p&gt;The replacement drive behaved the same way. I&amp;#39;m powering it using an external enclosure as a temporary solution until I get a better setup. The enclosure works perfectly for my 4TB Barracuda drives, and it&amp;#39;s rated for drives up to 18TB.\nI also tried both drives in a desktop environment and had the same result.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious if I&amp;#39;m just unlucky or if I&amp;#39;m missing something. I know that sometimes 3.3v can cause issues if the drive supports PWDIS, but I haven&amp;#39;t found any documentation that states that this drive has PWDIS, so I haven&amp;#39;t tried the tape method yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gu903", "is_robot_indexable": true, "report_reasons": null, "author": "Scrambles225", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gu903/my_new_ironwolf_drives_beep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gu903/my_new_ironwolf_drives_beep/", "subreddit_subscribers": 671891, "created_utc": 1677827433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for software that will scan every photo in my library and add tags to them to allow searching. Ideally, this will function similarly to the photo search from iOS where you can search \"shoe\" and shoes will appear.", "author_fullname": "t2_50n4upyn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo Library Scan &amp; Organization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11go6ou", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677809271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for software that will scan every photo in my library and add tags to them to allow searching. Ideally, this will function similarly to the photo search from iOS where you can search &amp;quot;shoe&amp;quot; and shoes will appear.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11go6ou", "is_robot_indexable": true, "report_reasons": null, "author": "c00pdwg", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11go6ou/photo_library_scan_organization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11go6ou/photo_library_scan_organization/", "subreddit_subscribers": 671891, "created_utc": 1677809271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What I really want is a program that overlays with explorer (Windows), so when I'm searching for a file to upload or send, I can see the ranking and tags that I've added to the file, without slowing things down too much. \n\n&amp;#x200B;\n\nDoes TagSpaces work like that, or is it only some browser interface where you can see your tags and ratings?", "author_fullname": "t2_fl91vcm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for software that lets me rank (and maybe tag) files? Is Tagspaces a good and safe option?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gkl0g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677800220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I really want is a program that overlays with explorer (Windows), so when I&amp;#39;m searching for a file to upload or send, I can see the ranking and tags that I&amp;#39;ve added to the file, without slowing things down too much. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does TagSpaces work like that, or is it only some browser interface where you can see your tags and ratings?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gkl0g", "is_robot_indexable": true, "report_reasons": null, "author": "Atlantic0ne", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gkl0g/looking_for_software_that_lets_me_rank_and_maybe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gkl0g/looking_for_software_that_lets_me_rank_and_maybe/", "subreddit_subscribers": 671891, "created_utc": 1677800220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Until now I've relied on various clouds for my important  data. I don't like  it but  it's easy &amp; ensures that I have all of this  data in 2 locations. Makes it easy to access on multiple machines or when I switch  to a new laptop as I  just did. But in the past I've had lots of large photos,  raw/jpg/etc that I've lost track of simply because  I moved to a new desktop and didn't secure the old data before wiping  out the disk. In those cases I  typically kept the jpgs &amp; ditched  the raw thinking I'd never need it. Now  I wish I had it. Then  I have data from  my company (1 person company, software programmer, accounting software) that I need to keep until  I wind  up the company  in a year or two. So it's all  of the source code, sample data etc. I will want this going forward because I have written software for myself to track  my investments so that needs to  be secure.\n\nI've decided to get a Synology box to start. I was thinking about the DS220+ but I see the DS223 just came out and  expect a DS223+ shortly so I'm waiting on that. It seems that the DS220+ is still preferable to the  DS223 but comparisons are hard  to come by. Ideally I'd like to get to a DS923+ but I'm thinking I'll start with a 22\\* and then gain some experience &amp; get a 923 in  the future and use the 22\\* to mirror data from the 923. There are very cheap WD Red Pro disks on sale in Canada right now ($399 for 18Tb) so  I  ordered 2 of those before the sale ends. I'll  start with those in whatever box I  get and do a RAID 1. It's way more space than  I  need but cheap enough  to  just bite  the bullet to start.\n\nI'd love  any suggestions you  may have.", "author_fullname": "t2_4mhep53l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting out focusing on securing my data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g71ie", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677774614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Until now I&amp;#39;ve relied on various clouds for my important  data. I don&amp;#39;t like  it but  it&amp;#39;s easy &amp;amp; ensures that I have all of this  data in 2 locations. Makes it easy to access on multiple machines or when I switch  to a new laptop as I  just did. But in the past I&amp;#39;ve had lots of large photos,  raw/jpg/etc that I&amp;#39;ve lost track of simply because  I moved to a new desktop and didn&amp;#39;t secure the old data before wiping  out the disk. In those cases I  typically kept the jpgs &amp;amp; ditched  the raw thinking I&amp;#39;d never need it. Now  I wish I had it. Then  I have data from  my company (1 person company, software programmer, accounting software) that I need to keep until  I wind  up the company  in a year or two. So it&amp;#39;s all  of the source code, sample data etc. I will want this going forward because I have written software for myself to track  my investments so that needs to  be secure.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve decided to get a Synology box to start. I was thinking about the DS220+ but I see the DS223 just came out and  expect a DS223+ shortly so I&amp;#39;m waiting on that. It seems that the DS220+ is still preferable to the  DS223 but comparisons are hard  to come by. Ideally I&amp;#39;d like to get to a DS923+ but I&amp;#39;m thinking I&amp;#39;ll start with a 22* and then gain some experience &amp;amp; get a 923 in  the future and use the 22* to mirror data from the 923. There are very cheap WD Red Pro disks on sale in Canada right now ($399 for 18Tb) so  I  ordered 2 of those before the sale ends. I&amp;#39;ll  start with those in whatever box I  get and do a RAID 1. It&amp;#39;s way more space than  I  need but cheap enough  to  just bite  the bullet to start.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love  any suggestions you  may have.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11g71ie", "is_robot_indexable": true, "report_reasons": null, "author": "DaveWpgC", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g71ie/starting_out_focusing_on_securing_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g71ie/starting_out_focusing_on_securing_my_data/", "subreddit_subscribers": 671891, "created_utc": 1677774614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! I'm running Windows. I have an external hard drive with my data on it using the ExFAT file system. I know this is a terrible idea due to lack of journaling, so I want to reformat it to NTFS.\n\nI do backups on the free version of VEEAM to a second external drive. \n\nIf I reformat my first drive to NTFS, can I then restore through VEEAM, and all my data will just be ready and waiting? Do I need to anything special to prepare for this (like, should I do a full backup in VEEAM as opposed to an incremental one before I do this?)\n\nThanks!", "author_fullname": "t2_2gu72213", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Veeam Restoring after Reformatting source disk to change file format?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g6msm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677773612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m running Windows. I have an external hard drive with my data on it using the ExFAT file system. I know this is a terrible idea due to lack of journaling, so I want to reformat it to NTFS.&lt;/p&gt;\n\n&lt;p&gt;I do backups on the free version of VEEAM to a second external drive. &lt;/p&gt;\n\n&lt;p&gt;If I reformat my first drive to NTFS, can I then restore through VEEAM, and all my data will just be ready and waiting? Do I need to anything special to prepare for this (like, should I do a full backup in VEEAM as opposed to an incremental one before I do this?)&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11g6msm", "is_robot_indexable": true, "report_reasons": null, "author": "exixius", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11g6msm/veeam_restoring_after_reformatting_source_disk_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11g6msm/veeam_restoring_after_reformatting_source_disk_to/", "subreddit_subscribers": 671891, "created_utc": 1677773612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm tinkering with some cloud storage and thought it might be interesting to combine a few of them into a \"RAID\" for benefits similar to local arrays. If one of the providers goes down, the account gets banned or policy changes or whatever, then I won't lose everything immediately like on a RAID 0 equivalent. And I'll be able to use the aggregated storage more effectively since I have don't have to worry about how full each individual drive is.\n\nI figure it's going to be annoying as hell to combine all the free 2 or 5GB of storage here and there so I mainly want to do this with large 3 disks; Oracle Cloud (150/200GB, using SFTP), Storj (150GB) and SFTP / local.\n\nOur other lord and savior (the primary one being zfs of course lol) rclone doesn't seem to have a trick for this one. The \"combine\" policies I found seem to be oriented towards where to place the data (lowest free space used, fewest objects stored etc) and doesn't have a function for RAID. Does rclone have a feature for this?\n\nIf not, I think I can try splitting the files up and semi manully adding 33% parity data. I don't know if that guarantees single drive redundancy though, since a data chunk and parity of that chunk could end up on the same disk. I don't suppose there's an rclone solution for this... Lol.", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is 1 drive redundancy possible with cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11gziv0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677846528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m tinkering with some cloud storage and thought it might be interesting to combine a few of them into a &amp;quot;RAID&amp;quot; for benefits similar to local arrays. If one of the providers goes down, the account gets banned or policy changes or whatever, then I won&amp;#39;t lose everything immediately like on a RAID 0 equivalent. And I&amp;#39;ll be able to use the aggregated storage more effectively since I have don&amp;#39;t have to worry about how full each individual drive is.&lt;/p&gt;\n\n&lt;p&gt;I figure it&amp;#39;s going to be annoying as hell to combine all the free 2 or 5GB of storage here and there so I mainly want to do this with large 3 disks; Oracle Cloud (150/200GB, using SFTP), Storj (150GB) and SFTP / local.&lt;/p&gt;\n\n&lt;p&gt;Our other lord and savior (the primary one being zfs of course lol) rclone doesn&amp;#39;t seem to have a trick for this one. The &amp;quot;combine&amp;quot; policies I found seem to be oriented towards where to place the data (lowest free space used, fewest objects stored etc) and doesn&amp;#39;t have a function for RAID. Does rclone have a feature for this?&lt;/p&gt;\n\n&lt;p&gt;If not, I think I can try splitting the files up and semi manully adding 33% parity data. I don&amp;#39;t know if that guarantees single drive redundancy though, since a data chunk and parity of that chunk could end up on the same disk. I don&amp;#39;t suppose there&amp;#39;s an rclone solution for this... Lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "vTrueNAS 72TB / Hyper-V", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gziv0", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11gziv0/is_1_drive_redundancy_possible_with_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gziv0/is_1_drive_redundancy_possible_with_cloud/", "subreddit_subscribers": 671891, "created_utc": 1677846528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using MultCloud right now. Here's my issue,.. I have 4 dropbox accounts using different emails... Can I can add them even though the email I used to create the MultCloud account is assigned to only one of the 4 dropbox accounts? \n\nSide note... I'd like to keep it organized if that's possible. Does this mean I need several MultCloud accounts too? \n\nThanks in advance.", "author_fullname": "t2_jb6dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "several email addresses using same clouds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gql0c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677815912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using MultCloud right now. Here&amp;#39;s my issue,.. I have 4 dropbox accounts using different emails... Can I can add them even though the email I used to create the MultCloud account is assigned to only one of the 4 dropbox accounts? &lt;/p&gt;\n\n&lt;p&gt;Side note... I&amp;#39;d like to keep it organized if that&amp;#39;s possible. Does this mean I need several MultCloud accounts too? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gql0c", "is_robot_indexable": true, "report_reasons": null, "author": "badcat130", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gql0c/several_email_addresses_using_same_clouds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gql0c/several_email_addresses_using_same_clouds/", "subreddit_subscribers": 671891, "created_utc": 1677815912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After some consideration, I\u2019ve decided to backup my mediaserver to BackBlaze B2.  Just doing an rclone sync for the time being, as I continue to clean up my local directories. \n\nI\u2019m not using encryption on thr client side, so I imagine that backblaze has the capability to see everything I\u2019ve backed up. \n\nMy question is, does anyone know if they scan private B2 buckets for copyrighted data? It would be might inconvienent if they did, and I\u2019d probably have to come up with a new strategy to encrypt and remove obfuscate before uploading\n\nThx!", "author_fullname": "t2_r6bjole0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Backup q?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gj5go", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677798281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After some consideration, I\u2019ve decided to backup my mediaserver to BackBlaze B2.  Just doing an rclone sync for the time being, as I continue to clean up my local directories. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m not using encryption on thr client side, so I imagine that backblaze has the capability to see everything I\u2019ve backed up. &lt;/p&gt;\n\n&lt;p&gt;My question is, does anyone know if they scan private B2 buckets for copyrighted data? It would be might inconvienent if they did, and I\u2019d probably have to come up with a new strategy to encrypt and remove obfuscate before uploading&lt;/p&gt;\n\n&lt;p&gt;Thx!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gj5go", "is_robot_indexable": true, "report_reasons": null, "author": "AuthenticImposter", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gj5go/cloud_backup_q/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gj5go/cloud_backup_q/", "subreddit_subscribers": 671891, "created_utc": 1677798281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently working on devising a plan for a way to store a good amount of media from across the different orgs and other groups on our university campus to go into a time capsule to be opened on our 50th reunion.\n\nI already did base research in figuring out roughly what storage types and methods I should be going for but wanted to get external opinions/questions answered before I commit to it.\n\nIdeally the plan is to go with optical disks such as BR or M-Disc (only concern with the latter is finding hardware for burning) and immediately after place it back in its factory case. I'm also toying with the idea of adding a vacuum sealed bag + silica packs since this will be buried underground along with other items in the capsule. \n\nMy main concerns are budget, both monetary and timewise, is this feasible with a lower $100-200 budget and a timeframe of about 1 month? also if there are any further precautions I should look into taking. \n\nIf there's anything I missed please lmk thank you", "author_fullname": "t2_3d2xhhyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time Capsule options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ghpb8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677796679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working on devising a plan for a way to store a good amount of media from across the different orgs and other groups on our university campus to go into a time capsule to be opened on our 50th reunion.&lt;/p&gt;\n\n&lt;p&gt;I already did base research in figuring out roughly what storage types and methods I should be going for but wanted to get external opinions/questions answered before I commit to it.&lt;/p&gt;\n\n&lt;p&gt;Ideally the plan is to go with optical disks such as BR or M-Disc (only concern with the latter is finding hardware for burning) and immediately after place it back in its factory case. I&amp;#39;m also toying with the idea of adding a vacuum sealed bag + silica packs since this will be buried underground along with other items in the capsule. &lt;/p&gt;\n\n&lt;p&gt;My main concerns are budget, both monetary and timewise, is this feasible with a lower $100-200 budget and a timeframe of about 1 month? also if there are any further precautions I should look into taking. &lt;/p&gt;\n\n&lt;p&gt;If there&amp;#39;s anything I missed please lmk thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ghpb8", "is_robot_indexable": true, "report_reasons": null, "author": "Mango_yoshi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ghpb8/time_capsule_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ghpb8/time_capsule_options/", "subreddit_subscribers": 671891, "created_utc": 1677796679.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}