{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Company recently made the decision to move to snowflake from a mishmash of on-prem databases (mostly sql server). \n\nData science team uses R and spark running on a handful of on prem workstations to build models. \n\nThe IT department is pushing hard to move everything into snowflake and the data science people are complaining that snowpark is half baked. Especially on R programming where they'll still have to take data out of snowflake into some other compute source to run R models, then insert results back into snowflake.\n\nTrying to think of solutions. Should we:\n\n1) convince IT so provision a small databricks account we can use just for running models?\n2) suck it up and convert everything to python, trying to adopt snowpark/anaconda as much as possible?\n3) keep our pile of on prem machines with do modeling by pulling data out of snowflake, computing locally, inserting model results back into snowflake for use in reports, decision dashboards, etc.", "author_fullname": "t2_5iwre", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "machine learning in snowflake, unhappy data scientists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121mm5c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679752633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Company recently made the decision to move to snowflake from a mishmash of on-prem databases (mostly sql server). &lt;/p&gt;\n\n&lt;p&gt;Data science team uses R and spark running on a handful of on prem workstations to build models. &lt;/p&gt;\n\n&lt;p&gt;The IT department is pushing hard to move everything into snowflake and the data science people are complaining that snowpark is half baked. Especially on R programming where they&amp;#39;ll still have to take data out of snowflake into some other compute source to run R models, then insert results back into snowflake.&lt;/p&gt;\n\n&lt;p&gt;Trying to think of solutions. Should we:&lt;/p&gt;\n\n&lt;p&gt;1) convince IT so provision a small databricks account we can use just for running models?\n2) suck it up and convert everything to python, trying to adopt snowpark/anaconda as much as possible?\n3) keep our pile of on prem machines with do modeling by pulling data out of snowflake, computing locally, inserting model results back into snowflake for use in reports, decision dashboards, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121mm5c", "is_robot_indexable": true, "report_reasons": null, "author": "CantGoogleMe", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121mm5c/machine_learning_in_snowflake_unhappy_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121mm5c/machine_learning_in_snowflake_unhappy_data/", "subreddit_subscribers": 94355, "created_utc": 1679752633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working as a Data Engineer for a bit now, but most of my background is in full stack development utilizing Node, dotNet, and front-end frameworks, and both noSQL and SQL databases etc., but rarely Python. Python was always used as a scripting language or for hobbies. Now, in the world of \"data fill-in-the-blank\" Python is the bee's knees. I do like it..., and have decided to move forward with a sticker on my Mac but at what point an I forcing a round peg in an octagon hole? I have been using Python to write small pipelines moving a million rows or less from various flat-files or DB tables to some data warehouse. Everything seems simple: Read in a CSV with Pandas, do some cleaning, upload to the whatever database, etc. However, some of the projects I'm working on are requiring a bit more sophistication or heavy lifting. Pandas and Sqlalchemy seem chunky and not great (slow) for migrating greater than a million rows from one source to another. Are there options other than Sqlalchemy or Pandas in the Python world for moving data or have I been reading too many articles by Data Scientists?", "author_fullname": "t2_74fehzbh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At what point is Python not answer for piping data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1211nsk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679698103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working as a Data Engineer for a bit now, but most of my background is in full stack development utilizing Node, dotNet, and front-end frameworks, and both noSQL and SQL databases etc., but rarely Python. Python was always used as a scripting language or for hobbies. Now, in the world of &amp;quot;data fill-in-the-blank&amp;quot; Python is the bee&amp;#39;s knees. I do like it..., and have decided to move forward with a sticker on my Mac but at what point an I forcing a round peg in an octagon hole? I have been using Python to write small pipelines moving a million rows or less from various flat-files or DB tables to some data warehouse. Everything seems simple: Read in a CSV with Pandas, do some cleaning, upload to the whatever database, etc. However, some of the projects I&amp;#39;m working on are requiring a bit more sophistication or heavy lifting. Pandas and Sqlalchemy seem chunky and not great (slow) for migrating greater than a million rows from one source to another. Are there options other than Sqlalchemy or Pandas in the Python world for moving data or have I been reading too many articles by Data Scientists?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1211nsk", "is_robot_indexable": true, "report_reasons": null, "author": "afivegallonbucket", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1211nsk/at_what_point_is_python_not_answer_for_piping_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1211nsk/at_what_point_is_python_not_answer_for_piping_data/", "subreddit_subscribers": 94355, "created_utc": 1679698103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\n\na few days ago in a thread here it was mentioned how much of a pain in the ass is to develop and test AWS Glue jobs locally.\n\nSo I created a devcontainer template repository that provides an out of the box experience to do exactly that.\n\nYou can find it here: [https://github.com/wtfzambo/glue-devcontainer-template](https://github.com/wtfzambo/glue-devcontainer-template)\n\nAlso, any feedback is much appreciated!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Glue jobs devcontainer - template repo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120uauq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679744739.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679684525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;a few days ago in a thread here it was mentioned how much of a pain in the ass is to develop and test AWS Glue jobs locally.&lt;/p&gt;\n\n&lt;p&gt;So I created a devcontainer template repository that provides an out of the box experience to do exactly that.&lt;/p&gt;\n\n&lt;p&gt;You can find it here: &lt;a href=\"https://github.com/wtfzambo/glue-devcontainer-template\"&gt;https://github.com/wtfzambo/glue-devcontainer-template&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also, any feedback is much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UoCH6o1tYaBUocy2bnCAr29AJZ7vKO7VGeNYbTdJtVQ.jpg?auto=webp&amp;v=enabled&amp;s=fed95134cd01b110a0552a0f0c6f31230b0fa842", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/UoCH6o1tYaBUocy2bnCAr29AJZ7vKO7VGeNYbTdJtVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04c9a7b66bbbbb9dcccb0ebebdda82bdd4fbe444", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/UoCH6o1tYaBUocy2bnCAr29AJZ7vKO7VGeNYbTdJtVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce730edfcb974d33ae35bb8c4f91608504010ba4", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/UoCH6o1tYaBUocy2bnCAr29AJZ7vKO7VGeNYbTdJtVQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b76226e4d921cda1f718b51a723faed98b354e53", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/UoCH6o1tYaBUocy2bnCAr29AJZ7vKO7VGeNYbTdJtVQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a0517f1a0a05691247a92ddbb43f72b73a040e9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/UoCH6o1tYaBUocy2bnCAr29AJZ7vKO7VGeNYbTdJtVQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cab3f49e10dec3d880f45e2cea065224f62eedbb", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/UoCH6o1tYaBUocy2bnCAr29AJZ7vKO7VGeNYbTdJtVQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41938ccb999bc6d556a9ed684a7f9d6752f02201", "width": 1080, "height": 540}], "variants": {}, "id": "PI6vLq5gsn2Faa2wk3Vxb99CPKpBD1nFBCd6twFbY-A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "120uauq", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/120uauq/aws_glue_jobs_devcontainer_template_repo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/120uauq/aws_glue_jobs_devcontainer_template_repo/", "subreddit_subscribers": 94355, "created_utc": 1679684525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, i am in the process of building a new data model. Our existing model is OBT, while it's good for analysts, it's a nightmare for dashboard use cases. I am thinking of building a traditional dimensional Model which I'll use in dashboards and build a OBT on top of it to give to analysts. But I had a few questions :\n\n1. It is said that dimensional models can be slow to process the data. Our current OBT model refreshes 8 times a day and the new model should be refreshed in the same frequency or most probably once every hour.\n\n2. We use databricks, while it could solve some of load time problems mentioned above, I am not really able to find any resources that tells me whether there's omr modelling approach that is more suitable to a spark platform like databricks compared to others.", "author_fullname": "t2_xap78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Platform considerations for modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121lqh0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679750588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, i am in the process of building a new data model. Our existing model is OBT, while it&amp;#39;s good for analysts, it&amp;#39;s a nightmare for dashboard use cases. I am thinking of building a traditional dimensional Model which I&amp;#39;ll use in dashboards and build a OBT on top of it to give to analysts. But I had a few questions :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;It is said that dimensional models can be slow to process the data. Our current OBT model refreshes 8 times a day and the new model should be refreshed in the same frequency or most probably once every hour.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We use databricks, while it could solve some of load time problems mentioned above, I am not really able to find any resources that tells me whether there&amp;#39;s omr modelling approach that is more suitable to a spark platform like databricks compared to others.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121lqh0", "is_robot_indexable": true, "report_reasons": null, "author": "totalsports1", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/121lqh0/platform_considerations_for_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121lqh0/platform_considerations_for_modelling/", "subreddit_subscribers": 94355, "created_utc": 1679750588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As per title. How are you using DuckDB, either privately for passion/side projects, hobbies, or professionally in production environments?\n\nWould love to just hear the innovative ways in which people are using it.", "author_fullname": "t2_4bjsc4oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you using DuckDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121jccb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679744601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per title. How are you using DuckDB, either privately for passion/side projects, hobbies, or professionally in production environments?&lt;/p&gt;\n\n&lt;p&gt;Would love to just hear the innovative ways in which people are using it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121jccb", "is_robot_indexable": true, "report_reasons": null, "author": "jb7834", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121jccb/how_are_you_using_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121jccb/how_are_you_using_duckdb/", "subreddit_subscribers": 94355, "created_utc": 1679744601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am dealing with some data containing user-ids that need to be counted in power-bi. However, due to various potential users in power bi, we have the requirement to not store plain-text user-ids in any table connected to power bi.\nI am wondering what would be the best practice in such a case? I suggested a concept in which user-id get hashed with a salt, and a mapping table between each user-id and its hash is stored in a table which is not available in power-bi.  \nWhat are your thoughts about this approach? Did you had similar requirements? Are there alternatives to solve that problem?", "author_fullname": "t2_8etbdwvd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to encrypt sensitive data from data lake to power bi?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121i1mv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679740981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am dealing with some data containing user-ids that need to be counted in power-bi. However, due to various potential users in power bi, we have the requirement to not store plain-text user-ids in any table connected to power bi.\nI am wondering what would be the best practice in such a case? I suggested a concept in which user-id get hashed with a salt, and a mapping table between each user-id and its hash is stored in a table which is not available in power-bi.&lt;br/&gt;\nWhat are your thoughts about this approach? Did you had similar requirements? Are there alternatives to solve that problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121i1mv", "is_robot_indexable": true, "report_reasons": null, "author": "Remote-Juice2527", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121i1mv/how_to_encrypt_sensitive_data_from_data_lake_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121i1mv/how_to_encrypt_sensitive_data_from_data_lake_to/", "subreddit_subscribers": 94355, "created_utc": 1679740981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm making a ELT Pipeline. First grabbing source data putting it i. S3 then transforming the data to be used in production systems and putting it in another s3 bucket.\n\nI'll have 2 lambdas to do this, one to grab the data and put it on s3 and another to do the transformation.\n\nMy question is, should I use an orchestrator like step functions to do this, or can I just put event bridge triggers to look for new files placed on the s3 bucket for the second lambda and a scheduled rate for the first lambda? \n\nBoth seem fine to me. Anyone have opinions about this?", "author_fullname": "t2_88w2s61v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS ELT Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120xk1d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679690012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m making a ELT Pipeline. First grabbing source data putting it i. S3 then transforming the data to be used in production systems and putting it in another s3 bucket.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll have 2 lambdas to do this, one to grab the data and put it on s3 and another to do the transformation.&lt;/p&gt;\n\n&lt;p&gt;My question is, should I use an orchestrator like step functions to do this, or can I just put event bridge triggers to look for new files placed on the s3 bucket for the second lambda and a scheduled rate for the first lambda? &lt;/p&gt;\n\n&lt;p&gt;Both seem fine to me. Anyone have opinions about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "120xk1d", "is_robot_indexable": true, "report_reasons": null, "author": "TheLoneKid", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/120xk1d/aws_elt_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/120xk1d/aws_elt_pipeline/", "subreddit_subscribers": 94355, "created_utc": 1679690012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I have been looking into IT as a career recently, and settled on working towards working in Cloud Engineering. I was told to start working on getting a CompTIA A+ certification, but I am unsure if that is the first step, where to take the exam or training, or what the next steps would be. \n\nI want to get there through certifications if possible as I am not in a position to go to college.", "author_fullname": "t2_5t16cove", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What steps and certifications should I get if I want to get into Cloud Engineering as a career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120xqne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679690300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I have been looking into IT as a career recently, and settled on working towards working in Cloud Engineering. I was told to start working on getting a CompTIA A+ certification, but I am unsure if that is the first step, where to take the exam or training, or what the next steps would be. &lt;/p&gt;\n\n&lt;p&gt;I want to get there through certifications if possible as I am not in a position to go to college.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "120xqne", "is_robot_indexable": true, "report_reasons": null, "author": "GingerKit07", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/120xqne/what_steps_and_certifications_should_i_get_if_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/120xqne/what_steps_and_certifications_should_i_get_if_i/", "subreddit_subscribers": 94355, "created_utc": 1679690300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\nI\u2019m exploring how to integrate Apache Airflow into our communicable disease unit workflow. We are mostly R users but are starting to use Python more. The basic workflow I have in mind is:\n\nTask 1 - trigger script 1 - pull immunization records from Snowflake &gt;&gt; Task 2 - trigger script 2 - clean and deduplicate, then add new records to historical dataset &gt;&gt; Update R Shiny\n\nI have Airflow up and running via docker, but I\u2019m stuck on how to trigger scripts via bash command. Can I have the script anywhere on our network drive, or does it have to be somewhere within the airflow folder? In the past I\u2019ve used windows task scheduler and .bat files for run scripts overnight, but trying to better automate some of our repetitive tasks.", "author_fullname": "t2_7e2bmavx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Airflow Triggering Python Scripts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_121rg90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679762604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI\u2019m exploring how to integrate Apache Airflow into our communicable disease unit workflow. We are mostly R users but are starting to use Python more. The basic workflow I have in mind is:&lt;/p&gt;\n\n&lt;p&gt;Task 1 - trigger script 1 - pull immunization records from Snowflake &amp;gt;&amp;gt; Task 2 - trigger script 2 - clean and deduplicate, then add new records to historical dataset &amp;gt;&amp;gt; Update R Shiny&lt;/p&gt;\n\n&lt;p&gt;I have Airflow up and running via docker, but I\u2019m stuck on how to trigger scripts via bash command. Can I have the script anywhere on our network drive, or does it have to be somewhere within the airflow folder? In the past I\u2019ve used windows task scheduler and .bat files for run scripts overnight, but trying to better automate some of our repetitive tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121rg90", "is_robot_indexable": true, "report_reasons": null, "author": "Senior-Athlete-0", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121rg90/apache_airflow_triggering_python_scripts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121rg90/apache_airflow_triggering_python_scripts/", "subreddit_subscribers": 94355, "created_utc": 1679762604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using dbt, and I have a use case at the company I work at which I don't think is supported exactly. We use a green/blue deploy strategy for each table. When all the dbt tests for that specific table have completed successfully, I'd like to run a sql snippet to swap/promote it. Kind of like a model post-hook, but not after the model is run, instead for collection after all per-model tests pass. Does anyone have any ideas for a workaround, hack, or a package that might work?\n\nCurrently I'm writing test results to the database and having a second step to parse them, but a single pass option seems like it should be possible.", "author_fullname": "t2_dzfsy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt post-hook for successful tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_121shh6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764674.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using dbt, and I have a use case at the company I work at which I don&amp;#39;t think is supported exactly. We use a green/blue deploy strategy for each table. When all the dbt tests for that specific table have completed successfully, I&amp;#39;d like to run a sql snippet to swap/promote it. Kind of like a model post-hook, but not after the model is run, instead for collection after all per-model tests pass. Does anyone have any ideas for a workaround, hack, or a package that might work?&lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m writing test results to the database and having a second step to parse them, but a single pass option seems like it should be possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121shh6", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_Again", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121shh6/dbt_posthook_for_successful_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121shh6/dbt_posthook_for_successful_tests/", "subreddit_subscribers": 94355, "created_utc": 1679764674.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Interested to hear how (and what) teams are doing for data pipeline documentation. Where is the proper balance between maintainability and enough detail to provide value. What items are you covering the document (source, maybe associated jira details, high level business logic overview, etc...)?  Where are you keeping it github-pages, confluence, G drive (oh the horror)...?\n\nI don't feel code documentation should be included as the code itself should be well commented and clean and with the changes frequent in code bases it'd never be kept up anyway.", "author_fullname": "t2_puuzgu6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline documentation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_121s86p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679764741.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interested to hear how (and what) teams are doing for data pipeline documentation. Where is the proper balance between maintainability and enough detail to provide value. What items are you covering the document (source, maybe associated jira details, high level business logic overview, etc...)?  Where are you keeping it github-pages, confluence, G drive (oh the horror)...?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t feel code documentation should be included as the code itself should be well commented and clean and with the changes frequent in code bases it&amp;#39;d never be kept up anyway.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121s86p", "is_robot_indexable": true, "report_reasons": null, "author": "getafterit123", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121s86p/pipeline_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121s86p/pipeline_documentation/", "subreddit_subscribers": 94355, "created_utc": 1679764169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone who has tried Polars in AWS Glue using the Shell script option, or whether it is even feasible at all? My work is dirt cheap and wants to minimize costs as much as possible lol. I'll be processing 500mb worth of data and not even sure if the 0.0625 DPU option would even cut it. For context, that is like 0.25 vCPU and 1GB RAM lol.", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars Library in AWS Glue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121ou6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679756928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone who has tried Polars in AWS Glue using the Shell script option, or whether it is even feasible at all? My work is dirt cheap and wants to minimize costs as much as possible lol. I&amp;#39;ll be processing 500mb worth of data and not even sure if the 0.0625 DPU option would even cut it. For context, that is like 0.25 vCPU and 1GB RAM lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121ou6t", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121ou6t/polars_library_in_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121ou6t/polars_library_in_aws_glue/", "subreddit_subscribers": 94355, "created_utc": 1679756928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nNew to ADF, I have an integration landing data in a container on a storage account, I use copy activity with wildcard path (and merge files in settings) to collect all the files every night, I copy them to another storage account where they are stored as parquet. After the copy activity I delete all the files in the first \"landing\" container.\n\nMy next step is to implement a python script I have, that implements scd and then I want to sink the result of that to the database. Two questions:\n\n1. The files arrive as json and I copy to parquet, sometimes it fails since copy activity does not understand what data type a specific column should be. How do I overcome this? Currently I use the advanced editor to read up some mapping from an sql-table where I explicitly pointing out what data type each column should be. However, this is not ideal since this will disregard any attribute on the json-object not defined in the mapping and I want this to be dynamic. I do not want to lose any data in this step.\n2. Where do you guys implement your python scripts on data in adf? What machine will execute it? My only demand is that the script can access the lake and the database. I know about databricks and the notebook activity in adf but spinning up a spark cluster feels a bit dramatic. The biggest parquet files is about 300-400 mb. I do use pyspark in my script though, but since it is pretty small data I think pyspark running on one simple machine would be enough? \n\nI only do the json -&gt; parquet to save storage btw, but maybe completely unnecessary? My goal here is a flow that is as minimalistic and cheap as possible where the only point of the data lake is to have all the data as \"raw\" as possible so I always can reload the database from that. I am completely open to use other tools.\n\n&amp;#x200B;\n\n Thank you in advance!", "author_fullname": "t2_7d6btbfhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ADF copy json to parquet and help on how to implement a python script on files in the lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121esja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679730692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;New to ADF, I have an integration landing data in a container on a storage account, I use copy activity with wildcard path (and merge files in settings) to collect all the files every night, I copy them to another storage account where they are stored as parquet. After the copy activity I delete all the files in the first &amp;quot;landing&amp;quot; container.&lt;/p&gt;\n\n&lt;p&gt;My next step is to implement a python script I have, that implements scd and then I want to sink the result of that to the database. Two questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The files arrive as json and I copy to parquet, sometimes it fails since copy activity does not understand what data type a specific column should be. How do I overcome this? Currently I use the advanced editor to read up some mapping from an sql-table where I explicitly pointing out what data type each column should be. However, this is not ideal since this will disregard any attribute on the json-object not defined in the mapping and I want this to be dynamic. I do not want to lose any data in this step.&lt;/li&gt;\n&lt;li&gt;Where do you guys implement your python scripts on data in adf? What machine will execute it? My only demand is that the script can access the lake and the database. I know about databricks and the notebook activity in adf but spinning up a spark cluster feels a bit dramatic. The biggest parquet files is about 300-400 mb. I do use pyspark in my script though, but since it is pretty small data I think pyspark running on one simple machine would be enough? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I only do the json -&amp;gt; parquet to save storage btw, but maybe completely unnecessary? My goal here is a flow that is as minimalistic and cheap as possible where the only point of the data lake is to have all the data as &amp;quot;raw&amp;quot; as possible so I always can reload the database from that. I am completely open to use other tools.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121esja", "is_robot_indexable": true, "report_reasons": null, "author": "pLebesgue", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121esja/adf_copy_json_to_parquet_and_help_on_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121esja/adf_copy_json_to_parquet_and_help_on_how_to/", "subreddit_subscribers": 94355, "created_utc": 1679730692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Interested in observability, ClickHouse, or both? Our upcoming webinar shows a top-to-bottom use case of using ClickHouse on Kubernetes at OpsVerse to offer better observability capabilities to end users. We'll cover both key ClickHouse capabilities as well as cloud native operation using the Altinity Operator for ClickHouse. The solution is deployed and working well. Join us to [find out more.](https://altinity.com/events/supercharging-observability-at-opsverse-using-clickhouse-realtime-analytics)", "author_fullname": "t2_s3zu6zpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Supercharging Observability at OpsVerse using ClickHouse Real-Time Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1212n6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679700131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interested in observability, ClickHouse, or both? Our upcoming webinar shows a top-to-bottom use case of using ClickHouse on Kubernetes at OpsVerse to offer better observability capabilities to end users. We&amp;#39;ll cover both key ClickHouse capabilities as well as cloud native operation using the Altinity Operator for ClickHouse. The solution is deployed and working well. Join us to &lt;a href=\"https://altinity.com/events/supercharging-observability-at-opsverse-using-clickhouse-realtime-analytics\"&gt;find out more.&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?auto=webp&amp;v=enabled&amp;s=34af15d35d9f21298c8ce61e9c609e2dcb3d7ed4", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc6a268670aab771ec14273e8014c7ba818d8378", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=813c76e4bd454d3e57918d666417360298e951ba", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68f047442b400f75f89acc8296532165c65f54cd", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e941d1bdd97bedb1f5cbbad6d902d4ca997bba3", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6354c15bf6a79652ae8d00ca7d89e19579df1690", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95a760900d9886bf1dc46f9640c015a448a91e00", "width": 1080, "height": 607}], "variants": {}, "id": "1DvOlwCOyWVgnZlqccNEhkvL8YcYma0EUWaPTbsVdUs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1212n6p", "is_robot_indexable": true, "report_reasons": null, "author": "RyhanSunny_Altinity", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1212n6p/supercharging_observability_at_opsverse_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1212n6p/supercharging_observability_at_opsverse_using/", "subreddit_subscribers": 94355, "created_utc": 1679700131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have a question regarding the most efficient method for data transformations. I have been tasked with performing ETL processes at my company and am the sole person responsible for the data.\n\nI have extracted additional information from a Dynamodb and RDS using AWS Glue. I then read the table with Athena, created a view, and loaded it into Power BI using the Athena Connector.\n\nMy question is, what is the best way to perform transformations on the data, such as adding columns with rank functions and others? Should I use Pyspark in the transform part while running the job in Glue, or should I use SQL to add the columns in the Athena view? Or should I try something different?\n\nThank you in advance!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transformations of ETL process on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12127c9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679699217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a question regarding the most efficient method for data transformations. I have been tasked with performing ETL processes at my company and am the sole person responsible for the data.&lt;/p&gt;\n\n&lt;p&gt;I have extracted additional information from a Dynamodb and RDS using AWS Glue. I then read the table with Athena, created a view, and loaded it into Power BI using the Athena Connector.&lt;/p&gt;\n\n&lt;p&gt;My question is, what is the best way to perform transformations on the data, such as adding columns with rank functions and others? Should I use Pyspark in the transform part while running the job in Glue, or should I use SQL to add the columns in the Athena view? Or should I try something different?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12127c9", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12127c9/transformations_of_etl_process_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12127c9/transformations_of_etl_process_on_aws/", "subreddit_subscribers": 94355, "created_utc": 1679699217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any non technical jobs out there to transition into (no coding, devops, tool management etc) in which DE skills might be a good fit for?", "author_fullname": "t2_foq4s5w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some transferable skills coming out of data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120v31a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679685777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any non technical jobs out there to transition into (no coding, devops, tool management etc) in which DE skills might be a good fit for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "120v31a", "is_robot_indexable": true, "report_reasons": null, "author": "sumApples", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/120v31a/what_are_some_transferable_skills_coming_out_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/120v31a/what_are_some_transferable_skills_coming_out_of/", "subreddit_subscribers": 94355, "created_utc": 1679685777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So really interested on thoughts here. There is a big cloud data warehouse provider. Well known, works well, sales oriented approach to building its user base.\n\nThere is a new solution on the market. Open source, kinda data lakehouse without the need for Spark. You can clone the git, or grab the docker image and just ise it. They also have a cloud offering (think dbt-cli vs. dbt cloud). Their cloud offering has more transparent pricing than the big provider, and their website has a direct comparison to that cloud provider.\n\nThe thing is, when you log in to their cloud offering, you\u2019d be forgiven for thinking it **was** the big provider. Layout, color scheme, even the way it returns values with a summary on the right. It\u2019s almost identical.\n\nSo, longevity/risk of collapse of the company aside, would you/your company consider using a \u2018clone\u2019 if it were cheaper?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ethics of using clones/knock-offs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1213bem", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679701571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So really interested on thoughts here. There is a big cloud data warehouse provider. Well known, works well, sales oriented approach to building its user base.&lt;/p&gt;\n\n&lt;p&gt;There is a new solution on the market. Open source, kinda data lakehouse without the need for Spark. You can clone the git, or grab the docker image and just ise it. They also have a cloud offering (think dbt-cli vs. dbt cloud). Their cloud offering has more transparent pricing than the big provider, and their website has a direct comparison to that cloud provider.&lt;/p&gt;\n\n&lt;p&gt;The thing is, when you log in to their cloud offering, you\u2019d be forgiven for thinking it &lt;strong&gt;was&lt;/strong&gt; the big provider. Layout, color scheme, even the way it returns values with a summary on the right. It\u2019s almost identical.&lt;/p&gt;\n\n&lt;p&gt;So, longevity/risk of collapse of the company aside, would you/your company consider using a \u2018clone\u2019 if it were cheaper?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1213bem", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1213bem/ethics_of_using_clonesknockoffs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1213bem/ethics_of_using_clonesknockoffs/", "subreddit_subscribers": 94355, "created_utc": 1679701571.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}