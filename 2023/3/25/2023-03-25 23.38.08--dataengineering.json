{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Company recently made the decision to move to snowflake from a mishmash of on-prem databases (mostly sql server). \n\nData science team uses R and spark running on a handful of on prem workstations to build models. \n\nThe IT department is pushing hard to move everything into snowflake and the data science people are complaining that snowpark is half baked. Especially on R programming where they'll still have to take data out of snowflake into some other compute source to run R models, then insert results back into snowflake.\n\nTrying to think of solutions. Should we:\n\n1) convince IT so provision a small databricks account we can use just for running models?\n2) suck it up and convert everything to python, trying to adopt snowpark/anaconda as much as possible?\n3) keep our pile of on prem machines with do modeling by pulling data out of snowflake, computing locally, inserting model results back into snowflake for use in reports, decision dashboards, etc.", "author_fullname": "t2_5iwre", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "machine learning in snowflake, unhappy data scientists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121mm5c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679752633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Company recently made the decision to move to snowflake from a mishmash of on-prem databases (mostly sql server). &lt;/p&gt;\n\n&lt;p&gt;Data science team uses R and spark running on a handful of on prem workstations to build models. &lt;/p&gt;\n\n&lt;p&gt;The IT department is pushing hard to move everything into snowflake and the data science people are complaining that snowpark is half baked. Especially on R programming where they&amp;#39;ll still have to take data out of snowflake into some other compute source to run R models, then insert results back into snowflake.&lt;/p&gt;\n\n&lt;p&gt;Trying to think of solutions. Should we:&lt;/p&gt;\n\n&lt;p&gt;1) convince IT so provision a small databricks account we can use just for running models?\n2) suck it up and convert everything to python, trying to adopt snowpark/anaconda as much as possible?\n3) keep our pile of on prem machines with do modeling by pulling data out of snowflake, computing locally, inserting model results back into snowflake for use in reports, decision dashboards, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121mm5c", "is_robot_indexable": true, "report_reasons": null, "author": "CantGoogleMe", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121mm5c/machine_learning_in_snowflake_unhappy_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121mm5c/machine_learning_in_snowflake_unhappy_data/", "subreddit_subscribers": 94381, "created_utc": 1679752633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As per title. How are you using DuckDB, either privately for passion/side projects, hobbies, or professionally in production environments?\n\nWould love to just hear the innovative ways in which people are using it.", "author_fullname": "t2_4bjsc4oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you using DuckDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121jccb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679744601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per title. How are you using DuckDB, either privately for passion/side projects, hobbies, or professionally in production environments?&lt;/p&gt;\n\n&lt;p&gt;Would love to just hear the innovative ways in which people are using it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121jccb", "is_robot_indexable": true, "report_reasons": null, "author": "jb7834", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121jccb/how_are_you_using_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121jccb/how_are_you_using_duckdb/", "subreddit_subscribers": 94381, "created_utc": 1679744601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, i am in the process of building a new data model. Our existing model is OBT, while it's good for analysts, it's a nightmare for dashboard use cases. I am thinking of building a traditional dimensional Model which I'll use in dashboards and build a OBT on top of it to give to analysts. But I had a few questions :\n\n1. It is said that dimensional models can be slow to process the data. Our current OBT model refreshes 8 times a day and the new model should be refreshed in the same frequency or most probably once every hour.\n\n2. We use databricks, while it could solve some of load time problems mentioned above, I am not really able to find any resources that tells me whether there's omr modelling approach that is more suitable to a spark platform like databricks compared to others.", "author_fullname": "t2_xap78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Platform considerations for modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121lqh0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679750588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, i am in the process of building a new data model. Our existing model is OBT, while it&amp;#39;s good for analysts, it&amp;#39;s a nightmare for dashboard use cases. I am thinking of building a traditional dimensional Model which I&amp;#39;ll use in dashboards and build a OBT on top of it to give to analysts. But I had a few questions :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;It is said that dimensional models can be slow to process the data. Our current OBT model refreshes 8 times a day and the new model should be refreshed in the same frequency or most probably once every hour.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We use databricks, while it could solve some of load time problems mentioned above, I am not really able to find any resources that tells me whether there&amp;#39;s omr modelling approach that is more suitable to a spark platform like databricks compared to others.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121lqh0", "is_robot_indexable": true, "report_reasons": null, "author": "totalsports1", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/121lqh0/platform_considerations_for_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121lqh0/platform_considerations_for_modelling/", "subreddit_subscribers": 94381, "created_utc": 1679750588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am dealing with some data containing user-ids that need to be counted in power-bi. However, due to various potential users in power bi, we have the requirement to not store plain-text user-ids in any table connected to power bi.\nI am wondering what would be the best practice in such a case? I suggested a concept in which user-id get hashed with a salt, and a mapping table between each user-id and its hash is stored in a table which is not available in power-bi.  \nWhat are your thoughts about this approach? Did you had similar requirements? Are there alternatives to solve that problem?", "author_fullname": "t2_8etbdwvd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to encrypt sensitive data from data lake to power bi?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121i1mv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679740981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am dealing with some data containing user-ids that need to be counted in power-bi. However, due to various potential users in power bi, we have the requirement to not store plain-text user-ids in any table connected to power bi.\nI am wondering what would be the best practice in such a case? I suggested a concept in which user-id get hashed with a salt, and a mapping table between each user-id and its hash is stored in a table which is not available in power-bi.&lt;br/&gt;\nWhat are your thoughts about this approach? Did you had similar requirements? Are there alternatives to solve that problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121i1mv", "is_robot_indexable": true, "report_reasons": null, "author": "Remote-Juice2527", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121i1mv/how_to_encrypt_sensitive_data_from_data_lake_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121i1mv/how_to_encrypt_sensitive_data_from_data_lake_to/", "subreddit_subscribers": 94381, "created_utc": 1679740981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Interested to hear how (and what) teams are doing for data pipeline documentation. Where is the proper balance between maintainability and enough detail to provide value. What items are you covering the document (source, maybe associated jira details, high level business logic overview, etc...)?  Where are you keeping it github-pages, confluence, G drive (oh the horror)...?\n\nI don't feel code documentation should be included as the code itself should be well commented and clean and with the changes frequent in code bases it'd never be kept up anyway.", "author_fullname": "t2_puuzgu6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline documentation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121s86p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679764741.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interested to hear how (and what) teams are doing for data pipeline documentation. Where is the proper balance between maintainability and enough detail to provide value. What items are you covering the document (source, maybe associated jira details, high level business logic overview, etc...)?  Where are you keeping it github-pages, confluence, G drive (oh the horror)...?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t feel code documentation should be included as the code itself should be well commented and clean and with the changes frequent in code bases it&amp;#39;d never be kept up anyway.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121s86p", "is_robot_indexable": true, "report_reasons": null, "author": "getafterit123", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121s86p/pipeline_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121s86p/pipeline_documentation/", "subreddit_subscribers": 94381, "created_utc": 1679764169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\nI\u2019m exploring how to integrate Apache Airflow into our communicable disease unit workflow. We are mostly R users but are starting to use Python more. The basic workflow I have in mind is:\n\nTask 1 - trigger script 1 - pull immunization records from Snowflake &gt;&gt; Task 2 - trigger script 2 - clean and deduplicate, then add new records to historical dataset &gt;&gt; Update R Shiny\n\nI have Airflow up and running via docker, but I\u2019m stuck on how to trigger scripts via bash command. Can I have the script anywhere on our network drive, or does it have to be somewhere within the airflow folder? In the past I\u2019ve used windows task scheduler and .bat files for run scripts overnight, but trying to better automate some of our repetitive tasks.", "author_fullname": "t2_7e2bmavx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Airflow Triggering Python Scripts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121rg90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679762604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI\u2019m exploring how to integrate Apache Airflow into our communicable disease unit workflow. We are mostly R users but are starting to use Python more. The basic workflow I have in mind is:&lt;/p&gt;\n\n&lt;p&gt;Task 1 - trigger script 1 - pull immunization records from Snowflake &amp;gt;&amp;gt; Task 2 - trigger script 2 - clean and deduplicate, then add new records to historical dataset &amp;gt;&amp;gt; Update R Shiny&lt;/p&gt;\n\n&lt;p&gt;I have Airflow up and running via docker, but I\u2019m stuck on how to trigger scripts via bash command. Can I have the script anywhere on our network drive, or does it have to be somewhere within the airflow folder? In the past I\u2019ve used windows task scheduler and .bat files for run scripts overnight, but trying to better automate some of our repetitive tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121rg90", "is_robot_indexable": true, "report_reasons": null, "author": "Senior-Athlete-0", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121rg90/apache_airflow_triggering_python_scripts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121rg90/apache_airflow_triggering_python_scripts/", "subreddit_subscribers": 94381, "created_utc": 1679762604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, Redditors!\n\nI applied for a junior DE position at one company. I saw they use Airflow as an orchestration tool and I started learning it recently but can't seem to understand something, so if you guys are willing to help out I would be really grateful.\n\n1. So in ETL, let's say I have a script that extracts data from some source (doesn't matter which one) and that data is now loaded. Where is it loaded? I mean the location? Is it in some repository on the company's cloud or does Airflow has its own place where it temporarily stores it?\n2. Now let's say I have 3 different python scripts (pandas) that perform some transformation each. So transform\\_script1 takes that data, performs transformation and how exactly does it pass it to the transform\\_script2? Is the same for transform\\_script3?\n3. From what I understand, in my dag script, I should specify the downstream or upstream scripts. So it would look something like this:  \nextract\\_script &gt;&gt; transform\\_script1 &gt;&gt; transform\\_script2 &gt;&gt; transform\\_script3 &gt;&gt; load\\_script  \nBut what I fail to understand is how is the data passed between them.\n4. If someone has extra time I would really like to see what transform\\_script(s) look like. I might be asking a lot of your time, but if you have some time I would really appreciate it.\n\nI realize I am a total beginner, but really want this interview to go well. Thank you, guys!", "author_fullname": "t2_2jtk54zi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121ywna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679777841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, Redditors!&lt;/p&gt;\n\n&lt;p&gt;I applied for a junior DE position at one company. I saw they use Airflow as an orchestration tool and I started learning it recently but can&amp;#39;t seem to understand something, so if you guys are willing to help out I would be really grateful.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;So in ETL, let&amp;#39;s say I have a script that extracts data from some source (doesn&amp;#39;t matter which one) and that data is now loaded. Where is it loaded? I mean the location? Is it in some repository on the company&amp;#39;s cloud or does Airflow has its own place where it temporarily stores it?&lt;/li&gt;\n&lt;li&gt;Now let&amp;#39;s say I have 3 different python scripts (pandas) that perform some transformation each. So transform_script1 takes that data, performs transformation and how exactly does it pass it to the transform_script2? Is the same for transform_script3?&lt;/li&gt;\n&lt;li&gt;From what I understand, in my dag script, I should specify the downstream or upstream scripts. So it would look something like this:&lt;br/&gt;\nextract_script &amp;gt;&amp;gt; transform_script1 &amp;gt;&amp;gt; transform_script2 &amp;gt;&amp;gt; transform_script3 &amp;gt;&amp;gt; load_script&lt;br/&gt;\nBut what I fail to understand is how is the data passed between them.&lt;/li&gt;\n&lt;li&gt;If someone has extra time I would really like to see what transform_script(s) look like. I might be asking a lot of your time, but if you have some time I would really appreciate it.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I realize I am a total beginner, but really want this interview to go well. Thank you, guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121ywna", "is_robot_indexable": true, "report_reasons": null, "author": "d_underdog", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/121ywna/airflow_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121ywna/airflow_question/", "subreddit_subscribers": 94381, "created_utc": 1679777841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m 16 and in the future I want to start an engineering company. I have been doing a lot of research and most sources say I should pursue an EngD degree. Do you think that an EngD degree is important to be an engineering entrepreneur or is a bachelors degree enough? Also around what age do most people finish their EngD degrees? Sorry if I don\u2019t sound very technical I don\u2019t know very much about engineering and engineering degrees at the moment", "author_fullname": "t2_9yhanmge", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I pursue a EngD degree?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12212uw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679782361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m 16 and in the future I want to start an engineering company. I have been doing a lot of research and most sources say I should pursue an EngD degree. Do you think that an EngD degree is important to be an engineering entrepreneur or is a bachelors degree enough? Also around what age do most people finish their EngD degrees? Sorry if I don\u2019t sound very technical I don\u2019t know very much about engineering and engineering degrees at the moment&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12212uw", "is_robot_indexable": true, "report_reasons": null, "author": "Training-Guess-9063", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12212uw/should_i_pursue_a_engd_degree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12212uw/should_i_pursue_a_engd_degree/", "subreddit_subscribers": 94381, "created_utc": 1679782361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_77yicu5nv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Git for Data - What, How, and Why Now\" Hey guys, have you ever heard of using Git for your data? It's pretty cool stuff. In this post, we're going to talk about what it is, how to use it, and why it's so important nowadays. So let's dive in!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_121ykcg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TEiwfMRhXCnZHBzHitVExtayvi2tnehRBkeIcbpwTXY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679777108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "lakefs.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://lakefs.io/blog/git-for-data/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QD9QeCeKNaJQVF4hWLZasWL_dGQfCZed7nUNRC7jWDU.jpg?auto=webp&amp;v=enabled&amp;s=fb8e87464fb11217761889b0a016cf08446b6882", "width": 2048, "height": 1536}, "resolutions": [{"url": "https://external-preview.redd.it/QD9QeCeKNaJQVF4hWLZasWL_dGQfCZed7nUNRC7jWDU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=730e5a32fb032126290c0ed4c6241b52e51e77cc", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QD9QeCeKNaJQVF4hWLZasWL_dGQfCZed7nUNRC7jWDU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f214ff483dbe8b9bfc60d27e1ca4b4137503255", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QD9QeCeKNaJQVF4hWLZasWL_dGQfCZed7nUNRC7jWDU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc9e7fd70b5cc3c0b5d0c680ca1601afe9a548a9", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/QD9QeCeKNaJQVF4hWLZasWL_dGQfCZed7nUNRC7jWDU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=71da66e94f7db135341cee4e116a541d11926dee", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/QD9QeCeKNaJQVF4hWLZasWL_dGQfCZed7nUNRC7jWDU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a82060c0cafb84b5bfaee8b75c81eb0d1ba65768", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/QD9QeCeKNaJQVF4hWLZasWL_dGQfCZed7nUNRC7jWDU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8777258112c56c642bf7cbdf2aeefe6f9b60ebd2", "width": 1080, "height": 810}], "variants": {}, "id": "xzBUah3HqGrenXqfj9dCOt3Xkr7R6FW2BBT46gHvu8w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "121ykcg", "is_robot_indexable": true, "report_reasons": null, "author": "Lydia_alice", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121ykcg/git_for_data_what_how_and_why_now_hey_guys_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://lakefs.io/blog/git-for-data/", "subreddit_subscribers": 94381, "created_utc": 1679777108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using dbt, and I have a use case at the company I work at which I don't think is supported exactly. We use a green/blue deploy strategy for each table. When all the dbt tests for that specific table have completed successfully, I'd like to run a sql snippet to swap/promote it. Kind of like a model post-hook, but not after the model is run, instead for collection after all per-model tests pass. Does anyone have any ideas for a workaround, hack, or a package that might work?\n\nCurrently I'm writing test results to the database and having a second step to parse them, but a single pass option seems like it should be possible.", "author_fullname": "t2_dzfsy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt post-hook for successful tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121shh6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764674.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using dbt, and I have a use case at the company I work at which I don&amp;#39;t think is supported exactly. We use a green/blue deploy strategy for each table. When all the dbt tests for that specific table have completed successfully, I&amp;#39;d like to run a sql snippet to swap/promote it. Kind of like a model post-hook, but not after the model is run, instead for collection after all per-model tests pass. Does anyone have any ideas for a workaround, hack, or a package that might work?&lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m writing test results to the database and having a second step to parse them, but a single pass option seems like it should be possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121shh6", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_Again", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121shh6/dbt_posthook_for_successful_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121shh6/dbt_posthook_for_successful_tests/", "subreddit_subscribers": 94381, "created_utc": 1679764674.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone who has tried Polars in AWS Glue using the Shell script option, or whether it is even feasible at all? My work is dirt cheap and wants to minimize costs as much as possible lol. I'll be processing 500mb worth of data and not even sure if the 0.0625 DPU option would even cut it. For context, that is like 0.25 vCPU and 1GB RAM lol.", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars Library in AWS Glue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121ou6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679756928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone who has tried Polars in AWS Glue using the Shell script option, or whether it is even feasible at all? My work is dirt cheap and wants to minimize costs as much as possible lol. I&amp;#39;ll be processing 500mb worth of data and not even sure if the 0.0625 DPU option would even cut it. For context, that is like 0.25 vCPU and 1GB RAM lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121ou6t", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121ou6t/polars_library_in_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121ou6t/polars_library_in_aws_glue/", "subreddit_subscribers": 94381, "created_utc": 1679756928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nNew to ADF, I have an integration landing data in a container on a storage account, I use copy activity with wildcard path (and merge files in settings) to collect all the files every night, I copy them to another storage account where they are stored as parquet. After the copy activity I delete all the files in the first \"landing\" container.\n\nMy next step is to implement a python script I have, that implements scd and then I want to sink the result of that to the database. Two questions:\n\n1. The files arrive as json and I copy to parquet, sometimes it fails since copy activity does not understand what data type a specific column should be. How do I overcome this? Currently I use the advanced editor to read up some mapping from an sql-table where I explicitly pointing out what data type each column should be. However, this is not ideal since this will disregard any attribute on the json-object not defined in the mapping and I want this to be dynamic. I do not want to lose any data in this step.\n2. Where do you guys implement your python scripts on data in adf? What machine will execute it? My only demand is that the script can access the lake and the database. I know about databricks and the notebook activity in adf but spinning up a spark cluster feels a bit dramatic. The biggest parquet files is about 300-400 mb. I do use pyspark in my script though, but since it is pretty small data I think pyspark running on one simple machine would be enough? \n\nI only do the json -&gt; parquet to save storage btw, but maybe completely unnecessary? My goal here is a flow that is as minimalistic and cheap as possible where the only point of the data lake is to have all the data as \"raw\" as possible so I always can reload the database from that. I am completely open to use other tools.\n\n&amp;#x200B;\n\n Thank you in advance!", "author_fullname": "t2_7d6btbfhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ADF copy json to parquet and help on how to implement a python script on files in the lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121esja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679730692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;New to ADF, I have an integration landing data in a container on a storage account, I use copy activity with wildcard path (and merge files in settings) to collect all the files every night, I copy them to another storage account where they are stored as parquet. After the copy activity I delete all the files in the first &amp;quot;landing&amp;quot; container.&lt;/p&gt;\n\n&lt;p&gt;My next step is to implement a python script I have, that implements scd and then I want to sink the result of that to the database. Two questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The files arrive as json and I copy to parquet, sometimes it fails since copy activity does not understand what data type a specific column should be. How do I overcome this? Currently I use the advanced editor to read up some mapping from an sql-table where I explicitly pointing out what data type each column should be. However, this is not ideal since this will disregard any attribute on the json-object not defined in the mapping and I want this to be dynamic. I do not want to lose any data in this step.&lt;/li&gt;\n&lt;li&gt;Where do you guys implement your python scripts on data in adf? What machine will execute it? My only demand is that the script can access the lake and the database. I know about databricks and the notebook activity in adf but spinning up a spark cluster feels a bit dramatic. The biggest parquet files is about 300-400 mb. I do use pyspark in my script though, but since it is pretty small data I think pyspark running on one simple machine would be enough? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I only do the json -&amp;gt; parquet to save storage btw, but maybe completely unnecessary? My goal here is a flow that is as minimalistic and cheap as possible where the only point of the data lake is to have all the data as &amp;quot;raw&amp;quot; as possible so I always can reload the database from that. I am completely open to use other tools.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121esja", "is_robot_indexable": true, "report_reasons": null, "author": "pLebesgue", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121esja/adf_copy_json_to_parquet_and_help_on_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121esja/adf_copy_json_to_parquet_and_help_on_how_to/", "subreddit_subscribers": 94381, "created_utc": 1679730692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Interested in observability, ClickHouse, or both? Our upcoming webinar shows a top-to-bottom use case of using ClickHouse on Kubernetes at OpsVerse to offer better observability capabilities to end users. We'll cover both key ClickHouse capabilities as well as cloud native operation using the Altinity Operator for ClickHouse. The solution is deployed and working well. Join us to [find out more.](https://altinity.com/events/supercharging-observability-at-opsverse-using-clickhouse-realtime-analytics)", "author_fullname": "t2_s3zu6zpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Supercharging Observability at OpsVerse using ClickHouse Real-Time Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1212n6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679700131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interested in observability, ClickHouse, or both? Our upcoming webinar shows a top-to-bottom use case of using ClickHouse on Kubernetes at OpsVerse to offer better observability capabilities to end users. We&amp;#39;ll cover both key ClickHouse capabilities as well as cloud native operation using the Altinity Operator for ClickHouse. The solution is deployed and working well. Join us to &lt;a href=\"https://altinity.com/events/supercharging-observability-at-opsverse-using-clickhouse-realtime-analytics\"&gt;find out more.&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?auto=webp&amp;v=enabled&amp;s=34af15d35d9f21298c8ce61e9c609e2dcb3d7ed4", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc6a268670aab771ec14273e8014c7ba818d8378", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=813c76e4bd454d3e57918d666417360298e951ba", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68f047442b400f75f89acc8296532165c65f54cd", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e941d1bdd97bedb1f5cbbad6d902d4ca997bba3", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6354c15bf6a79652ae8d00ca7d89e19579df1690", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/DEYX6YSkXZ4ODNyc1yL_TkAtzMcL19R2RER6w0bejt0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95a760900d9886bf1dc46f9640c015a448a91e00", "width": 1080, "height": 607}], "variants": {}, "id": "1DvOlwCOyWVgnZlqccNEhkvL8YcYma0EUWaPTbsVdUs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1212n6p", "is_robot_indexable": true, "report_reasons": null, "author": "RyhanSunny_Altinity", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1212n6p/supercharging_observability_at_opsverse_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1212n6p/supercharging_observability_at_opsverse_using/", "subreddit_subscribers": 94381, "created_utc": 1679700131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have a question regarding the most efficient method for data transformations. I have been tasked with performing ETL processes at my company and am the sole person responsible for the data.\n\nI have extracted additional information from a Dynamodb and RDS using AWS Glue. I then read the table with Athena, created a view, and loaded it into Power BI using the Athena Connector.\n\nMy question is, what is the best way to perform transformations on the data, such as adding columns with rank functions and others? Should I use Pyspark in the transform part while running the job in Glue, or should I use SQL to add the columns in the Athena view? Or should I try something different?\n\nThank you in advance!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transformations of ETL process on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12127c9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679699217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a question regarding the most efficient method for data transformations. I have been tasked with performing ETL processes at my company and am the sole person responsible for the data.&lt;/p&gt;\n\n&lt;p&gt;I have extracted additional information from a Dynamodb and RDS using AWS Glue. I then read the table with Athena, created a view, and loaded it into Power BI using the Athena Connector.&lt;/p&gt;\n\n&lt;p&gt;My question is, what is the best way to perform transformations on the data, such as adding columns with rank functions and others? Should I use Pyspark in the transform part while running the job in Glue, or should I use SQL to add the columns in the Athena view? Or should I try something different?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12127c9", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12127c9/transformations_of_etl_process_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12127c9/transformations_of_etl_process_on_aws/", "subreddit_subscribers": 94381, "created_utc": 1679699217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So really interested on thoughts here. There is a big cloud data warehouse provider. Well known, works well, sales oriented approach to building its user base.\n\nThere is a new solution on the market. Open source, kinda data lakehouse without the need for Spark. You can clone the git, or grab the docker image and just ise it. They also have a cloud offering (think dbt-cli vs. dbt cloud). Their cloud offering has more transparent pricing than the big provider, and their website has a direct comparison to that cloud provider.\n\nThe thing is, when you log in to their cloud offering, you\u2019d be forgiven for thinking it **was** the big provider. Layout, color scheme, even the way it returns values with a summary on the right. It\u2019s almost identical.\n\nSo, longevity/risk of collapse of the company aside, would you/your company consider using a \u2018clone\u2019 if it were cheaper?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ethics of using clones/knock-offs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1213bem", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679701571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So really interested on thoughts here. There is a big cloud data warehouse provider. Well known, works well, sales oriented approach to building its user base.&lt;/p&gt;\n\n&lt;p&gt;There is a new solution on the market. Open source, kinda data lakehouse without the need for Spark. You can clone the git, or grab the docker image and just ise it. They also have a cloud offering (think dbt-cli vs. dbt cloud). Their cloud offering has more transparent pricing than the big provider, and their website has a direct comparison to that cloud provider.&lt;/p&gt;\n\n&lt;p&gt;The thing is, when you log in to their cloud offering, you\u2019d be forgiven for thinking it &lt;strong&gt;was&lt;/strong&gt; the big provider. Layout, color scheme, even the way it returns values with a summary on the right. It\u2019s almost identical.&lt;/p&gt;\n\n&lt;p&gt;So, longevity/risk of collapse of the company aside, would you/your company consider using a \u2018clone\u2019 if it were cheaper?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1213bem", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1213bem/ethics_of_using_clonesknockoffs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1213bem/ethics_of_using_clonesknockoffs/", "subreddit_subscribers": 94381, "created_utc": 1679701571.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}