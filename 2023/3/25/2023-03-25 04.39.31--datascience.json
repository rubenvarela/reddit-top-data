{"kind": "Listing", "data": {"after": "t3_120dwpr", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "PSA: data scientists are not data engineers trained to write sophisticated data ETL pipelines or design data architecture. Some of us do learn due to our companies cheaping out on hiring, but this should not be a requirement if you're posting for a data analyst / data scientist job.\n\nJust finished a coding round for a Data Science (Analytics) position where 50% of the questions were on data eng non sense. Want me to do the work of an analyst and an eng? Sure, then pay me double the salary too. I'm just sick of these companies lumping everything data related into one role.", "author_fullname": "t2_sk84vwlh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sick of companies hiding behind Data Scientists to do Engineering work!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120qzg1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 355, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 355, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679677937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;PSA: data scientists are not data engineers trained to write sophisticated data ETL pipelines or design data architecture. Some of us do learn due to our companies cheaping out on hiring, but this should not be a requirement if you&amp;#39;re posting for a data analyst / data scientist job.&lt;/p&gt;\n\n&lt;p&gt;Just finished a coding round for a Data Science (Analytics) position where 50% of the questions were on data eng non sense. Want me to do the work of an analyst and an eng? Sure, then pay me double the salary too. I&amp;#39;m just sick of these companies lumping everything data related into one role.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120qzg1", "is_robot_indexable": true, "report_reasons": null, "author": "Away-Assignment-2173", "discussion_type": null, "num_comments": 114, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120qzg1/sick_of_companies_hiding_behind_data_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120qzg1/sick_of_companies_hiding_behind_data_scientists/", "subreddit_subscribers": 862080, "created_utc": 1679677937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Out of 2,700 surveyed businesses, only 7% reported having the correct blend of culture and operations to draw out full growth potential from digital tools. The businesses, from the US, UK, France, Germany, Australia, New Zealand, China and India, indicate a global problem of organisations investing heavily in digital tools and data capabilities, but lacking in the literacy and culture required to utilise the tools.\n\nThe businesses interviewed showed that those aiming to make decisions based on high quality, transparent data that had built a data culture that incubated responsible risk taking were more likely to deliver profitable growth.\u00a0Data culture permeates through all six of DataIQ\u2019s key pillars and can be readily developed as a result of core [**training programmes**](https://www.dataiq.global/workshops) and [**education**](https://www.dataiq.global/knowledge).\n\nRead more: [https://www.dataiq.global/articles/nine-in-ten-companies-lack-data-culture-and-organisation-to-facilitate-growth](https://www.dataiq.global/articles/nine-in-ten-companies-lack-data-culture-and-organisation-to-facilitate-growth)", "author_fullname": "t2_fzqj063z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nine in ten companies lack data culture and organisation to facilitate growth", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120nnyr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679671610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Out of 2,700 surveyed businesses, only 7% reported having the correct blend of culture and operations to draw out full growth potential from digital tools. The businesses, from the US, UK, France, Germany, Australia, New Zealand, China and India, indicate a global problem of organisations investing heavily in digital tools and data capabilities, but lacking in the literacy and culture required to utilise the tools.&lt;/p&gt;\n\n&lt;p&gt;The businesses interviewed showed that those aiming to make decisions based on high quality, transparent data that had built a data culture that incubated responsible risk taking were more likely to deliver profitable growth.\u00a0Data culture permeates through all six of DataIQ\u2019s key pillars and can be readily developed as a result of core &lt;a href=\"https://www.dataiq.global/workshops\"&gt;&lt;strong&gt;training programmes&lt;/strong&gt;&lt;/a&gt; and &lt;a href=\"https://www.dataiq.global/knowledge\"&gt;&lt;strong&gt;education&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Read more: &lt;a href=\"https://www.dataiq.global/articles/nine-in-ten-companies-lack-data-culture-and-organisation-to-facilitate-growth\"&gt;https://www.dataiq.global/articles/nine-in-ten-companies-lack-data-culture-and-organisation-to-facilitate-growth&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120nnyr", "is_robot_indexable": true, "report_reasons": null, "author": "Heavy_Hearing_7997", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120nnyr/nine_in_ten_companies_lack_data_culture_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120nnyr/nine_in_ten_companies_lack_data_culture_and/", "subreddit_subscribers": 862080, "created_utc": 1679671610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_3hyi3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Critical remote takeover vulnerability in MLflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_120i0se", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mykIrGTCQHVx6ez-c1P6I5cYKZxdoe3hKujc1ExtVrY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679659062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "protectai.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://protectai.com/blog/hacking-ai-system-takeover-exploit-in-mlflow", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ULls-Aq6CIN5CKye1qQwGewSR2fgKfEE5npM6_8lUoI.jpg?auto=webp&amp;v=enabled&amp;s=d022636d2c6b15d85b7971445f5659dfe8410aed", "width": 2500, "height": 1313}, "resolutions": [{"url": "https://external-preview.redd.it/ULls-Aq6CIN5CKye1qQwGewSR2fgKfEE5npM6_8lUoI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8fe82873d69ee4637474ce63b3523a91b4870d3", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ULls-Aq6CIN5CKye1qQwGewSR2fgKfEE5npM6_8lUoI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a93f9c0825cd914d964d751f21c02432c859188", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ULls-Aq6CIN5CKye1qQwGewSR2fgKfEE5npM6_8lUoI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41fef85d1fabc0dcd958485f2a8ebc43c8258a57", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ULls-Aq6CIN5CKye1qQwGewSR2fgKfEE5npM6_8lUoI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=015b41b4213f879303e73038171e81e3a8a3206f", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/ULls-Aq6CIN5CKye1qQwGewSR2fgKfEE5npM6_8lUoI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8e3bc7f0ef02b2ba12eb3d1184f6347387e0c26", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/ULls-Aq6CIN5CKye1qQwGewSR2fgKfEE5npM6_8lUoI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7409dfd9139e85d16b85941f1dc64f670652ef91", "width": 1080, "height": 567}], "variants": {}, "id": "VcwgPuj_bDToB8abZYooBkNm4Pnb5jQJWUIzFjzDvoE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "120i0se", "is_robot_indexable": true, "report_reasons": null, "author": "FlyingTriangle", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120i0se/critical_remote_takeover_vulnerability_in_mlflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://protectai.com/blog/hacking-ai-system-takeover-exploit-in-mlflow", "subreddit_subscribers": 862080, "created_utc": 1679659062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Personally, I\u2019ve experienced data scientist and analyst interviews change. 5 years ago (and even 3 years ago) when I was job hunting for DS and DA positions, the interview process was INCREDIBLY long, with panel interviews ranging from 6-8 people back-to-back, live coding sessions, take-home projects that would take days, multiple technical rounds, and even some with Fermi problems. And these were for JUNIOR positions!\n\nI just finished another successful job hunt and landed a DS position. I actually had 3 job offers. But in all 3 of those companies, none had a crazy amount of panel interviews (2 max), none involved take-home projects, and none involved live coding. They all focused on talking about past experiences and technical questions or case problems sprinkled in.\n\nGranted, the 3 companies in question are non-tech (yet have mature data capabilities) while the interviews from 3-5 years ago were mostly from tech. But even the non-tech companies from that time period had grueling interview processes.\n\nJust wondering if anyone else has experienced the same, and if it\u2019s any indicator that DS interviews are finally \u201cleveling out\u201d and cutting out all the BS.", "author_fullname": "t2_h5j8s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have Data Scientist &amp; Analyst interviews changed over the past 5+ years?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120svtx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679681732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Personally, I\u2019ve experienced data scientist and analyst interviews change. 5 years ago (and even 3 years ago) when I was job hunting for DS and DA positions, the interview process was INCREDIBLY long, with panel interviews ranging from 6-8 people back-to-back, live coding sessions, take-home projects that would take days, multiple technical rounds, and even some with Fermi problems. And these were for JUNIOR positions!&lt;/p&gt;\n\n&lt;p&gt;I just finished another successful job hunt and landed a DS position. I actually had 3 job offers. But in all 3 of those companies, none had a crazy amount of panel interviews (2 max), none involved take-home projects, and none involved live coding. They all focused on talking about past experiences and technical questions or case problems sprinkled in.&lt;/p&gt;\n\n&lt;p&gt;Granted, the 3 companies in question are non-tech (yet have mature data capabilities) while the interviews from 3-5 years ago were mostly from tech. But even the non-tech companies from that time period had grueling interview processes.&lt;/p&gt;\n\n&lt;p&gt;Just wondering if anyone else has experienced the same, and if it\u2019s any indicator that DS interviews are finally \u201cleveling out\u201d and cutting out all the BS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120svtx", "is_robot_indexable": true, "report_reasons": null, "author": "redditisthenewblak", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120svtx/have_data_scientist_analyst_interviews_changed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120svtx/have_data_scientist_analyst_interviews_changed/", "subreddit_subscribers": 862080, "created_utc": 1679681732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone,\n\nDoes any redditor here works as a DS in civil engineer or construction?\n\nI have experience in that industry and I was wondering what uses DS can have or what companies merge these two areas.\n\nCongrats on the sub!", "author_fullname": "t2_4hwr77yk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science in Civil Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120xg7n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679689849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Does any redditor here works as a DS in civil engineer or construction?&lt;/p&gt;\n\n&lt;p&gt;I have experience in that industry and I was wondering what uses DS can have or what companies merge these two areas.&lt;/p&gt;\n\n&lt;p&gt;Congrats on the sub!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120xg7n", "is_robot_indexable": true, "report_reasons": null, "author": "_r_u_i_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120xg7n/data_science_in_civil_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120xg7n/data_science_in_civil_engineering/", "subreddit_subscribers": 862080, "created_utc": 1679689849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello guys.\n\nI am missing some obvious thing that is stopping me from plotting what is needed for my work.\n\nThere is a table that was grouped by some variables to reduce the dimensionality since the original dataset was too large to use in pandas.\nNow i have around 30k rows with a count, the result from the group by process.\n\nNow it's needed to plot a line, with the total amount of a specific category for comparison sake.\n\nWhen i pass that category as x and y as the count, the graph is made doing one bar for each value in count.\n\nIf i try to do a count plot, the product is a graph that count the total ROWS of each category.\n\nSns.catplot(x=\"cat1\",data=pd_df,kind=\"count\".\n\nHow i can plot what i need?", "author_fullname": "t2_axhjl6nf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to effectively plot grouped data with seaborn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120s8p0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679680441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys.&lt;/p&gt;\n\n&lt;p&gt;I am missing some obvious thing that is stopping me from plotting what is needed for my work.&lt;/p&gt;\n\n&lt;p&gt;There is a table that was grouped by some variables to reduce the dimensionality since the original dataset was too large to use in pandas.\nNow i have around 30k rows with a count, the result from the group by process.&lt;/p&gt;\n\n&lt;p&gt;Now it&amp;#39;s needed to plot a line, with the total amount of a specific category for comparison sake.&lt;/p&gt;\n\n&lt;p&gt;When i pass that category as x and y as the count, the graph is made doing one bar for each value in count.&lt;/p&gt;\n\n&lt;p&gt;If i try to do a count plot, the product is a graph that count the total ROWS of each category.&lt;/p&gt;\n\n&lt;p&gt;Sns.catplot(x=&amp;quot;cat1&amp;quot;,data=pd_df,kind=&amp;quot;count&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;How i can plot what i need?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120s8p0", "is_robot_indexable": true, "report_reasons": null, "author": "One_Kaleidoscope_271", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120s8p0/how_to_effectively_plot_grouped_data_with_seaborn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120s8p0/how_to_effectively_plot_grouped_data_with_seaborn/", "subreddit_subscribers": 862080, "created_utc": 1679680441.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys,\n\nCan you recommend good introductory courses on azure synapse ?\nI would like to get an introduction into the topic and maybe some hands on experience.\n\nThank you !", "author_fullname": "t2_80lmsg10", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Course on azure synapse.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120q5ol", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679676718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;Can you recommend good introductory courses on azure synapse ?\nI would like to get an introduction into the topic and maybe some hands on experience.&lt;/p&gt;\n\n&lt;p&gt;Thank you !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120q5ol", "is_robot_indexable": true, "report_reasons": null, "author": "peppe95ggez", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120q5ol/course_on_azure_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120q5ol/course_on_azure_synapse/", "subreddit_subscribers": 862080, "created_utc": 1679676718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\"I am regressing as a data scientist in my role\" says a recently promoted data scientist on the team.\n\nThere are 4 teams with data engineers and data scientists. The engineering leaders are great. But the leaders above them don't show experience with large-scale data projects.\n\n4 org shuffles in one year. A couple of data scientists have got a good amount of influence with leadership, and most of the conversations are around research and new models.\n\nThere is no tangible improvement in terms of the software that is being delivered to the customers. Every new customer within the first few months has several complaints regarding the data that they see, inexplicable spikes and randomness in the data, and when they compare the data with their own reports, they find gaps and lose trust in the product.\n\nThere is always talk about more sophisticated models, and we are lacking in basic issues of software development practices, code quality, data quality.\n\nFor folks in this group who have experienced similar challenges, what has worked for you to improve the quality of the projects?", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for recommendations regarding challenges in data practice...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120p32k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679674527.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;I am regressing as a data scientist in my role&amp;quot; says a recently promoted data scientist on the team.&lt;/p&gt;\n\n&lt;p&gt;There are 4 teams with data engineers and data scientists. The engineering leaders are great. But the leaders above them don&amp;#39;t show experience with large-scale data projects.&lt;/p&gt;\n\n&lt;p&gt;4 org shuffles in one year. A couple of data scientists have got a good amount of influence with leadership, and most of the conversations are around research and new models.&lt;/p&gt;\n\n&lt;p&gt;There is no tangible improvement in terms of the software that is being delivered to the customers. Every new customer within the first few months has several complaints regarding the data that they see, inexplicable spikes and randomness in the data, and when they compare the data with their own reports, they find gaps and lose trust in the product.&lt;/p&gt;\n\n&lt;p&gt;There is always talk about more sophisticated models, and we are lacking in basic issues of software development practices, code quality, data quality.&lt;/p&gt;\n\n&lt;p&gt;For folks in this group who have experienced similar challenges, what has worked for you to improve the quality of the projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120p32k", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120p32k/looking_for_recommendations_regarding_challenges/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120p32k/looking_for_recommendations_regarding_challenges/", "subreddit_subscribers": 862080, "created_utc": 1679674527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Question from a friend: \n\nCurrently work at IBM as a data scientist. Salary is $115k\n\nGot an offer to work for a large government contractor for $135k fully remote. \n\nIBM told me they\u2019d try to bump salary up to $155k but in person three times a week.\n\nLooking for stable, longer term job. Whatever stable means if you\u2019re a data scientist in the Age of AI.", "author_fullname": "t2_4x52hdwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Federal Contractor or IBM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120902t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679632036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question from a friend: &lt;/p&gt;\n\n&lt;p&gt;Currently work at IBM as a data scientist. Salary is $115k&lt;/p&gt;\n\n&lt;p&gt;Got an offer to work for a large government contractor for $135k fully remote. &lt;/p&gt;\n\n&lt;p&gt;IBM told me they\u2019d try to bump salary up to $155k but in person three times a week.&lt;/p&gt;\n\n&lt;p&gt;Looking for stable, longer term job. Whatever stable means if you\u2019re a data scientist in the Age of AI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120902t", "is_robot_indexable": true, "report_reasons": null, "author": "Beautiful-Cancel6235", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120902t/federal_contractor_or_ibm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120902t/federal_contractor_or_ibm/", "subreddit_subscribers": 862080, "created_utc": 1679632036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I am currently dealing with a dataset with different customer as each row, and customer\u2019s bank different categories of information as the columns. My goal is to predict customer\u2019s monthly overdue payments in the three beginning months of 2020. I have the data from 2018-2019 as the training set, and an macro-dataset  from 2020 for generalizing my model before 2020 to the COVID period. I think I will be using the KaplanMeier Estimator at the end, but right now I am clueless on how to do the generalization with that macro dataset. I will appreciate any insights.", "author_fullname": "t2_b50cm4sj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking insights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1217ffi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679710777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am currently dealing with a dataset with different customer as each row, and customer\u2019s bank different categories of information as the columns. My goal is to predict customer\u2019s monthly overdue payments in the three beginning months of 2020. I have the data from 2018-2019 as the training set, and an macro-dataset  from 2020 for generalizing my model before 2020 to the COVID period. I think I will be using the KaplanMeier Estimator at the end, but right now I am clueless on how to do the generalization with that macro dataset. I will appreciate any insights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1217ffi", "is_robot_indexable": true, "report_reasons": null, "author": "lunaticAKE", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1217ffi/seeking_insights/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1217ffi/seeking_insights/", "subreddit_subscribers": 862080, "created_utc": 1679710777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work as a Data Scientist Consultant for an IT consulting firm for nearly 2 years now. Most of the projects with the client (signed contract) usually last about 3-6 months like my CV (around 4 clients now) but some people actually stay longer with the client continuing to phrase 2 or even 3. When I ask my friend from another Consulting firm as a Strategist, he said it is normal to actually change that fast for his position but not so sure about mine. Is it a red flag to jump clients that much or would it be better to stay at least 9-12 months with one client exclusively?", "author_fullname": "t2_3ct6vq4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the appropriate duration for a new account/project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1212myx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679700118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a Data Scientist Consultant for an IT consulting firm for nearly 2 years now. Most of the projects with the client (signed contract) usually last about 3-6 months like my CV (around 4 clients now) but some people actually stay longer with the client continuing to phrase 2 or even 3. When I ask my friend from another Consulting firm as a Strategist, he said it is normal to actually change that fast for his position but not so sure about mine. Is it a red flag to jump clients that much or would it be better to stay at least 9-12 months with one client exclusively?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1212myx", "is_robot_indexable": true, "report_reasons": null, "author": "taylortiki", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1212myx/what_is_the_appropriate_duration_for_a_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1212myx/what_is_the_appropriate_duration_for_a_new/", "subreddit_subscribers": 862080, "created_utc": 1679700118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a line graph with x/y axis. X refers to time and Y refers to a count.\n\nNow the correct graph looks very similar to a gamma distribution, but not quite. This is what is considered the \"standard/expected\" shape.\n\nWhen I graph new data, I need to detect if the shape deviates significantly from the standard. The shape is the most important and not necessarily the values.\n\nFor example, the standard shape could go up to 2000 on the y-axis on one graph, and only 200 on the other, but it's the shape that matters.\n\nSo if you took the standard shape, copied it, and moved it +100 in the positive x-direction, and then scaled the y-axis up 1000, it would still follow the standaed shape, so it would be labeled as \"standard\".\n\nI tried a Whitney-Mann U test and wasn't successful. I'm a little at a loss. Any suggestions?", "author_fullname": "t2_bzw9ivz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a statistical test to compare two sets of data based on the shape of the graph and not necessarily the difference in the data itself?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1212g4u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679699727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a line graph with x/y axis. X refers to time and Y refers to a count.&lt;/p&gt;\n\n&lt;p&gt;Now the correct graph looks very similar to a gamma distribution, but not quite. This is what is considered the &amp;quot;standard/expected&amp;quot; shape.&lt;/p&gt;\n\n&lt;p&gt;When I graph new data, I need to detect if the shape deviates significantly from the standard. The shape is the most important and not necessarily the values.&lt;/p&gt;\n\n&lt;p&gt;For example, the standard shape could go up to 2000 on the y-axis on one graph, and only 200 on the other, but it&amp;#39;s the shape that matters.&lt;/p&gt;\n\n&lt;p&gt;So if you took the standard shape, copied it, and moved it +100 in the positive x-direction, and then scaled the y-axis up 1000, it would still follow the standaed shape, so it would be labeled as &amp;quot;standard&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I tried a Whitney-Mann U test and wasn&amp;#39;t successful. I&amp;#39;m a little at a loss. Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1212g4u", "is_robot_indexable": true, "report_reasons": null, "author": "yukobeam", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1212g4u/is_there_a_statistical_test_to_compare_two_sets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1212g4u/is_there_a_statistical_test_to_compare_two_sets/", "subreddit_subscribers": 862080, "created_utc": 1679699727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everyone, I am working on a demand forecasting model. It predicts the demand in the upcoming month. How can I produce quantiles as forecast instead of a single estimate. I need qunatiles to account for the capital availability. If the available capital is low for example take the 50th percentile as the forecast. If it's high on the other hand take the 75th.", "author_fullname": "t2_8jmib0lw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quantiles as predictions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120zqr5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679694159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am working on a demand forecasting model. It predicts the demand in the upcoming month. How can I produce quantiles as forecast instead of a single estimate. I need qunatiles to account for the capital availability. If the available capital is low for example take the 50th percentile as the forecast. If it&amp;#39;s high on the other hand take the 75th.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120zqr5", "is_robot_indexable": true, "report_reasons": null, "author": "Riolite55", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120zqr5/quantiles_as_predictions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120zqr5/quantiles_as_predictions/", "subreddit_subscribers": 862080, "created_utc": 1679694159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm looking for an entry level job as a Data Analyst and was wondering: what is the best way to showcase my abilities on LinkedIn? This sound like a silly question, but as a newbie, I wonder what do the recruiters want to see? Do they want to see Power BI dashboards? Do they want to see my thinking process? Should I post my SQL/python code lines? \n\nThere are so many possibilities that it's hard for me to find focus.", "author_fullname": "t2_ax0lvvdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I draw recruiters attention?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120yovb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679691998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for an entry level job as a Data Analyst and was wondering: what is the best way to showcase my abilities on LinkedIn? This sound like a silly question, but as a newbie, I wonder what do the recruiters want to see? Do they want to see Power BI dashboards? Do they want to see my thinking process? Should I post my SQL/python code lines? &lt;/p&gt;\n\n&lt;p&gt;There are so many possibilities that it&amp;#39;s hard for me to find focus.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120yovb", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional-Reach818", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120yovb/how_can_i_draw_recruiters_attention/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120yovb/how_can_i_draw_recruiters_attention/", "subreddit_subscribers": 862080, "created_utc": 1679691998.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have 2.5 YOE as a DS in academia. I am now working as a DE in consulting. I took the DE role to get some experience in that area. I am completing my MS in Data Science. What is the optimal path to becoming a CDO at a medium to large company? Continue in consulting? Get an MBA after my MS?\n\nI know the path is not set in stone. Just wondering if anyone knows the general way people become CDOs.", "author_fullname": "t2_dbas4m3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to become a Chief Data Officer. What is the typical path to CDO?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120wj8t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679688156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 2.5 YOE as a DS in academia. I am now working as a DE in consulting. I took the DE role to get some experience in that area. I am completing my MS in Data Science. What is the optimal path to becoming a CDO at a medium to large company? Continue in consulting? Get an MBA after my MS?&lt;/p&gt;\n\n&lt;p&gt;I know the path is not set in stone. Just wondering if anyone knows the general way people become CDOs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120wj8t", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Hyena4223", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120wj8t/i_want_to_become_a_chief_data_officer_what_is_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120wj8t/i_want_to_become_a_chief_data_officer_what_is_the/", "subreddit_subscribers": 862080, "created_utc": 1679688156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to export my tableau dashboards to a pdf and email them to my clients on a weekly basis. What options do I have for doing this that don\u2019t involve my clients having to download anything?", "author_fullname": "t2_da67df26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automating Tableau Reports", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120vvi6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679687216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to export my tableau dashboards to a pdf and email them to my clients on a weekly basis. What options do I have for doing this that don\u2019t involve my clients having to download anything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120vvi6", "is_robot_indexable": true, "report_reasons": null, "author": "Subject-Resort5893", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120vvi6/automating_tableau_reports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120vvi6/automating_tableau_reports/", "subreddit_subscribers": 862080, "created_utc": 1679687216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey fellow data scientists! \nSorry if this is against the rules , but I didn\u2019t see anything explicitly banning this, so here goes. I\u2019m working in the US and my visa runs out at the end of the year. That\u2019s fine, because I\u2019m planning to move to the UK with my partner. I\u2019m originally from India, so I\u2019ll have to be sponsored for a visa. I have a PhD in explainable machine learning and have been a lead Data Scientist at a small startup for the last year or so. I have a fair bit of experience in document processing and structured data handling. If you or anyone you know is hiring in the UK and is willing to sponsor a work visa, I\u2019d love to hear about it. I have also worked with huggingface\u2019s BigScience initiative including BigCode, BigBio and BLOOM. Looking forward to any pointers you might have. \nThanks!", "author_fullname": "t2_nmj51", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job opportunities in UK", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120tzl3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679683929.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow data scientists! \nSorry if this is against the rules , but I didn\u2019t see anything explicitly banning this, so here goes. I\u2019m working in the US and my visa runs out at the end of the year. That\u2019s fine, because I\u2019m planning to move to the UK with my partner. I\u2019m originally from India, so I\u2019ll have to be sponsored for a visa. I have a PhD in explainable machine learning and have been a lead Data Scientist at a small startup for the last year or so. I have a fair bit of experience in document processing and structured data handling. If you or anyone you know is hiring in the UK and is willing to sponsor a work visa, I\u2019d love to hear about it. I have also worked with huggingface\u2019s BigScience initiative including BigCode, BigBio and BLOOM. Looking forward to any pointers you might have. \nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120tzl3", "is_robot_indexable": true, "report_reasons": null, "author": "thatphotoguy89", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120tzl3/job_opportunities_in_uk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120tzl3/job_opportunities_in_uk/", "subreddit_subscribers": 862080, "created_utc": 1679683929.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys,\nI am trying to get a certification in data science from data camp. I failed the final project and the reason is that the data validation was insufficient. I did all the required steps to clean out the data : outliers, unique values, no null values, log method etc. what could be the extra steps for a better data validation ?\nThank you in advance", "author_fullname": "t2_t1ls7aqf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Steps for data validation (please help)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120t4s8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679682220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,\nI am trying to get a certification in data science from data camp. I failed the final project and the reason is that the data validation was insufficient. I did all the required steps to clean out the data : outliers, unique values, no null values, log method etc. what could be the extra steps for a better data validation ?\nThank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120t4s8", "is_robot_indexable": true, "report_reasons": null, "author": "bach678", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120t4s8/steps_for_data_validation_please_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120t4s8/steps_for_data_validation_please_help/", "subreddit_subscribers": 862080, "created_utc": 1679682220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I just started a new role as a Demand Planner.  I have been asked to update and maintain a file that calculates forecast \"accuracy\", which was created by a person who retired.  I have my doubts that the file has been set up properly.  The company's goal is for forecasts to reach 80% \"accuracy\" (where 100% would be perfect forecasts).  But shouldn't it be the other way around?  Shouldn't the KPI be 20% Forecast Error or lower?  Because in cases of extreme under-forecasting or over-forecasting, the error will exceed 100%, so representing the accuracy by subtracting the error from 100% is meaningless.  \n\n&amp;#x200B;\n\nTo get around this issue, the file calculates the percent error as (Actual - Forecast)/Actual if the Absolute Error is less than the Actual, and it calculates the percent error as (Actual - Forecast)/Forecast if the Absolute Error is greater than the Actual.  Then it subtracts the Abs % Error from 100% to obtain the Forecast Accuracy.\n\n&amp;#x200B;\n\nHere are some examples of how the current file calculates forecast accuracy:\n\n&amp;#x200B;\n\n|Forecast|Actual|Abs Error|Abs % Error (Actual as Denominator)|Abs % Error (Forecast as Denominator)|Abs % Error (shown on the file)|Accuracy|\n|:-|:-|:-|:-|:-|:-|:-|\n|100|0|100|error|100%|100%|0%|\n|100|10|90|900%|90%|90%|10%|\n|100|75|25|33%|25%|33%|67%|\n|100|100|0|0%|0%|0%|100%|\n|100|250|150|60%|150%|60%|40%|\n|100|1000|900|90%|900%|90%|10%|\n|0|100|100|100%|error|100%|0%|\n\n&amp;#x200B;\n\nIs this the correct way to calculate Forecast Accuracy?", "author_fullname": "t2_tmaq0p4n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forecast Accuracy = 1 - Forecast Error", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120sznw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679681946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started a new role as a Demand Planner.  I have been asked to update and maintain a file that calculates forecast &amp;quot;accuracy&amp;quot;, which was created by a person who retired.  I have my doubts that the file has been set up properly.  The company&amp;#39;s goal is for forecasts to reach 80% &amp;quot;accuracy&amp;quot; (where 100% would be perfect forecasts).  But shouldn&amp;#39;t it be the other way around?  Shouldn&amp;#39;t the KPI be 20% Forecast Error or lower?  Because in cases of extreme under-forecasting or over-forecasting, the error will exceed 100%, so representing the accuracy by subtracting the error from 100% is meaningless.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;To get around this issue, the file calculates the percent error as (Actual - Forecast)/Actual if the Absolute Error is less than the Actual, and it calculates the percent error as (Actual - Forecast)/Forecast if the Absolute Error is greater than the Actual.  Then it subtracts the Abs % Error from 100% to obtain the Forecast Accuracy.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here are some examples of how the current file calculates forecast accuracy:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Forecast&lt;/th&gt;\n&lt;th align=\"left\"&gt;Actual&lt;/th&gt;\n&lt;th align=\"left\"&gt;Abs Error&lt;/th&gt;\n&lt;th align=\"left\"&gt;Abs % Error (Actual as Denominator)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Abs % Error (Forecast as Denominator)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Abs % Error (shown on the file)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Accuracy&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;error&lt;/td&gt;\n&lt;td align=\"left\"&gt;100%&lt;/td&gt;\n&lt;td align=\"left\"&gt;100%&lt;/td&gt;\n&lt;td align=\"left\"&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;90&lt;/td&gt;\n&lt;td align=\"left\"&gt;900%&lt;/td&gt;\n&lt;td align=\"left\"&gt;90%&lt;/td&gt;\n&lt;td align=\"left\"&gt;90%&lt;/td&gt;\n&lt;td align=\"left\"&gt;10%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;75&lt;/td&gt;\n&lt;td align=\"left\"&gt;25&lt;/td&gt;\n&lt;td align=\"left\"&gt;33%&lt;/td&gt;\n&lt;td align=\"left\"&gt;25%&lt;/td&gt;\n&lt;td align=\"left\"&gt;33%&lt;/td&gt;\n&lt;td align=\"left\"&gt;67%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;0%&lt;/td&gt;\n&lt;td align=\"left\"&gt;0%&lt;/td&gt;\n&lt;td align=\"left\"&gt;0%&lt;/td&gt;\n&lt;td align=\"left\"&gt;100%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;250&lt;/td&gt;\n&lt;td align=\"left\"&gt;150&lt;/td&gt;\n&lt;td align=\"left\"&gt;60%&lt;/td&gt;\n&lt;td align=\"left\"&gt;150%&lt;/td&gt;\n&lt;td align=\"left\"&gt;60%&lt;/td&gt;\n&lt;td align=\"left\"&gt;40%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;td align=\"left\"&gt;900&lt;/td&gt;\n&lt;td align=\"left\"&gt;90%&lt;/td&gt;\n&lt;td align=\"left\"&gt;900%&lt;/td&gt;\n&lt;td align=\"left\"&gt;90%&lt;/td&gt;\n&lt;td align=\"left\"&gt;10%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;100&lt;/td&gt;\n&lt;td align=\"left\"&gt;100%&lt;/td&gt;\n&lt;td align=\"left\"&gt;error&lt;/td&gt;\n&lt;td align=\"left\"&gt;100%&lt;/td&gt;\n&lt;td align=\"left\"&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this the correct way to calculate Forecast Accuracy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120sznw", "is_robot_indexable": true, "report_reasons": null, "author": "iamindemand", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120sznw/forecast_accuracy_1_forecast_error/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120sznw/forecast_accuracy_1_forecast_error/", "subreddit_subscribers": 862080, "created_utc": 1679681946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm an experienced data scientist(4+ years). Recently joined a 100-150 people tech startup company that went bankrupt last two years. They're back in the game after receiving external fundinf. Lately I've been thinking if the expectations set upon my role are realistically achievable. My manager is more of a middle management guy with zero experience in data. When I was interviewing for them, it seemed like a match for my skills. They're looking to build predictive and prescriptive analytics using machine learning on a large scale.I was under the impression that they have  data engineers maintaining the data pipeline. I was also open to double hat data science and MLops as I've had experience doing both. I was surprised when I started working here that it's a one man data team job. Apparently, all of the data team left during layoffs last two years. The data science projects were also scattered across different locations(i.e cloud, local environment, etc.) and built using a custom library. Management wanted me to improve the current machine learning powered features of the app such that it can be reused by other companies. Unfortunately, it's tightly coupled with our backend services. The experiment platform for online testing was also built without understanding about the statistics behind it. It's a whole revamp of the data infrastructure if they wanted to achieve their goal to deploy machine learning solutions in large scale. I learned from the ex-data scientist working here that the current infra was built with tech debt since they wanted to ship things fast before(i.e series A funding). I've communicated what needs to be done first before they can achieve what they want. I asked for a data engineer since the ETL pipeline has been acting up lately and the staging environment crashes when I deploy new experiments. Nobody in the current team knows why it crashes so I'm left with very limited options on how to deliver value. I tried debugging it out on my own but it's beyond my understanding too. As of the moment, I'm finding out how they built their current model so I can at least tweak it and deliver value. Most of the notebooks are missing from the codebase and the remaining few are long notebooks with no context or comments on the purpose of the analysis done. There was also no documentation left. I guess because it was sudden layoffs and everyone didn't have time to document etc. I've been trying my best to do it all on my own hoping to make things happen for the past three months. I've pitched ideas for resources to achieve the goals but they always end up rejected by the engineers but approved by our manager. The hierarchy of decision making in the team is still confusing to me right now. I've also proposed solutions to make it easier to maintain the data infra as it's only me doing a one man data team job. It has been frustrating for me as my job became completely different as I've expected it to be when I joined here. To be fair, I think they didn't know what they need when they were hiring and they were also not aware of things that need fixing in the data pipeline. \n\nJust last week, I learned there are restructuring plans in the making. They're trying to get the manager fired as he is always missing and rarely communicating with the software engineers. Team's morale has been low since then.", "author_fullname": "t2_896476q8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One man data team, should I resign?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120srqt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679681497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an experienced data scientist(4+ years). Recently joined a 100-150 people tech startup company that went bankrupt last two years. They&amp;#39;re back in the game after receiving external fundinf. Lately I&amp;#39;ve been thinking if the expectations set upon my role are realistically achievable. My manager is more of a middle management guy with zero experience in data. When I was interviewing for them, it seemed like a match for my skills. They&amp;#39;re looking to build predictive and prescriptive analytics using machine learning on a large scale.I was under the impression that they have  data engineers maintaining the data pipeline. I was also open to double hat data science and MLops as I&amp;#39;ve had experience doing both. I was surprised when I started working here that it&amp;#39;s a one man data team job. Apparently, all of the data team left during layoffs last two years. The data science projects were also scattered across different locations(i.e cloud, local environment, etc.) and built using a custom library. Management wanted me to improve the current machine learning powered features of the app such that it can be reused by other companies. Unfortunately, it&amp;#39;s tightly coupled with our backend services. The experiment platform for online testing was also built without understanding about the statistics behind it. It&amp;#39;s a whole revamp of the data infrastructure if they wanted to achieve their goal to deploy machine learning solutions in large scale. I learned from the ex-data scientist working here that the current infra was built with tech debt since they wanted to ship things fast before(i.e series A funding). I&amp;#39;ve communicated what needs to be done first before they can achieve what they want. I asked for a data engineer since the ETL pipeline has been acting up lately and the staging environment crashes when I deploy new experiments. Nobody in the current team knows why it crashes so I&amp;#39;m left with very limited options on how to deliver value. I tried debugging it out on my own but it&amp;#39;s beyond my understanding too. As of the moment, I&amp;#39;m finding out how they built their current model so I can at least tweak it and deliver value. Most of the notebooks are missing from the codebase and the remaining few are long notebooks with no context or comments on the purpose of the analysis done. There was also no documentation left. I guess because it was sudden layoffs and everyone didn&amp;#39;t have time to document etc. I&amp;#39;ve been trying my best to do it all on my own hoping to make things happen for the past three months. I&amp;#39;ve pitched ideas for resources to achieve the goals but they always end up rejected by the engineers but approved by our manager. The hierarchy of decision making in the team is still confusing to me right now. I&amp;#39;ve also proposed solutions to make it easier to maintain the data infra as it&amp;#39;s only me doing a one man data team job. It has been frustrating for me as my job became completely different as I&amp;#39;ve expected it to be when I joined here. To be fair, I think they didn&amp;#39;t know what they need when they were hiring and they were also not aware of things that need fixing in the data pipeline. &lt;/p&gt;\n\n&lt;p&gt;Just last week, I learned there are restructuring plans in the making. They&amp;#39;re trying to get the manager fired as he is always missing and rarely communicating with the software engineers. Team&amp;#39;s morale has been low since then.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120srqt", "is_robot_indexable": true, "report_reasons": null, "author": "ramenmaster2021", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120srqt/one_man_data_team_should_i_resign/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120srqt/one_man_data_team_should_i_resign/", "subreddit_subscribers": 862080, "created_utc": 1679681497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, so if you are not a Baseball Nerd, OOTP is a baseball simulation game that lets you manage an MLB or other professional baseball team. It\u2019s basically a glorified spreadsheet since it keeps tons of stats. \n\nI\u2019ve been thinking of how fun it would be to create some machine learning models to predict player performance and create basically a mini fangraphs for our fictional online league.  \n\nThe question I have, is would this be too silly to put on my public GitHub? Would employers think it\u2019s a little silly to have models based off a video game? Should I just keep the repo private? I\u2019m about to graduate with my Masters and I\u2019m job hunting.", "author_fullname": "t2_dozq1t0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OOTP Machine Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120sr9b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679681471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, so if you are not a Baseball Nerd, OOTP is a baseball simulation game that lets you manage an MLB or other professional baseball team. It\u2019s basically a glorified spreadsheet since it keeps tons of stats. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been thinking of how fun it would be to create some machine learning models to predict player performance and create basically a mini fangraphs for our fictional online league.  &lt;/p&gt;\n\n&lt;p&gt;The question I have, is would this be too silly to put on my public GitHub? Would employers think it\u2019s a little silly to have models based off a video game? Should I just keep the repo private? I\u2019m about to graduate with my Masters and I\u2019m job hunting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120sr9b", "is_robot_indexable": true, "report_reasons": null, "author": "Nooneofsignificance2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120sr9b/ootp_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120sr9b/ootp_machine_learning/", "subreddit_subscribers": 862080, "created_utc": 1679681471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How-to-Fine-Tune GPT-3-Model-for-Named-Entity-Recognition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_120rm54", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_erazjrbo", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iEH3VwCvYGday3yXLGkixCy8cZE_EdyA39Nlal8obkY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "nlp_knowledge_sharing", "selftext": "Are you interested in fine-tuning pre-trained models like GPT-3 to suit your organization's specific needs?\n\nCheck out this must-read article on \"How-to-Fine-Tune GPT-3-Model-for-Named-Entity-Recognition.\" and Learn about the critical process of fine-tuning, which allows you to customize pre-trained models to achieve exceptional performance on your unique use cases.\n\nThe article breaks down the fundamental steps of fine-tuning, including preparing training data in the form of JSONL documents and designing prompts and completions. \nRead the full article here : https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition", "author_fullname": "t2_erazjrbo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How-to-Fine-Tune GPT-3-Model-for-Named-Entity-Recognition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/nlp_knowledge_sharing", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_120rl6f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1679679110.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ubiai.tools", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are you interested in fine-tuning pre-trained models like GPT-3 to suit your organization&amp;#39;s specific needs?&lt;/p&gt;\n\n&lt;p&gt;Check out this must-read article on &amp;quot;How-to-Fine-Tune GPT-3-Model-for-Named-Entity-Recognition.&amp;quot; and Learn about the critical process of fine-tuning, which allows you to customize pre-trained models to achieve exceptional performance on your unique use cases.&lt;/p&gt;\n\n&lt;p&gt;The article breaks down the fundamental steps of fine-tuning, including preparing training data in the form of JSONL documents and designing prompts and completions. \nRead the full article here : &lt;a href=\"https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition\"&gt;https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?auto=webp&amp;v=enabled&amp;s=8589973933b8a3d98f94770efc6b03fc0bf51d9d", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08f5bda42462327fe951a5c9830a966264d78738", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2deb84eb43c25f5feea0e7ddc596ae839f806c22", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d26bf86ae373d50e6c92bea40edbc5ef4f8fa2b2", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91e8fdb39b0ecc15eee3b51c79d6b072ed337b57", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d9bb0c74ca7b008ca2d474ed86f7dc180ed0a98", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=636277a8dfc8f20e73200643c91121e4f74c5f02", "width": 1080, "height": 564}], "variants": {}, "id": "fTeOd0VyYPSKz6U0rOsuA68u5NR6-meqB9jGZ-AMOzw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2ulgo", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120rl6f", "is_robot_indexable": true, "report_reasons": null, "author": "Lilith-Smol", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/nlp_knowledge_sharing/comments/120rl6f/howtofinetune_gpt3modelfornamedentityrecognition/", "parent_whitelist_status": null, "stickied": false, "url": "https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition", "subreddit_subscribers": 1777, "created_utc": 1679679110.0, "num_crossposts": 6, "media": null, "is_video": false}], "created": 1679679165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ubiai.tools", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?auto=webp&amp;v=enabled&amp;s=8589973933b8a3d98f94770efc6b03fc0bf51d9d", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08f5bda42462327fe951a5c9830a966264d78738", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2deb84eb43c25f5feea0e7ddc596ae839f806c22", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d26bf86ae373d50e6c92bea40edbc5ef4f8fa2b2", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91e8fdb39b0ecc15eee3b51c79d6b072ed337b57", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d9bb0c74ca7b008ca2d474ed86f7dc180ed0a98", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/niVLMBR-FFCwOoBrO1wRm_NhgNgqjggEw-_Hagphz0M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=636277a8dfc8f20e73200643c91121e4f74c5f02", "width": 1080, "height": 564}], "variants": {}, "id": "fTeOd0VyYPSKz6U0rOsuA68u5NR6-meqB9jGZ-AMOzw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120rm54", "is_robot_indexable": true, "report_reasons": null, "author": "Lilith-Smol", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_120rl6f", "author_flair_text_color": null, "permalink": "/r/datascience/comments/120rm54/howtofinetune_gpt3modelfornamedentityrecognition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition", "subreddit_subscribers": 862080, "created_utc": 1679679165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all,\n\nI am training an ML model where the features are coordinates of points on the human body for activity recognition. I used Mediapipe's model to get the coordinates for some 90000 frames obtained from videos. For each frame, I have 100 features which are the coordinates. The dataset is, therefore, huge. I am doing this on Colab using OpenCV and Mediapipe, and getting the coordinates itself is taking forever. Every time I run it, the execution stops unexpectedly after 80000+ frames are analyzed. How can I speed this up or make it more efficient?\n\n1. I was thinking of using every 5th frame instead of every single one.\n2. Batch processing is an option but would that really help? In the end, we are still analyzing every single frame, right?\n3. After obtaining the coordinates, should I run the classification model on the the dataset directly? Or should I do some sort of feature extraction and engineering? It feels like coming up with feature engineering rules for anatomical motion would not be straightforward.\n\nAppreciate your suggestions. \nThanks in advance :)", "author_fullname": "t2_gaxqj6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Speeding up multiclass classification ML model with 100+ features?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120r53n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679678198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I am training an ML model where the features are coordinates of points on the human body for activity recognition. I used Mediapipe&amp;#39;s model to get the coordinates for some 90000 frames obtained from videos. For each frame, I have 100 features which are the coordinates. The dataset is, therefore, huge. I am doing this on Colab using OpenCV and Mediapipe, and getting the coordinates itself is taking forever. Every time I run it, the execution stops unexpectedly after 80000+ frames are analyzed. How can I speed this up or make it more efficient?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I was thinking of using every 5th frame instead of every single one.&lt;/li&gt;\n&lt;li&gt;Batch processing is an option but would that really help? In the end, we are still analyzing every single frame, right?&lt;/li&gt;\n&lt;li&gt;After obtaining the coordinates, should I run the classification model on the the dataset directly? Or should I do some sort of feature extraction and engineering? It feels like coming up with feature engineering rules for anatomical motion would not be straightforward.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Appreciate your suggestions. \nThanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120r53n", "is_robot_indexable": true, "report_reasons": null, "author": "yipra97", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120r53n/speeding_up_multiclass_classification_ml_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120r53n/speeding_up_multiclass_classification_ml_model/", "subreddit_subscribers": 862080, "created_utc": 1679678198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\nI\u2019m currently working on a project to apply regression on a set of relational data from our ERP. As my data contains a significant number of 1:M relations, I feel that manually aggregating features into a single vector will either end up losing me a lot of information, or I would have to include an enormous amount of features to mitigate this.\nI remember reading a research paper that tried to embed relational data into heterogenous graphs and used graph neural network to inductively learn representations for the whole graph, but am not able to find it again on Scopus or WoS. I was wondering if anyone here  had any experience with this topic, and would be able to point me in the direction of some resources?\nAlternatively, how do you guys approach data from highly relational sources? Do you just aggregate features, even when you have to aggregate multiple times? Would love to hear you guys\u2019 opinion on this.\n\nThanks in advance!", "author_fullname": "t2_htj6ydlz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Embedding relational data into graphs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120hdi9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679657222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,\nI\u2019m currently working on a project to apply regression on a set of relational data from our ERP. As my data contains a significant number of 1:M relations, I feel that manually aggregating features into a single vector will either end up losing me a lot of information, or I would have to include an enormous amount of features to mitigate this.\nI remember reading a research paper that tried to embed relational data into heterogenous graphs and used graph neural network to inductively learn representations for the whole graph, but am not able to find it again on Scopus or WoS. I was wondering if anyone here  had any experience with this topic, and would be able to point me in the direction of some resources?\nAlternatively, how do you guys approach data from highly relational sources? Do you just aggregate features, even when you have to aggregate multiple times? Would love to hear you guys\u2019 opinion on this.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120hdi9", "is_robot_indexable": true, "report_reasons": null, "author": "Master-Pension-6441", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120hdi9/embedding_relational_data_into_graphs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120hdi9/embedding_relational_data_into_graphs/", "subreddit_subscribers": 862080, "created_utc": 1679657222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi \ud83d\udc4b all! We\u2019re building a marketplace for web data (https://www.databoutique.com).\n\nIf you need web data for training models or app development, you can ask the community for it. \nThe goal is to save time and cut down on scraping costs. \n\nThe basic idea is that most of the times, you\u2019ll need data that someone is already scraping, so it\u2019s faster and easier to ask for it, instead of doing again the scrape yourself. \n\nWe\u2019re in early phase, any feedback is welcome. We hope this helps lower the barriers to data.", "author_fullname": "t2_2xu6ursr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databoutique.com - a marketplace for web data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_120dwpr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679646535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi \ud83d\udc4b all! We\u2019re building a marketplace for web data (&lt;a href=\"https://www.databoutique.com\"&gt;https://www.databoutique.com&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;If you need web data for training models or app development, you can ask the community for it. \nThe goal is to save time and cut down on scraping costs. &lt;/p&gt;\n\n&lt;p&gt;The basic idea is that most of the times, you\u2019ll need data that someone is already scraping, so it\u2019s faster and easier to ask for it, instead of doing again the scrape yourself. &lt;/p&gt;\n\n&lt;p&gt;We\u2019re in early phase, any feedback is welcome. We hope this helps lower the barriers to data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "120dwpr", "is_robot_indexable": true, "report_reasons": null, "author": "Pigik83", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/120dwpr/databoutiquecom_a_marketplace_for_web_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/120dwpr/databoutiquecom_a_marketplace_for_web_data/", "subreddit_subscribers": 862080, "created_utc": 1679646535.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}