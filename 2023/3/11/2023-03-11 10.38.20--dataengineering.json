{"kind": "Listing", "data": {"after": "t3_11nt7aa", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This article is co-written by me and my colleague Kai Dai. We are both data platform engineers at Tencent Music (NYSE: TME), a music streaming service provider with a whopping 800 million monthly active users. To drop the number here is not to brag but to give a hint of the sea of data that my poor coworkers and I have to deal with everyday.\n\n# What We Use ClickHouse For?\n\nThe music library of Tencent Music contains data of all forms and types: recorded music, live music, audios, videos, etc. As data platform engineers, our job is to distill information from the data, based on which our teammates can make better decisions to support our users and musical partners.\n\nSpecifically, we do all-round analysis of the songs, lyrics, melodies, albums, and artists, turn all this information into data assets, and pass them to our internal data users for inventory counting, user profiling, metrics analysis, and group targeting.\n\nhttps://preview.redd.it/y36uy7do4xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=64ccd7ccb884196cd95b7f5633dbe39db72c0ec2\n\nWe stored and processed most of our data in Tencent Data Warehouse (TDW), an offline data platform where we put the data into various tag and metric systems and then created flat tables centering each object (songs, artists, etc.).\n\nThen we imported the flat tables into ClickHouse for analysis and Elasticsearch for data searching and group targeting.\n\nAfter that, our data analysts used the data under the tags and metrics they needed to form datasets for different usage scenarios, during which they could create their own tags and metrics.\n\nThe data processing pipeline looked like this:\n\nhttps://preview.redd.it/18em4jjr4xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=df6a63d7ae38287c56573894090fb912caad0d24\n\n# Why ClickHouse is Not a Good Fit\n\nWhen working with the above pipeline, we encountered a few difficulties:\n\n1. **Partial Update**: Partial update of columns was not supported. Therefore, any latency from any one of the data sources could delay the creation of flat tables, and thus undermine data timeliness.\n2. **High storage cost**: Data under different tags and metrics was updated at different frequencies. As much as ClickHouse excelled in dealing with flat tables, it was a huge waste of storage resources to just pour all data into a flat table and partition it by day, not to mention the maintenance cost coming with it.\n3. **High maintenance cost**: Architecturally speaking, ClickHouse was characterized by the strong coupling of storage nodes and compute nodes. Its components were heavily interdependent, adding to the risks of cluster instability. Plus, for federated queries across ClickHouse and Elasticsearch, we had to take care of a huge amount of connection issues. That was just tedious.\n\n# Transition to Apache Doris\n\n[Apache Doris](https://github.com/apache/doris), a real-time analytical database, boasts a few features that are exactly what we needed in solving our problems:\n\n1. **Partial update**: Doris supports a wide variety of data models, among which the Aggregate Model supports real-time partial update of columns. Building on this, we can directly ingest raw data into Doris and create flat tables there. The ingestion goes like this: Firstly, we use Spark to load data into Kafka; then, any incremental data will be updated to Doris and Elasticsearch via Flink. Meanwhile, Flink will pre-aggregate the data so as to release burden on Doris and Elasticsearch.\n2. **Storage cost**: Doris supports multi-table join queries and federated queries across Hive, Iceberg, Hudi, MySQL, and Elasticsearch. This allows us to split the large flat tables into smaller ones and partition them by update frequency. The benefits of doing so include a relief of storage burden and an increase of query throughput.\n3. **Maintenance cost**: Doris is of simple architecture and is compatible with MySQL protocol. Deploying Doris only involves two processes (FE and BE) with no dependency on other systems, making it easy to operate and maintain. Also, Doris supports querying external ES data tables. It can easily interface with the metadata in ES and automatically map the table schema from ES so we can conduct queries on Elasticsearch data via Doris without grappling with complex connections.\n\nWhat\u2019s more, Doris supports multiple data ingestion methods, including batch import from remote storage such as HDFS and S3, data reads from MySQL binlog and Kafka, and real-time data synchronization or batch import from MySQL, Oracle, and PostgreSQL. It ensures service availability and data reliability through a consistency protocol and is capable of auto debugging. This is great news for our operators and maintainers.\n\nStatistically speaking, these features have cut our storage cost by 42% and development cost by 40%.\n\nDuring our usage of Doris, we have received lots of support from the open source Apache Doris community and timely help from the SelectDB team, which is now running a commercial version of Apache Doris.\n\nhttps://preview.redd.it/4epkzulg5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d335d4db9f4d456d04db4e00a203b8d025a1430d\n\n# Further Improvement to Serve Our NeedsIntroduce a Semantic Layer\n\nSpeaking of the datasets, on the bright side, our data analysts are given the liberty of redefining and combining the tags and metrics at their convenience. But on the dark side, high heterogeneity of the tag and metric systems leads to more difficulty in their usage and management.\n\nOur solution is to introduce a semantic layer in our data processing pipeline. The semantic layer is where all the technical terms are translated into more comprehensible concepts for our internal data users. In other words, we are turning the tags and metrics into first-class citizens for data definement and management.\n\nhttps://preview.redd.it/7yxzag2k5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=49d662ddeb2b7fe0a5992c8a596c72449fcb404f\n\n**Why would this help?**\n\nFor data analysts, all tags and metrics will be created and shared at the semantic layer so there will be less confusion and higher efficiency.\n\nFor data users, they no longer need to create their own datasets or figure out which one is applicable for each scenario but can simply conduct queries on their specified tagset and metricset.\n\n# Upgrade the Semantic Layer\n\nExplicitly defining the tags and metrics at the semantic layer was not enough. In order to build a standardized data processing system, our next goal was to ensure consistent definition of tags and metrics throughout the whole data processing pipeline.\n\nFor this sake, we made the semantic layer the heart of our data management system:\n\nhttps://preview.redd.it/yitj349p5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1074b3ed4ac2f286d56ecb8a48ec9edcb2d8ba8f\n\n**How does it work?**\n\nAll computing logics in TDW will be defined at the semantic layer in the form of a single tag or metric.\n\nThe semantic layer receives logic queries from the application side, selects an engine accordingly, and generates SQL. Then it sends the SQL command to TDW for execution. Meanwhile, it might also send configuration and data ingestion tasks to Doris and decide which metrics and tags should be accelerated.\n\nIn this way, we have made the tags and metrics more manageable. A fly in the ointment is that since each tag and metric is individually defined, we are struggling with automating the generation of a valid SQL statement for the queries. If you have any idea about this, you are more than welcome to talk to us.\n\n# Give Full Play to Apache Doris\n\nAs you can see, Apache Doris has played a pivotal role in our solution. Optimizing the usage of Doris can largely improve our overall data processing efficiency. So in this part, we are going to share with you what we do with Doris to accelerate data ingestion and queries and reduce costs.\n\n**What We Want?**\n\nhttps://preview.redd.it/1s4n2nls5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6958bf9609e4b4d0b3b38f281e2b593029cf384b\n\nCurrently, we have 800+ tags and 1300+ metrics derived from the 80+ source tables in TDW.\n\nWhen importing data from TDW to Doris, we hope to achieve:\n\n* **Real-time availability:** In addition to the traditional T+1 offline data ingestion, we require real-time tagging.\n* **Partial update**: Each source table generates data through its own ETL task at various paces and involves only part of the tags and metrics, so we require the support for partial update of columns.\n* **High performance**: We need a response time of only a few seconds in group targeting, analysis and reporting scenarios.\n* **Low costs**: We hope to reduce costs as much as possible.\n\n**What We Do?**\n\n1. **Generate Flat Tables in Flink Instead of TDW**\n\nhttps://preview.redd.it/of8zcyyu5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ffeddbb65b3ec4cccbceb1c135ec5455b38a7050\n\nGenerating flat tables in TDW has a few downsides:\n\n* **High storage cost**: TDW has to maintain an extra flat table apart from the discrete 80+ source tables. That\u2019s huge redundancy.\n* **Low real-timeliness**: Any delay in the source tables will be augmented and retard the whole data link.\n* **High development cost**: To achieve real-timeliness would require extra development efforts and resources.\n\nOn the contrary, generating flat tables in Doris is much easier and less expensive. The process is as follows:\n\n* Use Spark to import new data into Kafka in an offline manner.\n* Use Flink to consume Kafka data.\n* Create a flat table via the primary key ID.\n* Import the flat table into Doris.\n\nAs is shown below, Flink has aggregated the five lines of data, of which \u201cID\u201d=1, into one line in Doris, reducing the data writing pressure on Doris.\n\nhttps://preview.redd.it/qk6mr24x5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a41e122d846e4f6efc9c04de30b011c8497763bd\n\nThis can largely reduce storage costs since TDW no long has to maintain two copies of data and KafKa only needs to store the new data pending for ingestion. What\u2019s more, we can add whatever ETL logic we want into Flink and reuse lots of development logic for offline and real-time data ingestion.\n\n**2. Name the Columns Smartly**\n\nAs we mentioned, the Aggregate Model of Doris allows partial update of columns. Here we provide a simple introduction to other data models in Doris for your reference:\n\n**Unique Model**: This is applicable for scenarios requiring primary key uniqueness. It only keeps the latest data of the same primary key ID. (As far as we know, the Apache Doris community is planning to include partial update of columns in the Unique Model, too.)\n\n**Duplicate Model**: This model stores all original data exactly as it is without any pre-aggregation or deduplication.\n\nAfter determining the data model, we had to think about how to name the columns. Using the tags or metrics as column names was not a choice because:\n\nI. Our internal data users might need to rename the metrics or tags, but Doris 1.1.3 does not support modification of column names.\n\nII. Tags might be taken online and offline frequently. If that involves the adding and dropping of columns, it will be not only time-consuming but also detrimental to query performance.\n\nInstead, we do the following:\n\n* **For flexible renaming of tags and metrics**, we use MySQL tables to store the metadata (name, globally unique ID, status, etc.). Any change to the names will only happen in the metadata but will not affect the table schema in Doris. For example, if a `song_name` is given an ID of 4, it will be stored with the column name of a4 in Doris. Then if the `song_name` is involved in a query, it will be converted to a4 in SQL.\n* **For the onlining and offlining of tags**, we sort out the tags based on how frequently they are being used. The least used ones will be given an offline mark in their metadata. No new data will be put under the offline tags but the existing data under those tags will still be available.\n* **For real-time availability of newly added tags and metrics**, we prebuild a few ID columns in Doris tables based on the mapping of name IDs. These reserved ID columns will be allocated to the newly added tags and metrics. Thus, we can avoid table schema change and the consequent overheads. Our experience shows that only 10 minutes after the tags and metrics are added, the data under them can be available.\n\nNoteworthily, the recently released Doris 1.2.0 supports Light Schema Change, which means that to add or remove columns, you only need to modify the metadata in FE. Also, you can rename the columns in data tables as long as you have enabled Light Schema Change for the tables. This is a big trouble saver for us.\n\n**3. Optimize Date Writing**\n\nHere are a few practices that have reduced our daily offline data ingestion time by 75% and our CUMU compaction score from 600+ to 100.\n\n* Flink pre-aggregation: as is mentioned above.\n* Auto-sizing of writing batch: To reduce Flink resource usage, we enable the data in one Kafka Topic to be written into various Doris tables and realize the automatic alteration of batch size based on the data amount.\n* Optimization of Doris data writing: fine-tune the the sizes of tablets and buckets as well as the compaction parameters for each scenario:\n\n`max_XXXX_compaction_thread`  \n`max_cumulative_compaction_num_singleton_deltas`\n\n* Optimization of the BE commit logic: conduct regular caching of BE lists, commit them to the BE nodes batch by batch, and use finer load balancing granularity.\n\nhttps://preview.redd.it/q5dqk7b76xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9d448a24018f450ba26dae50cb8fe2d815183267\n\n**4. Use Dori-on-ES in Queries**\n\nAbout 60% of our data queries involve group targeting. Group targeting is to find our target data by using a set of tags as filters. It poses a few requirements for our data processing architecture:\n\n* Group targeting related to APP users can involve very complicated logic. That means the system must support hundreds of tags as filters simultaneously.\n* Most group targeting scenarios only require the latest tag data. However, metric queries need to support historical data.\n* Data users might need to perform further aggregated analysis of metric data after group targeting.\n* Data users might also need to perform detailed queries on tags and metrics after group targeting.\n\nAfter consideration, we decided to adopt Doris-on-ES. Doris is where we store the metric data for each scenario as a partition table, while Elasticsearch stores all tag data. The Doris-on-ES solution combines the distributed query planning capability of Doris and the full-text search capability of Elasticsearch. The query pattern is as follows:\n\n`SELECT tag, agg(metric)`   \n `FROM Doris`   \n `WHERE id in (select id from Es where tagFilter)`  \n `GROUP BY tag`\n\nAs is shown, the ID data located in Elasticsearch will be used in the sub-query in Doris for metric analysis.\n\nIn practice, we find that the query response time is related to the size of the target group. If the target group contains over one million objects, the query will take up to 60 seconds. If it is even larger, a timeout error might occur.\n\nAfter investigation, we identified our two biggest time wasters:\n\nI. When Doris BE pulls data from Elasticsearch (1024 lines at a time by default), for a target group of over one million objects, the network I/O overhead can be huge.\n\nII. After the data pulling, Doris BE needs to conduct Join operations with local metric tables via SHUFFLE/BROADCAST, which can cost a lot.\n\nhttps://preview.redd.it/yhfjj9uh6xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=efbf60f8f864da8bded55c36089a6146dcab1251\n\nThus, we make the following optimizations:\n\n* Add a query session variable `es_optimize` that specifies whether to enable optimization.\n* In data writing into ES, add a BK column to store the bucket number after the primary key ID is hashed. The algorithm is the same as the bucketing algorithm in Doris (CRC32).\n* Use Doris BE to generate a Bucket Join execution plan, dispatch the bucket number to BE ScanNode and push it down to ES.\n* Use ES to compress the queried data; turn multiple data fetch into one and reduce network I/O overhead.\n* Make sure that Doris BE only pulls the data of buckets related to the local metric tables and conducts local Join operations directly to avoid data shuffling between Doris BEs.\n\nhttps://preview.redd.it/or4nmhsk6xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=96666009eab7269ee818903d8f20df2a50ac7956\n\nAs a result, we reduce the query response time for large group targeting from 60 seconds to a surprising 3.7 seconds.\n\nCommunity information shows that Doris is going to support inverted indexing since version 2.0.0, which is soon to be released. With this new version, we will be able to conduct full-text search on text types, equivalence or range filtering of texts, numbers, and datetime, and conveniently combine AND, OR, NOT logic in filtering since the inverted indexing supports array types. This new feature of Doris is expected to deliver 3\\~5 times better performance than Elasticsearch on the same task.\n\n**5. Refine the Management of Data**\n\nDoris\u2019 capability of cold and hot data separation provides the foundation of our cost reduction strategies in data processing.\n\n* Based on the TTL mechanism of Doris, we only store data of the current year in Doris and put the historical data before that in TDW for lower storage cost.\n* We vary the numbers of copies for different data partitions. For example, we set three copies for data of the recent three months, which is used frequently, one copy for data older than six months, and two copies for data in between.\n* Doris supports turning hot data into cold data so we only store data of the past seven days in SSD and transfer data older than that to HDD for less expensive storage.\n\n# Conclusion\n\nThank you for scrolling all the way down here and finishing this long read. We\u2019ve shared our cheers and tears, lessons learned, and a few practices that might be of some value to you during our transition from ClickHouse to Doris. We really appreciate the help from the Apache Doris community and the SelectDB team, but we might still be chasing them around for a while since we attempt to realize auto-identification of cold and hot data, pre-computation of frequently used tags/metrics, simplification of code logic using Materialized Views, and so on and so forth.", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Tencent Data Engineer: Why We Went from ClickHouse to Apache Doris?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "media_metadata": {"1s4n2nls5xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/1s4n2nls5xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffc82a6c81bb76aecc2509a7add4e00f5388eef0"}, {"y": 74, "x": 216, "u": "https://preview.redd.it/1s4n2nls5xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c54dedbd6a673fdc0a89903c15986fdc58508f67"}, {"y": 111, "x": 320, "u": "https://preview.redd.it/1s4n2nls5xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb1a6f84bedcb4b93d0b646a37cbd92317d3f9a0"}, {"y": 222, "x": 640, "u": "https://preview.redd.it/1s4n2nls5xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=27d030c1f442c08bca0495fb8b115692b6e54af9"}, {"y": 333, "x": 960, "u": "https://preview.redd.it/1s4n2nls5xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb64638bb1b62a81667f82c47d16326b5dc33f31"}, {"y": 374, "x": 1080, "u": "https://preview.redd.it/1s4n2nls5xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3a710db2801869632ca3efb6988328d0e9aa9d8"}], "s": {"y": 444, "x": 1280, "u": "https://preview.redd.it/1s4n2nls5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6958bf9609e4b4d0b3b38f281e2b593029cf384b"}, "id": "1s4n2nls5xma1"}, "of8zcyyu5xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/of8zcyyu5xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bdb5c25334656f9118bccf99c247e93671f27fd6"}, {"y": 95, "x": 216, "u": "https://preview.redd.it/of8zcyyu5xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c83c696cc1591ce908c8b3baebe13bfd2ae81a26"}, {"y": 141, "x": 320, "u": "https://preview.redd.it/of8zcyyu5xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a11396fa6142177870a105472915efc5418dc7b8"}, {"y": 283, "x": 640, "u": "https://preview.redd.it/of8zcyyu5xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1fc72b0e8d95cfa37ffafb764aaad3bd338155bc"}, {"y": 425, "x": 960, "u": "https://preview.redd.it/of8zcyyu5xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00ed7dc3bb77a356f67d69fb101a91934a3ba74a"}, {"y": 478, "x": 1080, "u": "https://preview.redd.it/of8zcyyu5xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=599a8c5586a9eee61284814ee5a766a89092e66e"}], "s": {"y": 567, "x": 1280, "u": "https://preview.redd.it/of8zcyyu5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ffeddbb65b3ec4cccbceb1c135ec5455b38a7050"}, "id": "of8zcyyu5xma1"}, "18em4jjr4xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/18em4jjr4xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37fd7969e1b056cf4edda6e751dd464e0c30c1d2"}, {"y": 125, "x": 216, "u": "https://preview.redd.it/18em4jjr4xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf9095b50cf2e72daf20e07a1eb37f6f4fbcded7"}, {"y": 185, "x": 320, "u": "https://preview.redd.it/18em4jjr4xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86cfcc5e6ac4c704b6b6d03023b9a701d44a6fde"}, {"y": 371, "x": 640, "u": "https://preview.redd.it/18em4jjr4xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd766a736e722f31d9517daa83cb51ff9c43f9e4"}, {"y": 557, "x": 960, "u": "https://preview.redd.it/18em4jjr4xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9332cc2ab61e491699f672abf5a4b4b0e894d10b"}, {"y": 626, "x": 1080, "u": "https://preview.redd.it/18em4jjr4xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66d19614866e4400fb4b51a06ff947ab91abce38"}], "s": {"y": 743, "x": 1280, "u": "https://preview.redd.it/18em4jjr4xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=df6a63d7ae38287c56573894090fb912caad0d24"}, "id": "18em4jjr4xma1"}, "q5dqk7b76xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 43, "x": 108, "u": "https://preview.redd.it/q5dqk7b76xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=382fcacdfbd4a6d575cea780d4e74387ed96ae03"}, {"y": 86, "x": 216, "u": "https://preview.redd.it/q5dqk7b76xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fba8143309190666af29b6d042dfc9648215248a"}, {"y": 127, "x": 320, "u": "https://preview.redd.it/q5dqk7b76xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebb6ab17d52894ab1bdade885762b728c1376c91"}, {"y": 255, "x": 640, "u": "https://preview.redd.it/q5dqk7b76xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1dd5e344a6460c690fbba7af968cbea2160e4f0a"}, {"y": 383, "x": 960, "u": "https://preview.redd.it/q5dqk7b76xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e76ac4f9278edc76ff029b22a027cb61e89f0a0"}, {"y": 431, "x": 1080, "u": "https://preview.redd.it/q5dqk7b76xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7ca2d4144e390043391705b6a6ffbaefd637b92"}], "s": {"y": 511, "x": 1280, "u": "https://preview.redd.it/q5dqk7b76xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9d448a24018f450ba26dae50cb8fe2d815183267"}, "id": "q5dqk7b76xma1"}, "yhfjj9uh6xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/yhfjj9uh6xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3dccbf42d93c8f9623a02c14bba871ddf4f5db7e"}, {"y": 149, "x": 216, "u": "https://preview.redd.it/yhfjj9uh6xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a51f23a8c11c8694166e39df7c0999d338dc86e"}, {"y": 220, "x": 320, "u": "https://preview.redd.it/yhfjj9uh6xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=845d2e4009b4ee7e27c0668be9d88b3c7136566e"}, {"y": 441, "x": 640, "u": "https://preview.redd.it/yhfjj9uh6xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=100ef73eb80728d9b1cdc013ff4d400e3aab22be"}, {"y": 662, "x": 960, "u": "https://preview.redd.it/yhfjj9uh6xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6303f650b3c61646fa17d8bfe2b7c2c34d3b768f"}, {"y": 745, "x": 1080, "u": "https://preview.redd.it/yhfjj9uh6xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c522c14c499a33be05488256959830a5419149f"}], "s": {"y": 883, "x": 1280, "u": "https://preview.redd.it/yhfjj9uh6xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=efbf60f8f864da8bded55c36089a6146dcab1251"}, "id": "yhfjj9uh6xma1"}, "7yxzag2k5xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/7yxzag2k5xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2cac78420b4fc9ded1397d2073aad6c861789ad1"}, {"y": 125, "x": 216, "u": "https://preview.redd.it/7yxzag2k5xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bcd9687048a44c5edf5e7c4fe3a8d9ead0a3ece0"}, {"y": 185, "x": 320, "u": "https://preview.redd.it/7yxzag2k5xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b51c98ad8abbc02cf8ab85207a8785463b49a44b"}, {"y": 371, "x": 640, "u": "https://preview.redd.it/7yxzag2k5xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=707375e30415450f60e9c4e341e4bd57a881f65c"}, {"y": 557, "x": 960, "u": "https://preview.redd.it/7yxzag2k5xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d30f0014eadd14953b486ed45c8cf665d19aeaf"}, {"y": 626, "x": 1080, "u": "https://preview.redd.it/7yxzag2k5xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5a9794271a539cad0ebe6dca76baf6ac62c8e5f"}], "s": {"y": 743, "x": 1280, "u": "https://preview.redd.it/7yxzag2k5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=49d662ddeb2b7fe0a5992c8a596c72449fcb404f"}, "id": "7yxzag2k5xma1"}, "or4nmhsk6xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 77, "x": 108, "u": "https://preview.redd.it/or4nmhsk6xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=162ecbce47ac71d5b83872ef978b088e5d98953a"}, {"y": 155, "x": 216, "u": "https://preview.redd.it/or4nmhsk6xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbe2c16d468c6184a99a8131037b0f9289b6f465"}, {"y": 231, "x": 320, "u": "https://preview.redd.it/or4nmhsk6xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d6cd506a851ef4be8dded24ad56e38b3242425db"}, {"y": 462, "x": 640, "u": "https://preview.redd.it/or4nmhsk6xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3d7c7a80414d4c607cb46e6f241478c525b98b6"}, {"y": 693, "x": 960, "u": "https://preview.redd.it/or4nmhsk6xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e9d98061d287de0405f30d22c53ef3e95116bc4"}, {"y": 779, "x": 1080, "u": "https://preview.redd.it/or4nmhsk6xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc14851ab6c5f274f983cfc8d61f0c743f20b662"}], "s": {"y": 924, "x": 1280, "u": "https://preview.redd.it/or4nmhsk6xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=96666009eab7269ee818903d8f20df2a50ac7956"}, "id": "or4nmhsk6xma1"}, "yitj349p5xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/yitj349p5xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d171a7d4821856fa33acf5785354be85f089f6f"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/yitj349p5xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c610224f0c3e747283c6138294b8b75fb69dfcbb"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/yitj349p5xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37d5007eae3697b3ce407426e3c919993ad28dc7"}, {"y": 357, "x": 640, "u": "https://preview.redd.it/yitj349p5xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=25b4bbeec50fe83cfbc09842632353e90ff64006"}, {"y": 535, "x": 960, "u": "https://preview.redd.it/yitj349p5xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b648f711adc70e8d9739f42739a8a6490bb23945"}, {"y": 602, "x": 1080, "u": "https://preview.redd.it/yitj349p5xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2956f9a2badb493e69614f17f212f62e7557f4f4"}], "s": {"y": 714, "x": 1280, "u": "https://preview.redd.it/yitj349p5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1074b3ed4ac2f286d56ecb8a48ec9edcb2d8ba8f"}, "id": "yitj349p5xma1"}, "y36uy7do4xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 58, "x": 108, "u": "https://preview.redd.it/y36uy7do4xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a8a1d58e97eb126e9f636b6b9ba960d2fb2e45a"}, {"y": 116, "x": 216, "u": "https://preview.redd.it/y36uy7do4xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f1fc425e5754960820e266c0b374f0fbe68f24c"}, {"y": 173, "x": 320, "u": "https://preview.redd.it/y36uy7do4xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75b2af81ffc6ff382f1b326ae169d06ee0f9848c"}, {"y": 346, "x": 640, "u": "https://preview.redd.it/y36uy7do4xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ba3afe5ee72fae495f1d32452eb32a6af24939a"}, {"y": 519, "x": 960, "u": "https://preview.redd.it/y36uy7do4xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17fb51d2cedc290ff5d6e2af499e1eb2bbe9ca73"}, {"y": 584, "x": 1080, "u": "https://preview.redd.it/y36uy7do4xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6eef8a193024f69e330cf907432214c5d3e6fecd"}], "s": {"y": 693, "x": 1280, "u": "https://preview.redd.it/y36uy7do4xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=64ccd7ccb884196cd95b7f5633dbe39db72c0ec2"}, "id": "y36uy7do4xma1"}, "4epkzulg5xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 61, "x": 108, "u": "https://preview.redd.it/4epkzulg5xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e857b51c17add8fe6bfd34129752793e6f7c5dec"}, {"y": 123, "x": 216, "u": "https://preview.redd.it/4epkzulg5xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a3f1f2cda4f5eafdadb01508c2a7a50ca7a2f44"}, {"y": 183, "x": 320, "u": "https://preview.redd.it/4epkzulg5xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4a409a836a4c06f00973d235b68273de942bab6"}, {"y": 367, "x": 640, "u": "https://preview.redd.it/4epkzulg5xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dbba1d94140ee73d9cdfa15704d90a9fdd22dfe6"}, {"y": 550, "x": 960, "u": "https://preview.redd.it/4epkzulg5xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ef84cadfddcea37d3278fe619e9efd96d0c14c8"}, {"y": 619, "x": 1080, "u": "https://preview.redd.it/4epkzulg5xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a86ebff34a5dfd162c93c1a0a2805cd022e3f30"}], "s": {"y": 734, "x": 1280, "u": "https://preview.redd.it/4epkzulg5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d335d4db9f4d456d04db4e00a203b8d025a1430d"}, "id": "4epkzulg5xma1"}, "qk6mr24x5xma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/qk6mr24x5xma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c01961add86b2f942896cd0240bab8247f9781f7"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/qk6mr24x5xma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c58cb57f2620265ef530205efd147ea7bab958e2"}, {"y": 155, "x": 320, "u": "https://preview.redd.it/qk6mr24x5xma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37c85053d651efc7d0e1b2e9867bc6d52f35bebd"}, {"y": 311, "x": 640, "u": "https://preview.redd.it/qk6mr24x5xma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a57843d7f940534dccd9dfbc023f57c3f54c727d"}, {"y": 466, "x": 960, "u": "https://preview.redd.it/qk6mr24x5xma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2a01c7e5697de9197ca6698e86134bd6660360c"}, {"y": 524, "x": 1080, "u": "https://preview.redd.it/qk6mr24x5xma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a15dd69191cf9ec531fab4dd1d3adfc8841709c"}], "s": {"y": 622, "x": 1280, "u": "https://preview.redd.it/qk6mr24x5xma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a41e122d846e4f6efc9c04de30b011c8497763bd"}, "id": "qk6mr24x5xma1"}}, "name": "t3_11nqc61", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 167, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 167, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://a.thumbs.redditmedia.com/wwKzqt9FU3c6VH9ad85VsOCZ1I4qem7GKP79e8Wb4c8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1678457655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This article is co-written by me and my colleague Kai Dai. We are both data platform engineers at Tencent Music (NYSE: TME), a music streaming service provider with a whopping 800 million monthly active users. To drop the number here is not to brag but to give a hint of the sea of data that my poor coworkers and I have to deal with everyday.&lt;/p&gt;\n\n&lt;h1&gt;What We Use ClickHouse For?&lt;/h1&gt;\n\n&lt;p&gt;The music library of Tencent Music contains data of all forms and types: recorded music, live music, audios, videos, etc. As data platform engineers, our job is to distill information from the data, based on which our teammates can make better decisions to support our users and musical partners.&lt;/p&gt;\n\n&lt;p&gt;Specifically, we do all-round analysis of the songs, lyrics, melodies, albums, and artists, turn all this information into data assets, and pass them to our internal data users for inventory counting, user profiling, metrics analysis, and group targeting.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y36uy7do4xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=64ccd7ccb884196cd95b7f5633dbe39db72c0ec2\"&gt;https://preview.redd.it/y36uy7do4xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=64ccd7ccb884196cd95b7f5633dbe39db72c0ec2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We stored and processed most of our data in Tencent Data Warehouse (TDW), an offline data platform where we put the data into various tag and metric systems and then created flat tables centering each object (songs, artists, etc.).&lt;/p&gt;\n\n&lt;p&gt;Then we imported the flat tables into ClickHouse for analysis and Elasticsearch for data searching and group targeting.&lt;/p&gt;\n\n&lt;p&gt;After that, our data analysts used the data under the tags and metrics they needed to form datasets for different usage scenarios, during which they could create their own tags and metrics.&lt;/p&gt;\n\n&lt;p&gt;The data processing pipeline looked like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/18em4jjr4xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=df6a63d7ae38287c56573894090fb912caad0d24\"&gt;https://preview.redd.it/18em4jjr4xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=df6a63d7ae38287c56573894090fb912caad0d24&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Why ClickHouse is Not a Good Fit&lt;/h1&gt;\n\n&lt;p&gt;When working with the above pipeline, we encountered a few difficulties:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Partial Update&lt;/strong&gt;: Partial update of columns was not supported. Therefore, any latency from any one of the data sources could delay the creation of flat tables, and thus undermine data timeliness.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High storage cost&lt;/strong&gt;: Data under different tags and metrics was updated at different frequencies. As much as ClickHouse excelled in dealing with flat tables, it was a huge waste of storage resources to just pour all data into a flat table and partition it by day, not to mention the maintenance cost coming with it.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High maintenance cost&lt;/strong&gt;: Architecturally speaking, ClickHouse was characterized by the strong coupling of storage nodes and compute nodes. Its components were heavily interdependent, adding to the risks of cluster instability. Plus, for federated queries across ClickHouse and Elasticsearch, we had to take care of a huge amount of connection issues. That was just tedious.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Transition to Apache Doris&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/apache/doris\"&gt;Apache Doris&lt;/a&gt;, a real-time analytical database, boasts a few features that are exactly what we needed in solving our problems:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Partial update&lt;/strong&gt;: Doris supports a wide variety of data models, among which the Aggregate Model supports real-time partial update of columns. Building on this, we can directly ingest raw data into Doris and create flat tables there. The ingestion goes like this: Firstly, we use Spark to load data into Kafka; then, any incremental data will be updated to Doris and Elasticsearch via Flink. Meanwhile, Flink will pre-aggregate the data so as to release burden on Doris and Elasticsearch.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Storage cost&lt;/strong&gt;: Doris supports multi-table join queries and federated queries across Hive, Iceberg, Hudi, MySQL, and Elasticsearch. This allows us to split the large flat tables into smaller ones and partition them by update frequency. The benefits of doing so include a relief of storage burden and an increase of query throughput.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Maintenance cost&lt;/strong&gt;: Doris is of simple architecture and is compatible with MySQL protocol. Deploying Doris only involves two processes (FE and BE) with no dependency on other systems, making it easy to operate and maintain. Also, Doris supports querying external ES data tables. It can easily interface with the metadata in ES and automatically map the table schema from ES so we can conduct queries on Elasticsearch data via Doris without grappling with complex connections.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What\u2019s more, Doris supports multiple data ingestion methods, including batch import from remote storage such as HDFS and S3, data reads from MySQL binlog and Kafka, and real-time data synchronization or batch import from MySQL, Oracle, and PostgreSQL. It ensures service availability and data reliability through a consistency protocol and is capable of auto debugging. This is great news for our operators and maintainers.&lt;/p&gt;\n\n&lt;p&gt;Statistically speaking, these features have cut our storage cost by 42% and development cost by 40%.&lt;/p&gt;\n\n&lt;p&gt;During our usage of Doris, we have received lots of support from the open source Apache Doris community and timely help from the SelectDB team, which is now running a commercial version of Apache Doris.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4epkzulg5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d335d4db9f4d456d04db4e00a203b8d025a1430d\"&gt;https://preview.redd.it/4epkzulg5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d335d4db9f4d456d04db4e00a203b8d025a1430d&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Further Improvement to Serve Our NeedsIntroduce a Semantic Layer&lt;/h1&gt;\n\n&lt;p&gt;Speaking of the datasets, on the bright side, our data analysts are given the liberty of redefining and combining the tags and metrics at their convenience. But on the dark side, high heterogeneity of the tag and metric systems leads to more difficulty in their usage and management.&lt;/p&gt;\n\n&lt;p&gt;Our solution is to introduce a semantic layer in our data processing pipeline. The semantic layer is where all the technical terms are translated into more comprehensible concepts for our internal data users. In other words, we are turning the tags and metrics into first-class citizens for data definement and management.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7yxzag2k5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=49d662ddeb2b7fe0a5992c8a596c72449fcb404f\"&gt;https://preview.redd.it/7yxzag2k5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=49d662ddeb2b7fe0a5992c8a596c72449fcb404f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why would this help?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For data analysts, all tags and metrics will be created and shared at the semantic layer so there will be less confusion and higher efficiency.&lt;/p&gt;\n\n&lt;p&gt;For data users, they no longer need to create their own datasets or figure out which one is applicable for each scenario but can simply conduct queries on their specified tagset and metricset.&lt;/p&gt;\n\n&lt;h1&gt;Upgrade the Semantic Layer&lt;/h1&gt;\n\n&lt;p&gt;Explicitly defining the tags and metrics at the semantic layer was not enough. In order to build a standardized data processing system, our next goal was to ensure consistent definition of tags and metrics throughout the whole data processing pipeline.&lt;/p&gt;\n\n&lt;p&gt;For this sake, we made the semantic layer the heart of our data management system:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yitj349p5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1074b3ed4ac2f286d56ecb8a48ec9edcb2d8ba8f\"&gt;https://preview.redd.it/yitj349p5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1074b3ed4ac2f286d56ecb8a48ec9edcb2d8ba8f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How does it work?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;All computing logics in TDW will be defined at the semantic layer in the form of a single tag or metric.&lt;/p&gt;\n\n&lt;p&gt;The semantic layer receives logic queries from the application side, selects an engine accordingly, and generates SQL. Then it sends the SQL command to TDW for execution. Meanwhile, it might also send configuration and data ingestion tasks to Doris and decide which metrics and tags should be accelerated.&lt;/p&gt;\n\n&lt;p&gt;In this way, we have made the tags and metrics more manageable. A fly in the ointment is that since each tag and metric is individually defined, we are struggling with automating the generation of a valid SQL statement for the queries. If you have any idea about this, you are more than welcome to talk to us.&lt;/p&gt;\n\n&lt;h1&gt;Give Full Play to Apache Doris&lt;/h1&gt;\n\n&lt;p&gt;As you can see, Apache Doris has played a pivotal role in our solution. Optimizing the usage of Doris can largely improve our overall data processing efficiency. So in this part, we are going to share with you what we do with Doris to accelerate data ingestion and queries and reduce costs.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What We Want?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1s4n2nls5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6958bf9609e4b4d0b3b38f281e2b593029cf384b\"&gt;https://preview.redd.it/1s4n2nls5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6958bf9609e4b4d0b3b38f281e2b593029cf384b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Currently, we have 800+ tags and 1300+ metrics derived from the 80+ source tables in TDW.&lt;/p&gt;\n\n&lt;p&gt;When importing data from TDW to Doris, we hope to achieve:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Real-time availability:&lt;/strong&gt; In addition to the traditional T+1 offline data ingestion, we require real-time tagging.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Partial update&lt;/strong&gt;: Each source table generates data through its own ETL task at various paces and involves only part of the tags and metrics, so we require the support for partial update of columns.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt;: We need a response time of only a few seconds in group targeting, analysis and reporting scenarios.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Low costs&lt;/strong&gt;: We hope to reduce costs as much as possible.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What We Do?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Generate Flat Tables in Flink Instead of TDW&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/of8zcyyu5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ffeddbb65b3ec4cccbceb1c135ec5455b38a7050\"&gt;https://preview.redd.it/of8zcyyu5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ffeddbb65b3ec4cccbceb1c135ec5455b38a7050&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Generating flat tables in TDW has a few downsides:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;High storage cost&lt;/strong&gt;: TDW has to maintain an extra flat table apart from the discrete 80+ source tables. That\u2019s huge redundancy.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Low real-timeliness&lt;/strong&gt;: Any delay in the source tables will be augmented and retard the whole data link.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High development cost&lt;/strong&gt;: To achieve real-timeliness would require extra development efforts and resources.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;On the contrary, generating flat tables in Doris is much easier and less expensive. The process is as follows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use Spark to import new data into Kafka in an offline manner.&lt;/li&gt;\n&lt;li&gt;Use Flink to consume Kafka data.&lt;/li&gt;\n&lt;li&gt;Create a flat table via the primary key ID.&lt;/li&gt;\n&lt;li&gt;Import the flat table into Doris.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As is shown below, Flink has aggregated the five lines of data, of which \u201cID\u201d=1, into one line in Doris, reducing the data writing pressure on Doris.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qk6mr24x5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a41e122d846e4f6efc9c04de30b011c8497763bd\"&gt;https://preview.redd.it/qk6mr24x5xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a41e122d846e4f6efc9c04de30b011c8497763bd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This can largely reduce storage costs since TDW no long has to maintain two copies of data and KafKa only needs to store the new data pending for ingestion. What\u2019s more, we can add whatever ETL logic we want into Flink and reuse lots of development logic for offline and real-time data ingestion.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Name the Columns Smartly&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;As we mentioned, the Aggregate Model of Doris allows partial update of columns. Here we provide a simple introduction to other data models in Doris for your reference:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Unique Model&lt;/strong&gt;: This is applicable for scenarios requiring primary key uniqueness. It only keeps the latest data of the same primary key ID. (As far as we know, the Apache Doris community is planning to include partial update of columns in the Unique Model, too.)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Duplicate Model&lt;/strong&gt;: This model stores all original data exactly as it is without any pre-aggregation or deduplication.&lt;/p&gt;\n\n&lt;p&gt;After determining the data model, we had to think about how to name the columns. Using the tags or metrics as column names was not a choice because:&lt;/p&gt;\n\n&lt;p&gt;I. Our internal data users might need to rename the metrics or tags, but Doris 1.1.3 does not support modification of column names.&lt;/p&gt;\n\n&lt;p&gt;II. Tags might be taken online and offline frequently. If that involves the adding and dropping of columns, it will be not only time-consuming but also detrimental to query performance.&lt;/p&gt;\n\n&lt;p&gt;Instead, we do the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;For flexible renaming of tags and metrics&lt;/strong&gt;, we use MySQL tables to store the metadata (name, globally unique ID, status, etc.). Any change to the names will only happen in the metadata but will not affect the table schema in Doris. For example, if a &lt;code&gt;song_name&lt;/code&gt; is given an ID of 4, it will be stored with the column name of a4 in Doris. Then if the &lt;code&gt;song_name&lt;/code&gt; is involved in a query, it will be converted to a4 in SQL.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;For the onlining and offlining of tags&lt;/strong&gt;, we sort out the tags based on how frequently they are being used. The least used ones will be given an offline mark in their metadata. No new data will be put under the offline tags but the existing data under those tags will still be available.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;For real-time availability of newly added tags and metrics&lt;/strong&gt;, we prebuild a few ID columns in Doris tables based on the mapping of name IDs. These reserved ID columns will be allocated to the newly added tags and metrics. Thus, we can avoid table schema change and the consequent overheads. Our experience shows that only 10 minutes after the tags and metrics are added, the data under them can be available.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Noteworthily, the recently released Doris 1.2.0 supports Light Schema Change, which means that to add or remove columns, you only need to modify the metadata in FE. Also, you can rename the columns in data tables as long as you have enabled Light Schema Change for the tables. This is a big trouble saver for us.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Optimize Date Writing&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Here are a few practices that have reduced our daily offline data ingestion time by 75% and our CUMU compaction score from 600+ to 100.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Flink pre-aggregation: as is mentioned above.&lt;/li&gt;\n&lt;li&gt;Auto-sizing of writing batch: To reduce Flink resource usage, we enable the data in one Kafka Topic to be written into various Doris tables and realize the automatic alteration of batch size based on the data amount.&lt;/li&gt;\n&lt;li&gt;Optimization of Doris data writing: fine-tune the the sizes of tablets and buckets as well as the compaction parameters for each scenario:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;code&gt;max_XXXX_compaction_thread&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;max_cumulative_compaction_num_singleton_deltas&lt;/code&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Optimization of the BE commit logic: conduct regular caching of BE lists, commit them to the BE nodes batch by batch, and use finer load balancing granularity.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q5dqk7b76xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9d448a24018f450ba26dae50cb8fe2d815183267\"&gt;https://preview.redd.it/q5dqk7b76xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9d448a24018f450ba26dae50cb8fe2d815183267&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4. Use Dori-on-ES in Queries&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;About 60% of our data queries involve group targeting. Group targeting is to find our target data by using a set of tags as filters. It poses a few requirements for our data processing architecture:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Group targeting related to APP users can involve very complicated logic. That means the system must support hundreds of tags as filters simultaneously.&lt;/li&gt;\n&lt;li&gt;Most group targeting scenarios only require the latest tag data. However, metric queries need to support historical data.&lt;/li&gt;\n&lt;li&gt;Data users might need to perform further aggregated analysis of metric data after group targeting.&lt;/li&gt;\n&lt;li&gt;Data users might also need to perform detailed queries on tags and metrics after group targeting.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;After consideration, we decided to adopt Doris-on-ES. Doris is where we store the metric data for each scenario as a partition table, while Elasticsearch stores all tag data. The Doris-on-ES solution combines the distributed query planning capability of Doris and the full-text search capability of Elasticsearch. The query pattern is as follows:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SELECT tag, agg(metric)&lt;/code&gt;&lt;br/&gt;\n &lt;code&gt;FROM Doris&lt;/code&gt;&lt;br/&gt;\n &lt;code&gt;WHERE id in (select id from Es where tagFilter)&lt;/code&gt;&lt;br/&gt;\n &lt;code&gt;GROUP BY tag&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;As is shown, the ID data located in Elasticsearch will be used in the sub-query in Doris for metric analysis.&lt;/p&gt;\n\n&lt;p&gt;In practice, we find that the query response time is related to the size of the target group. If the target group contains over one million objects, the query will take up to 60 seconds. If it is even larger, a timeout error might occur.&lt;/p&gt;\n\n&lt;p&gt;After investigation, we identified our two biggest time wasters:&lt;/p&gt;\n\n&lt;p&gt;I. When Doris BE pulls data from Elasticsearch (1024 lines at a time by default), for a target group of over one million objects, the network I/O overhead can be huge.&lt;/p&gt;\n\n&lt;p&gt;II. After the data pulling, Doris BE needs to conduct Join operations with local metric tables via SHUFFLE/BROADCAST, which can cost a lot.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yhfjj9uh6xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=efbf60f8f864da8bded55c36089a6146dcab1251\"&gt;https://preview.redd.it/yhfjj9uh6xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=efbf60f8f864da8bded55c36089a6146dcab1251&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thus, we make the following optimizations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Add a query session variable &lt;code&gt;es_optimize&lt;/code&gt; that specifies whether to enable optimization.&lt;/li&gt;\n&lt;li&gt;In data writing into ES, add a BK column to store the bucket number after the primary key ID is hashed. The algorithm is the same as the bucketing algorithm in Doris (CRC32).&lt;/li&gt;\n&lt;li&gt;Use Doris BE to generate a Bucket Join execution plan, dispatch the bucket number to BE ScanNode and push it down to ES.&lt;/li&gt;\n&lt;li&gt;Use ES to compress the queried data; turn multiple data fetch into one and reduce network I/O overhead.&lt;/li&gt;\n&lt;li&gt;Make sure that Doris BE only pulls the data of buckets related to the local metric tables and conducts local Join operations directly to avoid data shuffling between Doris BEs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/or4nmhsk6xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=96666009eab7269ee818903d8f20df2a50ac7956\"&gt;https://preview.redd.it/or4nmhsk6xma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=96666009eab7269ee818903d8f20df2a50ac7956&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As a result, we reduce the query response time for large group targeting from 60 seconds to a surprising 3.7 seconds.&lt;/p&gt;\n\n&lt;p&gt;Community information shows that Doris is going to support inverted indexing since version 2.0.0, which is soon to be released. With this new version, we will be able to conduct full-text search on text types, equivalence or range filtering of texts, numbers, and datetime, and conveniently combine AND, OR, NOT logic in filtering since the inverted indexing supports array types. This new feature of Doris is expected to deliver 3~5 times better performance than Elasticsearch on the same task.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5. Refine the Management of Data&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Doris\u2019 capability of cold and hot data separation provides the foundation of our cost reduction strategies in data processing.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Based on the TTL mechanism of Doris, we only store data of the current year in Doris and put the historical data before that in TDW for lower storage cost.&lt;/li&gt;\n&lt;li&gt;We vary the numbers of copies for different data partitions. For example, we set three copies for data of the recent three months, which is used frequently, one copy for data older than six months, and two copies for data in between.&lt;/li&gt;\n&lt;li&gt;Doris supports turning hot data into cold data so we only store data of the past seven days in SSD and transfer data older than that to HDD for less expensive storage.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Conclusion&lt;/h1&gt;\n\n&lt;p&gt;Thank you for scrolling all the way down here and finishing this long read. We\u2019ve shared our cheers and tears, lessons learned, and a few practices that might be of some value to you during our transition from ClickHouse to Doris. We really appreciate the help from the Apache Doris community and the SelectDB team, but we might still be chasing them around for a while since we attempt to realize auto-identification of cold and hot data, pre-computation of frequently used tags/metrics, simplification of code logic using Materialized Views, and so on and so forth.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Aj2-clFYyOOfZWMZ3dMpfqJXhmJw58pb6qGbSDjWn6w.jpg?auto=webp&amp;v=enabled&amp;s=384b6b84b0fc6a77a25f550810e3390535e2627d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Aj2-clFYyOOfZWMZ3dMpfqJXhmJw58pb6qGbSDjWn6w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b6fc78f5d1de1da26bc7358bed0f2fad2a74236", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Aj2-clFYyOOfZWMZ3dMpfqJXhmJw58pb6qGbSDjWn6w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fcf57dadb661965001578b9222311e5a8f05064", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Aj2-clFYyOOfZWMZ3dMpfqJXhmJw58pb6qGbSDjWn6w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16f8e4551daba7d5cede8939b4691b300730d37e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Aj2-clFYyOOfZWMZ3dMpfqJXhmJw58pb6qGbSDjWn6w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc93db0d6969e18b2642db4979ccbc57d5ab2856", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Aj2-clFYyOOfZWMZ3dMpfqJXhmJw58pb6qGbSDjWn6w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f266db0c39ae4b317d488df9ba3bdb2318d5a59", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Aj2-clFYyOOfZWMZ3dMpfqJXhmJw58pb6qGbSDjWn6w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c93451827c716c6e47fc2c675dcf2cd7aa5633b", "width": 1080, "height": 540}], "variants": {}, "id": "-Os5QMyTpDQVBGQ2tysXs3cJbMldKtmp9X2iLv3Hy7I"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11nqc61", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nqc61/tencent_data_engineer_why_we_went_from_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nqc61/tencent_data_engineer_why_we_went_from_clickhouse/", "subreddit_subscribers": 92687, "created_utc": 1678457655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Without giving too many details about our company, we are a B2B SaaS product that has not prioritized our data catalog, as most is shared via Google Sheets or not at all. Hope some of you have some general ideas of where to start, as I know this is very vague. Thank you!", "author_fullname": "t2_gc5p1w37", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Our data catalog is difficult to manage and not built for the wider org - what can we do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nwrys", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 77, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 77, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678472871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Without giving too many details about our company, we are a B2B SaaS product that has not prioritized our data catalog, as most is shared via Google Sheets or not at all. Hope some of you have some general ideas of where to start, as I know this is very vague. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nwrys", "is_robot_indexable": true, "report_reasons": null, "author": "husky_misconception", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nwrys/our_data_catalog_is_difficult_to_manage_and_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nwrys/our_data_catalog_is_difficult_to_manage_and_not/", "subreddit_subscribers": 92687, "created_utc": 1678472871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working as a Data Engineer for more than a year now (since I graduated from university) and all of a sudden I can't find meaning in my job.\n\nI feel that all I'm doing is taking data from some data sources, transforming them and loading them in another data source (I'd be tempted to say that we have a data warehouse, but we simply don't).\n\nWhile I am in the midst of the action I seem to be enjoying myself, but when I attempt to make a connection between my job and how it does have an impact / contributes to something meaningful I feel desperate.\n\nI'm not completely sure if it's the sector that the company I work at is involved (user indoor location validation) or it's the job itself. When drawing comparisons to other software-related things I've done in the past (e.g. solving a Vehicle Routing Problem) I feel that what I'm currently doing has less of an impact.\n\nCould you share what it is in this profession (or hobby for some) that you find meaning at? Also, if someone at some point in their lives were at a similar situation, please do share your thoughts / what helped you keep going.\n\nNot sure if it's the right place to post, but I'm so immersed in it that even the process of gathering my thoughts, transforming them to a somewhat coherent text and posting here seemed like an ETL to me.\n\nThanks in advance!", "author_fullname": "t2_5fufo8h0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suddenly I can't find that much of a \"meaning\" in DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nuumt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678468378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working as a Data Engineer for more than a year now (since I graduated from university) and all of a sudden I can&amp;#39;t find meaning in my job.&lt;/p&gt;\n\n&lt;p&gt;I feel that all I&amp;#39;m doing is taking data from some data sources, transforming them and loading them in another data source (I&amp;#39;d be tempted to say that we have a data warehouse, but we simply don&amp;#39;t).&lt;/p&gt;\n\n&lt;p&gt;While I am in the midst of the action I seem to be enjoying myself, but when I attempt to make a connection between my job and how it does have an impact / contributes to something meaningful I feel desperate.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not completely sure if it&amp;#39;s the sector that the company I work at is involved (user indoor location validation) or it&amp;#39;s the job itself. When drawing comparisons to other software-related things I&amp;#39;ve done in the past (e.g. solving a Vehicle Routing Problem) I feel that what I&amp;#39;m currently doing has less of an impact.&lt;/p&gt;\n\n&lt;p&gt;Could you share what it is in this profession (or hobby for some) that you find meaning at? Also, if someone at some point in their lives were at a similar situation, please do share your thoughts / what helped you keep going.&lt;/p&gt;\n\n&lt;p&gt;Not sure if it&amp;#39;s the right place to post, but I&amp;#39;m so immersed in it that even the process of gathering my thoughts, transforming them to a somewhat coherent text and posting here seemed like an ETL to me.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11nuumt", "is_robot_indexable": true, "report_reasons": null, "author": "Smooth-Upstairs5640", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nuumt/suddenly_i_cant_find_that_much_of_a_meaning_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nuumt/suddenly_i_cant_find_that_much_of_a_meaning_in_de/", "subreddit_subscribers": 92687, "created_utc": 1678468378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to get a sense of what the market is looking to like.\n\nI got 3% bonus and 3% bonus, imo, it's not bad not good, so okay !!", "author_fullname": "t2_vtx6qjs2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much raise and bonus are you getting this year ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11npctm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678455239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to get a sense of what the market is looking to like.&lt;/p&gt;\n\n&lt;p&gt;I got 3% bonus and 3% bonus, imo, it&amp;#39;s not bad not good, so okay !!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11npctm", "is_robot_indexable": true, "report_reasons": null, "author": "Budget_Assignment457", "discussion_type": null, "num_comments": 71, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11npctm/how_much_raise_and_bonus_are_you_getting_this_year/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11npctm/how_much_raise_and_bonus_are_you_getting_this_year/", "subreddit_subscribers": 92687, "created_utc": 1678455239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm interested in possibly using dbt for a personal project that I'm working on, and to learn it just because it seems to be a valuable skills to have.\n\nIf you were starting from scratch, how would you recommend someone go about learning dbt? the docs, youtube, udemy, etc...", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to learn dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nwo53", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678472622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in possibly using dbt for a personal project that I&amp;#39;m working on, and to learn it just because it seems to be a valuable skills to have.&lt;/p&gt;\n\n&lt;p&gt;If you were starting from scratch, how would you recommend someone go about learning dbt? the docs, youtube, udemy, etc...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11nwo53", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nwo53/whats_the_best_way_to_learn_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nwo53/whats_the_best_way_to_learn_dbt/", "subreddit_subscribers": 92687, "created_utc": 1678472622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have just joined a company as a Junior Data Engineer, I have 3 years industry experience in software dev, systems administration and DevSecOps but this is my first DE role. To begin my company want me to complete the Google Professional Data Engineer cert. I would be able to dedicate my full working week each week to this. \n\nHow doable do you think this is and what sort of time frame should I be looking at? TIA", "author_fullname": "t2_3fxv004y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Professional Data Engineer Certification without prior data engineering experience - how possible is it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nnj4x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678450433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have just joined a company as a Junior Data Engineer, I have 3 years industry experience in software dev, systems administration and DevSecOps but this is my first DE role. To begin my company want me to complete the Google Professional Data Engineer cert. I would be able to dedicate my full working week each week to this. &lt;/p&gt;\n\n&lt;p&gt;How doable do you think this is and what sort of time frame should I be looking at? TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nnj4x", "is_robot_indexable": true, "report_reasons": null, "author": "J1010H", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nnj4x/google_professional_data_engineer_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nnj4x/google_professional_data_engineer_certification/", "subreddit_subscribers": 92687, "created_utc": 1678450433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a graduate student studying in Midwest about to graduate, and I am going to pick up my first data analyst job to build up my experience while also preparing myself to apply for data engineering job. One thing I know in my next career search is that I would like to move out of Midwest, I currently work in banking industry and have a passion for the finance/banking industry, and I know NY is probably the best place to be at, but part of me also don\u2019t think I would like it because of the crazy cost of living. In terms of the job search in data engineering, what state in the US do you think has the most opportunities? I am single and I have no family here, so I can move anywhere I want to.", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best location for data eng career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11obldn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678511925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a graduate student studying in Midwest about to graduate, and I am going to pick up my first data analyst job to build up my experience while also preparing myself to apply for data engineering job. One thing I know in my next career search is that I would like to move out of Midwest, I currently work in banking industry and have a passion for the finance/banking industry, and I know NY is probably the best place to be at, but part of me also don\u2019t think I would like it because of the crazy cost of living. In terms of the job search in data engineering, what state in the US do you think has the most opportunities? I am single and I have no family here, so I can move anywhere I want to.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11obldn", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11obldn/best_location_for_data_eng_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11obldn/best_location_for_data_eng_career/", "subreddit_subscribers": 92687, "created_utc": 1678511925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "well, today i was taken by surprise, that my team expects me to do a presentation about something relevant to our \"data world\", i would like some help to decide my subject, the presentation will be only to my team, so just people with some degree of knowledge in data engineering, the subject is free, the last guy spoke about some aws services and how some companies were using it, for now we only use google cloud, i was hoping some of you could suggest some cool techs and maybe use cases that you believe could give a hour of presentation + discussion, would be nice to be related to data warehouse, ETL/ELT and or BQ cost reduction but is not required, please help me to have a job after next friday haha", "author_fullname": "t2_r720n4nk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what could i talk about in my \"data talk\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nt18v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678464214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;well, today i was taken by surprise, that my team expects me to do a presentation about something relevant to our &amp;quot;data world&amp;quot;, i would like some help to decide my subject, the presentation will be only to my team, so just people with some degree of knowledge in data engineering, the subject is free, the last guy spoke about some aws services and how some companies were using it, for now we only use google cloud, i was hoping some of you could suggest some cool techs and maybe use cases that you believe could give a hour of presentation + discussion, would be nice to be related to data warehouse, ETL/ELT and or BQ cost reduction but is not required, please help me to have a job after next friday haha&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nt18v", "is_robot_indexable": true, "report_reasons": null, "author": "lost-without-hope", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11nt18v/what_could_i_talk_about_in_my_data_talk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nt18v/what_could_i_talk_about_in_my_data_talk/", "subreddit_subscribers": 92687, "created_utc": 1678464214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'd argue the main difference is their goals and, more specifically, the targets they serve:\n\n* data analysts' insights support humans to make decisions\n* data scientists' iterations improve algorithms to take decisions\n\nAnd supporting both of these roles as a data engineer is very similar:\n\n* facilitate the exploration of data \n* \"productionize\" the models they've designed (feeding viz or other tools (via rETL) or ML)\n* improve their workflows (collaborative tooling, CI/CD, ...)\n\nOnly the actual tools used change, mainly because they're still being seen as very separate verticals in the data tooling landscape.\n\n&amp;#x200B;\n\nWhat do you think??", "author_fullname": "t2_hizqfv2o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The difference between data scientists &amp; analysts and what it changes for data engineers...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nmrai", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678448246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d argue the main difference is their goals and, more specifically, the targets they serve:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;data analysts&amp;#39; insights support humans to make decisions&lt;/li&gt;\n&lt;li&gt;data scientists&amp;#39; iterations improve algorithms to take decisions&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;And supporting both of these roles as a data engineer is very similar:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;facilitate the exploration of data &lt;/li&gt;\n&lt;li&gt;&amp;quot;productionize&amp;quot; the models they&amp;#39;ve designed (feeding viz or other tools (via rETL) or ML)&lt;/li&gt;\n&lt;li&gt;improve their workflows (collaborative tooling, CI/CD, ...)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Only the actual tools used change, mainly because they&amp;#39;re still being seen as very separate verticals in the data tooling landscape.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What do you think??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11nmrai", "is_robot_indexable": true, "report_reasons": null, "author": "briceluu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nmrai/the_difference_between_data_scientists_analysts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nmrai/the_difference_between_data_scientists_analysts/", "subreddit_subscribers": 92687, "created_utc": 1678448246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I've started a new GCP project for a part-time client a few months ago and I had to build a lot of code to extract order information from Amazon.\n\nThey were in a bit of a \"red alert\" because their previous agency had left after doing a very bad job and their business was growing and they needed something to take control of their data and get better insights on how the business was going. When I started I was pretty much left with a GCP project containing a VM and a BigQuery data warehouse that was a mess.\n\nThe VM had a NiFi instance installed where the ETL was done but it kept crashing all the time, while the BigQuery data warehouse contained numerous datasets and tables like \"sales\\_1\", \"sales\\_2\", and \"orders\\_view1\" (which was actually a table not a view), \"orders-view2\" etc.\n\nThe problem was that their API tokens were expiring and they were hitting throttling limits for their endpoints (each endpoint has a different limit), so I ended up doing most of the Python coding work to get all the data that they needed and save it to BigQuery.\n\nBecause at some point they want to do ML and predictions using their data (but they don't have any idea, yet), I created a Cloud Storage bucket where I save all of the response data I get from doing the API requests, following the \"data lake\" principles, so that later we can process them with Spark or Databricks or something similar.\n\nHowever, apart from building new functionality I had to fix a lot of the old code that the previous agency left behind, views and scheduled query jobs that were failing, because most of them were with SELECT \\* and when we added new columns with data to certain tables, those jobs failed, etc.\n\nAnyway, I've ended up building an architecture which I feel is a bit of a Frankenstein, and I am looking for some help in finding, maybe, better services to run them on and a combination that will easily scale.\n\n[Bad architecture](https://preview.redd.it/mivsz45d6wma1.jpg?width=734&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=695d8d484046d7b40eeab24c416db1e998a9132a)\n\nI will explain below the purpose of each service:\n\n1. VM (a compute Engine VM where all the Python code that reads data from the API is stored). This won't really scale since we're not allowed to call the same endpoint multiple times from different services and we only download the data once every 24h. I plan to go down to 1h increments, but the \"problem\" is that the business just can't react as fast to hour-level changing data. And this is orders and sales information and they don't havea a sharp drop-off, so daily is fine for now. *(I would love to look into streaming one day, but idk if it's necessary or what the advantages for the business team would be, since they don't care &lt;right now&gt; about new data coming in much faster).*\n2. Firestore - most of the API endpoints have a parameter called \"last\\_updated\\_on\", which I use to only take in new data from when the previous API call was made. The \"previous\" and \"current\" timestamps for when I called the endpoints are stored in Firestore. Each endpoints has its own collection and a \"last\\_executed\" field in a document with metadata, from where I get this information and use it in my API call. Then, once the call is successfully completed, I overwrite the \"last\\_executed\" date with the timestamp of when I started my latest call. (**1a in my diagram**)\n3. PubSub - some API endpoints depend on data returned by previous endpoints. For example: ***Orders*** and ***OrderDetails*** are 2 different endpoints. ***Orders*** takes the \"last\\_updated\\_on\" parameter, whereas **OrderDetails** takes an OrderID, which I get from the response of ***Orders*** (I actually get all orders that were updated in the last 24 hours). I save all these OrderID's into PubSub, and call ***OrderDetails*** again, with each OrderID. (**1b in my diagram**)\n4. Cloud Storage - I save all of the JSON response files to Cloud Storage to also have the \"raw\" data I get from the API and so that I can also reference back to the raw data, if I find any strange things in the BigQuery data warehouse (sometimes it happens).\n5. BigQuery - where all the API data from the Amazon Service and other 3rd party Services are saved.\n\nEach of these API calls and ETL processing jobs are being triggered by Cron jobs inside the VM, fact which I find quite annoying. I was thinking of implementing Prefect, because even a Small Cloud Composer instance would 4x our spending bill, and there's not a lot of benefit on top.\n\nAll the data download and ETL processing is done inside the VM, which runs 24/7. I don't know if moving to Cloud Functions would necessarily be better (sadly never built one before, and I've extremely stretched with time in my day-to-day), and also I don't know how to build an architecture that would scale, tbh.\n\nI'm looking for any kind of advice that would help me figure out a better architecture, because I don't want the code debt to pile up too much along the way. I know the current architecture is quite \"cringy\", but I'm not sure which way to go next.\n\nImprovements I was thinking about, but don't know if it's a good idea:\n\n\\- I was thinking of Google Workflows to trigger my jobs, but I don't know what are the limitations of those\n\n\\- Move some of the code to Cloud Run, although the current implementation works and the costs / month aren't that high, since we're running a very small custom instance and saving an extra $15-$20 on a $30 per month bill is not worth the development effort, for now.\n\n\\- Keep the API download in the VM and move the ETL part in Databricks, since I know I can schedule, monitor, debug and make code changes to the ETL much faster there, if necessary. Also, job scheduling in Databricks is readily available so I don't have to do much there to make sure this runs fine.\n\n\\- Get rid of Firestore, because I feel I'm using it for the wrong reasons, but idk what other service I can use. Maybe Redis?\n\n\\- Change PubSub with something else, since Idk if the pattern I'm using is a \"publisher-subscriber\" pattern. I feel like it kind of is, but I'm not a very good software engineer, so I'm not sure.\n\nAnyway, any roast, feedback, idea or guidance is very much appreciated. Sadly I have nobody around me who to ask for feedback on this, since I am the only dev.", "author_fullname": "t2_v4yctwkv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please help me improve my architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mivsz45d6wma1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 84, "x": 108, "u": "https://preview.redd.it/mivsz45d6wma1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7f2f62ae135897d3c5456b51643e3926e6606cd"}, {"y": 168, "x": 216, "u": "https://preview.redd.it/mivsz45d6wma1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=873be1041ed552c25c96befc2fb4369e16c1b55a"}, {"y": 249, "x": 320, "u": "https://preview.redd.it/mivsz45d6wma1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c6712e3540d31df6067c79edb84934b147e0e32"}, {"y": 499, "x": 640, "u": "https://preview.redd.it/mivsz45d6wma1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2856e2125b5c8eb9ab007eab0646a35e27a0c5e7"}], "s": {"y": 573, "x": 734, "u": "https://preview.redd.it/mivsz45d6wma1.jpg?width=734&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=695d8d484046d7b40eeab24c416db1e998a9132a"}, "id": "mivsz45d6wma1"}}, "name": "t3_11nlsw1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jc7lfBbC__JCpVtUTVaXmVj4SqKQ515JvKh3XEaUwFU.jpg", "edited": 1678445742.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678445279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;ve started a new GCP project for a part-time client a few months ago and I had to build a lot of code to extract order information from Amazon.&lt;/p&gt;\n\n&lt;p&gt;They were in a bit of a &amp;quot;red alert&amp;quot; because their previous agency had left after doing a very bad job and their business was growing and they needed something to take control of their data and get better insights on how the business was going. When I started I was pretty much left with a GCP project containing a VM and a BigQuery data warehouse that was a mess.&lt;/p&gt;\n\n&lt;p&gt;The VM had a NiFi instance installed where the ETL was done but it kept crashing all the time, while the BigQuery data warehouse contained numerous datasets and tables like &amp;quot;sales_1&amp;quot;, &amp;quot;sales_2&amp;quot;, and &amp;quot;orders_view1&amp;quot; (which was actually a table not a view), &amp;quot;orders-view2&amp;quot; etc.&lt;/p&gt;\n\n&lt;p&gt;The problem was that their API tokens were expiring and they were hitting throttling limits for their endpoints (each endpoint has a different limit), so I ended up doing most of the Python coding work to get all the data that they needed and save it to BigQuery.&lt;/p&gt;\n\n&lt;p&gt;Because at some point they want to do ML and predictions using their data (but they don&amp;#39;t have any idea, yet), I created a Cloud Storage bucket where I save all of the response data I get from doing the API requests, following the &amp;quot;data lake&amp;quot; principles, so that later we can process them with Spark or Databricks or something similar.&lt;/p&gt;\n\n&lt;p&gt;However, apart from building new functionality I had to fix a lot of the old code that the previous agency left behind, views and scheduled query jobs that were failing, because most of them were with SELECT * and when we added new columns with data to certain tables, those jobs failed, etc.&lt;/p&gt;\n\n&lt;p&gt;Anyway, I&amp;#39;ve ended up building an architecture which I feel is a bit of a Frankenstein, and I am looking for some help in finding, maybe, better services to run them on and a combination that will easily scale.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mivsz45d6wma1.jpg?width=734&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=695d8d484046d7b40eeab24c416db1e998a9132a\"&gt;Bad architecture&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I will explain below the purpose of each service:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;VM (a compute Engine VM where all the Python code that reads data from the API is stored). This won&amp;#39;t really scale since we&amp;#39;re not allowed to call the same endpoint multiple times from different services and we only download the data once every 24h. I plan to go down to 1h increments, but the &amp;quot;problem&amp;quot; is that the business just can&amp;#39;t react as fast to hour-level changing data. And this is orders and sales information and they don&amp;#39;t havea a sharp drop-off, so daily is fine for now. &lt;em&gt;(I would love to look into streaming one day, but idk if it&amp;#39;s necessary or what the advantages for the business team would be, since they don&amp;#39;t care &amp;lt;right now&amp;gt; about new data coming in much faster).&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Firestore - most of the API endpoints have a parameter called &amp;quot;last_updated_on&amp;quot;, which I use to only take in new data from when the previous API call was made. The &amp;quot;previous&amp;quot; and &amp;quot;current&amp;quot; timestamps for when I called the endpoints are stored in Firestore. Each endpoints has its own collection and a &amp;quot;last_executed&amp;quot; field in a document with metadata, from where I get this information and use it in my API call. Then, once the call is successfully completed, I overwrite the &amp;quot;last_executed&amp;quot; date with the timestamp of when I started my latest call. (&lt;strong&gt;1a in my diagram&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;PubSub - some API endpoints depend on data returned by previous endpoints. For example: &lt;strong&gt;&lt;em&gt;Orders&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;OrderDetails&lt;/em&gt;&lt;/strong&gt; are 2 different endpoints. &lt;strong&gt;&lt;em&gt;Orders&lt;/em&gt;&lt;/strong&gt; takes the &amp;quot;last_updated_on&amp;quot; parameter, whereas &lt;strong&gt;OrderDetails&lt;/strong&gt; takes an OrderID, which I get from the response of &lt;strong&gt;&lt;em&gt;Orders&lt;/em&gt;&lt;/strong&gt; (I actually get all orders that were updated in the last 24 hours). I save all these OrderID&amp;#39;s into PubSub, and call &lt;strong&gt;&lt;em&gt;OrderDetails&lt;/em&gt;&lt;/strong&gt; again, with each OrderID. (&lt;strong&gt;1b in my diagram&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;Cloud Storage - I save all of the JSON response files to Cloud Storage to also have the &amp;quot;raw&amp;quot; data I get from the API and so that I can also reference back to the raw data, if I find any strange things in the BigQuery data warehouse (sometimes it happens).&lt;/li&gt;\n&lt;li&gt;BigQuery - where all the API data from the Amazon Service and other 3rd party Services are saved.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Each of these API calls and ETL processing jobs are being triggered by Cron jobs inside the VM, fact which I find quite annoying. I was thinking of implementing Prefect, because even a Small Cloud Composer instance would 4x our spending bill, and there&amp;#39;s not a lot of benefit on top.&lt;/p&gt;\n\n&lt;p&gt;All the data download and ETL processing is done inside the VM, which runs 24/7. I don&amp;#39;t know if moving to Cloud Functions would necessarily be better (sadly never built one before, and I&amp;#39;ve extremely stretched with time in my day-to-day), and also I don&amp;#39;t know how to build an architecture that would scale, tbh.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for any kind of advice that would help me figure out a better architecture, because I don&amp;#39;t want the code debt to pile up too much along the way. I know the current architecture is quite &amp;quot;cringy&amp;quot;, but I&amp;#39;m not sure which way to go next.&lt;/p&gt;\n\n&lt;p&gt;Improvements I was thinking about, but don&amp;#39;t know if it&amp;#39;s a good idea:&lt;/p&gt;\n\n&lt;p&gt;- I was thinking of Google Workflows to trigger my jobs, but I don&amp;#39;t know what are the limitations of those&lt;/p&gt;\n\n&lt;p&gt;- Move some of the code to Cloud Run, although the current implementation works and the costs / month aren&amp;#39;t that high, since we&amp;#39;re running a very small custom instance and saving an extra $15-$20 on a $30 per month bill is not worth the development effort, for now.&lt;/p&gt;\n\n&lt;p&gt;- Keep the API download in the VM and move the ETL part in Databricks, since I know I can schedule, monitor, debug and make code changes to the ETL much faster there, if necessary. Also, job scheduling in Databricks is readily available so I don&amp;#39;t have to do much there to make sure this runs fine.&lt;/p&gt;\n\n&lt;p&gt;- Get rid of Firestore, because I feel I&amp;#39;m using it for the wrong reasons, but idk what other service I can use. Maybe Redis?&lt;/p&gt;\n\n&lt;p&gt;- Change PubSub with something else, since Idk if the pattern I&amp;#39;m using is a &amp;quot;publisher-subscriber&amp;quot; pattern. I feel like it kind of is, but I&amp;#39;m not a very good software engineer, so I&amp;#39;m not sure.&lt;/p&gt;\n\n&lt;p&gt;Anyway, any roast, feedback, idea or guidance is very much appreciated. Sadly I have nobody around me who to ask for feedback on this, since I am the only dev.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nlsw1", "is_robot_indexable": true, "report_reasons": null, "author": "jack-in-the-sack", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nlsw1/please_help_me_improve_my_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nlsw1/please_help_me_improve_my_architecture/", "subreddit_subscribers": 92687, "created_utc": 1678445279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, we are trying to cut cost in our company. We are trying to reduce the number of nodes in our cluster.\n\nWe have decided on only keeping the recent data that would be 6 months and deleting all the records before that.\n\n  \nI want to develop an efficient solution or architecture to implement this feature. I am thinking of designing a script using python.\n\n&amp;#x200B;\n\nI have thought of two solutions :\n\n* Getting a data range and create a date list and delete data on day by day basis and at the end running a vaccum and analyze.\n* Moving all the required records to a new table and dropping the table.\n\n&amp;#x200B;\n\nOther Noes:\n\n* Table size is around 40gb and 40M records.\n* Daily elt jobs are running which sync the tables, so putting a halt on the etl jobs for the specific table would be a good idea or the delete command won't hinder the upsert on the table.", "author_fullname": "t2_l38csc3b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deleting data efficiently from Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nn64i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678449444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, we are trying to cut cost in our company. We are trying to reduce the number of nodes in our cluster.&lt;/p&gt;\n\n&lt;p&gt;We have decided on only keeping the recent data that would be 6 months and deleting all the records before that.&lt;/p&gt;\n\n&lt;p&gt;I want to develop an efficient solution or architecture to implement this feature. I am thinking of designing a script using python.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have thought of two solutions :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Getting a data range and create a date list and delete data on day by day basis and at the end running a vaccum and analyze.&lt;/li&gt;\n&lt;li&gt;Moving all the required records to a new table and dropping the table.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Other Noes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Table size is around 40gb and 40M records.&lt;/li&gt;\n&lt;li&gt;Daily elt jobs are running which sync the tables, so putting a halt on the etl jobs for the specific table would be a good idea or the delete command won&amp;#39;t hinder the upsert on the table.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nn64i", "is_robot_indexable": true, "report_reasons": null, "author": "AdSure744", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nn64i/deleting_data_efficiently_from_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nn64i/deleting_data_efficiently_from_redshift/", "subreddit_subscribers": 92687, "created_utc": 1678449444.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "They are written in rust so are very performant but how real is that they will be future of big data engineering/science?", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Apache Arrow DataFusion and Ballista the future of big data engineering/science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11od6q9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678517459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They are written in rust so are very performant but how real is that they will be future of big data engineering/science?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11od6q9", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11od6q9/is_apache_arrow_datafusion_and_ballista_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11od6q9/is_apache_arrow_datafusion_and_ballista_the/", "subreddit_subscribers": 92687, "created_utc": 1678517459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My firm is moving away from a database/platform I'm pretty familiar with and highly encouraging my team learn kdb+/q. Learning a new technology is always a bit of a challenge for me without a fair amount of structure. Where should I get started? Are there good lesson plans/video series I can follow? I see this as an opportunity to get ahead at my firm and I'd really like to got the ground running. We plan on serving very low latency internal clients if that helps point in the right direction. \n\nI'm very familiar with python, sql, JavaScript, bash if that provides any use.", "author_fullname": "t2_44l31", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to approach learning kdb/q?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nx5m7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678473740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My firm is moving away from a database/platform I&amp;#39;m pretty familiar with and highly encouraging my team learn kdb+/q. Learning a new technology is always a bit of a challenge for me without a fair amount of structure. Where should I get started? Are there good lesson plans/video series I can follow? I see this as an opportunity to get ahead at my firm and I&amp;#39;d really like to got the ground running. We plan on serving very low latency internal clients if that helps point in the right direction. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m very familiar with python, sql, JavaScript, bash if that provides any use.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nx5m7", "is_robot_indexable": true, "report_reasons": null, "author": "uhndeyha", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nx5m7/best_way_to_approach_learning_kdbq/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nx5m7/best_way_to_approach_learning_kdbq/", "subreddit_subscribers": 92687, "created_utc": 1678473740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is looking at Reverse ETL tools. What is the best on the market and most potential long term?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reverse ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nt2mj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678464302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is looking at Reverse ETL tools. What is the best on the market and most potential long term?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11nt2mj", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nt2mj/reverse_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nt2mj/reverse_etl/", "subreddit_subscribers": 92687, "created_utc": 1678464302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi data engeneers!\n\nI'm trying to analyse some adf pipelines performance, do you have some log analytics helpful for this work? Or some interesting queries to have in the box for future monitoring?\n\nThanks in advance!\n\nSergi", "author_fullname": "t2_3ientplx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory - Log Analytics Queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11ofs0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678526723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi data engeneers!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to analyse some adf pipelines performance, do you have some log analytics helpful for this work? Or some interesting queries to have in the box for future monitoring?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;Sergi&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11ofs0t", "is_robot_indexable": true, "report_reasons": null, "author": "f1se4", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ofs0t/azure_data_factory_log_analytics_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ofs0t/azure_data_factory_log_analytics_queries/", "subreddit_subscribers": 92687, "created_utc": 1678526723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI\u2019m aware of PyIceberg, however if I understand correctly it can only be used on existing iceberg tables.\n\nIs there a way to take a file in a dataframe or duckdb table, or anything else in Python and write it out to an Iceberg table?\n\nIn the same though process, it it possible to upsert the contents of an Iceberg table with Python?\n\nI\u2019m fairly new to it so may have missed something obvious, but would like some organisation (and ACID compliance) within my s3 lake and can\u2019t get sign-off to set up a Spark cluster or Trinio or anything like that.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Create/write Iceberg with Python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11of9jr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678524871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m aware of PyIceberg, however if I understand correctly it can only be used on existing iceberg tables.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to take a file in a dataframe or duckdb table, or anything else in Python and write it out to an Iceberg table?&lt;/p&gt;\n\n&lt;p&gt;In the same though process, it it possible to upsert the contents of an Iceberg table with Python?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m fairly new to it so may have missed something obvious, but would like some organisation (and ACID compliance) within my s3 lake and can\u2019t get sign-off to set up a Spark cluster or Trinio or anything like that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11of9jr", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11of9jr/createwrite_iceberg_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11of9jr/createwrite_iceberg_with_python/", "subreddit_subscribers": 92687, "created_utc": 1678524871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any advantage in having both DS and DE skills together?", "author_fullname": "t2_m7gb996u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I also learn Data Science while learning DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11odfkf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678518307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any advantage in having both DS and DE skills together?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11odfkf", "is_robot_indexable": true, "report_reasons": null, "author": "RNGesusDoesntLoveMe", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11odfkf/should_i_also_learn_data_science_while_learning_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11odfkf/should_i_also_learn_data_science_while_learning_de/", "subreddit_subscribers": 92687, "created_utc": 1678518307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Engineers,\n\n&amp;#x200B;\n\nI'm fairly new to this software field and I'm from a non-cs background so I don't have a lot of experience in working in the backend or building data pipelines with different tools. My main objective is to build a self-service kind of thing to ingest CSV(mostly) files from GCS buckets to snowflake(Data Warehouse). \n\n&amp;#x200B;\n\nCSV file in GCS bucket ---&gt; Table with same column names, data types, and data in snowflake(vaguely)\n\n&amp;#x200B;\n\nThese are the steps and solutions our team came up with so far:\n\n1. Table schema translation from source CSV files to final tables(like CREATE TABLE command)\n   1. Config files that contain the table definition like column names and data types provided by the customer\n   2. Terraform\n   3. Kafka\n   4. Python\n2. How to create internal STAGE in Snowflake\n   1. Python\n   2. Kafka\n   3. Stored procedures/tasks\n3. Data transfer from the external stage(GCS buckets) to the internal stage(Can this be done dynamically?)\n   1. Snowpipe\n   2. SF tasks\n   3. Python\n4. Data merging or insertion into the final table(Truncate/Load or Insert/Update/Delete)\n   1. Python\n   2. Config files\n   3. Stored procedures/tasks\n\nI know that I'm looking for a very specific solution for a specific problem but if you have done this type of work before or have any high-level suggestions they'll be greatly appreciated. I'm open to any open-source software\n\n&amp;#x200B;\n\nPS: I haven't worked with most of the tools above, the above steps are just a result of a brainstorming session. I know Snowflake and Python but I'll learn and try out any of the suggestions given. If it comes across as desperate that's because I am. Please help me.", "author_fullname": "t2_2yutl8h5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automating Data Ingestion from GCS buckets to Snowflake(Data Warehouse)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11od75p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678517499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Engineers,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fairly new to this software field and I&amp;#39;m from a non-cs background so I don&amp;#39;t have a lot of experience in working in the backend or building data pipelines with different tools. My main objective is to build a self-service kind of thing to ingest CSV(mostly) files from GCS buckets to snowflake(Data Warehouse). &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;CSV file in GCS bucket ---&amp;gt; Table with same column names, data types, and data in snowflake(vaguely)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;These are the steps and solutions our team came up with so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Table schema translation from source CSV files to final tables(like CREATE TABLE command)\n\n&lt;ol&gt;\n&lt;li&gt;Config files that contain the table definition like column names and data types provided by the customer&lt;/li&gt;\n&lt;li&gt;Terraform&lt;/li&gt;\n&lt;li&gt;Kafka&lt;/li&gt;\n&lt;li&gt;Python&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;How to create internal STAGE in Snowflake\n\n&lt;ol&gt;\n&lt;li&gt;Python&lt;/li&gt;\n&lt;li&gt;Kafka&lt;/li&gt;\n&lt;li&gt;Stored procedures/tasks&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;Data transfer from the external stage(GCS buckets) to the internal stage(Can this be done dynamically?)\n\n&lt;ol&gt;\n&lt;li&gt;Snowpipe&lt;/li&gt;\n&lt;li&gt;SF tasks&lt;/li&gt;\n&lt;li&gt;Python&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;Data merging or insertion into the final table(Truncate/Load or Insert/Update/Delete)\n\n&lt;ol&gt;\n&lt;li&gt;Python&lt;/li&gt;\n&lt;li&gt;Config files&lt;/li&gt;\n&lt;li&gt;Stored procedures/tasks&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I know that I&amp;#39;m looking for a very specific solution for a specific problem but if you have done this type of work before or have any high-level suggestions they&amp;#39;ll be greatly appreciated. I&amp;#39;m open to any open-source software&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;PS: I haven&amp;#39;t worked with most of the tools above, the above steps are just a result of a brainstorming session. I know Snowflake and Python but I&amp;#39;ll learn and try out any of the suggestions given. If it comes across as desperate that&amp;#39;s because I am. Please help me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11od75p", "is_robot_indexable": true, "report_reasons": null, "author": "vgowthamvk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11od75p/automating_data_ingestion_from_gcs_buckets_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11od75p/automating_data_ingestion_from_gcs_buckets_to/", "subreddit_subscribers": 92687, "created_utc": 1678517499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have searched everywhere but I am really not sure if  it\u2019s a presales role or Technical one. Can anyone help me with your interview process if you had a similar experience. Any help is appreciated", "author_fullname": "t2_4nl1hv1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have gotten a interview with databrics field engineering team SA role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11oci3y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678515012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have searched everywhere but I am really not sure if  it\u2019s a presales role or Technical one. Can anyone help me with your interview process if you had a similar experience. Any help is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11oci3y", "is_robot_indexable": true, "report_reasons": null, "author": "ChickenOk7367", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11oci3y/i_have_gotten_a_interview_with_databrics_field/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11oci3y/i_have_gotten_a_interview_with_databrics_field/", "subreddit_subscribers": 92687, "created_utc": 1678515012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_psxnzxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inquery: An open-source tool for making safe SQL updates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_11o1vty", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/s_2os-jybU0w1avEPiwgOjgmkt82LcovumlFL55MGMA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678484845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/inqueryio/inquery", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3cNtz-PUCKdO8wiDUXSuLa1gDQeKdHXYuAxjcCYnewI.jpg?auto=webp&amp;v=enabled&amp;s=9550aa01e53ed1a3d16a74b9b7e562d3f27047a2", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/3cNtz-PUCKdO8wiDUXSuLa1gDQeKdHXYuAxjcCYnewI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c194597ed388f02b94b6a75ecd2411189216280", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/3cNtz-PUCKdO8wiDUXSuLa1gDQeKdHXYuAxjcCYnewI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68d18c6b15f5a78c0f1dfbc7c4171d62914d93f0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/3cNtz-PUCKdO8wiDUXSuLa1gDQeKdHXYuAxjcCYnewI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a19a0b96e34e5a87d10ea32ed81bf130b3dcdd42", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/3cNtz-PUCKdO8wiDUXSuLa1gDQeKdHXYuAxjcCYnewI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d36d9a298c106cfd2eb109a9f3f3e90c409fc42", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/3cNtz-PUCKdO8wiDUXSuLa1gDQeKdHXYuAxjcCYnewI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3544cac61f3015ac62c5021d07860847a9e150af", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/3cNtz-PUCKdO8wiDUXSuLa1gDQeKdHXYuAxjcCYnewI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd093929726f08181d8a702e715ab8b6d2fd12b5", "width": 1080, "height": 540}], "variants": {}, "id": "qy690939jTx-esdKhZ-eZVVCcSYEc4kI3cMS0pHl2Ok"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11o1vty", "is_robot_indexable": true, "report_reasons": null, "author": "PM_ME_YOUR_JAVA", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11o1vty/inquery_an_opensource_tool_for_making_safe_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/inqueryio/inquery", "subreddit_subscribers": 92687, "created_utc": 1678484845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! \nWe have an on-prem infra and really need a tool for data governance, lineage and catalog. I would like to present a solution for our company. We have a diversified architecture, we store data into SQL server and HDFS. We use SQL server agent, Kafka and Talend to move data. Poweri Bi and Qlik are being used to store reports. \nI was going to propose OpenMetaData since it\u2019s quite valid as candidate, but we recently began meetings to buy a licensed version of Cloudera that will take care of Hadoop and Kafka other than bringing new stuff. \nCloudera comes with Apache Atlas and it seems convenient enough at this point. \n\nBut I\u2019m wondering if OpenMetaData is valid enough to outshine Atlas in our use case. \n\nDo you have any suggestions/observation? Should I stick to my original proposal? I think yes, but don\u2019t have enough hands on experience with atlas to judge. \n\nThanks a lot for every suggestion!", "author_fullname": "t2_d0ifg2cb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Atlas or OpenMetaData?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nxelq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678474329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! \nWe have an on-prem infra and really need a tool for data governance, lineage and catalog. I would like to present a solution for our company. We have a diversified architecture, we store data into SQL server and HDFS. We use SQL server agent, Kafka and Talend to move data. Poweri Bi and Qlik are being used to store reports. \nI was going to propose OpenMetaData since it\u2019s quite valid as candidate, but we recently began meetings to buy a licensed version of Cloudera that will take care of Hadoop and Kafka other than bringing new stuff. \nCloudera comes with Apache Atlas and it seems convenient enough at this point. &lt;/p&gt;\n\n&lt;p&gt;But I\u2019m wondering if OpenMetaData is valid enough to outshine Atlas in our use case. &lt;/p&gt;\n\n&lt;p&gt;Do you have any suggestions/observation? Should I stick to my original proposal? I think yes, but don\u2019t have enough hands on experience with atlas to judge. &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for every suggestion!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nxelq", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward-Cupcake6219", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nxelq/apache_atlas_or_openmetadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nxelq/apache_atlas_or_openmetadata/", "subreddit_subscribers": 92687, "created_utc": 1678474329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, is there any good alternative for azure data explorer?\n\nAzure Data Explorer is a big data platform optimized for analytical queries. It is used to store and do analysis on streaming data like logs, clicks stream and telemetry in near real time.", "author_fullname": "t2_pp0zirow", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Explorer alternative?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nxbu4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678474157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, is there any good alternative for azure data explorer?&lt;/p&gt;\n\n&lt;p&gt;Azure Data Explorer is a big data platform optimized for analytical queries. It is used to store and do analysis on streaming data like logs, clicks stream and telemetry in near real time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11nxbu4", "is_robot_indexable": true, "report_reasons": null, "author": "These_Rip_9327", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nxbu4/azure_data_explorer_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nxbu4/azure_data_explorer_alternative/", "subreddit_subscribers": 92687, "created_utc": 1678474157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI am a big data engineer. I am new to banking domain. I primarily work with GCP &amp; Azure cloud. I need to understand few things about banking process:\n\n1. Sometime we have to replicate production issues in our dev or staging env. What is the best way to use production data or replicate prod data in lower env?\n2. Currently,I am using Azure data stack. for data quality we are using SQL &amp; databricks code to handle missing/null values, domain logic, duplicate records. is there any better way to do it? I mean any special azure of google service to ensure data quality?\n\nI hope to get some fruitful answers. \n\nThank you!!", "author_fullname": "t2_a4t6qg8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HELP POST!! Big Data Engineering - Banking Domain", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nvete", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678469703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I am a big data engineer. I am new to banking domain. I primarily work with GCP &amp;amp; Azure cloud. I need to understand few things about banking process:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Sometime we have to replicate production issues in our dev or staging env. What is the best way to use production data or replicate prod data in lower env?&lt;/li&gt;\n&lt;li&gt;Currently,I am using Azure data stack. for data quality we are using SQL &amp;amp; databricks code to handle missing/null values, domain logic, duplicate records. is there any better way to do it? I mean any special azure of google service to ensure data quality?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I hope to get some fruitful answers. &lt;/p&gt;\n\n&lt;p&gt;Thank you!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11nvete", "is_robot_indexable": true, "report_reasons": null, "author": "AutomaticMorning2095", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nvete/help_post_big_data_engineering_banking_domain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nvete/help_post_big_data_engineering_banking_domain/", "subreddit_subscribers": 92687, "created_utc": 1678469703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hey all, I'm wondering if anyone familiar with Azure data factory can help me figure this out. In SSIS I use the \"data conversion transformation\" task. It might be staring me right in the face, but I can't find the equivalent task in azure data factory. Anyone know?", "author_fullname": "t2_ubo6r40p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory equivalent to SSIS 'data conversion transformation' task?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nuq1z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678468087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;m wondering if anyone familiar with Azure data factory can help me figure this out. In SSIS I use the &amp;quot;data conversion transformation&amp;quot; task. It might be staring me right in the face, but I can&amp;#39;t find the equivalent task in azure data factory. Anyone know?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nuq1z", "is_robot_indexable": true, "report_reasons": null, "author": "Successful_Coconut42", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nuq1z/azure_data_factory_equivalent_to_ssis_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nuq1z/azure_data_factory_equivalent_to_ssis_data/", "subreddit_subscribers": 92687, "created_utc": 1678468087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I was a Java/SQL developer for 6 years (total 12 years of experience), then started working on a proprietary low code ETL framework (columnar NoSQL database).  It has been 6 years, and I worked on two big projects with the same product. I also had to do a lot of Java coding in these last 6 years (mainly for process orchestration, custom control checks, preprocessing as ETL involves loading flat files received over SFTP)\n\nRecently passed Google Certified Professional Data Engineer exam. My current role is kind of independent contributor, which involves end to end delivery (building interfaces to extract data, transformation logic using low code platform, pre/post processing using Java/Shell scripting, technical design, BI development).\n\nI am trying for various DE roles (GCP/Azure Cloud data engineering and Hadoop/Spark). To prepare for this, I cleared the Google exam and completed a few courses like Azure Data Engineering, PySpark, Hadoop, and Spark Developer courses. \n\nI am really passionate about data roles, but I believe my profile is not that attractive for recruiters to get an interview chance. Only a couple of interview calls after hundreds of job applications. \n\nLooks like Azure is the market leader in the city where I am living. My next plan is to try to do a personal project in GCP or Azure to gain more hands-on experience. Do I need to do anything else?", "author_fullname": "t2_idc7jz7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Low code ETL platform/Java developer trying for DE roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nt7aa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678464597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I was a Java/SQL developer for 6 years (total 12 years of experience), then started working on a proprietary low code ETL framework (columnar NoSQL database).  It has been 6 years, and I worked on two big projects with the same product. I also had to do a lot of Java coding in these last 6 years (mainly for process orchestration, custom control checks, preprocessing as ETL involves loading flat files received over SFTP)&lt;/p&gt;\n\n&lt;p&gt;Recently passed Google Certified Professional Data Engineer exam. My current role is kind of independent contributor, which involves end to end delivery (building interfaces to extract data, transformation logic using low code platform, pre/post processing using Java/Shell scripting, technical design, BI development).&lt;/p&gt;\n\n&lt;p&gt;I am trying for various DE roles (GCP/Azure Cloud data engineering and Hadoop/Spark). To prepare for this, I cleared the Google exam and completed a few courses like Azure Data Engineering, PySpark, Hadoop, and Spark Developer courses. &lt;/p&gt;\n\n&lt;p&gt;I am really passionate about data roles, but I believe my profile is not that attractive for recruiters to get an interview chance. Only a couple of interview calls after hundreds of job applications. &lt;/p&gt;\n\n&lt;p&gt;Looks like Azure is the market leader in the city where I am living. My next plan is to try to do a personal project in GCP or Azure to gain more hands-on experience. Do I need to do anything else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11nt7aa", "is_robot_indexable": true, "report_reasons": null, "author": "Mindless_Piglet_9478", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nt7aa/low_code_etl_platformjava_developer_trying_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nt7aa/low_code_etl_platformjava_developer_trying_for_de/", "subreddit_subscribers": 92687, "created_utc": 1678464597.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}