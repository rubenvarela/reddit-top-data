{"kind": "Listing", "data": {"after": "t3_11q7abl", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideal Data Stack - architecture , whats your view?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_11q6ouz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 88, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 88, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/QI_7VI-4DQwMtrhpCBz3HNFMf-ECuTQb8C1xc9Ujw30.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678705560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/q5mn1rnwnhna1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/q5mn1rnwnhna1.png?auto=webp&amp;v=enabled&amp;s=e70a4e82570a8ab246caf255a5c46a464378b27c", "width": 958, "height": 637}, "resolutions": [{"url": "https://preview.redd.it/q5mn1rnwnhna1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a6d7e5a3f56746c8e4e4eb49d7cef7acdca5ae00", "width": 108, "height": 71}, {"url": "https://preview.redd.it/q5mn1rnwnhna1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7400ac5c140aef28f96db747d70a52275002beb1", "width": 216, "height": 143}, {"url": "https://preview.redd.it/q5mn1rnwnhna1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=400939337922ab91ad10a36ac79d86f4eb73c6fc", "width": 320, "height": 212}, {"url": "https://preview.redd.it/q5mn1rnwnhna1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b687883c72e4e6d1c39be3bc9d6cd799648bd0a6", "width": 640, "height": 425}], "variants": {}, "id": "NvVJXWUoSBiU0xfFu6vTDsb_QZM2Onm7i8vjiOSW1mQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11q6ouz", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11q6ouz/ideal_data_stack_architecture_whats_your_view/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/q5mn1rnwnhna1.png", "subreddit_subscribers": 92976, "created_utc": 1678705560.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks,I am researching the challenges of data engineering to develop a deeper understanding of the problems and challenges faced by the teams building data products.\n\nI used to be a data engineer between 2009 and 2014. In 2014, I got into product management and I am currently in a product lead role. I have always worked products which included a large amount of analytics, insights, predictive models, machine learning in software companies serving healthcare, surveillance, automotive, and ecommerce.\n\nIn my experience over the past 14 years, the data tooling ecosystem has expanded a lot.However, I am still in a scenario where the cost of data infrastructure and tooling is expensive.\n\n* Projects are complex and long drawn.\n* It is super hard to solve basic issues of data quality even reactively.\n* Maintaining trust in the data assets is really hard.\n* Data literacy of decision makers is not up to the mark in a lot of cases.\n* Stakeholders expect miracles and stuff to just work.\n* Very few people can explain the attributes, the calculations, the metrics, the insights, and the implications.\n* Everyone is just promising stuff and punting the inevitable reality of face the hard problem and solve it properly.\n\nI am just trying to research and gather inputs from the community on the nagging challenges of building products now to inform my product development and to inform a course that I am building to develop data product managers (because it is really difficult to find candidates to hire)\n\nMy question for you is:\n\n* What are your top 3 challenges in engineering data flows and pipelines?\n   * Is it the data inventory, quality, governance, accessibility, etc.?\n   * Is the infrastructure, the complexity of building, deploying, administering the systems?\n   * Is it the challenge of organizational structure, talent, capacity, leadership?\n   * Is it communication, silos, lack of alignment?\n   * Is it cost, performance, complexity of infrastructure?\n* What is preventing you from building valuable data products?\n\nFor me, infrastructure cost, performance of existing tools, spaghetti code, lack of data expertise among leadership stakeholders has been the biggest headwinds to progress.\n\nLast year, at one point, our AWS costs were $1.6 for every $1 a customer paid us. After working on a year and reducing substantial tech debt, we got to AWS cost of $0.6 for every $1 revenue. Still, there is no recognition, leadership is reluctant to fix data quality issues.\n\nAs a product lead, I have been able to influence some, but it's a lot of compounding challenges.\n\nDoes this resonate with folks?\n\nWhat are the top challenges you are facing?\n\nWhat are some solutions or workaround that have worked for you?\n\nLooking forward to your responses.", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some of the sticky problems in your data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qpfwo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678762283.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678749600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,I am researching the challenges of data engineering to develop a deeper understanding of the problems and challenges faced by the teams building data products.&lt;/p&gt;\n\n&lt;p&gt;I used to be a data engineer between 2009 and 2014. In 2014, I got into product management and I am currently in a product lead role. I have always worked products which included a large amount of analytics, insights, predictive models, machine learning in software companies serving healthcare, surveillance, automotive, and ecommerce.&lt;/p&gt;\n\n&lt;p&gt;In my experience over the past 14 years, the data tooling ecosystem has expanded a lot.However, I am still in a scenario where the cost of data infrastructure and tooling is expensive.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Projects are complex and long drawn.&lt;/li&gt;\n&lt;li&gt;It is super hard to solve basic issues of data quality even reactively.&lt;/li&gt;\n&lt;li&gt;Maintaining trust in the data assets is really hard.&lt;/li&gt;\n&lt;li&gt;Data literacy of decision makers is not up to the mark in a lot of cases.&lt;/li&gt;\n&lt;li&gt;Stakeholders expect miracles and stuff to just work.&lt;/li&gt;\n&lt;li&gt;Very few people can explain the attributes, the calculations, the metrics, the insights, and the implications.&lt;/li&gt;\n&lt;li&gt;Everyone is just promising stuff and punting the inevitable reality of face the hard problem and solve it properly.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am just trying to research and gather inputs from the community on the nagging challenges of building products now to inform my product development and to inform a course that I am building to develop data product managers (because it is really difficult to find candidates to hire)&lt;/p&gt;\n\n&lt;p&gt;My question for you is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are your top 3 challenges in engineering data flows and pipelines?\n\n&lt;ul&gt;\n&lt;li&gt;Is it the data inventory, quality, governance, accessibility, etc.?&lt;/li&gt;\n&lt;li&gt;Is the infrastructure, the complexity of building, deploying, administering the systems?&lt;/li&gt;\n&lt;li&gt;Is it the challenge of organizational structure, talent, capacity, leadership?&lt;/li&gt;\n&lt;li&gt;Is it communication, silos, lack of alignment?&lt;/li&gt;\n&lt;li&gt;Is it cost, performance, complexity of infrastructure?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;What is preventing you from building valuable data products?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For me, infrastructure cost, performance of existing tools, spaghetti code, lack of data expertise among leadership stakeholders has been the biggest headwinds to progress.&lt;/p&gt;\n\n&lt;p&gt;Last year, at one point, our AWS costs were $1.6 for every $1 a customer paid us. After working on a year and reducing substantial tech debt, we got to AWS cost of $0.6 for every $1 revenue. Still, there is no recognition, leadership is reluctant to fix data quality issues.&lt;/p&gt;\n\n&lt;p&gt;As a product lead, I have been able to influence some, but it&amp;#39;s a lot of compounding challenges.&lt;/p&gt;\n\n&lt;p&gt;Does this resonate with folks?&lt;/p&gt;\n\n&lt;p&gt;What are the top challenges you are facing?&lt;/p&gt;\n\n&lt;p&gt;What are some solutions or workaround that have worked for you?&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your responses.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qpfwo", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qpfwo/what_are_some_of_the_sticky_problems_in_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qpfwo/what_are_some_of_the_sticky_problems_in_your_data/", "subreddit_subscribers": 92976, "created_utc": 1678749600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A data warehouse was defined by Bill Inmon as \"a subject-oriented, integrated, nonvolatile, and time-variant collection of data in support of management's decisions\" over 30 years ago. However, the initial data warehouses\u00a0were unable to store massive heterogeneous data, hence the creation of data lakes. In modern times,\u00a0data lakehouse emerges as a new paradigm. It is an open data management architecture featured by strong data analytics and governance capabilities, high flexibility, and open storage.\n\nIf I could only use one word to describe the next-gen data lakehouse, it would be **unification:**\n\n* **Unified data storage** to avoid the trouble and risks brought by redundant storage and cross-system ETL.\n* **Unified governance** of both data and metadata with support for ACID, Schema Evolution, and Snapshot.\n* **Unified data application** that supports data access via a single interface for multiple engines and workloads.\n\nLet's look into the architecture of a data lakehouse. We will find that it is not only supported by table formats such as Apache Iceberg, Apache Hudi, and Delta Lake, but more importantly, it is powered by a high-performance query engine to extract value from data.\n\nUsers are looking for a query engine that allows quick and smooth access to the most popular data sources. What they don't want is for their data to be locked in a certain database and rendered unavailable for other engines or to spend extra time and computing costs on data transfer and format conversion.\n\nTo turn these visions into reality, a data query engine needs to figure out the following questions:\n\n* How to access more data sources and acquire metadata more easily?\n* How to improve query performance on data coming from various sources?\n* How to enable more flexible resource scheduling and workload management?\n\n[Apache Doris](https://github.com/apache/doris) provides a possible answer to these questions. It is a real-time OLAP database that aspires to build itself into a unified data analysis gateway. This means it needs to be easily connected to various RDBMS, data warehouses, and data lake engines (such as Hive, Iceberg, Hudi, Delta Lake, and Flink Table Store) and allow for quick data writing from and queries on these heterogeneous data sources. The rest of this article is an in-depth explanation of Apache Doris' techniques in the above three aspects: metadata acquisition, query performance optimization, and resource scheduling.\n\n# Metadata Acquisition and Data Access\n\nApache Doris 1.2.2 supports a wide variety of data lake formats and data access from various external data sources. Besides, via the Table Value Function, users can analyze files in object storage or HDFS directly.\n\nhttps://preview.redd.it/ftgtvwgp3ina1.png?width=1598&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bd9f1b2cebcf0e7ff7f183a7af8b66aaa7705662\n\nTo support multiple data sources, Apache Doris puts efforts into metadata acquisition and data access.\n\n**Metadata Acquisition**\n\nMetadata consists of information about the databases, tables, partitions, indexes, and files from the data source. Thus, metadata of various data sources come in different formats and patterns, adding to the difficulty of metadata connection. An ideal metadata acquisition service should include the following:\n\n1. A **metadata structure** that can accommodate heterogeneous metadata.\n2. An **extensible metadata connection framework** that enables quick and low-cost data connection.\n3. Reliable and **efficient\u00a0metadata access** that supports real-time metadata capture.\n4. **Custom authentication** services to interface with external privilege management systems and thus reduce migration costs.\u00a0\n\n**Metadata Structure**\n\nOlder versions of Doris support a two-tiered metadata structure: database and table. As a result, users need to create mappings for external databases and tables one by one, which is heavy work. Thus, Apache Doris 1.2.0 introduced the Multi-Catalog functionality. With this, you can map to external data at the catalog level, which means:\n\n1. You can map to the whole external data source and ingest all metadata from it.\n2. You can manage the properties of the specified data source at the catalog level, such as connection, privileges, and data ingestion details, and easily handle multiple data sources.\n\nData in Doris falls into two types of catalogs:\n\n1. Internal Catalog: Existing Doris databases and tables all belong to the Internal Catalog.\n2. External Catalog: This is used to interface with external data sources. For example, HMS External Catalog can be connected to a cluster managed by Hive Metastore, and Iceberg External Catalog can be connected to an Iceberg cluster.\n\nYou can use the `SWITCH` statement to switch catalogs. You can also conduct federated queries using fully qualified names. For example:\n\n    SELECT * FROM hive.db1.tbl1 a JOIN iceberg.db2.tbl2 b ON a.k1 = b.k1;\n\n**Extensible Metadata Connection Framework**\n\nThe introduction of the catalog level also enables users to add new data sources simply by using the CREATE CATALOG  \n statement:\n\n    CREATE CATALOG hive PROPERTIES (\n         'type'='hms',\n         'hive.metastore.uris' = 'thrift://172.21.0.1:7004',\n     );\n\nIn data lake scenarios, Apache Doris currently supports the following metadata services:\n\n* Hive Metastore-compatible metadata services\n* Alibaba Cloud Data Lake Formation\n* AWS Glue\n\nThis also paves the way for developers who want to connect to more data sources via External Catalog. All they need is to implement the access interface.\n\n**Efficient Metadata Access**\n\nAccess to external data sources is often hindered by network conditions and data resources. This requires extra efforts of a data query engine to guarantee reliability, stability, and real-timeliness in metadata access.\n\nhttps://preview.redd.it/5uk9n6vi4ina1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=16201ec6a497f4acfa0f963b2fa18f010b717d77\n\nDoris enables high efficiency in metadata access by Meta Cache, which includes Schema Cache, Partition Cache, and File Cache. This means that Doris can respond to metadata queries on thousands of tables in milliseconds. In addition, Doris supports manual refresh of metadata at the Catalog/Database/Table level. Meanwhile, it enables auto synchronization of metadata in Hive Metastore by monitoring Hive Metastore Event, so any changes can be updated within seconds.\n\n**Custom Authorization**\n\nExternal data sources usually come with their own privilege management services. Many companies use one single tool (such as Apache Ranger) to provide authorization for their multiple data systems. Doris supports a custom authorization plugin, which can be connected to the user's own privilege management system via the Doris Access Controller interface. As a user, you only need to specify the authorization plugin for a newly created catalog, and then you can readily perform authorization, audit, and data encryption on external data in Doris.\n\nhttps://preview.redd.it/ksssq9an4ina1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b845c8edf26e8c540783c69579444f80285bed5b\n\n**Data Access**\n\nDoris supports data access to external storage systems, including HDFS and S3-compatible object storage:\n\nhttps://preview.redd.it/qdiik8wk4ina1.png?width=1490&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=19b490d8bbda6e3b09b4d353427853f1eb0730e4\n\n# Query Performance Optimization\n\nAfter clearing the way for external data access, the next step for a query engine would be to accelerate data queries. In the case of Apache Doris, efforts are made in data reading, execution engine, and optimizer.\n\n**Data Reading**\n\nReading data on remote storage systems is often bottlenecked by access latency, concurrency, and I/O bandwidth, so reducing reading frequency will be a better choice.\n\n**Native File Format Reader**\n\nImproving data reading efficiency entails optimizing the reading of Parquet files and ORC files, which are the most commonly seen data files. Doris has refactored its File Reader, which is fine-tuned for each data format. Take the Native Parquet Reader as an example:\n\n* Reduce format conversion: It can directly convert files to the Doris storage format or to a format of higher performance using dictionary encoding.\u00a0\n* Smart indexing of finer granularity: It supports Page Index for Parquet files, so it can utilize Page-level smart indexing to filter Pages.\u00a0\n* Predicate pushdown and late materialization: It first reads columns with filters first and then reads the other columns of the filtered rows. This remarkably reduces file read volume since it avoids reading irrelevant data.\n* Lower read frequency: Building on the high throughput and low concurrency of remote storage, it combines multiple data reads into one in order to improve overall data reading efficiency.\n\n**File Cache**\n\nDoris caches files from remote storage in local high-performance disks as a way to reduce overhead and increase performance in data reading. In addition, it has developed two new features that make queries on remote files as quick as those on local files:\n\n1. Block cache: Doris supports the block cache of remote files and can automatically adjust the block size from 4KB to 4MB based on the read request. The block cache method reduces read/write amplification and read latency in cold caches.\n2. Consistent hashing for caching: Doris applies consistent hashing to manage cache locations and schedule data scanning. By doing so, it prevents cache failures brought about by the online and offlining of nodes. It can also increase cache hit rate and query service stability.\n\nhttps://preview.redd.it/flhystx05ina1.png?width=1080&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4bafba530e801c63ec68b436350528ba6e5475db\n\n**Execution Engine**\n\nDevelopers surely don't want to rebuild all the general features for every new data source. Instead, they hope to reuse the vectorized execution engine and all operators in Doris in the data lakehouse scenario. Thus, Doris has refactored the scan nodes:\n\n* Layer the logic: All data queries in Doris, including those on internal tables, use the same operators, such as Join, Sort, and Agg. The only difference between queries on internal and external data lies in data access. In Doris, anything above the scan nodes follows the same query logic, while below the scan nodes, the implementation classes will take care of access to different data sources.\n* Use a general framework for scan operators: Even for the scan nodes, different data sources have a lot in common, such as task splitting logic, scheduling of sub-tasks and I/O, predicate pushdown, and Runtime Filter. Therefore, Doris uses interfaces to handle them. Then, it implements a unified scheduling logic for all sub-tasks. The scheduler is in charge of all scanning tasks in the node. With global information of the node in hand, the schedular is able to do fine-grained management. Such a general framework makes it easy to connect a new data source to Doris, which will only take a week of work for one developer.\n\nhttps://preview.redd.it/beii5ic45ina1.png?width=830&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=27474588a55fefae022d125decc651ba4d059fe1\n\n**Query Optimizer**\n\nDoris supports a range of statistical information from various data sources, including Hive Metastore, Iceberg Metafile, and Hudi MetaTable. It has also refined its cost model inference based on the characteristics of different data sources to enhance its query planning capability. \n\n**Performance**\n\nWe tested Doris and Presto/Trino on HDFS in flat table scenarios (ClickBench) and multi-table scenarios (TPC-H). Here are the results: \n\n&amp;#x200B;\n\n[Doris vs Trino : Clickbench](https://preview.redd.it/0msrv3575ina1.png?width=1925&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7dc272558a7ab29faf4d434476d79e0f122219ba)\n\n&amp;#x200B;\n\n[TPC-H](https://preview.redd.it/va3rntv85ina1.png?width=1688&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=146be3b1ce95383510293a2be5b71c1a1e58c4de)\n\nAs is shown, with the same computing resources and on the same dataset, Apache Doris takes much less time to respond to SQL queries in both scenarios, delivering a 3\\~10 times higher performance than Presto/Trino.\n\n# Workload Management and Elastic Computing\n\nQuerying external data sources requires no internal storage of Doris. This makes elastic stateless computing nodes possible. Apache Doris 2.0 is going to implement Elastic Compute Node, which is dedicated to supporting query workloads of external data sources.\n\nhttps://preview.redd.it/t8ttffmc5ina1.png?width=1960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=88a20cb777d80cac0fc3a9e0e60914505c93167a\n\nStateless computing nodes are open for quick scaling so users can easily cope with query workloads during peaks and valleys and strike a balance between performance and cost. In addition, Doris has optimized itself for Kubernetes cluster management and node scheduling. Now Master nodes can automatically manage the onlining and offlining of Elastic Compute Nodes, so users can govern their cluster workloads in cloud-native and hybrid cloud scenarios without difficulty.\n\n# Use Case\n\nApache Doris has been adopted by a financial institution for risk management. The user's high demands for data timeliness makes their data mart built on Greenplum and CDH, which could only process data from one day ago, no longer a great fit. In 2022, they incorporated Apache Doris in their data production and application pipeline, which allowed them to perform federated queries across Elasticsearch, Greenplum, and Hive. A few highlights from the user's feedback include:\n\n* Doris allows them to create one Hive Catalog that maps to tens of thousands of external Hive tables and conducts fast queries on them.\n* Doris makes it possible to perform real-time federated queries using Elasticsearch Catalog and achieve a response time of mere milliseconds.\n* Doris enables the decoupling of daily batch processing and statistical analysis, bringing less resource consumption and higher system stability.\n\n&amp;#x200B;\n\n**Links:**\n\nWebsite : [https://doris.apache.org](https://doris.apache.org)\n\nRepo : [https://github.com/apache/doris](https://github.com/apache/doris)", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building the Next-Generation Data Lakehouse: 10X Performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "media_metadata": {"va3rntv85ina1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 26, "x": 108, "u": "https://preview.redd.it/va3rntv85ina1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39c6e79c570236ccc12db0b1f8c87c26c22f4f09"}, {"y": 53, "x": 216, "u": "https://preview.redd.it/va3rntv85ina1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3dcfc3767bc5b270dd8b711511b91d978113426d"}, {"y": 79, "x": 320, "u": "https://preview.redd.it/va3rntv85ina1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1223b1c09e86e35c4a97bf82854c94ea2035823b"}, {"y": 159, "x": 640, "u": "https://preview.redd.it/va3rntv85ina1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87bcb7e03f52f837201374c6b3306c91d460e2e6"}, {"y": 239, "x": 960, "u": "https://preview.redd.it/va3rntv85ina1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4135910b15ea510666e4e982cf2dd82d4a23eb28"}, {"y": 269, "x": 1080, "u": "https://preview.redd.it/va3rntv85ina1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a2cf2b2d3a20e8dbf7844c1809ea767df89add6"}], "s": {"y": 421, "x": 1688, "u": "https://preview.redd.it/va3rntv85ina1.png?width=1688&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=146be3b1ce95383510293a2be5b71c1a1e58c4de"}, "id": "va3rntv85ina1"}, "0msrv3575ina1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 19, "x": 108, "u": "https://preview.redd.it/0msrv3575ina1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4380aa7362af9517707a4c6bea2bc07166b3971d"}, {"y": 38, "x": 216, "u": "https://preview.redd.it/0msrv3575ina1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bd1236dc02f0c5933899d0a1a4e9165c945bda1"}, {"y": 57, "x": 320, "u": "https://preview.redd.it/0msrv3575ina1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9da9126e6c3a7cd27c7697cc62c00853fdef738e"}, {"y": 114, "x": 640, "u": "https://preview.redd.it/0msrv3575ina1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=875bf7aed4fe088e0a2b01d6046cb90069ce02a3"}, {"y": 172, "x": 960, "u": "https://preview.redd.it/0msrv3575ina1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f0381c3ccb8d6a702f3fb8d7e77fae479b48fa9"}, {"y": 193, "x": 1080, "u": "https://preview.redd.it/0msrv3575ina1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7064d961df53f4061d45ff2c79f091da91c229b2"}], "s": {"y": 345, "x": 1925, "u": "https://preview.redd.it/0msrv3575ina1.png?width=1925&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7dc272558a7ab29faf4d434476d79e0f122219ba"}, "id": "0msrv3575ina1"}, "t8ttffmc5ina1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 48, "x": 108, "u": "https://preview.redd.it/t8ttffmc5ina1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=52ff3aceaf1d8ee5a3b4d9ad446986b19a12be74"}, {"y": 97, "x": 216, "u": "https://preview.redd.it/t8ttffmc5ina1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35ba05795f181b4e8f831ac0027297ff14ae191b"}, {"y": 144, "x": 320, "u": "https://preview.redd.it/t8ttffmc5ina1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfac00855e77cff73b1b2c13f17c3c66fb6d4c3d"}, {"y": 288, "x": 640, "u": "https://preview.redd.it/t8ttffmc5ina1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=612abb8ff2496f4ed3cbbfdbd8e236757e89232a"}, {"y": 432, "x": 960, "u": "https://preview.redd.it/t8ttffmc5ina1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=caa161d9f76c01da1350831fae868b866204bac7"}, {"y": 487, "x": 1080, "u": "https://preview.redd.it/t8ttffmc5ina1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3344aa4a66a8f6f2f559a07ebd01c194e052990"}], "s": {"y": 884, "x": 1960, "u": "https://preview.redd.it/t8ttffmc5ina1.png?width=1960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=88a20cb777d80cac0fc3a9e0e60914505c93167a"}, "id": "t8ttffmc5ina1"}, "ftgtvwgp3ina1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 59, "x": 108, "u": "https://preview.redd.it/ftgtvwgp3ina1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1813900cd2785db3bd68d9032214d5d52cfd25f2"}, {"y": 119, "x": 216, "u": "https://preview.redd.it/ftgtvwgp3ina1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77fe6ba2e29b2ad2d3e2689b32f5e994532c7409"}, {"y": 176, "x": 320, "u": "https://preview.redd.it/ftgtvwgp3ina1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e41398a63a77f5d1be7c3b1f9beb272566e0c2d"}, {"y": 353, "x": 640, "u": "https://preview.redd.it/ftgtvwgp3ina1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0804a6b6c5b5ea907f1302edee7cbb29da9e12b6"}, {"y": 529, "x": 960, "u": "https://preview.redd.it/ftgtvwgp3ina1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29df47f2055ab6dfcfba475ebf8bd0f82bda0e7e"}, {"y": 596, "x": 1080, "u": "https://preview.redd.it/ftgtvwgp3ina1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a38dc3c9b53835c849d31af4090aaed7bfe917d"}], "s": {"y": 882, "x": 1598, "u": "https://preview.redd.it/ftgtvwgp3ina1.png?width=1598&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bd9f1b2cebcf0e7ff7f183a7af8b66aaa7705662"}, "id": "ftgtvwgp3ina1"}, "ksssq9an4ina1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/ksssq9an4ina1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea3e7df30931b74ef3f262857e77f6a01fe1a9aa"}, {"y": 95, "x": 216, "u": "https://preview.redd.it/ksssq9an4ina1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c51abf5d51403253b5ce047a78de7afea50affd4"}, {"y": 142, "x": 320, "u": "https://preview.redd.it/ksssq9an4ina1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c19c3fc00da50a8bc3c731542f63ff9ee00ef15"}, {"y": 284, "x": 640, "u": "https://preview.redd.it/ksssq9an4ina1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=781b2d72a69696ecad200b1bb33f0b3d2661cccc"}, {"y": 426, "x": 960, "u": "https://preview.redd.it/ksssq9an4ina1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=657f9985416b4219e9a6cd613455d92e22682627"}, {"y": 479, "x": 1080, "u": "https://preview.redd.it/ksssq9an4ina1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff8a28327df9b16850f029178ac1bc351b7fdae8"}], "s": {"y": 568, "x": 1280, "u": "https://preview.redd.it/ksssq9an4ina1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b845c8edf26e8c540783c69579444f80285bed5b"}, "id": "ksssq9an4ina1"}, "flhystx05ina1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/flhystx05ina1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2efa4f2c110362f95ce3e1938ee7ea934c245951"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/flhystx05ina1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=425d1a0a57ae2b14f2e05b598513fc852dc59785"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/flhystx05ina1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88b1926401ac994773ea276c8e1e67fb0486a198"}, {"y": 378, "x": 640, "u": "https://preview.redd.it/flhystx05ina1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca6e60bce13f02b747384929eeee0f88062fa9a8"}, {"y": 567, "x": 960, "u": "https://preview.redd.it/flhystx05ina1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b9f9b26bb42b813f9b6910b0bf01adf610d0580"}, {"y": 638, "x": 1080, "u": "https://preview.redd.it/flhystx05ina1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cc52c6fd4e8ec184af8848ec5fe62d96a8800fc"}], "s": {"y": 638, "x": 1080, "u": "https://preview.redd.it/flhystx05ina1.png?width=1080&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4bafba530e801c63ec68b436350528ba6e5475db"}, "id": "flhystx05ina1"}, "qdiik8wk4ina1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 44, "x": 108, "u": "https://preview.redd.it/qdiik8wk4ina1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c469588b46c8af2e5a37ad39197505fa88818e8"}, {"y": 88, "x": 216, "u": "https://preview.redd.it/qdiik8wk4ina1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f21165d9fba4cc22772181fb8fc1368676bf0a66"}, {"y": 131, "x": 320, "u": "https://preview.redd.it/qdiik8wk4ina1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd0180e63a0c6df7ec304f0f0b610776f9d0f1ee"}, {"y": 262, "x": 640, "u": "https://preview.redd.it/qdiik8wk4ina1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d209c6f98f6477ddb7900071a7d75629ae4ad467"}, {"y": 393, "x": 960, "u": "https://preview.redd.it/qdiik8wk4ina1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f00ff03d4fdf95cc8cab5c6ca47a5450bdfda2a"}, {"y": 442, "x": 1080, "u": "https://preview.redd.it/qdiik8wk4ina1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9230116b0e436e5867eee262f88fa2b5774b7bb"}], "s": {"y": 610, "x": 1490, "u": "https://preview.redd.it/qdiik8wk4ina1.png?width=1490&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=19b490d8bbda6e3b09b4d353427853f1eb0730e4"}, "id": "qdiik8wk4ina1"}, "beii5ic45ina1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 109, "x": 108, "u": "https://preview.redd.it/beii5ic45ina1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f106c66781a4bb2ccc04719b4ad7769f173e048d"}, {"y": 219, "x": 216, "u": "https://preview.redd.it/beii5ic45ina1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=994c245352f94762638a5c3c4f8dae231b73d6cf"}, {"y": 325, "x": 320, "u": "https://preview.redd.it/beii5ic45ina1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de98c77e8b32d41acea8cf8e3f0f3c13adc784f7"}, {"y": 650, "x": 640, "u": "https://preview.redd.it/beii5ic45ina1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ebce8ef86720ced80e9645183ecf42450317e0f"}], "s": {"y": 844, "x": 830, "u": "https://preview.redd.it/beii5ic45ina1.png?width=830&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=27474588a55fefae022d125decc651ba4d059fe1"}, "id": "beii5ic45ina1"}, "5uk9n6vi4ina1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/5uk9n6vi4ina1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=422a8069ccf8182eddb34c88aa34efa2fec6aefb"}, {"y": 79, "x": 216, "u": "https://preview.redd.it/5uk9n6vi4ina1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35010a7baea869a0efe2c4bb8c8b5583a141eec2"}, {"y": 118, "x": 320, "u": "https://preview.redd.it/5uk9n6vi4ina1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8982a2c7cb47c6b05e9572f30498c245387f8cec"}, {"y": 236, "x": 640, "u": "https://preview.redd.it/5uk9n6vi4ina1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d19d6c07ecb8d97b656a25a3ca1cb133e75af63f"}, {"y": 354, "x": 960, "u": "https://preview.redd.it/5uk9n6vi4ina1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfa0587697a5725731aabab003d41e6fa2f02938"}, {"y": 398, "x": 1080, "u": "https://preview.redd.it/5uk9n6vi4ina1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4b00ac76ec76b83423b8739a0605a5ff9c035e7"}], "s": {"y": 472, "x": 1280, "u": "https://preview.redd.it/5uk9n6vi4ina1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=16201ec6a497f4acfa0f963b2fa18f010b717d77"}, "id": "5uk9n6vi4ina1"}}, "name": "t3_11q8qol", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/rEugS-SdxZunJm-0_N68MO3dOYxnipm3xUvJz-qS9gw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1678711596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A data warehouse was defined by Bill Inmon as &amp;quot;a subject-oriented, integrated, nonvolatile, and time-variant collection of data in support of management&amp;#39;s decisions&amp;quot; over 30 years ago. However, the initial data warehouses\u00a0were unable to store massive heterogeneous data, hence the creation of data lakes. In modern times,\u00a0data lakehouse emerges as a new paradigm. It is an open data management architecture featured by strong data analytics and governance capabilities, high flexibility, and open storage.&lt;/p&gt;\n\n&lt;p&gt;If I could only use one word to describe the next-gen data lakehouse, it would be &lt;strong&gt;unification:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Unified data storage&lt;/strong&gt; to avoid the trouble and risks brought by redundant storage and cross-system ETL.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Unified governance&lt;/strong&gt; of both data and metadata with support for ACID, Schema Evolution, and Snapshot.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Unified data application&lt;/strong&gt; that supports data access via a single interface for multiple engines and workloads.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Let&amp;#39;s look into the architecture of a data lakehouse. We will find that it is not only supported by table formats such as Apache Iceberg, Apache Hudi, and Delta Lake, but more importantly, it is powered by a high-performance query engine to extract value from data.&lt;/p&gt;\n\n&lt;p&gt;Users are looking for a query engine that allows quick and smooth access to the most popular data sources. What they don&amp;#39;t want is for their data to be locked in a certain database and rendered unavailable for other engines or to spend extra time and computing costs on data transfer and format conversion.&lt;/p&gt;\n\n&lt;p&gt;To turn these visions into reality, a data query engine needs to figure out the following questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How to access more data sources and acquire metadata more easily?&lt;/li&gt;\n&lt;li&gt;How to improve query performance on data coming from various sources?&lt;/li&gt;\n&lt;li&gt;How to enable more flexible resource scheduling and workload management?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/apache/doris\"&gt;Apache Doris&lt;/a&gt; provides a possible answer to these questions. It is a real-time OLAP database that aspires to build itself into a unified data analysis gateway. This means it needs to be easily connected to various RDBMS, data warehouses, and data lake engines (such as Hive, Iceberg, Hudi, Delta Lake, and Flink Table Store) and allow for quick data writing from and queries on these heterogeneous data sources. The rest of this article is an in-depth explanation of Apache Doris&amp;#39; techniques in the above three aspects: metadata acquisition, query performance optimization, and resource scheduling.&lt;/p&gt;\n\n&lt;h1&gt;Metadata Acquisition and Data Access&lt;/h1&gt;\n\n&lt;p&gt;Apache Doris 1.2.2 supports a wide variety of data lake formats and data access from various external data sources. Besides, via the Table Value Function, users can analyze files in object storage or HDFS directly.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ftgtvwgp3ina1.png?width=1598&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bd9f1b2cebcf0e7ff7f183a7af8b66aaa7705662\"&gt;https://preview.redd.it/ftgtvwgp3ina1.png?width=1598&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bd9f1b2cebcf0e7ff7f183a7af8b66aaa7705662&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To support multiple data sources, Apache Doris puts efforts into metadata acquisition and data access.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Metadata Acquisition&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Metadata consists of information about the databases, tables, partitions, indexes, and files from the data source. Thus, metadata of various data sources come in different formats and patterns, adding to the difficulty of metadata connection. An ideal metadata acquisition service should include the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A &lt;strong&gt;metadata structure&lt;/strong&gt; that can accommodate heterogeneous metadata.&lt;/li&gt;\n&lt;li&gt;An &lt;strong&gt;extensible metadata connection framework&lt;/strong&gt; that enables quick and low-cost data connection.&lt;/li&gt;\n&lt;li&gt;Reliable and &lt;strong&gt;efficient\u00a0metadata access&lt;/strong&gt; that supports real-time metadata capture.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Custom authentication&lt;/strong&gt; services to interface with external privilege management systems and thus reduce migration costs.\u00a0&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Metadata Structure&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Older versions of Doris support a two-tiered metadata structure: database and table. As a result, users need to create mappings for external databases and tables one by one, which is heavy work. Thus, Apache Doris 1.2.0 introduced the Multi-Catalog functionality. With this, you can map to external data at the catalog level, which means:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;You can map to the whole external data source and ingest all metadata from it.&lt;/li&gt;\n&lt;li&gt;You can manage the properties of the specified data source at the catalog level, such as connection, privileges, and data ingestion details, and easily handle multiple data sources.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Data in Doris falls into two types of catalogs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Internal Catalog: Existing Doris databases and tables all belong to the Internal Catalog.&lt;/li&gt;\n&lt;li&gt;External Catalog: This is used to interface with external data sources. For example, HMS External Catalog can be connected to a cluster managed by Hive Metastore, and Iceberg External Catalog can be connected to an Iceberg cluster.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;You can use the &lt;code&gt;SWITCH&lt;/code&gt; statement to switch catalogs. You can also conduct federated queries using fully qualified names. For example:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT * FROM hive.db1.tbl1 a JOIN iceberg.db2.tbl2 b ON a.k1 = b.k1;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Extensible Metadata Connection Framework&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The introduction of the catalog level also enables users to add new data sources simply by using the CREATE CATALOG&lt;br/&gt;\n statement:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE CATALOG hive PROPERTIES (\n     &amp;#39;type&amp;#39;=&amp;#39;hms&amp;#39;,\n     &amp;#39;hive.metastore.uris&amp;#39; = &amp;#39;thrift://172.21.0.1:7004&amp;#39;,\n );\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;In data lake scenarios, Apache Doris currently supports the following metadata services:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hive Metastore-compatible metadata services&lt;/li&gt;\n&lt;li&gt;Alibaba Cloud Data Lake Formation&lt;/li&gt;\n&lt;li&gt;AWS Glue&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This also paves the way for developers who want to connect to more data sources via External Catalog. All they need is to implement the access interface.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Efficient Metadata Access&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Access to external data sources is often hindered by network conditions and data resources. This requires extra efforts of a data query engine to guarantee reliability, stability, and real-timeliness in metadata access.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5uk9n6vi4ina1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=16201ec6a497f4acfa0f963b2fa18f010b717d77\"&gt;https://preview.redd.it/5uk9n6vi4ina1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=16201ec6a497f4acfa0f963b2fa18f010b717d77&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Doris enables high efficiency in metadata access by Meta Cache, which includes Schema Cache, Partition Cache, and File Cache. This means that Doris can respond to metadata queries on thousands of tables in milliseconds. In addition, Doris supports manual refresh of metadata at the Catalog/Database/Table level. Meanwhile, it enables auto synchronization of metadata in Hive Metastore by monitoring Hive Metastore Event, so any changes can be updated within seconds.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Custom Authorization&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;External data sources usually come with their own privilege management services. Many companies use one single tool (such as Apache Ranger) to provide authorization for their multiple data systems. Doris supports a custom authorization plugin, which can be connected to the user&amp;#39;s own privilege management system via the Doris Access Controller interface. As a user, you only need to specify the authorization plugin for a newly created catalog, and then you can readily perform authorization, audit, and data encryption on external data in Doris.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ksssq9an4ina1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b845c8edf26e8c540783c69579444f80285bed5b\"&gt;https://preview.redd.it/ksssq9an4ina1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b845c8edf26e8c540783c69579444f80285bed5b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Access&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Doris supports data access to external storage systems, including HDFS and S3-compatible object storage:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qdiik8wk4ina1.png?width=1490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=19b490d8bbda6e3b09b4d353427853f1eb0730e4\"&gt;https://preview.redd.it/qdiik8wk4ina1.png?width=1490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=19b490d8bbda6e3b09b4d353427853f1eb0730e4&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Query Performance Optimization&lt;/h1&gt;\n\n&lt;p&gt;After clearing the way for external data access, the next step for a query engine would be to accelerate data queries. In the case of Apache Doris, efforts are made in data reading, execution engine, and optimizer.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Reading&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Reading data on remote storage systems is often bottlenecked by access latency, concurrency, and I/O bandwidth, so reducing reading frequency will be a better choice.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Native File Format Reader&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Improving data reading efficiency entails optimizing the reading of Parquet files and ORC files, which are the most commonly seen data files. Doris has refactored its File Reader, which is fine-tuned for each data format. Take the Native Parquet Reader as an example:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Reduce format conversion: It can directly convert files to the Doris storage format or to a format of higher performance using dictionary encoding.\u00a0&lt;/li&gt;\n&lt;li&gt;Smart indexing of finer granularity: It supports Page Index for Parquet files, so it can utilize Page-level smart indexing to filter Pages.\u00a0&lt;/li&gt;\n&lt;li&gt;Predicate pushdown and late materialization: It first reads columns with filters first and then reads the other columns of the filtered rows. This remarkably reduces file read volume since it avoids reading irrelevant data.&lt;/li&gt;\n&lt;li&gt;Lower read frequency: Building on the high throughput and low concurrency of remote storage, it combines multiple data reads into one in order to improve overall data reading efficiency.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;File Cache&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Doris caches files from remote storage in local high-performance disks as a way to reduce overhead and increase performance in data reading. In addition, it has developed two new features that make queries on remote files as quick as those on local files:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Block cache: Doris supports the block cache of remote files and can automatically adjust the block size from 4KB to 4MB based on the read request. The block cache method reduces read/write amplification and read latency in cold caches.&lt;/li&gt;\n&lt;li&gt;Consistent hashing for caching: Doris applies consistent hashing to manage cache locations and schedule data scanning. By doing so, it prevents cache failures brought about by the online and offlining of nodes. It can also increase cache hit rate and query service stability.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/flhystx05ina1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=4bafba530e801c63ec68b436350528ba6e5475db\"&gt;https://preview.redd.it/flhystx05ina1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=4bafba530e801c63ec68b436350528ba6e5475db&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Execution Engine&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Developers surely don&amp;#39;t want to rebuild all the general features for every new data source. Instead, they hope to reuse the vectorized execution engine and all operators in Doris in the data lakehouse scenario. Thus, Doris has refactored the scan nodes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Layer the logic: All data queries in Doris, including those on internal tables, use the same operators, such as Join, Sort, and Agg. The only difference between queries on internal and external data lies in data access. In Doris, anything above the scan nodes follows the same query logic, while below the scan nodes, the implementation classes will take care of access to different data sources.&lt;/li&gt;\n&lt;li&gt;Use a general framework for scan operators: Even for the scan nodes, different data sources have a lot in common, such as task splitting logic, scheduling of sub-tasks and I/O, predicate pushdown, and Runtime Filter. Therefore, Doris uses interfaces to handle them. Then, it implements a unified scheduling logic for all sub-tasks. The scheduler is in charge of all scanning tasks in the node. With global information of the node in hand, the schedular is able to do fine-grained management. Such a general framework makes it easy to connect a new data source to Doris, which will only take a week of work for one developer.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/beii5ic45ina1.png?width=830&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=27474588a55fefae022d125decc651ba4d059fe1\"&gt;https://preview.redd.it/beii5ic45ina1.png?width=830&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=27474588a55fefae022d125decc651ba4d059fe1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Query Optimizer&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Doris supports a range of statistical information from various data sources, including Hive Metastore, Iceberg Metafile, and Hudi MetaTable. It has also refined its cost model inference based on the characteristics of different data sources to enhance its query planning capability. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We tested Doris and Presto/Trino on HDFS in flat table scenarios (ClickBench) and multi-table scenarios (TPC-H). Here are the results: &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0msrv3575ina1.png?width=1925&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7dc272558a7ab29faf4d434476d79e0f122219ba\"&gt;Doris vs Trino : Clickbench&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/va3rntv85ina1.png?width=1688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=146be3b1ce95383510293a2be5b71c1a1e58c4de\"&gt;TPC-H&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As is shown, with the same computing resources and on the same dataset, Apache Doris takes much less time to respond to SQL queries in both scenarios, delivering a 3~10 times higher performance than Presto/Trino.&lt;/p&gt;\n\n&lt;h1&gt;Workload Management and Elastic Computing&lt;/h1&gt;\n\n&lt;p&gt;Querying external data sources requires no internal storage of Doris. This makes elastic stateless computing nodes possible. Apache Doris 2.0 is going to implement Elastic Compute Node, which is dedicated to supporting query workloads of external data sources.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/t8ttffmc5ina1.png?width=1960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=88a20cb777d80cac0fc3a9e0e60914505c93167a\"&gt;https://preview.redd.it/t8ttffmc5ina1.png?width=1960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=88a20cb777d80cac0fc3a9e0e60914505c93167a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Stateless computing nodes are open for quick scaling so users can easily cope with query workloads during peaks and valleys and strike a balance between performance and cost. In addition, Doris has optimized itself for Kubernetes cluster management and node scheduling. Now Master nodes can automatically manage the onlining and offlining of Elastic Compute Nodes, so users can govern their cluster workloads in cloud-native and hybrid cloud scenarios without difficulty.&lt;/p&gt;\n\n&lt;h1&gt;Use Case&lt;/h1&gt;\n\n&lt;p&gt;Apache Doris has been adopted by a financial institution for risk management. The user&amp;#39;s high demands for data timeliness makes their data mart built on Greenplum and CDH, which could only process data from one day ago, no longer a great fit. In 2022, they incorporated Apache Doris in their data production and application pipeline, which allowed them to perform federated queries across Elasticsearch, Greenplum, and Hive. A few highlights from the user&amp;#39;s feedback include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Doris allows them to create one Hive Catalog that maps to tens of thousands of external Hive tables and conducts fast queries on them.&lt;/li&gt;\n&lt;li&gt;Doris makes it possible to perform real-time federated queries using Elasticsearch Catalog and achieve a response time of mere milliseconds.&lt;/li&gt;\n&lt;li&gt;Doris enables the decoupling of daily batch processing and statistical analysis, bringing less resource consumption and higher system stability.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Website : &lt;a href=\"https://doris.apache.org\"&gt;https://doris.apache.org&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Repo : &lt;a href=\"https://github.com/apache/doris\"&gt;https://github.com/apache/doris&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TWPgZtrNm_F6Lvu1TE2FacnhwA5BQqoDma12IvM20sk.jpg?auto=webp&amp;v=enabled&amp;s=87a30f85cd5688ea63191fbf026ac83f743b4ab2", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/TWPgZtrNm_F6Lvu1TE2FacnhwA5BQqoDma12IvM20sk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=584f3a131f8830da3d62524f0c9357819ac5a4a0", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/TWPgZtrNm_F6Lvu1TE2FacnhwA5BQqoDma12IvM20sk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=71693bd704be6a965ba274e45d06ab943fda6590", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/TWPgZtrNm_F6Lvu1TE2FacnhwA5BQqoDma12IvM20sk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56b3263dfca88e35f48614bb267a75869461c3c3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/TWPgZtrNm_F6Lvu1TE2FacnhwA5BQqoDma12IvM20sk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d783f2289ac2db8a4cc3c6e157d270aa0e0d0342", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/TWPgZtrNm_F6Lvu1TE2FacnhwA5BQqoDma12IvM20sk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03190330193f0af37c252828b001c5821f7b7f20", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/TWPgZtrNm_F6Lvu1TE2FacnhwA5BQqoDma12IvM20sk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8eb1a3f3b0774e2f8ca35a0b7c12360255307e76", "width": 1080, "height": 540}], "variants": {}, "id": "MbJUbsCf6G-J9hic_pI199R1VVrV8ZZbnAvjldcvHvY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11q8qol", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11q8qol/building_the_nextgeneration_data_lakehouse_10x/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11q8qol/building_the_nextgeneration_data_lakehouse_10x/", "subreddit_subscribers": 92976, "created_utc": 1678711596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I'm one of the contributors to OpenDataDiscovery platform and would like to share a release update and/or introduce you to ODD as well!\n\n&amp;#x200B;\n\nFor the ones who're not familiar, the ODD Platform is an open-source data discovery and observability tool that enables businesses to standardize data collection, improve the compatibility of different data catalogs, augment and expand their data lineage capabilities, and improve the way data quality and observability are implemented.\n\n&amp;#x200B;\n\nSo, the v0.11 release of the ODD Platform introduces the following enhancements:\n\n\\- Metrics: With the implementation of Open Metrics Specification, ODD Platform introduces enhanced Metrics support. Users can ingest metrics for their data entities or dataset fields and observe the last metric values, right within the platform.\n\n\\- Search explanations: It is crucial to understand why a certain entity has appeared in search results. The ODD Platform can now display all user query entries in a specific entity in the Search API.\n\n\\- New Dataset Structure UI: The Dataset Structure UI has been reworked and enhanced, to provide a better experience for users in the Data Discovery phase.\n\n&amp;#x200B;\n\nGet to know about OpenDataDiscovery: [https://opendatadiscovery.org/](https://opendatadiscovery.org/)\n\nSource code: [https://github.com/opendatadiscovery/odd-platform](https://github.com/opendatadiscovery/odd-platform)", "author_fullname": "t2_ttr4u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Release 0.11 of OpenDataDiscovery Platform w/ metrics, search explanations &amp; new dataset structure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q5zrv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678703954.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678703185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I&amp;#39;m one of the contributors to OpenDataDiscovery platform and would like to share a release update and/or introduce you to ODD as well!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For the ones who&amp;#39;re not familiar, the ODD Platform is an open-source data discovery and observability tool that enables businesses to standardize data collection, improve the compatibility of different data catalogs, augment and expand their data lineage capabilities, and improve the way data quality and observability are implemented.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So, the v0.11 release of the ODD Platform introduces the following enhancements:&lt;/p&gt;\n\n&lt;p&gt;- Metrics: With the implementation of Open Metrics Specification, ODD Platform introduces enhanced Metrics support. Users can ingest metrics for their data entities or dataset fields and observe the last metric values, right within the platform.&lt;/p&gt;\n\n&lt;p&gt;- Search explanations: It is crucial to understand why a certain entity has appeared in search results. The ODD Platform can now display all user query entries in a specific entity in the Search API.&lt;/p&gt;\n\n&lt;p&gt;- New Dataset Structure UI: The Dataset Structure UI has been reworked and enhanced, to provide a better experience for users in the Data Discovery phase.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Get to know about OpenDataDiscovery: &lt;a href=\"https://opendatadiscovery.org/\"&gt;https://opendatadiscovery.org/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Source code: &lt;a href=\"https://github.com/opendatadiscovery/odd-platform\"&gt;https://github.com/opendatadiscovery/odd-platform&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11q5zrv", "is_robot_indexable": true, "report_reasons": null, "author": "Haarolean", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11q5zrv/release_011_of_opendatadiscovery_platform_w/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11q5zrv/release_011_of_opendatadiscovery_platform_w/", "subreddit_subscribers": 92976, "created_utc": 1678703185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Junior data engineer here, currently i'm debugging my data pipelines by adding logging statements in between the code like a caveman. I'm sure there is a better way to debug pipelines, but I'm not sure debugging works in data engineering like it does for the rest of software development? Am i wrong? Is there a better way?", "author_fullname": "t2_m2jhpzk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you debug your pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qw5jt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678766907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Junior data engineer here, currently i&amp;#39;m debugging my data pipelines by adding logging statements in between the code like a caveman. I&amp;#39;m sure there is a better way to debug pipelines, but I&amp;#39;m not sure debugging works in data engineering like it does for the rest of software development? Am i wrong? Is there a better way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11qw5jt", "is_robot_indexable": true, "report_reasons": null, "author": "IntraspeciesFever", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qw5jt/how_do_you_debug_your_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qw5jt/how_do_you_debug_your_pipelines/", "subreddit_subscribers": 92976, "created_utc": 1678766907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitHub - sbalnojan/FDE-airflow-tutorial: Functional Data Engineering tutorial in Python &amp; Airflow.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_11qmhaj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9QtqKuBwcDTiYFrErfytfCQ3NAW0uZRJVnbWOOo9RzM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678743132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/sbalnojan/FDE-airflow-tutorial", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?auto=webp&amp;v=enabled&amp;s=4a992deebea52f66a4c5e88e6466156d32a0dd98", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3e2bc5b35a61ced16cb4fa063b571f5bd9f3940", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e171412602c623c7e60b4b533b88a7a5159894ee", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff46d52191e2c54884ed3e8a9b177239f3cea110", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b2641103341efa415e10499b0ff0d034c1fd4bd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6491b8af7551fd7d56f00b5f6379866f335ebc95", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44a03be959c6da453c8bb7d4dba2a75894d1f83b", "width": 1080, "height": 540}], "variants": {}, "id": "Nca_dx-vFQi3_J7fErsS6JRyq0Xp24ZjcUheqZ6Ol5c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11qmhaj", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qmhaj/github_sbalnojanfdeairflowtutorial_functional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/sbalnojan/FDE-airflow-tutorial", "subreddit_subscribers": 92976, "created_utc": 1678743132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have made a script to transform excel workbook and output analysis in one worksheet and a pivot table in the other worksheet. \n\nMy initial thought is as below \n1. Script on lambda function\n2. User upload the excel data to S3 host website, then the data is sent to S3\n3. Once the excel data is landed in S3, it triggers lambda to process the data, and it generates the downloadable link to the user. \n\nI would like to keep the website as an internal tool. So shall I create some limited access user to use this flow. \n\nIt seems I am over complicating this data flow. \ud83e\udd7a", "author_fullname": "t2_pd2piq1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting Python data processing scripts on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q4i43", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678697795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have made a script to transform excel workbook and output analysis in one worksheet and a pivot table in the other worksheet. &lt;/p&gt;\n\n&lt;p&gt;My initial thought is as below \n1. Script on lambda function\n2. User upload the excel data to S3 host website, then the data is sent to S3\n3. Once the excel data is landed in S3, it triggers lambda to process the data, and it generates the downloadable link to the user. &lt;/p&gt;\n\n&lt;p&gt;I would like to keep the website as an internal tool. So shall I create some limited access user to use this flow. &lt;/p&gt;\n\n&lt;p&gt;It seems I am over complicating this data flow. \ud83e\udd7a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11q4i43", "is_robot_indexable": true, "report_reasons": null, "author": "uk_dataguy", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11q4i43/hosting_python_data_processing_scripts_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11q4i43/hosting_python_data_processing_scripts_on_aws/", "subreddit_subscribers": 92976, "created_utc": 1678697795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm just wondering what everyone's opinions are on this. \n\nI've found that most data pipelines that involve some scripting in my job really doesn't require you to build out classes and methods. I think that data pipeline programming tends to be more linear and less dependent on other code, compared to some front end and backend. \n\nShould I be including proper usage of classes and methods, if name = main, and other programming best practices for all my data pipelines, even the ones that are fairly simple? \n\n\\^\\^ I use Python", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is object oriented programming necessary for most data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qd3fx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678722119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just wondering what everyone&amp;#39;s opinions are on this. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found that most data pipelines that involve some scripting in my job really doesn&amp;#39;t require you to build out classes and methods. I think that data pipeline programming tends to be more linear and less dependent on other code, compared to some front end and backend. &lt;/p&gt;\n\n&lt;p&gt;Should I be including proper usage of classes and methods, if name = main, and other programming best practices for all my data pipelines, even the ones that are fairly simple? &lt;/p&gt;\n\n&lt;p&gt;^^ I use Python&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qd3fx", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11qd3fx/is_object_oriented_programming_necessary_for_most/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qd3fx/is_object_oriented_programming_necessary_for_most/", "subreddit_subscribers": 92976, "created_utc": 1678722119.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \n\nCan you please help me with one problem. \n\nWe have a datalake in Aws. In general we are consuming data with Athena. Recently we get a requirement to build Rest Api to serve data for UI. \nWhat approach would you use, lambda + Athena or dynamodb/documentdb (rel database?) + lambda or maybe something different. \n\nDataset size: 5 GB\n\nDid somebody have experience with something similar? \n\nThanks", "author_fullname": "t2_zfbp4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Api on DataLake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q8hnx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678710911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;Can you please help me with one problem. &lt;/p&gt;\n\n&lt;p&gt;We have a datalake in Aws. In general we are consuming data with Athena. Recently we get a requirement to build Rest Api to serve data for UI. \nWhat approach would you use, lambda + Athena or dynamodb/documentdb (rel database?) + lambda or maybe something different. &lt;/p&gt;\n\n&lt;p&gt;Dataset size: 5 GB&lt;/p&gt;\n\n&lt;p&gt;Did somebody have experience with something similar? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11q8hnx", "is_robot_indexable": true, "report_reasons": null, "author": "yurchenkoiv", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11q8hnx/api_on_datalake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11q8hnx/api_on_datalake/", "subreddit_subscribers": 92976, "created_utc": 1678710911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI started a new job couple of months ago and I have questions for you.\n\nCompany : Retail company with millions and millions of rows of sales. Thousand of sales daily\n\nCurrently : Each morning Qlik automations retrieve data for the DB and store it in .qvd files (qlik format) and then the different dashboard read those files\n\nScenario :\n\nI'm the only BI person of the company and my job consist to retrieve data from the so called PostgreSQL \"data warehouse\" (but for me it's more of a database because it's straight raw facts and dimensions tables that we need to join) that our ERP company made available for us, transform that data and create different dashboards with Qlik.\n\nAt the moment, the company decided to do everything in Qlik, so the data is extract from the DB with qlik, transform in qlik load script and then use in the different reports.\n\nI'm looking to modernize the stack, implement a more robust ELT/ETL process and create a more scalable approach because the company plan to grow a lot in the future.\n\nWhat would you suggest to use at each moment of the process ? (preferably open-source)\n\nJust so you know, intermediate person here (great ELT/ETL, SQL and Python knowledge).\n\nThanks !", "author_fullname": "t2_itk7ibdg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New job - stuck with Qlik", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qkoa6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678739265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I started a new job couple of months ago and I have questions for you.&lt;/p&gt;\n\n&lt;p&gt;Company : Retail company with millions and millions of rows of sales. Thousand of sales daily&lt;/p&gt;\n\n&lt;p&gt;Currently : Each morning Qlik automations retrieve data for the DB and store it in .qvd files (qlik format) and then the different dashboard read those files&lt;/p&gt;\n\n&lt;p&gt;Scenario :&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m the only BI person of the company and my job consist to retrieve data from the so called PostgreSQL &amp;quot;data warehouse&amp;quot; (but for me it&amp;#39;s more of a database because it&amp;#39;s straight raw facts and dimensions tables that we need to join) that our ERP company made available for us, transform that data and create different dashboards with Qlik.&lt;/p&gt;\n\n&lt;p&gt;At the moment, the company decided to do everything in Qlik, so the data is extract from the DB with qlik, transform in qlik load script and then use in the different reports.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to modernize the stack, implement a more robust ELT/ETL process and create a more scalable approach because the company plan to grow a lot in the future.&lt;/p&gt;\n\n&lt;p&gt;What would you suggest to use at each moment of the process ? (preferably open-source)&lt;/p&gt;\n\n&lt;p&gt;Just so you know, intermediate person here (great ELT/ETL, SQL and Python knowledge).&lt;/p&gt;\n\n&lt;p&gt;Thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11qkoa6", "is_robot_indexable": true, "report_reasons": null, "author": "TheAthleticGeek_", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qkoa6/new_job_stuck_with_qlik/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qkoa6/new_job_stuck_with_qlik/", "subreddit_subscribers": 92976, "created_utc": 1678739265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Data Warehouse to Data Cloud: The Snowflake Story", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "name": "t3_11qcrkv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3nKlDMq3WTPacK1-eNkH-MkuvgHg06SEKau6zKWowuA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678721349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/snowflake-data-cloud", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?auto=webp&amp;v=enabled&amp;s=a72fe69de06710651abf0c44c8f2556ef8a71a65", "width": 1270, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c669e0bd35b89a5020783503e6df4726032340cf", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbc75e35c6a198146b2838bc69fa39959cd5856a", "width": 216, "height": 122}, {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=105c574c39e86666861ee8bc864057dfdc9cc8ac", "width": 320, "height": 181}, {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e85924e3efbbee7f9c6c8291192aa5f3ca3b2f12", "width": 640, "height": 362}, {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ed46006d6d7b9a5311eabbcdd308d81201747c3", "width": 960, "height": 544}, {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e1f75d519364073b701e7fe54c0b9bce6a7e89f", "width": 1080, "height": 612}], "variants": {}, "id": "KsjjRxGPbrKT-0-TRLTcnJhDqHkQDqFSLrvZR4ZVSVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11qcrkv", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qcrkv/from_data_warehouse_to_data_cloud_the_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/snowflake-data-cloud", "subreddit_subscribers": 92976, "created_utc": 1678721349.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working on this issue for the past few years. Here are some resources I find helpful!\n\nSpreadsheets to Python: (I'm an author) [https://blog.trymito.io/automating-spreadsheets-with-python-101/](https://blog.trymito.io/automating-spreadsheets-with-python-101/)\n\nConnecting google sheets to Amazon S3 [https://www.youtube.com/watch?v=ksnrlkY55fI](https://www.youtube.com/watch?v=ksnrlkY55fI)\n\nExcel to google sheets: [https://hevodata.com/learn/sync-excel-to-google-sheets/](https://hevodata.com/learn/sync-excel-to-google-sheets/)\n\nSpreadsheets to Data Warehouse:[https://docs.getdbt.com/blog/moving-spreadsheet-data](https://docs.getdbt.com/blog/moving-spreadsheet-data)", "author_fullname": "t2_7vpi3es9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources on Connecting Spreadsheets to Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q3qwm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678694880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working on this issue for the past few years. Here are some resources I find helpful!&lt;/p&gt;\n\n&lt;p&gt;Spreadsheets to Python: (I&amp;#39;m an author) &lt;a href=\"https://blog.trymito.io/automating-spreadsheets-with-python-101/\"&gt;https://blog.trymito.io/automating-spreadsheets-with-python-101/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Connecting google sheets to Amazon S3 &lt;a href=\"https://www.youtube.com/watch?v=ksnrlkY55fI\"&gt;https://www.youtube.com/watch?v=ksnrlkY55fI&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Excel to google sheets: &lt;a href=\"https://hevodata.com/learn/sync-excel-to-google-sheets/\"&gt;https://hevodata.com/learn/sync-excel-to-google-sheets/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Spreadsheets to Data Warehouse:&lt;a href=\"https://docs.getdbt.com/blog/moving-spreadsheet-data\"&gt;https://docs.getdbt.com/blog/moving-spreadsheet-data&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?auto=webp&amp;v=enabled&amp;s=9ff7051f337e335d18563c726d7c244b133f26f4", "width": 5000, "height": 3500}, "resolutions": [{"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=423a950d1ce38a96ec3d95381ba4ba06f8832d06", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88c25a286ea85640bd3cb07259c63ea6cbfe83e0", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a7eedcb5bf84c9d992b1f3717f71362f6c7d461", "width": 320, "height": 224}, {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f93fd7b09f37f6edca1890c7772ae3885179953d", "width": 640, "height": 448}, {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3ea872b87df9eceff2d292b887f72a4ffb69c6f", "width": 960, "height": 672}, {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7dcf46e9d404e8246243dac4c10b27d234c2806f", "width": 1080, "height": 756}], "variants": {}, "id": "4Y55QpCpbLy1dJdjtwk3WSaENOBK22AizVs8dftlcFI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11q3qwm", "is_robot_indexable": true, "report_reasons": null, "author": "Jake_Stack808", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11q3qwm/resources_on_connecting_spreadsheets_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11q3qwm/resources_on_connecting_spreadsheets_to_data/", "subreddit_subscribers": 92976, "created_utc": 1678694880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to understand how our customers are using our webapp (where they are clicking, what tabs are they visiting, time they login, etc) to drive a good dataset that can infer what features are useful and where our end-users spend most of their time.\n\nFor context, we are a B2B SaaS.\n\nWhat have you used in the past and recommend ? \n\nOur warehouse is BigQuery and the front-end webapp is Typescript if this helps. \n\nThank you !", "author_fullname": "t2_ilw3j5ci", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Webapp analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qbiad", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678718359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to understand how our customers are using our webapp (where they are clicking, what tabs are they visiting, time they login, etc) to drive a good dataset that can infer what features are useful and where our end-users spend most of their time.&lt;/p&gt;\n\n&lt;p&gt;For context, we are a B2B SaaS.&lt;/p&gt;\n\n&lt;p&gt;What have you used in the past and recommend ? &lt;/p&gt;\n\n&lt;p&gt;Our warehouse is BigQuery and the front-end webapp is Typescript if this helps. &lt;/p&gt;\n\n&lt;p&gt;Thank you !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11qbiad", "is_robot_indexable": true, "report_reasons": null, "author": "GiacomoLeopardi6", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qbiad/webapp_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qbiad/webapp_analytics/", "subreddit_subscribers": 92976, "created_utc": 1678718359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For a school project I'm looking into the difference when it comes to traditional and modern data platforms. Now I have a hard time finding what defines each of them and some possible examples. \n\nAt first I thought it was comparing data warehouses and data lakes, but that doesnt seems to be it. Can I get some explanations or examples as to what to look for?", "author_fullname": "t2_11u3oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Traditional data platform vs modern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q7g01", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678707967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For a school project I&amp;#39;m looking into the difference when it comes to traditional and modern data platforms. Now I have a hard time finding what defines each of them and some possible examples. &lt;/p&gt;\n\n&lt;p&gt;At first I thought it was comparing data warehouses and data lakes, but that doesnt seems to be it. Can I get some explanations or examples as to what to look for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11q7g01", "is_robot_indexable": true, "report_reasons": null, "author": "Clumsywolfy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11q7g01/traditional_data_platform_vs_modern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11q7g01/traditional_data_platform_vs_modern/", "subreddit_subscribers": 92976, "created_utc": 1678707967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nAs you can see the title,  I wanted to know how Data modelling happens in real project. I have been in Data Engineering for few years, but never got chance to work on data modelling.\n\nHow do you learn data modelling in this case? Only going through courses or a book doesnt give the real experience. Im also asking as interviews point of view.", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modelling in real project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qjhtj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678736749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;As you can see the title,  I wanted to know how Data modelling happens in real project. I have been in Data Engineering for few years, but never got chance to work on data modelling.&lt;/p&gt;\n\n&lt;p&gt;How do you learn data modelling in this case? Only going through courses or a book doesnt give the real experience. Im also asking as interviews point of view.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qjhtj", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qjhtj/data_modelling_in_real_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qjhtj/data_modelling_in_real_project/", "subreddit_subscribers": 92976, "created_utc": 1678736749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to use AWS Glue (unless there is a better approach) to migrate 3000 tables. I\u2019m not sure if this is a great idea because there are a lot of tables totaling 142G of data. Can I create Glue jobs to migrate data from SQL server (on premise) to MariaDB (on premise)? Do I have to migrate to the cloud THEN back down to MariaDB? I\u2019m confused, I\u2019ve only used Glue to create crawlers to migrate data from parquet files to tables in Athena, not something big like this.", "author_fullname": "t2_8ewmlf41", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I migrate data from SQL to MariaDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qi8ug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678734041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to use AWS Glue (unless there is a better approach) to migrate 3000 tables. I\u2019m not sure if this is a great idea because there are a lot of tables totaling 142G of data. Can I create Glue jobs to migrate data from SQL server (on premise) to MariaDB (on premise)? Do I have to migrate to the cloud THEN back down to MariaDB? I\u2019m confused, I\u2019ve only used Glue to create crawlers to migrate data from parquet files to tables in Athena, not something big like this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11qi8ug", "is_robot_indexable": true, "report_reasons": null, "author": "jumpfordespair", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qi8ug/how_do_i_migrate_data_from_sql_to_mariadb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qi8ug/how_do_i_migrate_data_from_sql_to_mariadb/", "subreddit_subscribers": 92976, "created_utc": 1678734041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a big advocate of Kafka so it is a bit of a surprise when a company that I recently joined has a retrofitted solution of using SNS+SQS in their stack to achieve what Kafka offers out of the box. In your opinion what is the advantage of using that? AFAIK Kafka provides everything that the company needs and it is not that hard to setup these days with confluent cloud or MSK.", "author_fullname": "t2_8n43xmar", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SNS/SQS vs Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11r01xo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678780215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a big advocate of Kafka so it is a bit of a surprise when a company that I recently joined has a retrofitted solution of using SNS+SQS in their stack to achieve what Kafka offers out of the box. In your opinion what is the advantage of using that? AFAIK Kafka provides everything that the company needs and it is not that hard to setup these days with confluent cloud or MSK.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11r01xo", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-River1467", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11r01xo/snssqs_vs_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11r01xo/snssqs_vs_kafka/", "subreddit_subscribers": 92976, "created_utc": 1678780215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nIs anyone aware of a downloadable example data warehouse built using Data Vault?\n\nThere are lots of concepts, and I feel like lots of important things that get skimmed over. Examples might include how many hashdiffs in sources if you\u2019re splitting into multiple satellites? How to handle source tables that join when the join isn\u2019t a business key? If a customer table has a created by and modified by field, does this mean the satellite will sit off a link between customer and team member (twice), or is it still just coming off the customer hub? etc.\n\nIn the Microsoft world there is the Contoso db, and the Jaffle Shop in dbt land.\n\nI don\u2019t really care about the technology - Postgres, MS SQL etc., but I do care about the examples of how it\u2019s actually put together.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Vault example db", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11qzdxo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678780571.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678777707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Is anyone aware of a downloadable example data warehouse built using Data Vault?&lt;/p&gt;\n\n&lt;p&gt;There are lots of concepts, and I feel like lots of important things that get skimmed over. Examples might include how many hashdiffs in sources if you\u2019re splitting into multiple satellites? How to handle source tables that join when the join isn\u2019t a business key? If a customer table has a created by and modified by field, does this mean the satellite will sit off a link between customer and team member (twice), or is it still just coming off the customer hub? etc.&lt;/p&gt;\n\n&lt;p&gt;In the Microsoft world there is the Contoso db, and the Jaffle Shop in dbt land.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t really care about the technology - Postgres, MS SQL etc., but I do care about the examples of how it\u2019s actually put together.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11qzdxo", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qzdxo/data_vault_example_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qzdxo/data_vault_example_db/", "subreddit_subscribers": 92976, "created_utc": 1678777707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nSo we have a table called vault in postres which has card num and it's token. This pair is unique.\n In our Hadoop we receive tokens and not cards as part of feeds. \n\nOne of the downstream requirements is that they can't handle tokens and need cards. We have implemented a spark code where we read the vault into a df and take a join with the source feed and replace tokens with cards and then send it to the downstream. \n\nNow this has to be done for multiple reports across multiple countries. So everytime I have to read the vault which is taking a lot of time and memory. \nPlease note we cannot save vault on Hadoop since it's a compliance issue...no cards should be visible to naked eye only tokens can be seen. Do you have any alternate solution to this?", "author_fullname": "t2_7le8j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with solutioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qwb02", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678767756.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678767385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;So we have a table called vault in postres which has card num and it&amp;#39;s token. This pair is unique.\n In our Hadoop we receive tokens and not cards as part of feeds. &lt;/p&gt;\n\n&lt;p&gt;One of the downstream requirements is that they can&amp;#39;t handle tokens and need cards. We have implemented a spark code where we read the vault into a df and take a join with the source feed and replace tokens with cards and then send it to the downstream. &lt;/p&gt;\n\n&lt;p&gt;Now this has to be done for multiple reports across multiple countries. So everytime I have to read the vault which is taking a lot of time and memory. \nPlease note we cannot save vault on Hadoop since it&amp;#39;s a compliance issue...no cards should be visible to naked eye only tokens can be seen. Do you have any alternate solution to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qwb02", "is_robot_indexable": true, "report_reasons": null, "author": "andkad", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qwb02/help_with_solutioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qwb02/help_with_solutioning/", "subreddit_subscribers": 92976, "created_utc": 1678767385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, so I just graduated last year June 2022 \\[23 years old\\] and I got a job offer by July-present as a Data Engineering Analyst at Company A. Currently my goal is to become a Cloud Engineer since I think that path will make my career future proof since most companies nowadays are using the Cloud platform and I also want to work for a foreign/US company to have dollar based salary.\n\n*I am currently getting offered with Company B*\n\n**Company A (July 2022 - Presently working):**An old and big Microfinance company, requires me to use T-SQL (SQL Server), Python and Batch Scripting. Most of the 3 are used for querying purposes, extract to CSV file and for Job Creation. Our company is only using old On-premise servers and we use SSIS for ETL (Which I barely touch). Currently I'm held up with a bond  because of Microsoft  trainings which are: SQL, SSIS, Power BI and Data story telling. (I hardly use the last 3 ).Will implement Cloud for Disaster Recovery of Data.***Monthly*** ***Salary: 35,000 PHP or 617.75 USD***\n\n**Company B:**A new microfinance company, wants me to join their company even without the experience because they are willing to help me. They are willing to invest in me with Azure Data Engineer trainings. As an overview, they are also a microfinance company, who doesn't have any idea how to make the ETL process, they want me to create the Database for them from scratch ( which I have no idea ). But they reassured me that their Azure DeVops Engineer ( 3 start ) will also guide me. The IT head was very kind to me.\n\n***Potential Monthly*** ***Salary: 45,000 PHP+ or 817.61USD***Below are the expertise I think they want me to train in:\n\n* Experience with big data tools: Hadoop, Spark, Kafka, etc.\n* Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.\n* Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.\n* Experience with Azure cloud services: Data Warehouse, Data Factory, Cosmo Databases, Azure Databases\n* Experience with stream-processing systems: Storm, Spark-Streaming, etc.\n* Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\n\n**Final Thoughts:**\n\nShould I risk it and job hop to Company B since I will be hoarding a lot of licenses, training and experience with them, so that after I leave their company I am more qualified for Cloud Engineer jobs?\n\nOr\n\nShould I stick with Company A, gain more experience and wait for their launch of their Cloud platform for Disaster Recovery?  \n\n\n**Note:** \n\nI have a pro-rated **75,000 php** training bond with Company A, if I leave &lt; 2 years.", "author_fullname": "t2_gmb1h0w2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Analyst vs Azure Data Engineer [Job Hop Help]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qvv7g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678766407.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678766079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, so I just graduated last year June 2022 [23 years old] and I got a job offer by July-present as a Data Engineering Analyst at Company A. Currently my goal is to become a Cloud Engineer since I think that path will make my career future proof since most companies nowadays are using the Cloud platform and I also want to work for a foreign/US company to have dollar based salary.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;I am currently getting offered with Company B&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Company A (July 2022 - Presently working):&lt;/strong&gt;An old and big Microfinance company, requires me to use T-SQL (SQL Server), Python and Batch Scripting. Most of the 3 are used for querying purposes, extract to CSV file and for Job Creation. Our company is only using old On-premise servers and we use SSIS for ETL (Which I barely touch). Currently I&amp;#39;m held up with a bond  because of Microsoft  trainings which are: SQL, SSIS, Power BI and Data story telling. (I hardly use the last 3 ).Will implement Cloud for Disaster Recovery of Data.&lt;strong&gt;&lt;em&gt;Monthly&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;Salary: 35,000 PHP or 617.75 USD&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Company B:&lt;/strong&gt;A new microfinance company, wants me to join their company even without the experience because they are willing to help me. They are willing to invest in me with Azure Data Engineer trainings. As an overview, they are also a microfinance company, who doesn&amp;#39;t have any idea how to make the ETL process, they want me to create the Database for them from scratch ( which I have no idea ). But they reassured me that their Azure DeVops Engineer ( 3 start ) will also guide me. The IT head was very kind to me.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Potential Monthly&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;Salary: 45,000 PHP+ or 817.61USD&lt;/em&gt;&lt;/strong&gt;Below are the expertise I think they want me to train in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Experience with big data tools: Hadoop, Spark, Kafka, etc.&lt;/li&gt;\n&lt;li&gt;Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.&lt;/li&gt;\n&lt;li&gt;Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.&lt;/li&gt;\n&lt;li&gt;Experience with Azure cloud services: Data Warehouse, Data Factory, Cosmo Databases, Azure Databases&lt;/li&gt;\n&lt;li&gt;Experience with stream-processing systems: Storm, Spark-Streaming, etc.&lt;/li&gt;\n&lt;li&gt;Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Final Thoughts:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Should I risk it and job hop to Company B since I will be hoarding a lot of licenses, training and experience with them, so that after I leave their company I am more qualified for Cloud Engineer jobs?&lt;/p&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;p&gt;Should I stick with Company A, gain more experience and wait for their launch of their Cloud platform for Disaster Recovery?  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;I have a pro-rated &lt;strong&gt;75,000 php&lt;/strong&gt; training bond with Company A, if I leave &amp;lt; 2 years.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11qvv7g", "is_robot_indexable": true, "report_reasons": null, "author": "Appropriate-Treat456", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qvv7g/data_engineering_analyst_vs_azure_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qvv7g/data_engineering_analyst_vs_azure_data_engineer/", "subreddit_subscribers": 92976, "created_utc": 1678766079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I've been wondering which OSS datalake format I should be looking for to build a greenfield project.\n\nI saw that Hudi has a bunch (and maybe too many) features and configurations to support: [https://hudi.apache.org/docs/configurations/](https://hudi.apache.org/docs/configurations/) \\- However, I've noticed many people complaining about the docs not being clear and hard to track the side effects of those configurations. Everyone makes it sound like is a complex tool and whenever you add a new use case, such as DeltaStreaming, then it has a different behaviour. \n\nDoes anyone mind sharing how it is the experience with Apache Hudi and if those statements make sense? Does still make sense to use Hudi for Lake projects or the easiness of setup with Delta/Iceberg makes them a better choice overall?", "author_fullname": "t2_w6ejzf7n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is Hudi a hard/complex tool as it seems?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qvpnl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678765612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been wondering which OSS datalake format I should be looking for to build a greenfield project.&lt;/p&gt;\n\n&lt;p&gt;I saw that Hudi has a bunch (and maybe too many) features and configurations to support: &lt;a href=\"https://hudi.apache.org/docs/configurations/\"&gt;https://hudi.apache.org/docs/configurations/&lt;/a&gt; - However, I&amp;#39;ve noticed many people complaining about the docs not being clear and hard to track the side effects of those configurations. Everyone makes it sound like is a complex tool and whenever you add a new use case, such as DeltaStreaming, then it has a different behaviour. &lt;/p&gt;\n\n&lt;p&gt;Does anyone mind sharing how it is the experience with Apache Hudi and if those statements make sense? Does still make sense to use Hudi for Lake projects or the easiness of setup with Delta/Iceberg makes them a better choice overall?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qvpnl", "is_robot_indexable": true, "report_reasons": null, "author": "rainyafternoon_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11qvpnl/is_hudi_a_hardcomplex_tool_as_it_seems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qvpnl/is_hudi_a_hardcomplex_tool_as_it_seems/", "subreddit_subscribers": 92976, "created_utc": 1678765612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am building something that collects post data every 5 minutes or so. This is what I envisioned my data strucutre to look like\n\n    {\n     'created_time': 2023-03-10 06:00:00,\n     'message': \"test\",\n     'id': '123456',\n     'data':[2023-03-10 06:05:00:{\n         'impression': 13071,\n         'comment': 9,\n         'like': 202,\n         'share': 21,\n         'click': 174},\n            2023-03-10 06:10:00:{\n         'impression': 13655,\n         'comment': 12,\n         'like': 212,\n         'share': 27,\n         'click': 187}\n    ]\n    \n     }\n\nWhere we would periodically update the data entry every certain periods with the new data, so the next entry would be\n\n    {\n     'created_time': 2023-03-10 06:00:00,\n     'message': \"test\",\n     'id': '123456',\n     'data':[2023-03-10 06:05:00:{\n         'impression': 13071,\n         'comment': 9,\n         'like': 202,\n         'share': 21,\n         'click': 174},\n            2023-03-10 06:10:00:{\n         'impression': 13655,\n         'comment': 12,\n         'like': 212,\n         'share': 27,\n         'click': 187},\n            2023-03-10 06:15:00:{\n         'impression': 130571,\n         'comment': 10,\n         'like': 212,\n         'share': 26,\n         'click': 174}\n    ]\n    \n     }\n\nOne of the main functionalities we are trying to use with the data is to aggregate data by date, e.g. impression in a month. Is this the best way to do it?", "author_fullname": "t2_2knag8t3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think about my data structure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qfnwf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678728165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building something that collects post data every 5 minutes or so. This is what I envisioned my data strucutre to look like&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n &amp;#39;created_time&amp;#39;: 2023-03-10 06:00:00,\n &amp;#39;message&amp;#39;: &amp;quot;test&amp;quot;,\n &amp;#39;id&amp;#39;: &amp;#39;123456&amp;#39;,\n &amp;#39;data&amp;#39;:[2023-03-10 06:05:00:{\n     &amp;#39;impression&amp;#39;: 13071,\n     &amp;#39;comment&amp;#39;: 9,\n     &amp;#39;like&amp;#39;: 202,\n     &amp;#39;share&amp;#39;: 21,\n     &amp;#39;click&amp;#39;: 174},\n        2023-03-10 06:10:00:{\n     &amp;#39;impression&amp;#39;: 13655,\n     &amp;#39;comment&amp;#39;: 12,\n     &amp;#39;like&amp;#39;: 212,\n     &amp;#39;share&amp;#39;: 27,\n     &amp;#39;click&amp;#39;: 187}\n]\n\n }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Where we would periodically update the data entry every certain periods with the new data, so the next entry would be&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n &amp;#39;created_time&amp;#39;: 2023-03-10 06:00:00,\n &amp;#39;message&amp;#39;: &amp;quot;test&amp;quot;,\n &amp;#39;id&amp;#39;: &amp;#39;123456&amp;#39;,\n &amp;#39;data&amp;#39;:[2023-03-10 06:05:00:{\n     &amp;#39;impression&amp;#39;: 13071,\n     &amp;#39;comment&amp;#39;: 9,\n     &amp;#39;like&amp;#39;: 202,\n     &amp;#39;share&amp;#39;: 21,\n     &amp;#39;click&amp;#39;: 174},\n        2023-03-10 06:10:00:{\n     &amp;#39;impression&amp;#39;: 13655,\n     &amp;#39;comment&amp;#39;: 12,\n     &amp;#39;like&amp;#39;: 212,\n     &amp;#39;share&amp;#39;: 27,\n     &amp;#39;click&amp;#39;: 187},\n        2023-03-10 06:15:00:{\n     &amp;#39;impression&amp;#39;: 130571,\n     &amp;#39;comment&amp;#39;: 10,\n     &amp;#39;like&amp;#39;: 212,\n     &amp;#39;share&amp;#39;: 26,\n     &amp;#39;click&amp;#39;: 174}\n]\n\n }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;One of the main functionalities we are trying to use with the data is to aggregate data by date, e.g. impression in a month. Is this the best way to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qfnwf", "is_robot_indexable": true, "report_reasons": null, "author": "lordgriefter", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qfnwf/what_do_you_think_about_my_data_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qfnwf/what_do_you_think_about_my_data_structure/", "subreddit_subscribers": 92976, "created_utc": 1678728165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New  to Data Engineering and came across Zeppelin &amp; Jupyter. Any recommendations, pro / cons?", "author_fullname": "t2_f4rnp5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zeppelin vs Jupyter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qde1c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678722825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New  to Data Engineering and came across Zeppelin &amp;amp; Jupyter. Any recommendations, pro / cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qde1c", "is_robot_indexable": true, "report_reasons": null, "author": "alwaysSearching23", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qde1c/zeppelin_vs_jupyter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qde1c/zeppelin_vs_jupyter/", "subreddit_subscribers": 92976, "created_utc": 1678722825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our systems have 20+ services. Each service has its own custom code to send email alerts when the script fails. I have been given the task to write a generic email alert service that can be used by all the other services. This is the design that I am planning to implement:\n\nEach service will write the final status for email to RabbitMQ\n\nMy email alert services will keep on continuously consuming data from the RabbitMQ queue and send the email alerts to the respective target. \n\nEven if email alert service goes down for some reason, the alerts will keep on getting captured in RabbitMQ and won't be lost. As soon as email alert service comes back online, all the backlog alerts will be consumed, and emails will be shared.\n\nIs this approach correct?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my approach for a fully decoupled email service correct? Please see the description inside.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qc9pg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678720182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our systems have 20+ services. Each service has its own custom code to send email alerts when the script fails. I have been given the task to write a generic email alert service that can be used by all the other services. This is the design that I am planning to implement:&lt;/p&gt;\n\n&lt;p&gt;Each service will write the final status for email to RabbitMQ&lt;/p&gt;\n\n&lt;p&gt;My email alert services will keep on continuously consuming data from the RabbitMQ queue and send the email alerts to the respective target. &lt;/p&gt;\n\n&lt;p&gt;Even if email alert service goes down for some reason, the alerts will keep on getting captured in RabbitMQ and won&amp;#39;t be lost. As soon as email alert service comes back online, all the backlog alerts will be consumed, and emails will be shared.&lt;/p&gt;\n\n&lt;p&gt;Is this approach correct?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qc9pg", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qc9pg/is_my_approach_for_a_fully_decoupled_email/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qc9pg/is_my_approach_for_a_fully_decoupled_email/", "subreddit_subscribers": 92976, "created_utc": 1678720182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm building a realtime analysis solution for our domain oriented microservice backend. All domain emit events in kafka. I'm looking for a solution to ingest data in an OLAP database based on processing those events  (enrichment, filtering etc.). I found [https://siddhi.io/](https://siddhi.io/) which looks promising. Since the last release (2019) the product is now part of WSo2 solution.   \nI'm also looking at [https://www.benthos.dev/](https://www.benthos.dev/).   \nI'm more interested in a declarative solution than code. \n\nI already try with [https://ksqldb.io/](https://ksqldb.io/) but it need too much ressources compare to other tools.\n\nAny feedback is more than welcome on similar architecture.", "author_fullname": "t2_yfmcf9k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Feedback on Siddhi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q7abl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678707476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m building a realtime analysis solution for our domain oriented microservice backend. All domain emit events in kafka. I&amp;#39;m looking for a solution to ingest data in an OLAP database based on processing those events  (enrichment, filtering etc.). I found &lt;a href=\"https://siddhi.io/\"&gt;https://siddhi.io/&lt;/a&gt; which looks promising. Since the last release (2019) the product is now part of WSo2 solution.&lt;br/&gt;\nI&amp;#39;m also looking at &lt;a href=\"https://www.benthos.dev/\"&gt;https://www.benthos.dev/&lt;/a&gt;.&lt;br/&gt;\nI&amp;#39;m more interested in a declarative solution than code. &lt;/p&gt;\n\n&lt;p&gt;I already try with &lt;a href=\"https://ksqldb.io/\"&gt;https://ksqldb.io/&lt;/a&gt; but it need too much ressources compare to other tools.&lt;/p&gt;\n\n&lt;p&gt;Any feedback is more than welcome on similar architecture.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gGuJ_TvlU8xzsFA1OftaR6DHf50CiuK6wMSvfikIbwk.jpg?auto=webp&amp;v=enabled&amp;s=0b655fa39ada3de91b4c285f789d741cc7575f45", "width": 2784, "height": 1031}, "resolutions": [{"url": "https://external-preview.redd.it/gGuJ_TvlU8xzsFA1OftaR6DHf50CiuK6wMSvfikIbwk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=552e8af85d5787582284ef7021acb24205ee743b", "width": 108, "height": 39}, {"url": "https://external-preview.redd.it/gGuJ_TvlU8xzsFA1OftaR6DHf50CiuK6wMSvfikIbwk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a5a37899a246d64d18786c5a464123509b7f7a9", "width": 216, "height": 79}, {"url": "https://external-preview.redd.it/gGuJ_TvlU8xzsFA1OftaR6DHf50CiuK6wMSvfikIbwk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a6d8fe9a90874d0c40b7d2f11b9c0fd76b5e913", "width": 320, "height": 118}, {"url": "https://external-preview.redd.it/gGuJ_TvlU8xzsFA1OftaR6DHf50CiuK6wMSvfikIbwk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=323ce159276e6c03a1d1a2ade2320d722c8ec5d4", "width": 640, "height": 237}, {"url": "https://external-preview.redd.it/gGuJ_TvlU8xzsFA1OftaR6DHf50CiuK6wMSvfikIbwk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36518d594b85490104f8f883f2379f83f8719f52", "width": 960, "height": 355}, {"url": "https://external-preview.redd.it/gGuJ_TvlU8xzsFA1OftaR6DHf50CiuK6wMSvfikIbwk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0411056c2888a10bc890fc305dc111aa9f619466", "width": 1080, "height": 399}], "variants": {}, "id": "yCRWOjwVJOaG3icYU-QrqLuxkBD6h2YnvYeoj8w34Q4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11q7abl", "is_robot_indexable": true, "report_reasons": null, "author": "Jbpin", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11q7abl/seeking_feedback_on_siddhi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11q7abl/seeking_feedback_on_siddhi/", "subreddit_subscribers": 92976, "created_utc": 1678707476.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}