{"kind": "Listing", "data": {"after": "t3_11qxvjl", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks,I am researching the challenges of data engineering to develop a deeper understanding of the problems and challenges faced by the teams building data products.\n\nI used to be a data engineer between 2009 and 2014. In 2014, I got into product management and I am currently in a product lead role. I have always worked products which included a large amount of analytics, insights, predictive models, machine learning in software companies serving healthcare, surveillance, automotive, and ecommerce.\n\nIn my experience over the past 14 years, the data tooling ecosystem has expanded a lot.However, I am still in a scenario where the cost of data infrastructure and tooling is expensive.\n\n* Projects are complex and long drawn.\n* It is super hard to solve basic issues of data quality even reactively.\n* Maintaining trust in the data assets is really hard.\n* Data literacy of decision makers is not up to the mark in a lot of cases.\n* Stakeholders expect miracles and stuff to just work.\n* Very few people can explain the attributes, the calculations, the metrics, the insights, and the implications.\n* Everyone is just promising stuff and punting the inevitable reality of face the hard problem and solve it properly.\n\nI am just trying to research and gather inputs from the community on the nagging challenges of building products now to inform my product development and to inform a course that I am building to develop data product managers (because it is really difficult to find candidates to hire)\n\nMy question for you is:\n\n* What are your top 3 challenges in engineering data flows and pipelines?\n   * Is it the data inventory, quality, governance, accessibility, etc.?\n   * Is the infrastructure, the complexity of building, deploying, administering the systems?\n   * Is it the challenge of organizational structure, talent, capacity, leadership?\n   * Is it communication, silos, lack of alignment?\n   * Is it cost, performance, complexity of infrastructure?\n* What is preventing you from building valuable data products?\n\nFor me, infrastructure cost, performance of existing tools, spaghetti code, lack of data expertise among leadership stakeholders has been the biggest headwinds to progress.\n\nLast year, at one point, our AWS costs were $1.6 for every $1 a customer paid us. After working on a year and reducing substantial tech debt, we got to AWS cost of $0.6 for every $1 revenue. Still, there is no recognition, leadership is reluctant to fix data quality issues.\n\nAs a product lead, I have been able to influence some, but it's a lot of compounding challenges.\n\nDoes this resonate with folks?\n\nWhat are the top challenges you are facing?\n\nWhat are some solutions or workaround that have worked for you?\n\nLooking forward to your responses.", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some of the sticky problems in your data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qpfwo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678762283.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678749600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,I am researching the challenges of data engineering to develop a deeper understanding of the problems and challenges faced by the teams building data products.&lt;/p&gt;\n\n&lt;p&gt;I used to be a data engineer between 2009 and 2014. In 2014, I got into product management and I am currently in a product lead role. I have always worked products which included a large amount of analytics, insights, predictive models, machine learning in software companies serving healthcare, surveillance, automotive, and ecommerce.&lt;/p&gt;\n\n&lt;p&gt;In my experience over the past 14 years, the data tooling ecosystem has expanded a lot.However, I am still in a scenario where the cost of data infrastructure and tooling is expensive.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Projects are complex and long drawn.&lt;/li&gt;\n&lt;li&gt;It is super hard to solve basic issues of data quality even reactively.&lt;/li&gt;\n&lt;li&gt;Maintaining trust in the data assets is really hard.&lt;/li&gt;\n&lt;li&gt;Data literacy of decision makers is not up to the mark in a lot of cases.&lt;/li&gt;\n&lt;li&gt;Stakeholders expect miracles and stuff to just work.&lt;/li&gt;\n&lt;li&gt;Very few people can explain the attributes, the calculations, the metrics, the insights, and the implications.&lt;/li&gt;\n&lt;li&gt;Everyone is just promising stuff and punting the inevitable reality of face the hard problem and solve it properly.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am just trying to research and gather inputs from the community on the nagging challenges of building products now to inform my product development and to inform a course that I am building to develop data product managers (because it is really difficult to find candidates to hire)&lt;/p&gt;\n\n&lt;p&gt;My question for you is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are your top 3 challenges in engineering data flows and pipelines?\n\n&lt;ul&gt;\n&lt;li&gt;Is it the data inventory, quality, governance, accessibility, etc.?&lt;/li&gt;\n&lt;li&gt;Is the infrastructure, the complexity of building, deploying, administering the systems?&lt;/li&gt;\n&lt;li&gt;Is it the challenge of organizational structure, talent, capacity, leadership?&lt;/li&gt;\n&lt;li&gt;Is it communication, silos, lack of alignment?&lt;/li&gt;\n&lt;li&gt;Is it cost, performance, complexity of infrastructure?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;What is preventing you from building valuable data products?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For me, infrastructure cost, performance of existing tools, spaghetti code, lack of data expertise among leadership stakeholders has been the biggest headwinds to progress.&lt;/p&gt;\n\n&lt;p&gt;Last year, at one point, our AWS costs were $1.6 for every $1 a customer paid us. After working on a year and reducing substantial tech debt, we got to AWS cost of $0.6 for every $1 revenue. Still, there is no recognition, leadership is reluctant to fix data quality issues.&lt;/p&gt;\n\n&lt;p&gt;As a product lead, I have been able to influence some, but it&amp;#39;s a lot of compounding challenges.&lt;/p&gt;\n\n&lt;p&gt;Does this resonate with folks?&lt;/p&gt;\n\n&lt;p&gt;What are the top challenges you are facing?&lt;/p&gt;\n\n&lt;p&gt;What are some solutions or workaround that have worked for you?&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your responses.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qpfwo", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qpfwo/what_are_some_of_the_sticky_problems_in_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qpfwo/what_are_some_of_the_sticky_problems_in_your_data/", "subreddit_subscribers": 92994, "created_utc": 1678749600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Junior data engineer here, currently i'm debugging my data pipelines by adding logging statements in between the code like a caveman. I'm sure there is a better way to debug pipelines, but I'm not sure debugging works in data engineering like it does for the rest of software development? Am i wrong? Is there a better way?", "author_fullname": "t2_m2jhpzk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you debug your pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qw5jt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678766907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Junior data engineer here, currently i&amp;#39;m debugging my data pipelines by adding logging statements in between the code like a caveman. I&amp;#39;m sure there is a better way to debug pipelines, but I&amp;#39;m not sure debugging works in data engineering like it does for the rest of software development? Am i wrong? Is there a better way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11qw5jt", "is_robot_indexable": true, "report_reasons": null, "author": "IntraspeciesFever", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qw5jt/how_do_you_debug_your_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qw5jt/how_do_you_debug_your_pipelines/", "subreddit_subscribers": 92994, "created_utc": 1678766907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitHub - sbalnojan/FDE-airflow-tutorial: Functional Data Engineering tutorial in Python &amp; Airflow.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_11qmhaj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9QtqKuBwcDTiYFrErfytfCQ3NAW0uZRJVnbWOOo9RzM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678743132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/sbalnojan/FDE-airflow-tutorial", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?auto=webp&amp;v=enabled&amp;s=4a992deebea52f66a4c5e88e6466156d32a0dd98", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3e2bc5b35a61ced16cb4fa063b571f5bd9f3940", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e171412602c623c7e60b4b533b88a7a5159894ee", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff46d52191e2c54884ed3e8a9b177239f3cea110", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b2641103341efa415e10499b0ff0d034c1fd4bd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6491b8af7551fd7d56f00b5f6379866f335ebc95", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ApAjrK-ZaiZ7tlhZpMaPKxcmjrDj-rgKGUL8n2hhIAM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44a03be959c6da453c8bb7d4dba2a75894d1f83b", "width": 1080, "height": 540}], "variants": {}, "id": "Nca_dx-vFQi3_J7fErsS6JRyq0Xp24ZjcUheqZ6Ol5c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11qmhaj", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qmhaj/github_sbalnojanfdeairflowtutorial_functional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/sbalnojan/FDE-airflow-tutorial", "subreddit_subscribers": 92994, "created_utc": 1678743132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm just wondering what everyone's opinions are on this. \n\nI've found that most data pipelines that involve some scripting in my job really doesn't require you to build out classes and methods. I think that data pipeline programming tends to be more linear and less dependent on other code, compared to some front end and backend. \n\nShould I be including proper usage of classes and methods, if name = main, and other programming best practices for all my data pipelines, even the ones that are fairly simple? \n\n\\^\\^ I use Python", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is object oriented programming necessary for most data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qd3fx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678722119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just wondering what everyone&amp;#39;s opinions are on this. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found that most data pipelines that involve some scripting in my job really doesn&amp;#39;t require you to build out classes and methods. I think that data pipeline programming tends to be more linear and less dependent on other code, compared to some front end and backend. &lt;/p&gt;\n\n&lt;p&gt;Should I be including proper usage of classes and methods, if name = main, and other programming best practices for all my data pipelines, even the ones that are fairly simple? &lt;/p&gt;\n\n&lt;p&gt;^^ I use Python&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qd3fx", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11qd3fx/is_object_oriented_programming_necessary_for_most/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qd3fx/is_object_oriented_programming_necessary_for_most/", "subreddit_subscribers": 92994, "created_utc": 1678722119.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a big advocate of Kafka so it is a bit of a surprise when a company that I recently joined has a retrofitted solution of using SNS+SQS in their stack to achieve what Kafka offers out of the box. In your opinion what is the advantage of using that? AFAIK Kafka provides everything that the company needs and it is not that hard to setup these days with confluent cloud or MSK.", "author_fullname": "t2_8n43xmar", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SNS/SQS vs Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11r01xo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678780215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a big advocate of Kafka so it is a bit of a surprise when a company that I recently joined has a retrofitted solution of using SNS+SQS in their stack to achieve what Kafka offers out of the box. In your opinion what is the advantage of using that? AFAIK Kafka provides everything that the company needs and it is not that hard to setup these days with confluent cloud or MSK.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11r01xo", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-River1467", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11r01xo/snssqs_vs_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11r01xo/snssqs_vs_kafka/", "subreddit_subscribers": 92994, "created_utc": 1678780215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Data Warehouse to Data Cloud: The Snowflake Story", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "name": "t3_11qcrkv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3nKlDMq3WTPacK1-eNkH-MkuvgHg06SEKau6zKWowuA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678721349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/snowflake-data-cloud", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?auto=webp&amp;v=enabled&amp;s=a72fe69de06710651abf0c44c8f2556ef8a71a65", "width": 1270, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c669e0bd35b89a5020783503e6df4726032340cf", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbc75e35c6a198146b2838bc69fa39959cd5856a", "width": 216, "height": 122}, {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=105c574c39e86666861ee8bc864057dfdc9cc8ac", "width": 320, "height": 181}, {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e85924e3efbbee7f9c6c8291192aa5f3ca3b2f12", "width": 640, "height": 362}, {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ed46006d6d7b9a5311eabbcdd308d81201747c3", "width": 960, "height": 544}, {"url": "https://external-preview.redd.it/DmpJEmzAq0p6TPWKt9yxUYQNn8NjeSTI_djZdoU86dA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e1f75d519364073b701e7fe54c0b9bce6a7e89f", "width": 1080, "height": 612}], "variants": {}, "id": "KsjjRxGPbrKT-0-TRLTcnJhDqHkQDqFSLrvZR4ZVSVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11qcrkv", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qcrkv/from_data_warehouse_to_data_cloud_the_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/snowflake-data-cloud", "subreddit_subscribers": 92994, "created_utc": 1678721349.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI started a new job couple of months ago and I have questions for you.\n\nCompany : Retail company with millions and millions of rows of sales. Thousand of sales daily\n\nCurrently : Each morning Qlik automations retrieve data for the DB and store it in .qvd files (qlik format) and then the different dashboard read those files\n\nScenario :\n\nI'm the only BI person of the company and my job consist to retrieve data from the so called PostgreSQL \"data warehouse\" (but for me it's more of a database because it's straight raw facts and dimensions tables that we need to join) that our ERP company made available for us, transform that data and create different dashboards with Qlik.\n\nAt the moment, the company decided to do everything in Qlik, so the data is extract from the DB with qlik, transform in qlik load script and then use in the different reports.\n\nI'm looking to modernize the stack, implement a more robust ELT/ETL process and create a more scalable approach because the company plan to grow a lot in the future.\n\nWhat would you suggest to use at each moment of the process ? (preferably open-source)\n\nJust so you know, intermediate person here (great ELT/ETL, SQL and Python knowledge).\n\nThanks !", "author_fullname": "t2_itk7ibdg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New job - stuck with Qlik", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qkoa6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678739265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I started a new job couple of months ago and I have questions for you.&lt;/p&gt;\n\n&lt;p&gt;Company : Retail company with millions and millions of rows of sales. Thousand of sales daily&lt;/p&gt;\n\n&lt;p&gt;Currently : Each morning Qlik automations retrieve data for the DB and store it in .qvd files (qlik format) and then the different dashboard read those files&lt;/p&gt;\n\n&lt;p&gt;Scenario :&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m the only BI person of the company and my job consist to retrieve data from the so called PostgreSQL &amp;quot;data warehouse&amp;quot; (but for me it&amp;#39;s more of a database because it&amp;#39;s straight raw facts and dimensions tables that we need to join) that our ERP company made available for us, transform that data and create different dashboards with Qlik.&lt;/p&gt;\n\n&lt;p&gt;At the moment, the company decided to do everything in Qlik, so the data is extract from the DB with qlik, transform in qlik load script and then use in the different reports.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to modernize the stack, implement a more robust ELT/ETL process and create a more scalable approach because the company plan to grow a lot in the future.&lt;/p&gt;\n\n&lt;p&gt;What would you suggest to use at each moment of the process ? (preferably open-source)&lt;/p&gt;\n\n&lt;p&gt;Just so you know, intermediate person here (great ELT/ETL, SQL and Python knowledge).&lt;/p&gt;\n\n&lt;p&gt;Thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11qkoa6", "is_robot_indexable": true, "report_reasons": null, "author": "TheAthleticGeek_", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qkoa6/new_job_stuck_with_qlik/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qkoa6/new_job_stuck_with_qlik/", "subreddit_subscribers": 92994, "created_utc": 1678739265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to understand how our customers are using our webapp (where they are clicking, what tabs are they visiting, time they login, etc) to drive a good dataset that can infer what features are useful and where our end-users spend most of their time.\n\nFor context, we are a B2B SaaS.\n\nWhat have you used in the past and recommend ? \n\nOur warehouse is BigQuery and the front-end webapp is Typescript if this helps. \n\nThank you !", "author_fullname": "t2_ilw3j5ci", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Webapp analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qbiad", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678718359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to understand how our customers are using our webapp (where they are clicking, what tabs are they visiting, time they login, etc) to drive a good dataset that can infer what features are useful and where our end-users spend most of their time.&lt;/p&gt;\n\n&lt;p&gt;For context, we are a B2B SaaS.&lt;/p&gt;\n\n&lt;p&gt;What have you used in the past and recommend ? &lt;/p&gt;\n\n&lt;p&gt;Our warehouse is BigQuery and the front-end webapp is Typescript if this helps. &lt;/p&gt;\n\n&lt;p&gt;Thank you !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11qbiad", "is_robot_indexable": true, "report_reasons": null, "author": "GiacomoLeopardi6", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qbiad/webapp_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qbiad/webapp_analytics/", "subreddit_subscribers": 92994, "created_utc": 1678718359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nIs anyone aware of a downloadable example data warehouse built using Data Vault?\n\nThere are lots of concepts, and I feel like lots of important things that get skimmed over. Examples might include how many hashdiffs in sources if you\u2019re splitting into multiple satellites? How to handle source tables that join when the join isn\u2019t a business key? If a customer table has a created by and modified by field, does this mean the satellite will sit off a link between customer and team member (twice), or is it still just coming off the customer hub? etc.\n\nIn the Microsoft world there is the Contoso db, and the Jaffle Shop in dbt land.\n\nI don\u2019t really care about the technology - Postgres, MS SQL etc., but I do care about the examples of how it\u2019s actually put together.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Vault example db", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qzdxo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678780571.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678777707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Is anyone aware of a downloadable example data warehouse built using Data Vault?&lt;/p&gt;\n\n&lt;p&gt;There are lots of concepts, and I feel like lots of important things that get skimmed over. Examples might include how many hashdiffs in sources if you\u2019re splitting into multiple satellites? How to handle source tables that join when the join isn\u2019t a business key? If a customer table has a created by and modified by field, does this mean the satellite will sit off a link between customer and team member (twice), or is it still just coming off the customer hub? etc.&lt;/p&gt;\n\n&lt;p&gt;In the Microsoft world there is the Contoso db, and the Jaffle Shop in dbt land.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t really care about the technology - Postgres, MS SQL etc., but I do care about the examples of how it\u2019s actually put together.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11qzdxo", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qzdxo/data_vault_example_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qzdxo/data_vault_example_db/", "subreddit_subscribers": 92994, "created_utc": 1678777707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I've been wondering which OSS datalake format I should be looking for to build a greenfield project.\n\nI saw that Hudi has a bunch (and maybe too many) features and configurations to support: [https://hudi.apache.org/docs/configurations/](https://hudi.apache.org/docs/configurations/) \\- However, I've noticed many people complaining about the docs not being clear and hard to track the side effects of those configurations. Everyone makes it sound like is a complex tool and whenever you add a new use case, such as DeltaStreaming, then it has a different behaviour. \n\nDoes anyone mind sharing how it is the experience with Apache Hudi and if those statements make sense? Does still make sense to use Hudi for Lake projects or the easiness of setup with Delta/Iceberg makes them a better choice overall?", "author_fullname": "t2_w6ejzf7n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is Hudi a hard/complex tool as it seems?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qvpnl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678765612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been wondering which OSS datalake format I should be looking for to build a greenfield project.&lt;/p&gt;\n\n&lt;p&gt;I saw that Hudi has a bunch (and maybe too many) features and configurations to support: &lt;a href=\"https://hudi.apache.org/docs/configurations/\"&gt;https://hudi.apache.org/docs/configurations/&lt;/a&gt; - However, I&amp;#39;ve noticed many people complaining about the docs not being clear and hard to track the side effects of those configurations. Everyone makes it sound like is a complex tool and whenever you add a new use case, such as DeltaStreaming, then it has a different behaviour. &lt;/p&gt;\n\n&lt;p&gt;Does anyone mind sharing how it is the experience with Apache Hudi and if those statements make sense? Does still make sense to use Hudi for Lake projects or the easiness of setup with Delta/Iceberg makes them a better choice overall?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qvpnl", "is_robot_indexable": true, "report_reasons": null, "author": "rainyafternoon_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11qvpnl/is_hudi_a_hardcomplex_tool_as_it_seems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qvpnl/is_hudi_a_hardcomplex_tool_as_it_seems/", "subreddit_subscribers": 92994, "created_utc": 1678765612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nAs you can see the title,  I wanted to know how Data modelling happens in real project. I have been in Data Engineering for few years, but never got chance to work on data modelling.\n\nHow do you learn data modelling in this case? Only going through courses or a book doesnt give the real experience. Im also asking as interviews point of view.", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modelling in real project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qjhtj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678736749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;As you can see the title,  I wanted to know how Data modelling happens in real project. I have been in Data Engineering for few years, but never got chance to work on data modelling.&lt;/p&gt;\n\n&lt;p&gt;How do you learn data modelling in this case? Only going through courses or a book doesnt give the real experience. Im also asking as interviews point of view.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qjhtj", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qjhtj/data_modelling_in_real_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qjhtj/data_modelling_in_real_project/", "subreddit_subscribers": 92994, "created_utc": 1678736749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to use AWS Glue (unless there is a better approach) to migrate 3000 tables. I\u2019m not sure if this is a great idea because there are a lot of tables totaling 142G of data. Can I create Glue jobs to migrate data from SQL server (on premise) to MariaDB (on premise)? Do I have to migrate to the cloud THEN back down to MariaDB? I\u2019m confused, I\u2019ve only used Glue to create crawlers to migrate data from parquet files to tables in Athena, not something big like this.", "author_fullname": "t2_8ewmlf41", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I migrate data from SQL to MariaDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qi8ug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678734041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to use AWS Glue (unless there is a better approach) to migrate 3000 tables. I\u2019m not sure if this is a great idea because there are a lot of tables totaling 142G of data. Can I create Glue jobs to migrate data from SQL server (on premise) to MariaDB (on premise)? Do I have to migrate to the cloud THEN back down to MariaDB? I\u2019m confused, I\u2019ve only used Glue to create crawlers to migrate data from parquet files to tables in Athena, not something big like this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11qi8ug", "is_robot_indexable": true, "report_reasons": null, "author": "jumpfordespair", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qi8ug/how_do_i_migrate_data_from_sql_to_mariadb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qi8ug/how_do_i_migrate_data_from_sql_to_mariadb/", "subreddit_subscribers": 92994, "created_utc": 1678734041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\n\nI have a Spark Glue job that uses jdbc to retrieve data from 10 lake formation tables. I then enter this data into an S3 bucket, use that bucket to create a MySQL query, and then insert that query back into an S3 bucket. The first stage of the Spark task takes longer than 4 hours to finish. Can someone give me some advice on how to optimise it?\n\nAnd in next step, I am using the loaded data to run complex Oracle SQL query for transformation by using Spark Sql, but spark Sql doesn't support a lot of Oracle SQL function and i Don't know how i am going to chnage it.\n\nAnd, i am using the right approach? Please suggest \n\nNote: For the larger databases, I already use Partition and S3 for data appending.", "author_fullname": "t2_a19a2f0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark on Glue for ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11r26ob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678788297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;\n\n&lt;p&gt;I have a Spark Glue job that uses jdbc to retrieve data from 10 lake formation tables. I then enter this data into an S3 bucket, use that bucket to create a MySQL query, and then insert that query back into an S3 bucket. The first stage of the Spark task takes longer than 4 hours to finish. Can someone give me some advice on how to optimise it?&lt;/p&gt;\n\n&lt;p&gt;And in next step, I am using the loaded data to run complex Oracle SQL query for transformation by using Spark Sql, but spark Sql doesn&amp;#39;t support a lot of Oracle SQL function and i Don&amp;#39;t know how i am going to chnage it.&lt;/p&gt;\n\n&lt;p&gt;And, i am using the right approach? Please suggest &lt;/p&gt;\n\n&lt;p&gt;Note: For the larger databases, I already use Partition and S3 for data appending.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11r26ob", "is_robot_indexable": true, "report_reasons": null, "author": "anurag_pandit1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11r26ob/spark_on_glue_for_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11r26ob/spark_on_glue_for_etl/", "subreddit_subscribers": 92994, "created_utc": 1678788297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4mcloivn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CNCF V6d: Zero-Copy In-Memory Sharing of Large (Distributed) Immutable Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11r1ysy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ommIxXhOwG222qnjLx6sMKQw4JhQvjIZTFtUr5df4Ts.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678787477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "infoq.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.infoq.com/news/2023/03/zero-copy-v6d/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HDLLP_IlHg-5H-4EFJR3U1JHkfGTs-Lb04S2tZA5i1k.jpg?auto=webp&amp;v=enabled&amp;s=ddfcbe3b1ae5a7fe4f552123b0c61dcbcd39548a", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/HDLLP_IlHg-5H-4EFJR3U1JHkfGTs-Lb04S2tZA5i1k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2c8027d53fbf771ae38279d4a9a58d6535d2679", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/HDLLP_IlHg-5H-4EFJR3U1JHkfGTs-Lb04S2tZA5i1k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3717aa4d9e6b38c3a3b586a23ab7e8289c031d4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/HDLLP_IlHg-5H-4EFJR3U1JHkfGTs-Lb04S2tZA5i1k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d747347e39ad4a2ab83c394b4e22ea989ea873f", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/HDLLP_IlHg-5H-4EFJR3U1JHkfGTs-Lb04S2tZA5i1k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1fb08955324624c3b8fd9258c82a743dcbe1e19e", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/HDLLP_IlHg-5H-4EFJR3U1JHkfGTs-Lb04S2tZA5i1k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d523e063be20e74b3ae6111ffe137a01feda2f6", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/HDLLP_IlHg-5H-4EFJR3U1JHkfGTs-Lb04S2tZA5i1k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bdd8a330d21ebde844bc830eddafd56ef9ab89c", "width": 1080, "height": 567}], "variants": {}, "id": "qV6JvCFADPX8WWVLFXzanKi3Nji5fB-K1tbd5HX4pBg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11r1ysy", "is_robot_indexable": true, "report_reasons": null, "author": "ElrasX", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11r1ysy/cncf_v6d_zerocopy_inmemory_sharing_of_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.infoq.com/news/2023/03/zero-copy-v6d/", "subreddit_subscribers": 92994, "created_utc": 1678787477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ffabopog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Release 0.3.2 of qbeast-spark!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_11r1rvv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vLGM2nBY967FIk5MwT_I8a3evKVU2-89wvBv7MzAT50.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678786733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/Qbeast-io/qbeast-spark/releases/tag/v0.3.2", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xoaps7fQrGI-RdO9BdALAz-PJ1imQSLUlNAWh8uS9j0.jpg?auto=webp&amp;v=enabled&amp;s=40774221d1d7609640287f4898a7ea01362475ec", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/xoaps7fQrGI-RdO9BdALAz-PJ1imQSLUlNAWh8uS9j0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cd2ce838278cab83bb59db8b37b7a83b679c08f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/xoaps7fQrGI-RdO9BdALAz-PJ1imQSLUlNAWh8uS9j0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da4f45b435cc24fb8c31eb9d4c9af8ea7985bdf5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/xoaps7fQrGI-RdO9BdALAz-PJ1imQSLUlNAWh8uS9j0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d86deeb3c26311d920a94007bc9cdd4916705ab5", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/xoaps7fQrGI-RdO9BdALAz-PJ1imQSLUlNAWh8uS9j0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fcb955610383ca9306a42e5a3855723ff1866135", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/xoaps7fQrGI-RdO9BdALAz-PJ1imQSLUlNAWh8uS9j0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06ce954553450a34086d63a9f3eb20e997c929da", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/xoaps7fQrGI-RdO9BdALAz-PJ1imQSLUlNAWh8uS9j0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e39fd0e2ac6886df8a0ef789ef67a03da872cbe", "width": 1080, "height": 540}], "variants": {}, "id": "zsPu7xyD_cgcv-Dld3-pRZ9zs0RcGQAHGo8eO30FFac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11r1rvv", "is_robot_indexable": true, "report_reasons": null, "author": "paolapardo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11r1rvv/release_032_of_qbeastspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/Qbeast-io/qbeast-spark/releases/tag/v0.3.2", "subreddit_subscribers": 92994, "created_utc": 1678786733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Put your thoughts on where are the current demands and what will be the future demand, Which will sustain for longer.\n\n**Traditional Data Engineering approach(On-Prem Tools) vs Cloud Based approach (Native features in Azure/AWS/GCP)**", "author_fullname": "t2_svzav4ot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Traditional Data Engineering approach(On-Prem Tools) vs Cloud Based approach", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11r0nx7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678782493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Put your thoughts on where are the current demands and what will be the future demand, Which will sustain for longer.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Traditional Data Engineering approach(On-Prem Tools) vs Cloud Based approach (Native features in Azure/AWS/GCP)&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11r0nx7", "is_robot_indexable": true, "report_reasons": null, "author": "SpaceIntelligent6910", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11r0nx7/traditional_data_engineering_approachonprem_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11r0nx7/traditional_data_engineering_approachonprem_tools/", "subreddit_subscribers": 92994, "created_utc": 1678782493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nSo we have a table called vault in postres which has card num and it's token. This pair is unique.\n In our Hadoop we receive tokens and not cards as part of feeds. \n\nOne of the downstream requirements is that they can't handle tokens and need cards. We have implemented a spark code where we read the vault into a df and take a join with the source feed and replace tokens with cards and then send it to the downstream. \n\nNow this has to be done for multiple reports across multiple countries. So everytime I have to read the vault which is taking a lot of time and memory. \nPlease note we cannot save vault on Hadoop since it's a compliance issue...no cards should be visible to naked eye only tokens can be seen. Do you have any alternate solution to this?", "author_fullname": "t2_7le8j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with solutioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qwb02", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678767756.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678767385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;So we have a table called vault in postres which has card num and it&amp;#39;s token. This pair is unique.\n In our Hadoop we receive tokens and not cards as part of feeds. &lt;/p&gt;\n\n&lt;p&gt;One of the downstream requirements is that they can&amp;#39;t handle tokens and need cards. We have implemented a spark code where we read the vault into a df and take a join with the source feed and replace tokens with cards and then send it to the downstream. &lt;/p&gt;\n\n&lt;p&gt;Now this has to be done for multiple reports across multiple countries. So everytime I have to read the vault which is taking a lot of time and memory. \nPlease note we cannot save vault on Hadoop since it&amp;#39;s a compliance issue...no cards should be visible to naked eye only tokens can be seen. Do you have any alternate solution to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qwb02", "is_robot_indexable": true, "report_reasons": null, "author": "andkad", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qwb02/help_with_solutioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qwb02/help_with_solutioning/", "subreddit_subscribers": 92994, "created_utc": 1678767385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, so I just graduated last year June 2022 \\[23 years old\\] and I got a job offer by July-present as a Data Engineering Analyst at Company A. Currently my goal is to become a Cloud Engineer since I think that path will make my career future proof since most companies nowadays are using the Cloud platform and I also want to work for a foreign/US company to have dollar based salary.\n\n*I am currently getting offered with Company B*\n\n**Company A (July 2022 - Presently working):**An old and big Microfinance company, requires me to use T-SQL (SQL Server), Python and Batch Scripting. Most of the 3 are used for querying purposes, extract to CSV file and for Job Creation. Our company is only using old On-premise servers and we use SSIS for ETL (Which I barely touch). Currently I'm held up with a bond  because of Microsoft  trainings which are: SQL, SSIS, Power BI and Data story telling. (I hardly use the last 3 ).Will implement Cloud for Disaster Recovery of Data.***Monthly*** ***Salary: 35,000 PHP or 617.75 USD***\n\n**Company B:**A new microfinance company, wants me to join their company even without the experience because they are willing to help me. They are willing to invest in me with Azure Data Engineer trainings. As an overview, they are also a microfinance company, who doesn't have any idea how to make the ETL process, they want me to create the Database for them from scratch ( which I have no idea ). But they reassured me that their Azure DeVops Engineer ( 3 start ) will also guide me. The IT head was very kind to me.\n\n***Potential Monthly*** ***Salary: 45,000 PHP+ or 817.61USD***Below are the expertise I think they want me to train in:\n\n* Experience with big data tools: Hadoop, Spark, Kafka, etc.\n* Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.\n* Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.\n* Experience with Azure cloud services: Data Warehouse, Data Factory, Cosmo Databases, Azure Databases\n* Experience with stream-processing systems: Storm, Spark-Streaming, etc.\n* Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\n\n**Final Thoughts:**\n\nShould I risk it and job hop to Company B since I will be hoarding a lot of licenses, training and experience with them, so that after I leave their company I am more qualified for Cloud Engineer jobs?\n\nOr\n\nShould I stick with Company A, gain more experience and wait for their launch of their Cloud platform for Disaster Recovery?  \n\n\n**Note:** \n\nI have a pro-rated **75,000 php** training bond with Company A, if I leave &lt; 2 years.", "author_fullname": "t2_gmb1h0w2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Analyst vs Azure Data Engineer [Job Hop Help]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qvv7g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678766407.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678766079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, so I just graduated last year June 2022 [23 years old] and I got a job offer by July-present as a Data Engineering Analyst at Company A. Currently my goal is to become a Cloud Engineer since I think that path will make my career future proof since most companies nowadays are using the Cloud platform and I also want to work for a foreign/US company to have dollar based salary.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;I am currently getting offered with Company B&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Company A (July 2022 - Presently working):&lt;/strong&gt;An old and big Microfinance company, requires me to use T-SQL (SQL Server), Python and Batch Scripting. Most of the 3 are used for querying purposes, extract to CSV file and for Job Creation. Our company is only using old On-premise servers and we use SSIS for ETL (Which I barely touch). Currently I&amp;#39;m held up with a bond  because of Microsoft  trainings which are: SQL, SSIS, Power BI and Data story telling. (I hardly use the last 3 ).Will implement Cloud for Disaster Recovery of Data.&lt;strong&gt;&lt;em&gt;Monthly&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;Salary: 35,000 PHP or 617.75 USD&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Company B:&lt;/strong&gt;A new microfinance company, wants me to join their company even without the experience because they are willing to help me. They are willing to invest in me with Azure Data Engineer trainings. As an overview, they are also a microfinance company, who doesn&amp;#39;t have any idea how to make the ETL process, they want me to create the Database for them from scratch ( which I have no idea ). But they reassured me that their Azure DeVops Engineer ( 3 start ) will also guide me. The IT head was very kind to me.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Potential Monthly&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;Salary: 45,000 PHP+ or 817.61USD&lt;/em&gt;&lt;/strong&gt;Below are the expertise I think they want me to train in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Experience with big data tools: Hadoop, Spark, Kafka, etc.&lt;/li&gt;\n&lt;li&gt;Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.&lt;/li&gt;\n&lt;li&gt;Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.&lt;/li&gt;\n&lt;li&gt;Experience with Azure cloud services: Data Warehouse, Data Factory, Cosmo Databases, Azure Databases&lt;/li&gt;\n&lt;li&gt;Experience with stream-processing systems: Storm, Spark-Streaming, etc.&lt;/li&gt;\n&lt;li&gt;Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Final Thoughts:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Should I risk it and job hop to Company B since I will be hoarding a lot of licenses, training and experience with them, so that after I leave their company I am more qualified for Cloud Engineer jobs?&lt;/p&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;p&gt;Should I stick with Company A, gain more experience and wait for their launch of their Cloud platform for Disaster Recovery?  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;I have a pro-rated &lt;strong&gt;75,000 php&lt;/strong&gt; training bond with Company A, if I leave &amp;lt; 2 years.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11qvv7g", "is_robot_indexable": true, "report_reasons": null, "author": "Appropriate-Treat456", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qvv7g/data_engineering_analyst_vs_azure_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qvv7g/data_engineering_analyst_vs_azure_data_engineer/", "subreddit_subscribers": 92994, "created_utc": 1678766079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am building something that collects post data every 5 minutes or so. This is what I envisioned my data strucutre to look like\n\n    {\n     'created_time': 2023-03-10 06:00:00,\n     'message': \"test\",\n     'id': '123456',\n     'data':[2023-03-10 06:05:00:{\n         'impression': 13071,\n         'comment': 9,\n         'like': 202,\n         'share': 21,\n         'click': 174},\n            2023-03-10 06:10:00:{\n         'impression': 13655,\n         'comment': 12,\n         'like': 212,\n         'share': 27,\n         'click': 187}\n    ]\n    \n     }\n\nWhere we would periodically update the data entry every certain periods with the new data, so the next entry would be\n\n    {\n     'created_time': 2023-03-10 06:00:00,\n     'message': \"test\",\n     'id': '123456',\n     'data':[2023-03-10 06:05:00:{\n         'impression': 13071,\n         'comment': 9,\n         'like': 202,\n         'share': 21,\n         'click': 174},\n            2023-03-10 06:10:00:{\n         'impression': 13655,\n         'comment': 12,\n         'like': 212,\n         'share': 27,\n         'click': 187},\n            2023-03-10 06:15:00:{\n         'impression': 130571,\n         'comment': 10,\n         'like': 212,\n         'share': 26,\n         'click': 174}\n    ]\n    \n     }\n\nOne of the main functionalities we are trying to use with the data is to aggregate data by date, e.g. impression in a month. Is this the best way to do it?", "author_fullname": "t2_2knag8t3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think about my data structure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qfnwf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678728165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building something that collects post data every 5 minutes or so. This is what I envisioned my data strucutre to look like&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n &amp;#39;created_time&amp;#39;: 2023-03-10 06:00:00,\n &amp;#39;message&amp;#39;: &amp;quot;test&amp;quot;,\n &amp;#39;id&amp;#39;: &amp;#39;123456&amp;#39;,\n &amp;#39;data&amp;#39;:[2023-03-10 06:05:00:{\n     &amp;#39;impression&amp;#39;: 13071,\n     &amp;#39;comment&amp;#39;: 9,\n     &amp;#39;like&amp;#39;: 202,\n     &amp;#39;share&amp;#39;: 21,\n     &amp;#39;click&amp;#39;: 174},\n        2023-03-10 06:10:00:{\n     &amp;#39;impression&amp;#39;: 13655,\n     &amp;#39;comment&amp;#39;: 12,\n     &amp;#39;like&amp;#39;: 212,\n     &amp;#39;share&amp;#39;: 27,\n     &amp;#39;click&amp;#39;: 187}\n]\n\n }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Where we would periodically update the data entry every certain periods with the new data, so the next entry would be&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n &amp;#39;created_time&amp;#39;: 2023-03-10 06:00:00,\n &amp;#39;message&amp;#39;: &amp;quot;test&amp;quot;,\n &amp;#39;id&amp;#39;: &amp;#39;123456&amp;#39;,\n &amp;#39;data&amp;#39;:[2023-03-10 06:05:00:{\n     &amp;#39;impression&amp;#39;: 13071,\n     &amp;#39;comment&amp;#39;: 9,\n     &amp;#39;like&amp;#39;: 202,\n     &amp;#39;share&amp;#39;: 21,\n     &amp;#39;click&amp;#39;: 174},\n        2023-03-10 06:10:00:{\n     &amp;#39;impression&amp;#39;: 13655,\n     &amp;#39;comment&amp;#39;: 12,\n     &amp;#39;like&amp;#39;: 212,\n     &amp;#39;share&amp;#39;: 27,\n     &amp;#39;click&amp;#39;: 187},\n        2023-03-10 06:15:00:{\n     &amp;#39;impression&amp;#39;: 130571,\n     &amp;#39;comment&amp;#39;: 10,\n     &amp;#39;like&amp;#39;: 212,\n     &amp;#39;share&amp;#39;: 26,\n     &amp;#39;click&amp;#39;: 174}\n]\n\n }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;One of the main functionalities we are trying to use with the data is to aggregate data by date, e.g. impression in a month. Is this the best way to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qfnwf", "is_robot_indexable": true, "report_reasons": null, "author": "lordgriefter", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qfnwf/what_do_you_think_about_my_data_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qfnwf/what_do_you_think_about_my_data_structure/", "subreddit_subscribers": 92994, "created_utc": 1678728165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New  to Data Engineering and came across Zeppelin &amp; Jupyter. Any recommendations, pro / cons?", "author_fullname": "t2_f4rnp5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zeppelin vs Jupyter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qde1c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678722825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New  to Data Engineering and came across Zeppelin &amp;amp; Jupyter. Any recommendations, pro / cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qde1c", "is_robot_indexable": true, "report_reasons": null, "author": "alwaysSearching23", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qde1c/zeppelin_vs_jupyter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qde1c/zeppelin_vs_jupyter/", "subreddit_subscribers": 92994, "created_utc": 1678722825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our systems have 20+ services. Each service has its own custom code to send email alerts when the script fails. I have been given the task to write a generic email alert service that can be used by all the other services. This is the design that I am planning to implement:\n\nEach service will write the final status for email to RabbitMQ\n\nMy email alert services will keep on continuously consuming data from the RabbitMQ queue and send the email alerts to the respective target. \n\nEven if email alert service goes down for some reason, the alerts will keep on getting captured in RabbitMQ and won't be lost. As soon as email alert service comes back online, all the backlog alerts will be consumed, and emails will be shared.\n\nIs this approach correct?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my approach for a fully decoupled email service correct? Please see the description inside.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qc9pg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678720182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our systems have 20+ services. Each service has its own custom code to send email alerts when the script fails. I have been given the task to write a generic email alert service that can be used by all the other services. This is the design that I am planning to implement:&lt;/p&gt;\n\n&lt;p&gt;Each service will write the final status for email to RabbitMQ&lt;/p&gt;\n\n&lt;p&gt;My email alert services will keep on continuously consuming data from the RabbitMQ queue and send the email alerts to the respective target. &lt;/p&gt;\n\n&lt;p&gt;Even if email alert service goes down for some reason, the alerts will keep on getting captured in RabbitMQ and won&amp;#39;t be lost. As soon as email alert service comes back online, all the backlog alerts will be consumed, and emails will be shared.&lt;/p&gt;\n\n&lt;p&gt;Is this approach correct?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11qc9pg", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qc9pg/is_my_approach_for_a_fully_decoupled_email/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11qc9pg/is_my_approach_for_a_fully_decoupled_email/", "subreddit_subscribers": 92994, "created_utc": 1678720182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. I'm new to data engineering and I'm a bit lost. I would like to know what resources you recommend me to learn data engineering. I've been writing code for over three years now mostly with Python and in the last year I've taken several data science trainings. It allowed me to discover a passion for data, its extraction, its processing, its storage... and all the technology around it. I noticed that there are a lot of tools to do all kinds of things but what I want to learn is the fundamentals, the components, the way of thinking, how to do the job, I want to have a good knowledge of data engineering and what it consists of before starting to specialize in different technologies. What I am looking for then is resources mainly books that will allow me to learn the job of data engineer in the right way. I await your answers and thank you.", "author_fullname": "t2_ed8urxes", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn data engineering the right way ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11r4bun", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678795521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I&amp;#39;m new to data engineering and I&amp;#39;m a bit lost. I would like to know what resources you recommend me to learn data engineering. I&amp;#39;ve been writing code for over three years now mostly with Python and in the last year I&amp;#39;ve taken several data science trainings. It allowed me to discover a passion for data, its extraction, its processing, its storage... and all the technology around it. I noticed that there are a lot of tools to do all kinds of things but what I want to learn is the fundamentals, the components, the way of thinking, how to do the job, I want to have a good knowledge of data engineering and what it consists of before starting to specialize in different technologies. What I am looking for then is resources mainly books that will allow me to learn the job of data engineer in the right way. I await your answers and thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11r4bun", "is_robot_indexable": true, "report_reasons": null, "author": "ParfaitD", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11r4bun/how_to_learn_data_engineering_the_right_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11r4bun/how_to_learn_data_engineering_the_right_way/", "subreddit_subscribers": 92994, "created_utc": 1678795521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, I had a piece of content I was working on. I'm an engineer and not a content writer so I would appreciate your feedback :D \n\n \n\nIn today\u2019s dynamic environment, the main data processing steps include: data ingestion from a source, data storage, data transformation, data cleansing, and data validation. After these steps, data can be stored and used in further analyses and data analytics applications.\n\nData analysts are eager to find new ways of processing data since this kind of information always grows in both volume and variety, and data-processing tools are being constantly updated (multiple times every year).\n\n\u200d\n\n##### What Is Data Ingestion?\n\nData ingestion is the process of transporting data from one or more sources to a target site for further processing and analysis. The data can be taken from multiple sources, including data pools, SaaS applications, IoT devices, on-premises databases, etc., and would usually end up in different target environments, including cloud data marts and data warehouses.\n\n\u200d\n\n##### Why Is Data Ingestion Important?\n\nData ingestion reorganizes company data to the desired format and helps ease its usage, especially during the extract, transform, and load (ETL) operations. Tools for data ingestion can both process a variety of data formats while simultaneously reorganizing large volumes of unstructured (raw) data.\n\nOnce data is ingested, organizations can employ analytical tools to get useful BI insights from multiple data sources. Companies can improve their applications and offer different features and services derived from the insights that are produced by ingested data sets. With proper data inputs, businesses can provide data analytics to authorized individuals more efficiently. Additionally, data ingestion brings the data to programs that need the most up-to-date data. For example, real-time data, when applied to the public transport system, can improve its efficiency (fuel consumption and traffic patterns), minimize arrival times, avoid congestion etc.\n\n\u200d\n\n##### How To Best Conduct Data Ingestion?\n\nData ingestion can be done in 3 different ways. More specifically, this can be completed through either real-time, batches or a combination of both processes, known otherwise as lambda (or micro-batch approach). Companies can choose one of the three types depending on their business objectives, IT infrastructures, and financial feasibility.\n\n1. **Real-time data ingestion** is the process of the collection and transfer of data from multiple sources in real-time using tools such as change data capture (CDC). CDC continually monitors the transaction logs and moves the changed data without interfering with the database workload. Real-time ingestion is crucial in time-limited use cases, such as power grid monitoring or stock market trading, especially when companies need to react rapidly to new information. Real-time data pipelines are also important in making quick operational decisions and defining actions based on new insights.\n2. **Batch-based data ingestion**, on the other hand, is the process of the collection and transfer of data in batches but in pre-specified time intervals. The ingestion process will collect data based on certain conditions, event triggers, or some forms of logical order. Batch-based ingestion is applicable when companies need to collect specific data on a less rigorous daily basis and or simply don\u2019t need a constant inflow of data for real-time decision-making. An example could be a printed newspaper that collects information over 24 hours and publishes it (part of it) at a certain time.\n3. **Micro-batch ingestion** is a data ingestion process that consists of both real-time and batch methods. The process includes the batch, serving, and speed layers. The first two layers index data in batches, and the speed layer instantly indexes the data that should otherwise be picked up by the slower batch and serving layers. This ongoing data transfer between different layers ensures that data is available for querying with no delay.\n\n \n\n##### The Benefits of Data Ingestion\n\nThese Data ingestion techniques provide various benefits, enabling firms to manage data while also improving their market positions effectively. Some of the advantages include the following:\n\n* Companies can save time and money: Data ingestion automates some of the tasks that are previously done manually by developers. With an automated system in place, however, critical developers can instead dedicate their time to other, more important tasks.\n* Dev-teams can improve their software applications: After implementation, dev-teams can utilize data ingestion techniques to ensure that their applications transfer data quickly and provide a smooth experience directly to the end-users.\n* Data is promptly available: Companies can gather data stored across various servers and move them all together to a unified environment available for immediate access and further analysis.\n* Data simplified: Data ingestion implementation, together with ETL tools, will convert different data types into pre-defined formats and then transfer them to a single data warehouse.\n* Improved decision-making: Real-time data ingestion allows businesses to uncover problems and opportunities on the spot, thereby making the right decisions at the right time.\n\n&amp;#x200B;\n\n \n\n##### The Must-Have Features For 2023 / Incoming Trends in 2023\n\nData ingestion tools can gather and transfer all structured, semi-structured, and unstructured data from multiple sources to target destinations. These tools automate manual ingestion processes and undertake processing steps that move data from one point to another. Other important features to pay attention to in the upcoming period are as follows:\n\n* **Data integration tools:** Traditional data integration platforms incorporate features for every step of the data value chain, and namely, the aforementioned data cleaning, data consolidation, ETL processes, data virtualization, and transfer and storage. They enable a regulated (and secure) flow of simplified data operations through increasing productivity without any processing delays.\n* **AI-powered search**: An AI-Powered Search can bring site visitors what they need right off the spot, and this will help business owners achieve better customer satisfaction, higher conversion rates, and increased revenues. An AI-based search engine will display results that are personalized to individual users based on their profiles, desires, and various other tendencies.\n* **Video-based search**: Implementing automated captions helps people consume media content effectively. With Omnisearch, you can utilize our advanced search functionality to find the exact video you need or navigate the database using filters such as topics, dates, and many more. Additionally, when you search for specific files, Omnisearch automatically tells you the relevance of various files to your search terms; this makes it quick and easy to navigate through your massive database to find and locate exactly what you need.", "author_fullname": "t2_o5qmz1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The importance of Data ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11r2wf0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678790911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I had a piece of content I was working on. I&amp;#39;m an engineer and not a content writer so I would appreciate your feedback :D &lt;/p&gt;\n\n&lt;p&gt;In today\u2019s dynamic environment, the main data processing steps include: data ingestion from a source, data storage, data transformation, data cleansing, and data validation. After these steps, data can be stored and used in further analyses and data analytics applications.&lt;/p&gt;\n\n&lt;p&gt;Data analysts are eager to find new ways of processing data since this kind of information always grows in both volume and variety, and data-processing tools are being constantly updated (multiple times every year).&lt;/p&gt;\n\n&lt;p&gt;\u200d&lt;/p&gt;\n\n&lt;h5&gt;What Is Data Ingestion?&lt;/h5&gt;\n\n&lt;p&gt;Data ingestion is the process of transporting data from one or more sources to a target site for further processing and analysis. The data can be taken from multiple sources, including data pools, SaaS applications, IoT devices, on-premises databases, etc., and would usually end up in different target environments, including cloud data marts and data warehouses.&lt;/p&gt;\n\n&lt;p&gt;\u200d&lt;/p&gt;\n\n&lt;h5&gt;Why Is Data Ingestion Important?&lt;/h5&gt;\n\n&lt;p&gt;Data ingestion reorganizes company data to the desired format and helps ease its usage, especially during the extract, transform, and load (ETL) operations. Tools for data ingestion can both process a variety of data formats while simultaneously reorganizing large volumes of unstructured (raw) data.&lt;/p&gt;\n\n&lt;p&gt;Once data is ingested, organizations can employ analytical tools to get useful BI insights from multiple data sources. Companies can improve their applications and offer different features and services derived from the insights that are produced by ingested data sets. With proper data inputs, businesses can provide data analytics to authorized individuals more efficiently. Additionally, data ingestion brings the data to programs that need the most up-to-date data. For example, real-time data, when applied to the public transport system, can improve its efficiency (fuel consumption and traffic patterns), minimize arrival times, avoid congestion etc.&lt;/p&gt;\n\n&lt;p&gt;\u200d&lt;/p&gt;\n\n&lt;h5&gt;How To Best Conduct Data Ingestion?&lt;/h5&gt;\n\n&lt;p&gt;Data ingestion can be done in 3 different ways. More specifically, this can be completed through either real-time, batches or a combination of both processes, known otherwise as lambda (or micro-batch approach). Companies can choose one of the three types depending on their business objectives, IT infrastructures, and financial feasibility.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Real-time data ingestion&lt;/strong&gt; is the process of the collection and transfer of data from multiple sources in real-time using tools such as change data capture (CDC). CDC continually monitors the transaction logs and moves the changed data without interfering with the database workload. Real-time ingestion is crucial in time-limited use cases, such as power grid monitoring or stock market trading, especially when companies need to react rapidly to new information. Real-time data pipelines are also important in making quick operational decisions and defining actions based on new insights.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Batch-based data ingestion&lt;/strong&gt;, on the other hand, is the process of the collection and transfer of data in batches but in pre-specified time intervals. The ingestion process will collect data based on certain conditions, event triggers, or some forms of logical order. Batch-based ingestion is applicable when companies need to collect specific data on a less rigorous daily basis and or simply don\u2019t need a constant inflow of data for real-time decision-making. An example could be a printed newspaper that collects information over 24 hours and publishes it (part of it) at a certain time.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Micro-batch ingestion&lt;/strong&gt; is a data ingestion process that consists of both real-time and batch methods. The process includes the batch, serving, and speed layers. The first two layers index data in batches, and the speed layer instantly indexes the data that should otherwise be picked up by the slower batch and serving layers. This ongoing data transfer between different layers ensures that data is available for querying with no delay.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h5&gt;The Benefits of Data Ingestion&lt;/h5&gt;\n\n&lt;p&gt;These Data ingestion techniques provide various benefits, enabling firms to manage data while also improving their market positions effectively. Some of the advantages include the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Companies can save time and money: Data ingestion automates some of the tasks that are previously done manually by developers. With an automated system in place, however, critical developers can instead dedicate their time to other, more important tasks.&lt;/li&gt;\n&lt;li&gt;Dev-teams can improve their software applications: After implementation, dev-teams can utilize data ingestion techniques to ensure that their applications transfer data quickly and provide a smooth experience directly to the end-users.&lt;/li&gt;\n&lt;li&gt;Data is promptly available: Companies can gather data stored across various servers and move them all together to a unified environment available for immediate access and further analysis.&lt;/li&gt;\n&lt;li&gt;Data simplified: Data ingestion implementation, together with ETL tools, will convert different data types into pre-defined formats and then transfer them to a single data warehouse.&lt;/li&gt;\n&lt;li&gt;Improved decision-making: Real-time data ingestion allows businesses to uncover problems and opportunities on the spot, thereby making the right decisions at the right time.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h5&gt;The Must-Have Features For 2023 / Incoming Trends in 2023&lt;/h5&gt;\n\n&lt;p&gt;Data ingestion tools can gather and transfer all structured, semi-structured, and unstructured data from multiple sources to target destinations. These tools automate manual ingestion processes and undertake processing steps that move data from one point to another. Other important features to pay attention to in the upcoming period are as follows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Data integration tools:&lt;/strong&gt; Traditional data integration platforms incorporate features for every step of the data value chain, and namely, the aforementioned data cleaning, data consolidation, ETL processes, data virtualization, and transfer and storage. They enable a regulated (and secure) flow of simplified data operations through increasing productivity without any processing delays.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AI-powered search&lt;/strong&gt;: An AI-Powered Search can bring site visitors what they need right off the spot, and this will help business owners achieve better customer satisfaction, higher conversion rates, and increased revenues. An AI-based search engine will display results that are personalized to individual users based on their profiles, desires, and various other tendencies.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Video-based search&lt;/strong&gt;: Implementing automated captions helps people consume media content effectively. With Omnisearch, you can utilize our advanced search functionality to find the exact video you need or navigate the database using filters such as topics, dates, and many more. Additionally, when you search for specific files, Omnisearch automatically tells you the relevance of various files to your search terms; this makes it quick and easy to navigate through your massive database to find and locate exactly what you need.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11r2wf0", "is_robot_indexable": true, "report_reasons": null, "author": "marin_smiljanic", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11r2wf0/the_importance_of_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11r2wf0/the_importance_of_data_ingestion/", "subreddit_subscribers": 92994, "created_utc": 1678790911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Put your thoughts on where are the current demands and what will be the future demand, Which will sustain for longer.\n\nTraditional Data Engineering approach(On-Prem Tools) vs Cloud Based approach (Native features in Azure/AWS/GCP)\n\n[View Poll](https://www.reddit.com/poll/11r1c0z)", "author_fullname": "t2_svzav4ot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Traditional Data Engineering approach(On-Prem Tools) vs Cloud Based approach", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11r1c0z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678785050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Put your thoughts on where are the current demands and what will be the future demand, Which will sustain for longer.&lt;/p&gt;\n\n&lt;p&gt;Traditional Data Engineering approach(On-Prem Tools) vs Cloud Based approach (Native features in Azure/AWS/GCP)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/11r1c0z\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11r1c0z", "is_robot_indexable": true, "report_reasons": null, "author": "SpaceIntelligent6910", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1679389850291, "options": [{"text": "Traditional Data Engineering Approach (Tool Based)", "id": "22056451"}, {"text": "Cloud based Data Engineering Approach (Cloud Services)", "id": "22056452"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 46, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11r1c0z/traditional_data_engineering_approachonprem_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/11r1c0z/traditional_data_engineering_approachonprem_tools/", "subreddit_subscribers": 92994, "created_utc": 1678785050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_b7f9ay9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Click Stream Analysis with Azure: Event Hub, Stream Analytics, Blob and Synapse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_11qxvjl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/X2ARvprwc5Y?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Click Stream Analysis with Azure: Event Hub, Stream Analytics, Blob and Synapse\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Click Stream Analysis with Azure: Event Hub, Stream Analytics, Blob and Synapse", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/X2ARvprwc5Y?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Click Stream Analysis with Azure: Event Hub, Stream Analytics, Blob and Synapse\"&gt;&lt;/iframe&gt;", "author_name": "SoftWiz Circle", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/X2ARvprwc5Y/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@SoftWizCircle"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/X2ARvprwc5Y?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Click Stream Analysis with Azure: Event Hub, Stream Analytics, Blob and Synapse\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/11qxvjl", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YL47h3i8Sog52RR2vGx1n2rEO003zYwVBaXbv0AWJmY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678772461.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/X2ARvprwc5Y", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/r1ujMr8JHjS4PhruOErBPrcXQ7fQ9TUopMWdsn-Qc1Q.jpg?auto=webp&amp;v=enabled&amp;s=e8eeefa4a287af02f91bfdae5917340d88a02b49", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/r1ujMr8JHjS4PhruOErBPrcXQ7fQ9TUopMWdsn-Qc1Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=024a9b60c6d890f0285364607c16002ea7f5e6a2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/r1ujMr8JHjS4PhruOErBPrcXQ7fQ9TUopMWdsn-Qc1Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5530ba07884644fb1f7bbc39b2d2090a274b8be0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/r1ujMr8JHjS4PhruOErBPrcXQ7fQ9TUopMWdsn-Qc1Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f148d8fd1b87e1a8b21fccf42ca2d44514ba8e93", "width": 320, "height": 240}], "variants": {}, "id": "DG1LMJdAolqkUL_K19OLzWFqqulRa1P2mP9DC_wFChc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11qxvjl", "is_robot_indexable": true, "report_reasons": null, "author": "balramprasad", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11qxvjl/click_stream_analysis_with_azure_event_hub_stream/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/X2ARvprwc5Y", "subreddit_subscribers": 92994, "created_utc": 1678772461.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Click Stream Analysis with Azure: Event Hub, Stream Analytics, Blob and Synapse", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/X2ARvprwc5Y?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Click Stream Analysis with Azure: Event Hub, Stream Analytics, Blob and Synapse\"&gt;&lt;/iframe&gt;", "author_name": "SoftWiz Circle", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/X2ARvprwc5Y/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@SoftWizCircle"}}, "is_video": false}}], "before": null}}