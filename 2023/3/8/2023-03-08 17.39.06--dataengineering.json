{"kind": "Listing", "data": {"after": "t3_11m0zqg", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't know about you, but I have plenty of data engineering horror stories to share. I'd love to hear the one that still gives you shivers.\n\n  \n**Here's my highlight:**  \n\n\n* I'm at a 500 people medium sized company, 300-400$m revenue. But we're growing strong, at 10-20% each year. Ingest data via python, dbt for transformations, storage in PostgreSQL and Tableau as BI tool on top of it.\n* We've been pushing the adoption of Tableau, and the company is eating it up, they love it. They are all over the dashboards.\n* What we're particular proud of is our **\"north star metric**\" dashboard, showing our new key metric, based on a recent business pivot.\n* In our most important customer segment, it looks like it's starting to grow **exponentially**! Everyone is excited!\n\nSuddenly an important manager calls me up\n\n\"*hey, something is wrong with the north star. The dashboard looked completely different yesterday! Our exponential growth is gone! Surely there is something wrong, please fix it by this evening. Tomorrow is the board meeting and I'm presenting the exponential growth.\"*\n\n  \nTook us some time to understand this one... Apparently, ALL data changed, the complete metric in this customer segment broke in, not just for today, but also for yesterday, the day before, and so on...\n\n  \nAfter some research, we realized a huge problem: The biggest customer in that segment left a few months ago, and filed a \"deletion request\". The upstream team responsible for this followed through, and basically \"detached the relevant data from the customer account\". \n\n  \nSo there we were. We didn't even know about this process, and had no chance to recover. The manager was left without his exponential growth. \n\n  \n*Aftermath: So what we did from then on is to turn on snapshotting of important data sources (using dbt). After being really unhappy, the manager was still convinced of the underlying exponential growth which shouldn't be reliant on one big customer, but the situation felt terrible. And I'm quite happy that only data from basically one customer went down the drain.*\n\n\\----\n\n  \nHow about you? Do you have a horror story to share?", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got a data engineering horror story, what is yours?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lvm8z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678279973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know about you, but I have plenty of data engineering horror stories to share. I&amp;#39;d love to hear the one that still gives you shivers.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here&amp;#39;s my highlight:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;m at a 500 people medium sized company, 300-400$m revenue. But we&amp;#39;re growing strong, at 10-20% each year. Ingest data via python, dbt for transformations, storage in PostgreSQL and Tableau as BI tool on top of it.&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve been pushing the adoption of Tableau, and the company is eating it up, they love it. They are all over the dashboards.&lt;/li&gt;\n&lt;li&gt;What we&amp;#39;re particular proud of is our &lt;strong&gt;&amp;quot;north star metric&lt;/strong&gt;&amp;quot; dashboard, showing our new key metric, based on a recent business pivot.&lt;/li&gt;\n&lt;li&gt;In our most important customer segment, it looks like it&amp;#39;s starting to grow &lt;strong&gt;exponentially&lt;/strong&gt;! Everyone is excited!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Suddenly an important manager calls me up&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&lt;em&gt;hey, something is wrong with the north star. The dashboard looked completely different yesterday! Our exponential growth is gone! Surely there is something wrong, please fix it by this evening. Tomorrow is the board meeting and I&amp;#39;m presenting the exponential growth.&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Took us some time to understand this one... Apparently, ALL data changed, the complete metric in this customer segment broke in, not just for today, but also for yesterday, the day before, and so on...&lt;/p&gt;\n\n&lt;p&gt;After some research, we realized a huge problem: The biggest customer in that segment left a few months ago, and filed a &amp;quot;deletion request&amp;quot;. The upstream team responsible for this followed through, and basically &amp;quot;detached the relevant data from the customer account&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;So there we were. We didn&amp;#39;t even know about this process, and had no chance to recover. The manager was left without his exponential growth. &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Aftermath: So what we did from then on is to turn on snapshotting of important data sources (using dbt). After being really unhappy, the manager was still convinced of the underlying exponential growth which shouldn&amp;#39;t be reliant on one big customer, but the situation felt terrible. And I&amp;#39;m quite happy that only data from basically one customer went down the drain.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;----&lt;/p&gt;\n\n&lt;p&gt;How about you? Do you have a horror story to share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11lvm8z", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lvm8z/i_got_a_data_engineering_horror_story_what_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lvm8z/i_got_a_data_engineering_horror_story_what_is/", "subreddit_subscribers": 92284, "created_utc": 1678279973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We all talk about data pipelines when we talk about DE or DE related positions. What how would you explain a data pipeline with examples to a layman(maybe). How many different examples can we think of?\n\nJust a random question that popped up.", "author_fullname": "t2_3nipc92b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you explain a data pipeline to a non techie?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lid59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678239481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We all talk about data pipelines when we talk about DE or DE related positions. What how would you explain a data pipeline with examples to a layman(maybe). How many different examples can we think of?&lt;/p&gt;\n\n&lt;p&gt;Just a random question that popped up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11lid59", "is_robot_indexable": true, "report_reasons": null, "author": "kausthab87", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lid59/how_would_you_explain_a_data_pipeline_to_a_non/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lid59/how_would_you_explain_a_data_pipeline_to_a_non/", "subreddit_subscribers": 92284, "created_utc": 1678239481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/](https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/)", "author_fullname": "t2_3pb20mxg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Kestra, the open source data orchestration and scheduling platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lra9t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678265803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/\"&gt;https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lra9t", "is_robot_indexable": true, "report_reasons": null, "author": "loicmathieu", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lra9t/introduction_to_kestra_the_open_source_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lra9t/introduction_to_kestra_the_open_source_data/", "subreddit_subscribers": 92284, "created_utc": 1678265803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How We Deploy 5X Faster with Warm Docker Containers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11lm6v8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3PsKOf3gXsXBRgBoWV9V2DNgR2NLi7bPiFVWQ3cvaZg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678249633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/fast-deploys-with-pex-and-docker", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?auto=webp&amp;v=enabled&amp;s=428399d39a1be1aded62d15ab2065cb88d8bf446", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=046e5911c908888ccaba4b46cc8a72296466e0ae", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7797460cbf8b1014dceba585394e955adc56ba5", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56b26029903e7b2bbe053dcfcbcb0d4d5f048f8d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a96e31d8af6bed29048f108a9830d27d5da6c04", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=165d741737d8fbc89be2fdb81121f8438e3352e8", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca8e3e0fd14e7b473ada1f4825f4c10e7f3e8009", "width": 1080, "height": 567}], "variants": {}, "id": "TCwPSvjxLSjXzKqkzC26JS0QPxvT4x7b0EnVA3jwC2I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lm6v8", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lm6v8/how_we_deploy_5x_faster_with_warm_docker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/fast-deploys-with-pex-and-docker", "subreddit_subscribers": 92284, "created_utc": 1678249633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am not sure how should I partition my data in the S3 data lake. \n\nI decided to go with \u201chive\u201d partitions format so I am gonna use \u201ckey=value\u201d style. But now, how the actual structure should look like?\n\na. year=2023/month=3/day=1/file.parquet (one file only)\n\nb. year=2023/month=3/day1.parquet (multiple files)\n\nc. date=2023-03-01/file.parquet\n\nWhen using b) - how would query like \u201cwhere year=2023 and month = 3 and day = 1\u201d work? It would read only the file \u201cday1\u201d or all files in this \u201cfolder\u201d and then filter the records from these files?\n\nAlso, if using the c) approach - how it would work if I would like to query one month of data? Would the app (say Spark or anything else) know it should take a look only at \u201c2023-03-*\u201d? Or it would scan all the folders?\n\nWhat is the strategy when designing data lake partitions?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lake partitioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l5aft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678209684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not sure how should I partition my data in the S3 data lake. &lt;/p&gt;\n\n&lt;p&gt;I decided to go with \u201chive\u201d partitions format so I am gonna use \u201ckey=value\u201d style. But now, how the actual structure should look like?&lt;/p&gt;\n\n&lt;p&gt;a. year=2023/month=3/day=1/file.parquet (one file only)&lt;/p&gt;\n\n&lt;p&gt;b. year=2023/month=3/day1.parquet (multiple files)&lt;/p&gt;\n\n&lt;p&gt;c. date=2023-03-01/file.parquet&lt;/p&gt;\n\n&lt;p&gt;When using b) - how would query like \u201cwhere year=2023 and month = 3 and day = 1\u201d work? It would read only the file \u201cday1\u201d or all files in this \u201cfolder\u201d and then filter the records from these files?&lt;/p&gt;\n\n&lt;p&gt;Also, if using the c) approach - how it would work if I would like to query one month of data? Would the app (say Spark or anything else) know it should take a look only at \u201c2023-03-*\u201d? Or it would scan all the folders?&lt;/p&gt;\n\n&lt;p&gt;What is the strategy when designing data lake partitions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11l5aft", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l5aft/data_lake_partitioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l5aft/data_lake_partitioning/", "subreddit_subscribers": 92284, "created_utc": 1678209684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone \ud83d\udc4b I\u2019m Ian \u2014 I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable ([https://www.turntable.so/](https://www.turntable.so/))\n\nTwo weeks ago we [demo'd some early features](https://www.reddit.com/r/dataengineering/comments/119oxil/building_a_better_local_dbt_experience/) and we were grateful for the community response.\n\nThe number one feature request from you all was a column level lineage view.\n\nToday, I'm excited to share that we now have column-level lineage to bring column understanding to dbt projects. Under the hood, we parse the dbt-compiled sql into an abstract syntax tree and then recurse through that tree to build the lineage.\n\nI\u2019d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM\u2026 thanks!\n\n[https://www.loom.com/share/c77689096ee14952a5e4cdea969eaf7c](https://www.loom.com/share/c77689096ee14952a5e4cdea969eaf7c)\n\nIn the upcoming weeks we're launching inline docs, smart autocomplete and an AI copilot!", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building Column Level Lineage for dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11llsqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678248521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone \ud83d\udc4b I\u2019m Ian \u2014 I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable (&lt;a href=\"https://www.turntable.so/\"&gt;https://www.turntable.so/&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Two weeks ago we &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/119oxil/building_a_better_local_dbt_experience/\"&gt;demo&amp;#39;d some early features&lt;/a&gt; and we were grateful for the community response.&lt;/p&gt;\n\n&lt;p&gt;The number one feature request from you all was a column level lineage view.&lt;/p&gt;\n\n&lt;p&gt;Today, I&amp;#39;m excited to share that we now have column-level lineage to bring column understanding to dbt projects. Under the hood, we parse the dbt-compiled sql into an abstract syntax tree and then recurse through that tree to build the lineage.&lt;/p&gt;\n\n&lt;p&gt;I\u2019d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM\u2026 thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.loom.com/share/c77689096ee14952a5e4cdea969eaf7c\"&gt;https://www.loom.com/share/c77689096ee14952a5e4cdea969eaf7c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In the upcoming weeks we&amp;#39;re launching inline docs, smart autocomplete and an AI copilot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11llsqn", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11llsqn/building_column_level_lineage_for_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11llsqn/building_column_level_lineage_for_dbt/", "subreddit_subscribers": 92284, "created_utc": 1678248521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had a design and architecture interview. Ended up being some predefined tables and data in a browser-based IDE. I was asked to \"model the data for a reporting database\". Kind of confused but started into a discussion on star schemas and why that is not the best on all databases but for a generic solution like this would fit.\n\nNOPE. Modeling the data meant writing some aggregate queries against those predefined tables. This was a design and architecture interview. If it had been a basic SQL interview, it would have been a fairly average problem set.\n\nI still feel confused. I did write some queries but not really sure I was in the right place or understanding the ask. I have to admit the mental switch very much threw me off my game. \n\nIf they wanted a new schema and then I should write the queries against that new schema (showing how it would work), I could understand that. I would expect that to be more of a white board or a db design tool rather than an IDE but I could work with that. But they specifically said to just write the selects against the tables as shown.\n\nAm I missing something? Do people consider writing SQL to be \"modeling the data\"?\n\nThe other interviews at that company have all been really interesting. I just feel like they weren't happy and I have no idea what they were looking for. I tried asking in different ways but in the end just wrote some queries. They didn't say they weren't happy and maybe my feeling just comes from my confusion as to what was being asked for. Just looking for insights I guess.", "author_fullname": "t2_nx1og3xx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modeling the data means writing queries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lg4f7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678233823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a design and architecture interview. Ended up being some predefined tables and data in a browser-based IDE. I was asked to &amp;quot;model the data for a reporting database&amp;quot;. Kind of confused but started into a discussion on star schemas and why that is not the best on all databases but for a generic solution like this would fit.&lt;/p&gt;\n\n&lt;p&gt;NOPE. Modeling the data meant writing some aggregate queries against those predefined tables. This was a design and architecture interview. If it had been a basic SQL interview, it would have been a fairly average problem set.&lt;/p&gt;\n\n&lt;p&gt;I still feel confused. I did write some queries but not really sure I was in the right place or understanding the ask. I have to admit the mental switch very much threw me off my game. &lt;/p&gt;\n\n&lt;p&gt;If they wanted a new schema and then I should write the queries against that new schema (showing how it would work), I could understand that. I would expect that to be more of a white board or a db design tool rather than an IDE but I could work with that. But they specifically said to just write the selects against the tables as shown.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something? Do people consider writing SQL to be &amp;quot;modeling the data&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;The other interviews at that company have all been really interesting. I just feel like they weren&amp;#39;t happy and I have no idea what they were looking for. I tried asking in different ways but in the end just wrote some queries. They didn&amp;#39;t say they weren&amp;#39;t happy and maybe my feeling just comes from my confusion as to what was being asked for. Just looking for insights I guess.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "11lg4f7", "is_robot_indexable": true, "report_reasons": null, "author": "Admirable-Shower2174", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lg4f7/modeling_the_data_means_writing_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lg4f7/modeling_the_data_means_writing_queries/", "subreddit_subscribers": 92284, "created_utc": 1678233823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently developing a presentation for a knowledge sharing session that goes over the history of data warehouses through data lakes. While researching the topic I found it interesting how the problems that lead to the creation of the data warehouse in the 80s/90s are the same problems we\u2019re talking about today. \n\nMainly, increase volume of data driven by increased storage capacity. And increase in velocity and variety of data. \n\nEveryone\u2019s still trying to figure out the best way to organize their data. I don\u2019t think this problem or our jobs are going away anytime soon.", "author_fullname": "t2_6cy1x7a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Same problems for decades", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lkfxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678244838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently developing a presentation for a knowledge sharing session that goes over the history of data warehouses through data lakes. While researching the topic I found it interesting how the problems that lead to the creation of the data warehouse in the 80s/90s are the same problems we\u2019re talking about today. &lt;/p&gt;\n\n&lt;p&gt;Mainly, increase volume of data driven by increased storage capacity. And increase in velocity and variety of data. &lt;/p&gt;\n\n&lt;p&gt;Everyone\u2019s still trying to figure out the best way to organize their data. I don\u2019t think this problem or our jobs are going away anytime soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lkfxv", "is_robot_indexable": true, "report_reasons": null, "author": "Pack_Otherwise", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lkfxv/same_problems_for_decades/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lkfxv/same_problems_for_decades/", "subreddit_subscribers": 92284, "created_utc": 1678244838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are a lot of questions about CDC in this sub... \n\nWould you contribute to a communal sheet to help guide tooling decisions? \n\ne.g. a managed spreadsheet enumerating all pros/cons/gotchas  of various tools?\n\nPls comment if so and on any modifcations to a v1 I'd share:\n\n**Criteria:**\n\n\\*Method (log / trigger / timestamp)\n\n\\*In-flights transforms in SQL\n\n\\*Requires windowing (e.g. can you join any historical data to a stream) \n\n\\*End-to-end exactly once? \n\n\\*Pricing\n\n\\*Automated Schema Evolution\n\n\\*DB size limit\n\n\\*Testing Capability\n\n\\*Environments (Cloud v. on-premise)\n\n\\*Automated Backfill\n\n\\*UI?\n\n\\*Pricing model\n\n\\*Environments (cloud, on-prem)\n\n\\*Dependencies\n\n\\*Sources/Destinations\n\n\\*User feedback\n\n**Solutions (limited to real-time)**\n\nDebezium\n\nConfluent\n\nStriim\n\nEstuary Flow\n\nArcion\n\nHVR (Fivetran)\n\nMatillion\n\nGoldenGate\n\nPrecisely\n\nQlik\n\nStreamSets\n\nEqualum\n\nNiFi\n\nFlink  \n\n\nShould we include batch tools (Airbyte, Hevo, Gather, Rivery, Stitch, etc...)", "author_fullname": "t2_sa3mbz4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Change Data Capture Eval Critera", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l84i9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678216028.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are a lot of questions about CDC in this sub... &lt;/p&gt;\n\n&lt;p&gt;Would you contribute to a communal sheet to help guide tooling decisions? &lt;/p&gt;\n\n&lt;p&gt;e.g. a managed spreadsheet enumerating all pros/cons/gotchas  of various tools?&lt;/p&gt;\n\n&lt;p&gt;Pls comment if so and on any modifcations to a v1 I&amp;#39;d share:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Criteria:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;*Method (log / trigger / timestamp)&lt;/p&gt;\n\n&lt;p&gt;*In-flights transforms in SQL&lt;/p&gt;\n\n&lt;p&gt;*Requires windowing (e.g. can you join any historical data to a stream) &lt;/p&gt;\n\n&lt;p&gt;*End-to-end exactly once? &lt;/p&gt;\n\n&lt;p&gt;*Pricing&lt;/p&gt;\n\n&lt;p&gt;*Automated Schema Evolution&lt;/p&gt;\n\n&lt;p&gt;*DB size limit&lt;/p&gt;\n\n&lt;p&gt;*Testing Capability&lt;/p&gt;\n\n&lt;p&gt;*Environments (Cloud v. on-premise)&lt;/p&gt;\n\n&lt;p&gt;*Automated Backfill&lt;/p&gt;\n\n&lt;p&gt;*UI?&lt;/p&gt;\n\n&lt;p&gt;*Pricing model&lt;/p&gt;\n\n&lt;p&gt;*Environments (cloud, on-prem)&lt;/p&gt;\n\n&lt;p&gt;*Dependencies&lt;/p&gt;\n\n&lt;p&gt;*Sources/Destinations&lt;/p&gt;\n\n&lt;p&gt;*User feedback&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Solutions (limited to real-time)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Debezium&lt;/p&gt;\n\n&lt;p&gt;Confluent&lt;/p&gt;\n\n&lt;p&gt;Striim&lt;/p&gt;\n\n&lt;p&gt;Estuary Flow&lt;/p&gt;\n\n&lt;p&gt;Arcion&lt;/p&gt;\n\n&lt;p&gt;HVR (Fivetran)&lt;/p&gt;\n\n&lt;p&gt;Matillion&lt;/p&gt;\n\n&lt;p&gt;GoldenGate&lt;/p&gt;\n\n&lt;p&gt;Precisely&lt;/p&gt;\n\n&lt;p&gt;Qlik&lt;/p&gt;\n\n&lt;p&gt;StreamSets&lt;/p&gt;\n\n&lt;p&gt;Equalum&lt;/p&gt;\n\n&lt;p&gt;NiFi&lt;/p&gt;\n\n&lt;p&gt;Flink  &lt;/p&gt;\n\n&lt;p&gt;Should we include batch tools (Airbyte, Hevo, Gather, Rivery, Stitch, etc...)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11l84i9", "is_robot_indexable": true, "report_reasons": null, "author": "MooJerseyCreamery", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l84i9/change_data_capture_eval_critera/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l84i9/change_data_capture_eval_critera/", "subreddit_subscribers": 92284, "created_utc": 1678216028.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For some weird reason, there is no documentation on how to paginate over the responses of certain endpoints of the databricks rest api (ie. Job runs). It\u2019ll return a \u201cnext page token\u201d, but I\u2019ve tried the common. Pagination workflows (return the token as a param, in the header, etc) and it seems as if they are all incorrect. Is there anybody who can detail how to paginate?", "author_fullname": "t2_3ixkfqzx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks REST API Pagination", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lnvpc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678254608.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some weird reason, there is no documentation on how to paginate over the responses of certain endpoints of the databricks rest api (ie. Job runs). It\u2019ll return a \u201cnext page token\u201d, but I\u2019ve tried the common. Pagination workflows (return the token as a param, in the header, etc) and it seems as if they are all incorrect. Is there anybody who can detail how to paginate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lnvpc", "is_robot_indexable": true, "report_reasons": null, "author": "kharigardner", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lnvpc/databricks_rest_api_pagination/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lnvpc/databricks_rest_api_pagination/", "subreddit_subscribers": 92284, "created_utc": 1678254608.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nMy team wants to enable an automated way to run SQL pipelines in our AWS EKS environment.\nOur first idea is to use dbt as the SQL framework. However we can't find an easy way to have a transient spark cluster running theses dbt pipelines.\n\nHave anybody already worked with dbt and AWS EKS transient cluster?\nAny thoughts about it?\n\nWhat we have in mind to test:\n\nOption A\n- Apply a spark cluter in AWS EKS with a thrift server\n- Find its host and use it to run dbt pipeline\n- Kill the cluster\n\nOption B\n- Add dbt to the EMR on EKS image with a spark thrift server (we need to understand how it can be done)\n- Run dbt pipeline as entrypoint for this image using the local thrift server\n\nThanks for any thoughts!", "author_fullname": "t2_n0qka4td", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using dbt with transient spark cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l7dz2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678214387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;My team wants to enable an automated way to run SQL pipelines in our AWS EKS environment.\nOur first idea is to use dbt as the SQL framework. However we can&amp;#39;t find an easy way to have a transient spark cluster running theses dbt pipelines.&lt;/p&gt;\n\n&lt;p&gt;Have anybody already worked with dbt and AWS EKS transient cluster?\nAny thoughts about it?&lt;/p&gt;\n\n&lt;p&gt;What we have in mind to test:&lt;/p&gt;\n\n&lt;p&gt;Option A\n- Apply a spark cluter in AWS EKS with a thrift server\n- Find its host and use it to run dbt pipeline\n- Kill the cluster&lt;/p&gt;\n\n&lt;p&gt;Option B\n- Add dbt to the EMR on EKS image with a spark thrift server (we need to understand how it can be done)\n- Run dbt pipeline as entrypoint for this image using the local thrift server&lt;/p&gt;\n\n&lt;p&gt;Thanks for any thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11l7dz2", "is_robot_indexable": true, "report_reasons": null, "author": "Data_Broccoli_PII", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l7dz2/using_dbt_with_transient_spark_cluster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l7dz2/using_dbt_with_transient_spark_cluster/", "subreddit_subscribers": 92284, "created_utc": 1678214387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently working on creating reports for business areas and each data engineer is developing their own data marts similar to this architecture - https://www.researchgate.net/figure/Kimballs-data-warehouse-architecture-7_fig2_312486486\n\nEach business area (Finance, HR, Sales etc) have their own data mart, could be multiple fact tables and then the developers create -the reports. The organisation will end up with 70-100 fact tables, I assumed this was the way it should be done.\n\nMy colleague advised that this is the 'old way' of working, he is a data architect who has implemented a one version of the truth and a single data model which consists of all information of all levels of granularity - Finance, Customer, HR, Sales etc. This makes life easy for the developers and allows you to present your KPI's in one single dashboard.\n\nI was stumped, I didn't think it was possible and sounds to good to be true. I have many questions but I would like to know your thoughts on this? Any advice? Experiences with implementing something similar in the past.", "author_fullname": "t2_12rfbr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am fairly new to Data Warehousing and need some advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lyuir", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678288412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently working on creating reports for business areas and each data engineer is developing their own data marts similar to this architecture - &lt;a href=\"https://www.researchgate.net/figure/Kimballs-data-warehouse-architecture-7_fig2_312486486\"&gt;https://www.researchgate.net/figure/Kimballs-data-warehouse-architecture-7_fig2_312486486&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Each business area (Finance, HR, Sales etc) have their own data mart, could be multiple fact tables and then the developers create -the reports. The organisation will end up with 70-100 fact tables, I assumed this was the way it should be done.&lt;/p&gt;\n\n&lt;p&gt;My colleague advised that this is the &amp;#39;old way&amp;#39; of working, he is a data architect who has implemented a one version of the truth and a single data model which consists of all information of all levels of granularity - Finance, Customer, HR, Sales etc. This makes life easy for the developers and allows you to present your KPI&amp;#39;s in one single dashboard.&lt;/p&gt;\n\n&lt;p&gt;I was stumped, I didn&amp;#39;t think it was possible and sounds to good to be true. I have many questions but I would like to know your thoughts on this? Any advice? Experiences with implementing something similar in the past.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lyuir", "is_robot_indexable": true, "report_reasons": null, "author": "That_Sweet_Science", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lyuir/i_am_fairly_new_to_data_warehousing_and_need_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lyuir/i_am_fairly_new_to_data_warehousing_and_need_some/", "subreddit_subscribers": 92284, "created_utc": 1678288412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151](https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151)\n\nWhat do you think of this? I think its really awesome they are going forwards using open source scheduler tools in ADF.\n\nAnyone had already experiences with it? However it is still depending on Data Factory", "author_fullname": "t2_tzseb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managed Airflow in Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lup54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678280042.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678277255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151\"&gt;https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you think of this? I think its really awesome they are going forwards using open source scheduler tools in ADF.&lt;/p&gt;\n\n&lt;p&gt;Anyone had already experiences with it? However it is still depending on Data Factory&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?auto=webp&amp;v=enabled&amp;s=377641832eecbd621b310ab9dd16d381993c449f", "width": 2288, "height": 1448}, "resolutions": [{"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc4ffca7b91875a7fa3134c0d86d66dcd449c179", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75220a5e620ee298b7b2d8329044b879daca4586", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f764681c03c53d1229c8ff8e3799ea7bec9e4899", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f124a2fdd04ffd44d572a4e01089662b94a5f067", "width": 640, "height": 405}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7d3af51b0e95fbe753302b6f4c7969990339dc6", "width": 960, "height": 607}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a177ca1e48aaead34281cb5f3e145a59154b95e", "width": 1080, "height": 683}], "variants": {}, "id": "S0ZC01nTKv9edIZMquYzX-iG3I-MSj166ECzeJEKDn8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11lup54", "is_robot_indexable": true, "report_reasons": null, "author": "Zouzzi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lup54/managed_airflow_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lup54/managed_airflow_in_azure/", "subreddit_subscribers": 92284, "created_utc": 1678277255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I am running into an issue with Redshift and would appreciate any suggestions.\n\nI am creating some views on redshift using dbt. One of the in particular controls the row level access per user, so all models are ultimately joined with it. But it still is just a view definition, there are no tables in the mix.\n\nEvery single time dbt tries to update this view, if there are any queries at that time that involve that view (which is often, since this is a client facing database), it deadlocks.\nBy that I mean - dbt is not able to re-create the view, no one is able to query it, and this situations persists until dbt's process is manually killed.\n\nIt seems to me that even if the view is under use, as long as there are a few seconds of downtime (which there are), then dbt should do its job and no deadlock should occur.\n\nAny ideas? It's driving me crazy!\n\nThanks", "author_fullname": "t2_8xg3h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "question - dbt and redshift deadlock?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lqmmr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678263507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I am running into an issue with Redshift and would appreciate any suggestions.&lt;/p&gt;\n\n&lt;p&gt;I am creating some views on redshift using dbt. One of the in particular controls the row level access per user, so all models are ultimately joined with it. But it still is just a view definition, there are no tables in the mix.&lt;/p&gt;\n\n&lt;p&gt;Every single time dbt tries to update this view, if there are any queries at that time that involve that view (which is often, since this is a client facing database), it deadlocks.\nBy that I mean - dbt is not able to re-create the view, no one is able to query it, and this situations persists until dbt&amp;#39;s process is manually killed.&lt;/p&gt;\n\n&lt;p&gt;It seems to me that even if the view is under use, as long as there are a few seconds of downtime (which there are), then dbt should do its job and no deadlock should occur.&lt;/p&gt;\n\n&lt;p&gt;Any ideas? It&amp;#39;s driving me crazy!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lqmmr", "is_robot_indexable": true, "report_reasons": null, "author": "Fredbull", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lqmmr/question_dbt_and_redshift_deadlock/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lqmmr/question_dbt_and_redshift_deadlock/", "subreddit_subscribers": 92284, "created_utc": 1678263507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I am a revenue assurance analyst at a telecommunications company where we use a niche CRM to process and tracks our financials as well as our orders, tickets, and audits. I know the CRM inside and out, only person at my company that understands how everything works. \n\nCompany just trained me in Tableau for my reporting with some others at my company, few days after the training I created a dashboard leveraging our data from the CRM and caught a lot of attention from c-suite.\n\nWe use salesforce as our data gathering CRM and there was an integration effort in progress with the niche CRM and Salesforce. I joined onto the effort a few months ago and have been a major point of contact. Two weeks ago I discovered that the way we were using Salesforce to link commissions data was not reliable and historical data was subject to change based on web hooks. Presented my efforts to the CFO, CSO, and Salesforce architect (who loves me) which ultimately led them to scraping the entire integration of the financial data into Salesforce since we would never have accurate historical data. \n\nWe decided to use redshift as our path forward and I have been trying to learn as much as I can before the ware house is ready. I think that there is a new role here but I was hoping for other input or experiences. My current boss wants me to. Focus on audits but I know how crucial this ware house will be for us. Any advice?", "author_fullname": "t2_cdom6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Company just moved to RedShift, how to leverage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ljgqt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678242313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am a revenue assurance analyst at a telecommunications company where we use a niche CRM to process and tracks our financials as well as our orders, tickets, and audits. I know the CRM inside and out, only person at my company that understands how everything works. &lt;/p&gt;\n\n&lt;p&gt;Company just trained me in Tableau for my reporting with some others at my company, few days after the training I created a dashboard leveraging our data from the CRM and caught a lot of attention from c-suite.&lt;/p&gt;\n\n&lt;p&gt;We use salesforce as our data gathering CRM and there was an integration effort in progress with the niche CRM and Salesforce. I joined onto the effort a few months ago and have been a major point of contact. Two weeks ago I discovered that the way we were using Salesforce to link commissions data was not reliable and historical data was subject to change based on web hooks. Presented my efforts to the CFO, CSO, and Salesforce architect (who loves me) which ultimately led them to scraping the entire integration of the financial data into Salesforce since we would never have accurate historical data. &lt;/p&gt;\n\n&lt;p&gt;We decided to use redshift as our path forward and I have been trying to learn as much as I can before the ware house is ready. I think that there is a new role here but I was hoping for other input or experiences. My current boss wants me to. Focus on audits but I know how crucial this ware house will be for us. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11ljgqt", "is_robot_indexable": true, "report_reasons": null, "author": "FXAFrank", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ljgqt/company_just_moved_to_redshift_how_to_leverage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ljgqt/company_just_moved_to_redshift_how_to_leverage/", "subreddit_subscribers": 92284, "created_utc": 1678242313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Obviously I have a dag for data extraction, dag for transformations and dag for \u201caggregations\u201d.\n\nNow, all dags are created manually - i. e. when we add new data source we add new task to the dag and we have to define the dependencies manually as well. \n\nHow common is such practice? Meaning - I am thinking of generating the dags from some yaml template but not sure it would make things easier. The main reason I am considering it is because of the need to manually define the dependencies in the dag which can lead to human errors. \n\nBtw. for the \u201caggregations\u201d it should be pretty easy to generate such dag as this data is in the RDBMS and its tables can be defined using SQLAlchemy or dbt. But how about the \u201cmiddle\u201d step which just takes raw data and standardizes it in the datalake to parquet format? \ud83e\udd14", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow dags generation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lxu63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678285898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously I have a dag for data extraction, dag for transformations and dag for \u201caggregations\u201d.&lt;/p&gt;\n\n&lt;p&gt;Now, all dags are created manually - i. e. when we add new data source we add new task to the dag and we have to define the dependencies manually as well. &lt;/p&gt;\n\n&lt;p&gt;How common is such practice? Meaning - I am thinking of generating the dags from some yaml template but not sure it would make things easier. The main reason I am considering it is because of the need to manually define the dependencies in the dag which can lead to human errors. &lt;/p&gt;\n\n&lt;p&gt;Btw. for the \u201caggregations\u201d it should be pretty easy to generate such dag as this data is in the RDBMS and its tables can be defined using SQLAlchemy or dbt. But how about the \u201cmiddle\u201d step which just takes raw data and standardizes it in the datalake to parquet format? \ud83e\udd14&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11lxu63", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lxu63/airflow_dags_generation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lxu63/airflow_dags_generation/", "subreddit_subscribers": 92284, "created_utc": 1678285898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m searching for a tool I can use to request data from an API (Spotify\u2019s Web API), transform it, and load it to a MongoDB Atlas Database. My jobs will need to run both on scheduled basis and on a triggered basis, on the cloud.\n\nI tried to implement Atlas Trigger Functions as a solution, but that was a dead end due to their execution-time and socket limitations. \n\nAdditionally, I\u2019ve looked into a handful of IPaaS platforms (SnapLogic, Zapier, Workato) but their pricing models seem pretty steep for my use case.\n\nSome of my jobs will need to make 1,000\u2019s calls to the API, and will likely have an execution time over an hour.\n\nAny ideas for a potential solution? Thoughts on ETL tools like AWS Glue?", "author_fullname": "t2_5l0hu6ke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need a Data Integration Tool (Spotify Web API =&gt; MongoDB Atlas DB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l7pqc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678215125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m searching for a tool I can use to request data from an API (Spotify\u2019s Web API), transform it, and load it to a MongoDB Atlas Database. My jobs will need to run both on scheduled basis and on a triggered basis, on the cloud.&lt;/p&gt;\n\n&lt;p&gt;I tried to implement Atlas Trigger Functions as a solution, but that was a dead end due to their execution-time and socket limitations. &lt;/p&gt;\n\n&lt;p&gt;Additionally, I\u2019ve looked into a handful of IPaaS platforms (SnapLogic, Zapier, Workato) but their pricing models seem pretty steep for my use case.&lt;/p&gt;\n\n&lt;p&gt;Some of my jobs will need to make 1,000\u2019s calls to the API, and will likely have an execution time over an hour.&lt;/p&gt;\n\n&lt;p&gt;Any ideas for a potential solution? Thoughts on ETL tools like AWS Glue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11l7pqc", "is_robot_indexable": true, "report_reasons": null, "author": "Level-Gur8656", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l7pqc/need_a_data_integration_tool_spotify_web_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l7pqc/need_a_data_integration_tool_spotify_web_api/", "subreddit_subscribers": 92284, "created_utc": 1678215125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5pxn0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Teams Survey 2023 Results", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11lzsrn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1678290680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jesse-anderson.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.jesse-anderson.com/2023/03/data-teams-survey-2023-results/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Mentor | Jesse Anderson", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lzsrn", "is_robot_indexable": true, "report_reasons": null, "author": "eljefe6a", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11lzsrn/data_teams_survey_2023_results/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.jesse-anderson.com/2023/03/data-teams-survey-2023-results/", "subreddit_subscribers": 92284, "created_utc": 1678290680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am building a data pipeline and dashboard app that I want to deploy online (fastapi + postgresql + svelte/vue). It will read all data from postgresql, do computations (numpy / pandas), store the output back into postgresql, and then the frontend app will communicate with the api to render the dashboard.\n\nBut what is the best way to deploy postgres in production? My platform will start small, but I'd like to have the infrastructure to scale quickly when / if the platform takes off.\n\nI am a data engineer and programmer and I used to do everything myself, but with kubernets, docker things got more complicated:\n\n* use kubernetes to have postgresql, api (fastapi), and data pipeline in separate dockers and communicate with each other over network?\n* vm for postgresql?\n* \"bare metal\" (dedicated server) for postgresql?\n\nI've read pros and cons for each, I really don't know what the best way is to do this.\n\nDoes anybody has experience with this?", "author_fullname": "t2_15vo19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "from files to db in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lrzi6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678268273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building a data pipeline and dashboard app that I want to deploy online (fastapi + postgresql + svelte/vue). It will read all data from postgresql, do computations (numpy / pandas), store the output back into postgresql, and then the frontend app will communicate with the api to render the dashboard.&lt;/p&gt;\n\n&lt;p&gt;But what is the best way to deploy postgres in production? My platform will start small, but I&amp;#39;d like to have the infrastructure to scale quickly when / if the platform takes off.&lt;/p&gt;\n\n&lt;p&gt;I am a data engineer and programmer and I used to do everything myself, but with kubernets, docker things got more complicated:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;use kubernetes to have postgresql, api (fastapi), and data pipeline in separate dockers and communicate with each other over network?&lt;/li&gt;\n&lt;li&gt;vm for postgresql?&lt;/li&gt;\n&lt;li&gt;&amp;quot;bare metal&amp;quot; (dedicated server) for postgresql?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve read pros and cons for each, I really don&amp;#39;t know what the best way is to do this.&lt;/p&gt;\n\n&lt;p&gt;Does anybody has experience with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lrzi6", "is_robot_indexable": true, "report_reasons": null, "author": "iLLucionist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lrzi6/from_files_to_db_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lrzi6/from_files_to_db_in_production/", "subreddit_subscribers": 92284, "created_utc": 1678268273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Questions for any librarians, professors, scientists, Library and Information Science graduates, or any other profession that requires handling large amounts of data that needs to be searchable and categorized correctly.\n\nCurrently looking to catalogue many different file types. Everything from different image formats, zip formats, video containers, audio formats, PDF, etc. Point of this is to make files I have collected easily searchable and usable when doing research without having to remember exactly what information or media I have collected and where it is located.\n\n&amp;#x200B;\n\nI currently do not have a metadata system so I am flexible with what can be implemented. The most coinvent way would be to embed metadata directly into different file types, however, I do not know if this is always possible with different file and container standards. The other solution would be having a sidecar associated to a file, currently looking at either JSON, XML, or XMP as formats. Gravitating to XML. Zotero will also be used to collected and categorize information, but this is a story for another day.\n\nObviously different institutions may have their own proprietary software that they use internally to create and search for metadata but what I am looking for is open software that does not create propriety files or formats that will have to be converted in future.\n\nDoes anyone have any recommendations for software that can create metadata, especially keywords? I also need recommendations for software that is able to search through all the metadata and present files based on keywords or any other rules that make it easy to locate files. Currently looking at [jEXIFToolGUI](https://github.com/hvdwolf/jExifToolGUI), I prefer the GUI because many different files (especially images) require me to be able to see a preview and using a command line tool would take a massive amount of time to create the metadata.\n\n&amp;#x200B;\n\nWhat I am looking for is a local tool that allows me to easily create keywords as I gather data or allows me to create metadata in an efficient and fast way. Obviously if I have 2000+ images or books/articles/research papers, I don't want to have to spend months creating universal keywords that takes hours to associate to different files. And also a tool that allows me to point to a main folder or a folder of just metadata files that will search through all of them and present files that match my search criteria. Library of Congress, etc. has to have an easy to create descriptions and search through their massive collections.\n\nPlease help :)", "author_fullname": "t2_o8wjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for Metadata Searching and Creation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lmqy1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678251537.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678251217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Questions for any librarians, professors, scientists, Library and Information Science graduates, or any other profession that requires handling large amounts of data that needs to be searchable and categorized correctly.&lt;/p&gt;\n\n&lt;p&gt;Currently looking to catalogue many different file types. Everything from different image formats, zip formats, video containers, audio formats, PDF, etc. Point of this is to make files I have collected easily searchable and usable when doing research without having to remember exactly what information or media I have collected and where it is located.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I currently do not have a metadata system so I am flexible with what can be implemented. The most coinvent way would be to embed metadata directly into different file types, however, I do not know if this is always possible with different file and container standards. The other solution would be having a sidecar associated to a file, currently looking at either JSON, XML, or XMP as formats. Gravitating to XML. Zotero will also be used to collected and categorize information, but this is a story for another day.&lt;/p&gt;\n\n&lt;p&gt;Obviously different institutions may have their own proprietary software that they use internally to create and search for metadata but what I am looking for is open software that does not create propriety files or formats that will have to be converted in future.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any recommendations for software that can create metadata, especially keywords? I also need recommendations for software that is able to search through all the metadata and present files based on keywords or any other rules that make it easy to locate files. Currently looking at &lt;a href=\"https://github.com/hvdwolf/jExifToolGUI\"&gt;jEXIFToolGUI&lt;/a&gt;, I prefer the GUI because many different files (especially images) require me to be able to see a preview and using a command line tool would take a massive amount of time to create the metadata.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What I am looking for is a local tool that allows me to easily create keywords as I gather data or allows me to create metadata in an efficient and fast way. Obviously if I have 2000+ images or books/articles/research papers, I don&amp;#39;t want to have to spend months creating universal keywords that takes hours to associate to different files. And also a tool that allows me to point to a main folder or a folder of just metadata files that will search through all of them and present files that match my search criteria. Library of Congress, etc. has to have an easy to create descriptions and search through their massive collections.&lt;/p&gt;\n\n&lt;p&gt;Please help :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uM7coqDn-9EXT9S7l7on3W1sCkx_HhUqoO7uYtZwnbU.jpg?auto=webp&amp;v=enabled&amp;s=5ef0341d25a2e3ee6b350aae7173ae7ee738ccfc", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/uM7coqDn-9EXT9S7l7on3W1sCkx_HhUqoO7uYtZwnbU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd4e272987814740a35cb0f105ccea51bbfa55b7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/uM7coqDn-9EXT9S7l7on3W1sCkx_HhUqoO7uYtZwnbU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c39555a3055ec8f694c2e5eb4910e941104f91d2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/uM7coqDn-9EXT9S7l7on3W1sCkx_HhUqoO7uYtZwnbU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e938b403e2d5f2e89de9f8518508630034257703", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/uM7coqDn-9EXT9S7l7on3W1sCkx_HhUqoO7uYtZwnbU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=020f8e2828fd2129091e6f9f2388fc6b7bce868c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/uM7coqDn-9EXT9S7l7on3W1sCkx_HhUqoO7uYtZwnbU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc045d90add13af65a83edbb794788d7f1ea993c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/uM7coqDn-9EXT9S7l7on3W1sCkx_HhUqoO7uYtZwnbU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=142c2331c1330e702f48c08f69ece46a9ce3baae", "width": 1080, "height": 540}], "variants": {}, "id": "IltM0kQHIx5DNjQ8Fh9N0UT1y3o3gMf95sLW8twPWRQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11lmqy1", "is_robot_indexable": true, "report_reasons": null, "author": "ReclusiveEagle", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lmqy1/software_for_metadata_searching_and_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lmqy1/software_for_metadata_searching_and_creation/", "subreddit_subscribers": 92284, "created_utc": 1678251217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not a data engineer, but I'm working at a logistics company that runs off of excel files and I want to move them all into a database.\n\nBasically, we get ocean freight rates from multiple sources, and they send them via Excel files. I want to be able to upload these files onto a searchable database that has uniform data.\n\nThe problem is that all these excel files names their categories differently and structure their data differently. \n\nSo, I'm wondering how difficult a task is it to scrape the data and make it into a uniform database?\n\nIs this even possible?", "author_fullname": "t2_oeaa0pet", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How hard is it to combine different excel sheets into one database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lfdkf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678232060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a data engineer, but I&amp;#39;m working at a logistics company that runs off of excel files and I want to move them all into a database.&lt;/p&gt;\n\n&lt;p&gt;Basically, we get ocean freight rates from multiple sources, and they send them via Excel files. I want to be able to upload these files onto a searchable database that has uniform data.&lt;/p&gt;\n\n&lt;p&gt;The problem is that all these excel files names their categories differently and structure their data differently. &lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m wondering how difficult a task is it to scrape the data and make it into a uniform database?&lt;/p&gt;\n\n&lt;p&gt;Is this even possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lfdkf", "is_robot_indexable": true, "report_reasons": null, "author": "KaulHilo", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lfdkf/how_hard_is_it_to_combine_different_excel_sheets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lfdkf/how_hard_is_it_to_combine_different_excel_sheets/", "subreddit_subscribers": 92284, "created_utc": 1678232060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smart Brokers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_11lcv76", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VnUhYAfBogJDgLmEY9uQ3fQyZkg1o7omS554n0uaV2Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678226512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/smart-brokers?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7n4e-fxDjrSv2uD4tzsWS15dvSoLmUDsCj_Hu7T4gwk.jpg?auto=webp&amp;v=enabled&amp;s=503176a402f8882ca260cbee565409e092a9de8e", "width": 764, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7n4e-fxDjrSv2uD4tzsWS15dvSoLmUDsCj_Hu7T4gwk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59fb9530e074bd3449b9bf7fa36060686644944a", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/7n4e-fxDjrSv2uD4tzsWS15dvSoLmUDsCj_Hu7T4gwk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70269d47e565acf7e2317ae6bde21c88dd2e330a", "width": 216, "height": 169}, {"url": "https://external-preview.redd.it/7n4e-fxDjrSv2uD4tzsWS15dvSoLmUDsCj_Hu7T4gwk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0475beac9f3c5c1fb846e1dcbc1fe5eb43cc939", "width": 320, "height": 251}, {"url": "https://external-preview.redd.it/7n4e-fxDjrSv2uD4tzsWS15dvSoLmUDsCj_Hu7T4gwk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c264576b79a90c074095c2b842bccbcf3fa9f46", "width": 640, "height": 502}], "variants": {}, "id": "d-tMNY9JPcSTWMi-uCmzyrJP6Ah2-geqtSl4HBqMgTk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lcv76", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lcv76/smart_brokers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/smart-brokers?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 92284, "created_utc": 1678226512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI recently received outdated databases from my uncle from 2010, but they need to be more organized and in better condition. There are 200 .xls and .xlxs files saved in various subfolders, some of which are corrupted and contain only readable values. The headers need to be standardized and sometimes appear on the second row.\n\nAs someone with limited knowledge of Python, I have struggled with the challenges of the task. Despite seeking assistance from ChatGPT, I still need help. My goal is to extract the names and email addresses from these files.\n\nI am searching for a program that can quickly identify the headers of each file, allowing me to select the relevant columns based on their titles.", "author_fullname": "t2_4esf6urf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Program to handle simple stuff in multiple excel sheets.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l87u7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678216234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently received outdated databases from my uncle from 2010, but they need to be more organized and in better condition. There are 200 .xls and .xlxs files saved in various subfolders, some of which are corrupted and contain only readable values. The headers need to be standardized and sometimes appear on the second row.&lt;/p&gt;\n\n&lt;p&gt;As someone with limited knowledge of Python, I have struggled with the challenges of the task. Despite seeking assistance from ChatGPT, I still need help. My goal is to extract the names and email addresses from these files.&lt;/p&gt;\n\n&lt;p&gt;I am searching for a program that can quickly identify the headers of each file, allowing me to select the relevant columns based on their titles.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11l87u7", "is_robot_indexable": true, "report_reasons": null, "author": "NonSenseAdventurer", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l87u7/program_to_handle_simple_stuff_in_multiple_excel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l87u7/program_to_handle_simple_stuff_in_multiple_excel/", "subreddit_subscribers": 92284, "created_utc": 1678216234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that data analysis is probably the main path to reaching DE. But are there alternative approaches to a DE job as well, if you add in professional certifications from either Google, MS, and/or AWS? I am saying this in the regard that the alternative role places some emphasis in SQL. If not, what are other alternative roles besides data analysis that can lead en eventual role in DE?", "author_fullname": "t2_shlqver7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm an engineering grad and have enough credentials skill-wise to work as an industrial engineer. from what i have observed, this job demands knowledge in sql. That being said, is taking any role that has some emphasis on sql eligible enough to make the transition to higher level data roles like DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11m1iuf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678294589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that data analysis is probably the main path to reaching DE. But are there alternative approaches to a DE job as well, if you add in professional certifications from either Google, MS, and/or AWS? I am saying this in the regard that the alternative role places some emphasis in SQL. If not, what are other alternative roles besides data analysis that can lead en eventual role in DE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11m1iuf", "is_robot_indexable": true, "report_reasons": null, "author": "Such-Assist4895", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m1iuf/im_an_engineering_grad_and_have_enough/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m1iuf/im_an_engineering_grad_and_have_enough/", "subreddit_subscribers": 92284, "created_utc": 1678294589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We got 10s to 100s of GBs of data in parquet and json files.\nAthena is slow and awful. Databricks serverless sql looks super interesting, but I'm uncertain regarding costs.\n\nAre there any other good alternatives to query well saved and partitioned data saved in s3?\n\nDon't want to load it to a data warehouse, but I want to query the data for analytics and ML.\n\nOhh and only a managed platform, not self hosting.", "author_fullname": "t2_9ijn2a1v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Platform to query S3 data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11m0zqg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678293399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We got 10s to 100s of GBs of data in parquet and json files.\nAthena is slow and awful. Databricks serverless sql looks super interesting, but I&amp;#39;m uncertain regarding costs.&lt;/p&gt;\n\n&lt;p&gt;Are there any other good alternatives to query well saved and partitioned data saved in s3?&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t want to load it to a data warehouse, but I want to query the data for analytics and ML.&lt;/p&gt;\n\n&lt;p&gt;Ohh and only a managed platform, not self hosting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11m0zqg", "is_robot_indexable": true, "report_reasons": null, "author": "Ordinary-Storm8567", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m0zqg/best_platform_to_query_s3_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m0zqg/best_platform_to_query_s3_data_lake/", "subreddit_subscribers": 92284, "created_utc": 1678293399.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}