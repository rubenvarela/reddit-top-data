{"kind": "Listing", "data": {"after": "t3_11lqpnb", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data engineers! After having struggled at work with frequently having to transfer files, or even entire directories, between different cloud storage services, I decided to try and make this task easy for me, so I created Fluke, a Python package that offers a simple API which hides away all this complexity regarding file transfer. If your day-to-day work is similar to mine, I think that this project can help you in a big way!\n\nAs of yet, you can use Fluke to transfer files to/from the following locations, but I'll try adding even more in the future:\n\n* Local file system\n* Remote file system (through SSH/SFTP)\n* Amazon S3 (through HTTP)\n* ADLSv2 (through HTTP)\n\nGive it a go and tell me what you think!\n\n* Github: [https://github.com/manoss96/fluke](https://github.com/manoss96/fluke)\n* Docs: [fluke.rtfd.io](https://fluke.rtfd.io/)\n* Example: [https://github.com/manoss96/fluke#usage-example](https://github.com/manoss96/fluke#usage-example)", "author_fullname": "t2_q7l1xoqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created an open source project to help you transfer files between various remote locations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kyceb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678193362.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678192931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data engineers! After having struggled at work with frequently having to transfer files, or even entire directories, between different cloud storage services, I decided to try and make this task easy for me, so I created Fluke, a Python package that offers a simple API which hides away all this complexity regarding file transfer. If your day-to-day work is similar to mine, I think that this project can help you in a big way!&lt;/p&gt;\n\n&lt;p&gt;As of yet, you can use Fluke to transfer files to/from the following locations, but I&amp;#39;ll try adding even more in the future:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Local file system&lt;/li&gt;\n&lt;li&gt;Remote file system (through SSH/SFTP)&lt;/li&gt;\n&lt;li&gt;Amazon S3 (through HTTP)&lt;/li&gt;\n&lt;li&gt;ADLSv2 (through HTTP)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Give it a go and tell me what you think!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Github: &lt;a href=\"https://github.com/manoss96/fluke\"&gt;https://github.com/manoss96/fluke&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Docs: &lt;a href=\"https://fluke.rtfd.io/\"&gt;fluke.rtfd.io&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Example: &lt;a href=\"https://github.com/manoss96/fluke#usage-example\"&gt;https://github.com/manoss96/fluke#usage-example&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?auto=webp&amp;v=enabled&amp;s=03923c7e18e6975fa0a3e01654a87a2b6f0cc2b3", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f521a8cb43c395aeca794cf0f3ec60f80f55269b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f34131bc19b7a2460d2a180839b9720bf985f3f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86ce753cf5795c7b4d99c1fe48a8ced1918f7952", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=678ed5c0637d5d5fc0a9955230dd5f28cab16df6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7620a8bdf7a387d600a8110309c627f2cc9946c4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=784adf700ac1116ddeddb06409d4e8f7022d314e", "width": 1080, "height": 540}], "variants": {}, "id": "jLtdjHFEEy8dK1PXEN0uUAKPCiwcnjgJmAi80-3-9Xw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11kyceb", "is_robot_indexable": true, "report_reasons": null, "author": "WerdenWissen", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kyceb/i_created_an_open_source_project_to_help_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kyceb/i_created_an_open_source_project_to_help_you/", "subreddit_subscribers": 92249, "created_utc": 1678192931.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_172g1e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Discord Stores Trillions of Messages", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "name": "t3_11kyewm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wlCEH9KOY9PKb0zBLXsGRK_cTvvVYsWiwgfU5Q6i9ZA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678193118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "discord.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://discord.com/blog/how-discord-stores-trillions-of-messages", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?auto=webp&amp;v=enabled&amp;s=b1fc9516d12abf7d5fa616af3ff87ade48a9bcd5", "width": 1800, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f34b671a3572c6eff7efbf99c11b0c2930d5595d", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bfd681691b142c52ae5548ab621e9eb4da4dfcb", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5c9cb59417ea4eed46919dc15bf294251c6dd4c", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60d656f86b1a63ba5119e5c7daf3b96dd8a725f3", "width": 640, "height": 256}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75f56faf6057b97e7ca7fcdf037130a66d9aedfc", "width": 960, "height": 384}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa538c60fc7d4c5bd81151e90fb222772908404f", "width": 1080, "height": 432}], "variants": {}, "id": "pyoGenD5-m13RONB-xGYn7K1gfpx0aUjaCSfI-OcLdE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11kyewm", "is_robot_indexable": true, "report_reasons": null, "author": "stevecrox0914", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kyewm/how_discord_stores_trillions_of_messages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://discord.com/blog/how-discord-stores-trillions-of-messages", "subreddit_subscribers": 92249, "created_utc": 1678193118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am not sure whether this question has been asked earlier or not. Just needed to know how most of data engineers started their journey\n\n1. Directly joined a data related field, say data analytics or data engineering\n2. Or transitioned from some other related backend role, such as server side or something similar\n3. Or some other domain\n\nI ask this because I wanted to know whether directly getting into data side without much grounding in other backend related technologies would be good choice or not (maybe it could be learned alongside)\n\nEdit: would like to add a bit about myself, came out of college and employed straight into de role. Do not have much footing in other software domains", "author_fullname": "t2_jx4zrwe0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did current data engineers start their software journey as data engineer or something else", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l0m2w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678201604.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678198857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am not sure whether this question has been asked earlier or not. Just needed to know how most of data engineers started their journey&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Directly joined a data related field, say data analytics or data engineering&lt;/li&gt;\n&lt;li&gt;Or transitioned from some other related backend role, such as server side or something similar&lt;/li&gt;\n&lt;li&gt;Or some other domain&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I ask this because I wanted to know whether directly getting into data side without much grounding in other backend related technologies would be good choice or not (maybe it could be learned alongside)&lt;/p&gt;\n\n&lt;p&gt;Edit: would like to add a bit about myself, came out of college and employed straight into de role. Do not have much footing in other software domains&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11l0m2w", "is_robot_indexable": true, "report_reasons": null, "author": "sjdevelop", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l0m2w/did_current_data_engineers_start_their_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l0m2w/did_current_data_engineers_start_their_software/", "subreddit_subscribers": 92249, "created_utc": 1678198857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How We Deploy 5X Faster with Warm Docker Containers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11lm6v8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3PsKOf3gXsXBRgBoWV9V2DNgR2NLi7bPiFVWQ3cvaZg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678249633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/fast-deploys-with-pex-and-docker", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?auto=webp&amp;v=enabled&amp;s=428399d39a1be1aded62d15ab2065cb88d8bf446", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=046e5911c908888ccaba4b46cc8a72296466e0ae", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7797460cbf8b1014dceba585394e955adc56ba5", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56b26029903e7b2bbe053dcfcbcb0d4d5f048f8d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a96e31d8af6bed29048f108a9830d27d5da6c04", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=165d741737d8fbc89be2fdb81121f8438e3352e8", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/G5SvrdIT_LHDOoR8LnHmSriv1V6i9PnaCinKRPepQjs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca8e3e0fd14e7b473ada1f4825f4c10e7f3e8009", "width": 1080, "height": 567}], "variants": {}, "id": "TCwPSvjxLSjXzKqkzC26JS0QPxvT4x7b0EnVA3jwC2I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lm6v8", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lm6v8/how_we_deploy_5x_faster_with_warm_docker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/fast-deploys-with-pex-and-docker", "subreddit_subscribers": 92249, "created_utc": 1678249633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We all talk about data pipelines when we talk about DE or DE related positions. What how would you explain a data pipeline with examples to a layman(maybe). How many different examples can we think of?\n\nJust a random question that popped up.", "author_fullname": "t2_3nipc92b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you explain a data pipeline to a non techie?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lid59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678239481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We all talk about data pipelines when we talk about DE or DE related positions. What how would you explain a data pipeline with examples to a layman(maybe). How many different examples can we think of?&lt;/p&gt;\n\n&lt;p&gt;Just a random question that popped up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11lid59", "is_robot_indexable": true, "report_reasons": null, "author": "kausthab87", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lid59/how_would_you_explain_a_data_pipeline_to_a_non/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lid59/how_would_you_explain_a_data_pipeline_to_a_non/", "subreddit_subscribers": 92249, "created_utc": 1678239481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am not sure how should I partition my data in the S3 data lake. \n\nI decided to go with \u201chive\u201d partitions format so I am gonna use \u201ckey=value\u201d style. But now, how the actual structure should look like?\n\na. year=2023/month=3/day=1/file.parquet (one file only)\n\nb. year=2023/month=3/day1.parquet (multiple files)\n\nc. date=2023-03-01/file.parquet\n\nWhen using b) - how would query like \u201cwhere year=2023 and month = 3 and day = 1\u201d work? It would read only the file \u201cday1\u201d or all files in this \u201cfolder\u201d and then filter the records from these files?\n\nAlso, if using the c) approach - how it would work if I would like to query one month of data? Would the app (say Spark or anything else) know it should take a look only at \u201c2023-03-*\u201d? Or it would scan all the folders?\n\nWhat is the strategy when designing data lake partitions?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lake partitioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l5aft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678209684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not sure how should I partition my data in the S3 data lake. &lt;/p&gt;\n\n&lt;p&gt;I decided to go with \u201chive\u201d partitions format so I am gonna use \u201ckey=value\u201d style. But now, how the actual structure should look like?&lt;/p&gt;\n\n&lt;p&gt;a. year=2023/month=3/day=1/file.parquet (one file only)&lt;/p&gt;\n\n&lt;p&gt;b. year=2023/month=3/day1.parquet (multiple files)&lt;/p&gt;\n\n&lt;p&gt;c. date=2023-03-01/file.parquet&lt;/p&gt;\n\n&lt;p&gt;When using b) - how would query like \u201cwhere year=2023 and month = 3 and day = 1\u201d work? It would read only the file \u201cday1\u201d or all files in this \u201cfolder\u201d and then filter the records from these files?&lt;/p&gt;\n\n&lt;p&gt;Also, if using the c) approach - how it would work if I would like to query one month of data? Would the app (say Spark or anything else) know it should take a look only at \u201c2023-03-*\u201d? Or it would scan all the folders?&lt;/p&gt;\n\n&lt;p&gt;What is the strategy when designing data lake partitions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11l5aft", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l5aft/data_lake_partitioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l5aft/data_lake_partitioning/", "subreddit_subscribers": 92249, "created_utc": 1678209684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/](https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/)", "author_fullname": "t2_3pb20mxg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Kestra, the open source data orchestration and scheduling platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lra9t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678265803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/\"&gt;https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lra9t", "is_robot_indexable": true, "report_reasons": null, "author": "loicmathieu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lra9t/introduction_to_kestra_the_open_source_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lra9t/introduction_to_kestra_the_open_source_data/", "subreddit_subscribers": 92249, "created_utc": 1678265803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are a lot of questions about CDC in this sub... \n\nWould you contribute to a communal sheet to help guide tooling decisions? \n\ne.g. a managed spreadsheet enumerating all pros/cons/gotchas  of various tools?\n\nPls comment if so and on any modifcations to a v1 I'd share:\n\n**Criteria:**\n\n\\*Method (log / trigger / timestamp)\n\n\\*In-flights transforms in SQL\n\n\\*Requires windowing (e.g. can you join any historical data to a stream) \n\n\\*End-to-end exactly once? \n\n\\*Pricing\n\n\\*Automated Schema Evolution\n\n\\*DB size limit\n\n\\*Testing Capability\n\n\\*Environments (Cloud v. on-premise)\n\n\\*Automated Backfill\n\n\\*UI?\n\n\\*Pricing model\n\n\\*Environments (cloud, on-prem)\n\n\\*Dependencies\n\n\\*Sources/Destinations\n\n\\*User feedback\n\n**Solutions (limited to real-time)**\n\nDebezium\n\nConfluent\n\nStriim\n\nEstuary Flow\n\nArcion\n\nHVR (Fivetran)\n\nMatillion\n\nGoldenGate\n\nPrecisely\n\nQlik\n\nStreamSets\n\nEqualum\n\nNiFi\n\nFlink  \n\n\nShould we include batch tools (Airbyte, Hevo, Gather, Rivery, Stitch, etc...)", "author_fullname": "t2_sa3mbz4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Change Data Capture Eval Critera", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l84i9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678216028.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are a lot of questions about CDC in this sub... &lt;/p&gt;\n\n&lt;p&gt;Would you contribute to a communal sheet to help guide tooling decisions? &lt;/p&gt;\n\n&lt;p&gt;e.g. a managed spreadsheet enumerating all pros/cons/gotchas  of various tools?&lt;/p&gt;\n\n&lt;p&gt;Pls comment if so and on any modifcations to a v1 I&amp;#39;d share:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Criteria:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;*Method (log / trigger / timestamp)&lt;/p&gt;\n\n&lt;p&gt;*In-flights transforms in SQL&lt;/p&gt;\n\n&lt;p&gt;*Requires windowing (e.g. can you join any historical data to a stream) &lt;/p&gt;\n\n&lt;p&gt;*End-to-end exactly once? &lt;/p&gt;\n\n&lt;p&gt;*Pricing&lt;/p&gt;\n\n&lt;p&gt;*Automated Schema Evolution&lt;/p&gt;\n\n&lt;p&gt;*DB size limit&lt;/p&gt;\n\n&lt;p&gt;*Testing Capability&lt;/p&gt;\n\n&lt;p&gt;*Environments (Cloud v. on-premise)&lt;/p&gt;\n\n&lt;p&gt;*Automated Backfill&lt;/p&gt;\n\n&lt;p&gt;*UI?&lt;/p&gt;\n\n&lt;p&gt;*Pricing model&lt;/p&gt;\n\n&lt;p&gt;*Environments (cloud, on-prem)&lt;/p&gt;\n\n&lt;p&gt;*Dependencies&lt;/p&gt;\n\n&lt;p&gt;*Sources/Destinations&lt;/p&gt;\n\n&lt;p&gt;*User feedback&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Solutions (limited to real-time)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Debezium&lt;/p&gt;\n\n&lt;p&gt;Confluent&lt;/p&gt;\n\n&lt;p&gt;Striim&lt;/p&gt;\n\n&lt;p&gt;Estuary Flow&lt;/p&gt;\n\n&lt;p&gt;Arcion&lt;/p&gt;\n\n&lt;p&gt;HVR (Fivetran)&lt;/p&gt;\n\n&lt;p&gt;Matillion&lt;/p&gt;\n\n&lt;p&gt;GoldenGate&lt;/p&gt;\n\n&lt;p&gt;Precisely&lt;/p&gt;\n\n&lt;p&gt;Qlik&lt;/p&gt;\n\n&lt;p&gt;StreamSets&lt;/p&gt;\n\n&lt;p&gt;Equalum&lt;/p&gt;\n\n&lt;p&gt;NiFi&lt;/p&gt;\n\n&lt;p&gt;Flink  &lt;/p&gt;\n\n&lt;p&gt;Should we include batch tools (Airbyte, Hevo, Gather, Rivery, Stitch, etc...)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11l84i9", "is_robot_indexable": true, "report_reasons": null, "author": "MooJerseyCreamery", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l84i9/change_data_capture_eval_critera/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l84i9/change_data_capture_eval_critera/", "subreddit_subscribers": 92249, "created_utc": 1678216028.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had a design and architecture interview. Ended up being some predefined tables and data in a browser-based IDE. I was asked to \"model the data for a reporting database\". Kind of confused but started into a discussion on star schemas and why that is not the best on all databases but for a generic solution like this would fit.\n\nNOPE. Modeling the data meant writing some aggregate queries against those predefined tables. This was a design and architecture interview. If it had been a basic SQL interview, it would have been a fairly average problem set.\n\nI still feel confused. I did write some queries but not really sure I was in the right place or understanding the ask. I have to admit the mental switch very much threw me off my game. \n\nIf they wanted a new schema and then I should write the queries against that new schema (showing how it would work), I could understand that. I would expect that to be more of a white board or a db design tool rather than an IDE but I could work with that. But they specifically said to just write the selects against the tables as shown.\n\nAm I missing something? Do people consider writing SQL to be \"modeling the data\"?\n\nThe other interviews at that company have all been really interesting. I just feel like they weren't happy and I have no idea what they were looking for. I tried asking in different ways but in the end just wrote some queries. They didn't say they weren't happy and maybe my feeling just comes from my confusion as to what was being asked for. Just looking for insights I guess.", "author_fullname": "t2_nx1og3xx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modeling the data means writing queries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lg4f7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678233823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a design and architecture interview. Ended up being some predefined tables and data in a browser-based IDE. I was asked to &amp;quot;model the data for a reporting database&amp;quot;. Kind of confused but started into a discussion on star schemas and why that is not the best on all databases but for a generic solution like this would fit.&lt;/p&gt;\n\n&lt;p&gt;NOPE. Modeling the data meant writing some aggregate queries against those predefined tables. This was a design and architecture interview. If it had been a basic SQL interview, it would have been a fairly average problem set.&lt;/p&gt;\n\n&lt;p&gt;I still feel confused. I did write some queries but not really sure I was in the right place or understanding the ask. I have to admit the mental switch very much threw me off my game. &lt;/p&gt;\n\n&lt;p&gt;If they wanted a new schema and then I should write the queries against that new schema (showing how it would work), I could understand that. I would expect that to be more of a white board or a db design tool rather than an IDE but I could work with that. But they specifically said to just write the selects against the tables as shown.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something? Do people consider writing SQL to be &amp;quot;modeling the data&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;The other interviews at that company have all been really interesting. I just feel like they weren&amp;#39;t happy and I have no idea what they were looking for. I tried asking in different ways but in the end just wrote some queries. They didn&amp;#39;t say they weren&amp;#39;t happy and maybe my feeling just comes from my confusion as to what was being asked for. Just looking for insights I guess.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "11lg4f7", "is_robot_indexable": true, "report_reasons": null, "author": "Admirable-Shower2174", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lg4f7/modeling_the_data_means_writing_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lg4f7/modeling_the_data_means_writing_queries/", "subreddit_subscribers": 92249, "created_utc": 1678233823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently developing a presentation for a knowledge sharing session that goes over the history of data warehouses through data lakes. While researching the topic I found it interesting how the problems that lead to the creation of the data warehouse in the 80s/90s are the same problems we\u2019re talking about today. \n\nMainly, increase volume of data driven by increased storage capacity. And increase in velocity and variety of data. \n\nEveryone\u2019s still trying to figure out the best way to organize their data. I don\u2019t think this problem or our jobs are going away anytime soon.", "author_fullname": "t2_6cy1x7a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Same problems for decades", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lkfxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678244838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently developing a presentation for a knowledge sharing session that goes over the history of data warehouses through data lakes. While researching the topic I found it interesting how the problems that lead to the creation of the data warehouse in the 80s/90s are the same problems we\u2019re talking about today. &lt;/p&gt;\n\n&lt;p&gt;Mainly, increase volume of data driven by increased storage capacity. And increase in velocity and variety of data. &lt;/p&gt;\n\n&lt;p&gt;Everyone\u2019s still trying to figure out the best way to organize their data. I don\u2019t think this problem or our jobs are going away anytime soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lkfxv", "is_robot_indexable": true, "report_reasons": null, "author": "Pack_Otherwise", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lkfxv/same_problems_for_decades/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lkfxv/same_problems_for_decades/", "subreddit_subscribers": 92249, "created_utc": 1678244838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone \ud83d\udc4b I\u2019m Ian \u2014 I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable ([https://www.turntable.so/](https://www.turntable.so/))\n\nTwo weeks ago we [demo'd some early features](https://www.reddit.com/r/dataengineering/comments/119oxil/building_a_better_local_dbt_experience/) and we were grateful for the community response.\n\nThe number one feature request from you all was a column level lineage view.\n\nToday, I'm excited to share that we now have column-level lineage to bring column understanding to dbt projects. Under the hood, we parse the dbt-compiled sql into an abstract syntax tree and then recurse through that tree to build the lineage.\n\nI\u2019d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM\u2026 thanks!\n\n[https://www.loom.com/share/c77689096ee14952a5e4cdea969eaf7c](https://www.loom.com/share/c77689096ee14952a5e4cdea969eaf7c)\n\nIn the upcoming weeks we're launching inline docs, smart autocomplete and an AI copilot!", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building Column Level Lineage for dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11llsqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678248521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone \ud83d\udc4b I\u2019m Ian \u2014 I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable (&lt;a href=\"https://www.turntable.so/\"&gt;https://www.turntable.so/&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Two weeks ago we &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/119oxil/building_a_better_local_dbt_experience/\"&gt;demo&amp;#39;d some early features&lt;/a&gt; and we were grateful for the community response.&lt;/p&gt;\n\n&lt;p&gt;The number one feature request from you all was a column level lineage view.&lt;/p&gt;\n\n&lt;p&gt;Today, I&amp;#39;m excited to share that we now have column-level lineage to bring column understanding to dbt projects. Under the hood, we parse the dbt-compiled sql into an abstract syntax tree and then recurse through that tree to build the lineage.&lt;/p&gt;\n\n&lt;p&gt;I\u2019d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM\u2026 thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.loom.com/share/c77689096ee14952a5e4cdea969eaf7c\"&gt;https://www.loom.com/share/c77689096ee14952a5e4cdea969eaf7c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In the upcoming weeks we&amp;#39;re launching inline docs, smart autocomplete and an AI copilot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "11llsqn", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11llsqn/building_column_level_lineage_for_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11llsqn/building_column_level_lineage_for_dbt/", "subreddit_subscribers": 92249, "created_utc": 1678248521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For some weird reason, there is no documentation on how to paginate over the responses of certain endpoints of the databricks rest api (ie. Job runs). It\u2019ll return a \u201cnext page token\u201d, but I\u2019ve tried the common. Pagination workflows (return the token as a param, in the header, etc) and it seems as if they are all incorrect. Is there anybody who can detail how to paginate?", "author_fullname": "t2_3ixkfqzx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks REST API Pagination", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lnvpc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678254608.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some weird reason, there is no documentation on how to paginate over the responses of certain endpoints of the databricks rest api (ie. Job runs). It\u2019ll return a \u201cnext page token\u201d, but I\u2019ve tried the common. Pagination workflows (return the token as a param, in the header, etc) and it seems as if they are all incorrect. Is there anybody who can detail how to paginate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lnvpc", "is_robot_indexable": true, "report_reasons": null, "author": "kharigardner", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lnvpc/databricks_rest_api_pagination/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lnvpc/databricks_rest_api_pagination/", "subreddit_subscribers": 92249, "created_utc": 1678254608.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nMy team wants to enable an automated way to run SQL pipelines in our AWS EKS environment.\nOur first idea is to use dbt as the SQL framework. However we can't find an easy way to have a transient spark cluster running theses dbt pipelines.\n\nHave anybody already worked with dbt and AWS EKS transient cluster?\nAny thoughts about it?\n\nWhat we have in mind to test:\n\nOption A\n- Apply a spark cluter in AWS EKS with a thrift server\n- Find its host and use it to run dbt pipeline\n- Kill the cluster\n\nOption B\n- Add dbt to the EMR on EKS image with a spark thrift server (we need to understand how it can be done)\n- Run dbt pipeline as entrypoint for this image using the local thrift server\n\nThanks for any thoughts!", "author_fullname": "t2_n0qka4td", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using dbt with transient spark cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l7dz2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678214387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;My team wants to enable an automated way to run SQL pipelines in our AWS EKS environment.\nOur first idea is to use dbt as the SQL framework. However we can&amp;#39;t find an easy way to have a transient spark cluster running theses dbt pipelines.&lt;/p&gt;\n\n&lt;p&gt;Have anybody already worked with dbt and AWS EKS transient cluster?\nAny thoughts about it?&lt;/p&gt;\n\n&lt;p&gt;What we have in mind to test:&lt;/p&gt;\n\n&lt;p&gt;Option A\n- Apply a spark cluter in AWS EKS with a thrift server\n- Find its host and use it to run dbt pipeline\n- Kill the cluster&lt;/p&gt;\n\n&lt;p&gt;Option B\n- Add dbt to the EMR on EKS image with a spark thrift server (we need to understand how it can be done)\n- Run dbt pipeline as entrypoint for this image using the local thrift server&lt;/p&gt;\n\n&lt;p&gt;Thanks for any thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11l7dz2", "is_robot_indexable": true, "report_reasons": null, "author": "Data_Broccoli_PII", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l7dz2/using_dbt_with_transient_spark_cluster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l7dz2/using_dbt_with_transient_spark_cluster/", "subreddit_subscribers": 92249, "created_utc": 1678214387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I am a revenue assurance analyst at a telecommunications company where we use a niche CRM to process and tracks our financials as well as our orders, tickets, and audits. I know the CRM inside and out, only person at my company that understands how everything works. \n\nCompany just trained me in Tableau for my reporting with some others at my company, few days after the training I created a dashboard leveraging our data from the CRM and caught a lot of attention from c-suite.\n\nWe use salesforce as our data gathering CRM and there was an integration effort in progress with the niche CRM and Salesforce. I joined onto the effort a few months ago and have been a major point of contact. Two weeks ago I discovered that the way we were using Salesforce to link commissions data was not reliable and historical data was subject to change based on web hooks. Presented my efforts to the CFO, CSO, and Salesforce architect (who loves me) which ultimately led them to scraping the entire integration of the financial data into Salesforce since we would never have accurate historical data. \n\nWe decided to use redshift as our path forward and I have been trying to learn as much as I can before the ware house is ready. I think that there is a new role here but I was hoping for other input or experiences. My current boss wants me to. Focus on audits but I know how crucial this ware house will be for us. Any advice?", "author_fullname": "t2_cdom6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Company just moved to RedShift, how to leverage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ljgqt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678242313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am a revenue assurance analyst at a telecommunications company where we use a niche CRM to process and tracks our financials as well as our orders, tickets, and audits. I know the CRM inside and out, only person at my company that understands how everything works. &lt;/p&gt;\n\n&lt;p&gt;Company just trained me in Tableau for my reporting with some others at my company, few days after the training I created a dashboard leveraging our data from the CRM and caught a lot of attention from c-suite.&lt;/p&gt;\n\n&lt;p&gt;We use salesforce as our data gathering CRM and there was an integration effort in progress with the niche CRM and Salesforce. I joined onto the effort a few months ago and have been a major point of contact. Two weeks ago I discovered that the way we were using Salesforce to link commissions data was not reliable and historical data was subject to change based on web hooks. Presented my efforts to the CFO, CSO, and Salesforce architect (who loves me) which ultimately led them to scraping the entire integration of the financial data into Salesforce since we would never have accurate historical data. &lt;/p&gt;\n\n&lt;p&gt;We decided to use redshift as our path forward and I have been trying to learn as much as I can before the ware house is ready. I think that there is a new role here but I was hoping for other input or experiences. My current boss wants me to. Focus on audits but I know how crucial this ware house will be for us. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11ljgqt", "is_robot_indexable": true, "report_reasons": null, "author": "FXAFrank", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ljgqt/company_just_moved_to_redshift_how_to_leverage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ljgqt/company_just_moved_to_redshift_how_to_leverage/", "subreddit_subscribers": 92249, "created_utc": 1678242313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team at RudderStack (https://github.com/rudderlabs/rudder-server) \nis organizing [an online challenge for data engineers](https://www.rudderstack.com/blog/join-the-transformations-challenge-for-a-chance-to-win/). There will be prizes for the winners. To make the evaluation fair, I am searching for judges who are not associated with RudderStack and are experts/leaders in data emgineering. As a judge, you will play a critical role in evaluating the submissions. Appreciate your feedback.", "author_fullname": "t2_cbh6ollo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Searching a judge to make a DE challenge evaluation fair", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l4f0m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678207737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team at RudderStack (&lt;a href=\"https://github.com/rudderlabs/rudder-server\"&gt;https://github.com/rudderlabs/rudder-server&lt;/a&gt;) \nis organizing &lt;a href=\"https://www.rudderstack.com/blog/join-the-transformations-challenge-for-a-chance-to-win/\"&gt;an online challenge for data engineers&lt;/a&gt;. There will be prizes for the winners. To make the evaluation fair, I am searching for judges who are not associated with RudderStack and are experts/leaders in data emgineering. As a judge, you will play a critical role in evaluating the submissions. Appreciate your feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?auto=webp&amp;v=enabled&amp;s=594d25c58b78bf4996f7f6ff00cc9a88712b91ca", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ea915fbdb742c7cba2c80924e732a6d3c5a5267", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c289244ad4033c689691d1512f857857c57ed5c2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=611d7dfef702d66f239cd39d44b7811efe88d49d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=085360398328ec0f370bf6a430cf29e0f6371009", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c449cbfad694616b0371e5c1e6e5f1f53f30c859", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45f421afdb5f19e221bed96ad4f0aaa9cd17bf0f", "width": 1080, "height": 540}], "variants": {}, "id": "r1MPAi3-_zC2mqW1Gq01mVQS0WYhEmD0wiuszDr-2yg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11l4f0m", "is_robot_indexable": true, "report_reasons": null, "author": "ephemeral404", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l4f0m/searching_a_judge_to_make_a_de_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l4f0m/searching_a_judge_to_make_a_de_challenge/", "subreddit_subscribers": 92249, "created_utc": 1678207737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I am running into an issue with Redshift and would appreciate any suggestions.\n\nI am creating some views on redshift using dbt. One of the in particular controls the row level access per user, so all models are ultimately joined with it. But it still is just a view definition, there are no tables in the mix.\n\nEvery single time dbt tries to update this view, if there are any queries at that time that involve that view (which is often, since this is a client facing database), it deadlocks.\nBy that I mean - dbt is not able to re-create the view, no one is able to query it, and this situations persists until dbt's process is manually killed.\n\nIt seems to me that even if the view is under use, as long as there are a few seconds of downtime (which there are), then dbt should do its job and no deadlock should occur.\n\nAny ideas? It's driving me crazy!\n\nThanks", "author_fullname": "t2_8xg3h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "question - dbt and redshift deadlock?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lqmmr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678263507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I am running into an issue with Redshift and would appreciate any suggestions.&lt;/p&gt;\n\n&lt;p&gt;I am creating some views on redshift using dbt. One of the in particular controls the row level access per user, so all models are ultimately joined with it. But it still is just a view definition, there are no tables in the mix.&lt;/p&gt;\n\n&lt;p&gt;Every single time dbt tries to update this view, if there are any queries at that time that involve that view (which is often, since this is a client facing database), it deadlocks.\nBy that I mean - dbt is not able to re-create the view, no one is able to query it, and this situations persists until dbt&amp;#39;s process is manually killed.&lt;/p&gt;\n\n&lt;p&gt;It seems to me that even if the view is under use, as long as there are a few seconds of downtime (which there are), then dbt should do its job and no deadlock should occur.&lt;/p&gt;\n\n&lt;p&gt;Any ideas? It&amp;#39;s driving me crazy!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lqmmr", "is_robot_indexable": true, "report_reasons": null, "author": "Fredbull", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lqmmr/question_dbt_and_redshift_deadlock/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lqmmr/question_dbt_and_redshift_deadlock/", "subreddit_subscribers": 92249, "created_utc": 1678263507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m searching for a tool I can use to request data from an API (Spotify\u2019s Web API), transform it, and load it to a MongoDB Atlas Database. My jobs will need to run both on scheduled basis and on a triggered basis, on the cloud.\n\nI tried to implement Atlas Trigger Functions as a solution, but that was a dead end due to their execution-time and socket limitations. \n\nAdditionally, I\u2019ve looked into a handful of IPaaS platforms (SnapLogic, Zapier, Workato) but their pricing models seem pretty steep for my use case.\n\nSome of my jobs will need to make 1,000\u2019s calls to the API, and will likely have an execution time over an hour.\n\nAny ideas for a potential solution? Thoughts on ETL tools like AWS Glue?", "author_fullname": "t2_5l0hu6ke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need a Data Integration Tool (Spotify Web API =&gt; MongoDB Atlas DB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l7pqc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678215125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m searching for a tool I can use to request data from an API (Spotify\u2019s Web API), transform it, and load it to a MongoDB Atlas Database. My jobs will need to run both on scheduled basis and on a triggered basis, on the cloud.&lt;/p&gt;\n\n&lt;p&gt;I tried to implement Atlas Trigger Functions as a solution, but that was a dead end due to their execution-time and socket limitations. &lt;/p&gt;\n\n&lt;p&gt;Additionally, I\u2019ve looked into a handful of IPaaS platforms (SnapLogic, Zapier, Workato) but their pricing models seem pretty steep for my use case.&lt;/p&gt;\n\n&lt;p&gt;Some of my jobs will need to make 1,000\u2019s calls to the API, and will likely have an execution time over an hour.&lt;/p&gt;\n\n&lt;p&gt;Any ideas for a potential solution? Thoughts on ETL tools like AWS Glue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11l7pqc", "is_robot_indexable": true, "report_reasons": null, "author": "Level-Gur8656", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l7pqc/need_a_data_integration_tool_spotify_web_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l7pqc/need_a_data_integration_tool_spotify_web_api/", "subreddit_subscribers": 92249, "created_utc": 1678215125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am building a data pipeline and dashboard app that I want to deploy online (fastapi + postgresql + svelte/vue). It will read all data from postgresql, do computations (numpy / pandas), store the output back into postgresql, and then the frontend app will communicate with the api to render the dashboard.\n\nBut what is the best way to deploy postgres in production? My platform will start small, but I'd like to have the infrastructure to scale quickly when / if the platform takes off.\n\nI am a data engineer and programmer and I used to do everything myself, but with kubernets, docker things got more complicated:\n\n* use kubernetes to have postgresql, api (fastapi), and data pipeline in separate dockers and communicate with each other over network?\n* vm for postgresql?\n* \"bare metal\" (dedicated server) for postgresql?\n\nI've read pros and cons for each, I really don't know what the best way is to do this.\n\nDoes anybody has experience with this?", "author_fullname": "t2_15vo19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "from files to db in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lrzi6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678268273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building a data pipeline and dashboard app that I want to deploy online (fastapi + postgresql + svelte/vue). It will read all data from postgresql, do computations (numpy / pandas), store the output back into postgresql, and then the frontend app will communicate with the api to render the dashboard.&lt;/p&gt;\n\n&lt;p&gt;But what is the best way to deploy postgres in production? My platform will start small, but I&amp;#39;d like to have the infrastructure to scale quickly when / if the platform takes off.&lt;/p&gt;\n\n&lt;p&gt;I am a data engineer and programmer and I used to do everything myself, but with kubernets, docker things got more complicated:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;use kubernetes to have postgresql, api (fastapi), and data pipeline in separate dockers and communicate with each other over network?&lt;/li&gt;\n&lt;li&gt;vm for postgresql?&lt;/li&gt;\n&lt;li&gt;&amp;quot;bare metal&amp;quot; (dedicated server) for postgresql?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve read pros and cons for each, I really don&amp;#39;t know what the best way is to do this.&lt;/p&gt;\n\n&lt;p&gt;Does anybody has experience with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lrzi6", "is_robot_indexable": true, "report_reasons": null, "author": "iLLucionist", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lrzi6/from_files_to_db_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lrzi6/from_files_to_db_in_production/", "subreddit_subscribers": 92249, "created_utc": 1678268273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not a data engineer, but I'm working at a logistics company that runs off of excel files and I want to move them all into a database.\n\nBasically, we get ocean freight rates from multiple sources, and they send them via Excel files. I want to be able to upload these files onto a searchable database that has uniform data.\n\nThe problem is that all these excel files names their categories differently and structure their data differently. \n\nSo, I'm wondering how difficult a task is it to scrape the data and make it into a uniform database?\n\nIs this even possible?", "author_fullname": "t2_oeaa0pet", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How hard is it to combine different excel sheets into one database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lfdkf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678232060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a data engineer, but I&amp;#39;m working at a logistics company that runs off of excel files and I want to move them all into a database.&lt;/p&gt;\n\n&lt;p&gt;Basically, we get ocean freight rates from multiple sources, and they send them via Excel files. I want to be able to upload these files onto a searchable database that has uniform data.&lt;/p&gt;\n\n&lt;p&gt;The problem is that all these excel files names their categories differently and structure their data differently. &lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m wondering how difficult a task is it to scrape the data and make it into a uniform database?&lt;/p&gt;\n\n&lt;p&gt;Is this even possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lfdkf", "is_robot_indexable": true, "report_reasons": null, "author": "KaulHilo", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lfdkf/how_hard_is_it_to_combine_different_excel_sheets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lfdkf/how_hard_is_it_to_combine_different_excel_sheets/", "subreddit_subscribers": 92249, "created_utc": 1678232060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smart Brokers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_11lcv76", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VnUhYAfBogJDgLmEY9uQ3fQyZkg1o7omS554n0uaV2Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678226512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/smart-brokers?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7n4e-fxDjrSv2uD4tzsWS15dvSoLmUDsCj_Hu7T4gwk.jpg?auto=webp&amp;v=enabled&amp;s=503176a402f8882ca260cbee565409e092a9de8e", "width": 764, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7n4e-fxDjrSv2uD4tzsWS15dvSoLmUDsCj_Hu7T4gwk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59fb9530e074bd3449b9bf7fa36060686644944a", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/7n4e-fxDjrSv2uD4tzsWS15dvSoLmUDsCj_Hu7T4gwk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70269d47e565acf7e2317ae6bde21c88dd2e330a", "width": 216, "height": 169}, {"url": "https://external-preview.redd.it/7n4e-fxDjrSv2uD4tzsWS15dvSoLmUDsCj_Hu7T4gwk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0475beac9f3c5c1fb846e1dcbc1fe5eb43cc939", "width": 320, "height": 251}, {"url": "https://external-preview.redd.it/7n4e-fxDjrSv2uD4tzsWS15dvSoLmUDsCj_Hu7T4gwk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c264576b79a90c074095c2b842bccbcf3fa9f46", "width": 640, "height": 502}], "variants": {}, "id": "d-tMNY9JPcSTWMi-uCmzyrJP6Ah2-geqtSl4HBqMgTk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lcv76", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lcv76/smart_brokers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/smart-brokers?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 92249, "created_utc": 1678226512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI recently received outdated databases from my uncle from 2010, but they need to be more organized and in better condition. There are 200 .xls and .xlxs files saved in various subfolders, some of which are corrupted and contain only readable values. The headers need to be standardized and sometimes appear on the second row.\n\nAs someone with limited knowledge of Python, I have struggled with the challenges of the task. Despite seeking assistance from ChatGPT, I still need help. My goal is to extract the names and email addresses from these files.\n\nI am searching for a program that can quickly identify the headers of each file, allowing me to select the relevant columns based on their titles.", "author_fullname": "t2_4esf6urf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Program to handle simple stuff in multiple excel sheets.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l87u7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678216234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently received outdated databases from my uncle from 2010, but they need to be more organized and in better condition. There are 200 .xls and .xlxs files saved in various subfolders, some of which are corrupted and contain only readable values. The headers need to be standardized and sometimes appear on the second row.&lt;/p&gt;\n\n&lt;p&gt;As someone with limited knowledge of Python, I have struggled with the challenges of the task. Despite seeking assistance from ChatGPT, I still need help. My goal is to extract the names and email addresses from these files.&lt;/p&gt;\n\n&lt;p&gt;I am searching for a program that can quickly identify the headers of each file, allowing me to select the relevant columns based on their titles.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11l87u7", "is_robot_indexable": true, "report_reasons": null, "author": "NonSenseAdventurer", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l87u7/program_to_handle_simple_stuff_in_multiple_excel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l87u7/program_to_handle_simple_stuff_in_multiple_excel/", "subreddit_subscribers": 92249, "created_utc": 1678216234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey - I know this topic has been discussed a million times, but I am still not convinced.   \n\n\nWhat would we want to load  structured data sets in a datalake first (s3, blob, gcp, ...) before our target dwh. I feel it's doubling the workload on data engineers. Instead, it would be just easier to take your data from the source and load it directly to the endpoint. I deal with structured data only and I don't get the point of data lakes. If someone can illuminate me on the question, I would appreciate it :) \n\nThank you !!!", "author_fullname": "t2_g0kwnodf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question of the Day", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l3bs7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678205253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey - I know this topic has been discussed a million times, but I am still not convinced.   &lt;/p&gt;\n\n&lt;p&gt;What would we want to load  structured data sets in a datalake first (s3, blob, gcp, ...) before our target dwh. I feel it&amp;#39;s doubling the workload on data engineers. Instead, it would be just easier to take your data from the source and load it directly to the endpoint. I deal with structured data only and I don&amp;#39;t get the point of data lakes. If someone can illuminate me on the question, I would appreciate it :) &lt;/p&gt;\n\n&lt;p&gt;Thank you !!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11l3bs7", "is_robot_indexable": true, "report_reasons": null, "author": "That-Refrigerator901", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l3bs7/question_of_the_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l3bs7/question_of_the_day/", "subreddit_subscribers": 92249, "created_utc": 1678205253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been doing Data Engineering and Architecture work for past 15 odd years. I have built various data warehouses / data Marts /data lakes across multiple big companies. I have managed people before but in my current role I am mainly responsible for architecture as per my job title. But I have been doing hiring , product management and workload management etc. To me my natural progression can be into more of leadership (Director ) role. But company (my boss ) wants me to stick to architecture path. I have seen in past where people managers have taken undue advantage of technical talent in my company and have say over hiring /firing etc. I am 39 and feel like if I focus on strategic side of data world, I might have longer run and more satisfying career in the end. Please give me your thoughts.", "author_fullname": "t2_jbyap7dk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architect Vs Leadership", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l2bjs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678202972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been doing Data Engineering and Architecture work for past 15 odd years. I have built various data warehouses / data Marts /data lakes across multiple big companies. I have managed people before but in my current role I am mainly responsible for architecture as per my job title. But I have been doing hiring , product management and workload management etc. To me my natural progression can be into more of leadership (Director ) role. But company (my boss ) wants me to stick to architecture path. I have seen in past where people managers have taken undue advantage of technical talent in my company and have say over hiring /firing etc. I am 39 and feel like if I focus on strategic side of data world, I might have longer run and more satisfying career in the end. Please give me your thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11l2bjs", "is_robot_indexable": true, "report_reasons": null, "author": "Dry_Damage_6629", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l2bjs/architect_vs_leadership/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l2bjs/architect_vs_leadership/", "subreddit_subscribers": 92249, "created_utc": 1678202972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our company is moving to the Medallion architecture using Databricks and I am a bit worried/confused about how we are going to handle so many joins in Spark. So many of our tables use anywhere between 5-20 joins to create them. This is mostly due to one of our main data sources being itself a DataWarehouse that we extract and rebuild every day and use that data to help build our datawarehouse. Are we just going to need to constantly performance tuning the joins to death or is there a better way to do it? Im in my first year of DE but its my understanding that Spark is not great at handling joins.", "author_fullname": "t2_4bsgo8fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Conceptual Handling of Joins to Gold Layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kzmx9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678196446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our company is moving to the Medallion architecture using Databricks and I am a bit worried/confused about how we are going to handle so many joins in Spark. So many of our tables use anywhere between 5-20 joins to create them. This is mostly due to one of our main data sources being itself a DataWarehouse that we extract and rebuild every day and use that data to help build our datawarehouse. Are we just going to need to constantly performance tuning the joins to death or is there a better way to do it? Im in my first year of DE but its my understanding that Spark is not great at handling joins.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11kzmx9", "is_robot_indexable": true, "report_reasons": null, "author": "Hexboy3", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kzmx9/conceptual_handling_of_joins_to_gold_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kzmx9/conceptual_handling_of_joins_to_gold_layer/", "subreddit_subscribers": 92249, "created_utc": 1678196446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to do so? I'm assuming all data processing is pushed onto the data warehouse instead of performed on a Spark cluster. I ask because a colleague mentioned that they used Kafka but do not use any cloud services, and I have never heard of anyone hosting and managing their own Spark cluster onsite. Would there be any reason to use Kafka, if everything is eventually processed and dumped into some other data store. Maybe someone here has implemented a solution on the job where Kafka was needed but not Spark, although most data archs I see online have them coupled together.", "author_fullname": "t2_e0uj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to develop a streaming big data architecture with Kafka, but without Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lqpnb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678263789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to do so? I&amp;#39;m assuming all data processing is pushed onto the data warehouse instead of performed on a Spark cluster. I ask because a colleague mentioned that they used Kafka but do not use any cloud services, and I have never heard of anyone hosting and managing their own Spark cluster onsite. Would there be any reason to use Kafka, if everything is eventually processed and dumped into some other data store. Maybe someone here has implemented a solution on the job where Kafka was needed but not Spark, although most data archs I see online have them coupled together.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11lqpnb", "is_robot_indexable": true, "report_reasons": null, "author": "sinuspane", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lqpnb/is_it_possible_to_develop_a_streaming_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lqpnb/is_it_possible_to_develop_a_streaming_big_data/", "subreddit_subscribers": 92249, "created_utc": 1678263789.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}