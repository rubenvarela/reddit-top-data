{"kind": "Listing", "data": {"after": "t3_11swovi", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vxxrqrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tracking the Fake Stars Market with Dagster, BigQuery and dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4j09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 75, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 75, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KuVo6LkTRmwrRlRoeKwFt7dkpiuezOmsmBagrxsDfbk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678996759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/fake-stars", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?auto=webp&amp;v=enabled&amp;s=6feffd3ff633a036f59a08f8594eb3f065bbe3b1", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e360f3ccc3f8c501c3fd34b0f17d0904a271132", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f629d6c060d739672ab5769feef32945ed0b547", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ce2426c05b366dc0e7390f6941fdcf369417314", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3282917840b4991ab1509ce8ba7daf2e8acba140", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70e65fb12076a711151ddba54cd9469adc644038", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b91cbdc9fd1f26d26ae4c64763b11a5f8453325a", "width": 1080, "height": 567}], "variants": {}, "id": "eKvtnMcXWrAKAAoUL_SszTvxbrdxPwsxAJNSYYV7yNQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11t4j09", "is_robot_indexable": true, "report_reasons": null, "author": "MrMosBiggestFan", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4j09/tracking_the_fake_stars_market_with_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/fake-stars", "subreddit_subscribers": 93363, "created_utc": 1678996759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title", "author_fullname": "t2_c6w52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "do you think Zhamak has any idea how much time has now been wasted on orgs discussing if we should 'do a data mesh'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t1e0u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678989600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t1e0u", "is_robot_indexable": true, "report_reasons": null, "author": "EmergenL", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t1e0u/do_you_think_zhamak_has_any_idea_how_much_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t1e0u/do_you_think_zhamak_has_any_idea_how_much_time/", "subreddit_subscribers": 93363, "created_utc": 1678989600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all. I recently started a position at a manufacturer where the entire IT and data process is done through SAP. They are trying to slowly get themselves off of SAP and hired me due to my experience in more modern ETL/Data platforms.\nI was just wondering if anyone here had experience in modernizing an SAP platform and the best tools for the job. Thanks", "author_fullname": "t2_6ra3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with SAP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11swnax", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all. I recently started a position at a manufacturer where the entire IT and data process is done through SAP. They are trying to slowly get themselves off of SAP and hired me due to my experience in more modern ETL/Data platforms.\nI was just wondering if anyone here had experience in modernizing an SAP platform and the best tools for the job. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11swnax", "is_robot_indexable": true, "report_reasons": null, "author": "deemerritt", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11swnax/working_with_sap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11swnax/working_with_sap/", "subreddit_subscribers": 93363, "created_utc": 1678979041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In case you\u2019re not aware, AWS snowmobile is a service where AWS will park up a 45ft shipping container full of SSDs in order to transfer your data\u2014 up to 100 PB of it \u2014 to S3.\n\nI\u2019m curious if anyone here\u2019s actually used this service?", "author_fullname": "t2_4wbqkds2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ever used AWS Snowmobile?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t3ic9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678994370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In case you\u2019re not aware, AWS snowmobile is a service where AWS will park up a 45ft shipping container full of SSDs in order to transfer your data\u2014 up to 100 PB of it \u2014 to S3.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m curious if anyone here\u2019s actually used this service?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t3ic9", "is_robot_indexable": true, "report_reasons": null, "author": "bolivlake", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t3ic9/ever_used_aws_snowmobile/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t3ic9/ever_used_aws_snowmobile/", "subreddit_subscribers": 93363, "created_utc": 1678994370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am aware of various data catalogs such as DataHub, Amundsen and so on that offer data lineage capability but is there any tool offering *solely* the lineage without all the stuff around? (I am thinking of building \"custom data catalog\" but I would need some \"help\" with the lineage)", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Standalone lineage tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4onj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678997107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am aware of various data catalogs such as DataHub, Amundsen and so on that offer data lineage capability but is there any tool offering &lt;em&gt;solely&lt;/em&gt; the lineage without all the stuff around? (I am thinking of building &amp;quot;custom data catalog&amp;quot; but I would need some &amp;quot;help&amp;quot; with the lineage)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t4onj", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4onj/standalone_lineage_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t4onj/standalone_lineage_tool/", "subreddit_subscribers": 93363, "created_utc": 1678997107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently export the data into excel and load as hive tables manually. I\u2019m wondering if anyone has automated this process.", "author_fullname": "t2_5jxm4ghg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone sourced sharepoint data dynamically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tlf2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679042825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently export the data into excel and load as hive tables manually. I\u2019m wondering if anyone has automated this process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tlf2v", "is_robot_indexable": true, "report_reasons": null, "author": "tentative_guy22", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tlf2v/has_anyone_sourced_sharepoint_data_dynamically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tlf2v/has_anyone_sourced_sharepoint_data_dynamically/", "subreddit_subscribers": 93363, "created_utc": 1679042825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At a former company, I had the chance to start building dashboards and thoroughly enjoyed it. I had a great boss who gave me a lot of opportunity to grow. Long story short, the company went bad and my boss referred me for a head of IT position at another firm, even though my experience overall in IT is quite short. \n\nI land the job, and have learned a TON. How to make value propositions and lead a department &amp; projects to name a few. I already take a leadership role in a non-profit Im involved in, so it hasnt been too steep of a learning curve personally. Ive done fairly well. \n\nHowever, I still have this itch to grow from a junior DE to a principal/Data Architect, functioning as someone who mentors junior DE\u2019s and has some strong technical chops. I definetly feel comfortable dealing with people-conflict, but am aloso versatile enough for technical conflict as well. \n\nWhats the best way to make this transition? I have an opportunity at my present company to help the company be data driven (no analytics available besides canned reports) and am reading the data engineering toolkit. I am also honing my SQL. Thanks!", "author_fullname": "t2_6x7d7af4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Head of IT at a 100-person sized company that always wanted to grow into an Individual Contributor as a DE. How can I make that transition?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11thrhl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679029887.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679029707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At a former company, I had the chance to start building dashboards and thoroughly enjoyed it. I had a great boss who gave me a lot of opportunity to grow. Long story short, the company went bad and my boss referred me for a head of IT position at another firm, even though my experience overall in IT is quite short. &lt;/p&gt;\n\n&lt;p&gt;I land the job, and have learned a TON. How to make value propositions and lead a department &amp;amp; projects to name a few. I already take a leadership role in a non-profit Im involved in, so it hasnt been too steep of a learning curve personally. Ive done fairly well. &lt;/p&gt;\n\n&lt;p&gt;However, I still have this itch to grow from a junior DE to a principal/Data Architect, functioning as someone who mentors junior DE\u2019s and has some strong technical chops. I definetly feel comfortable dealing with people-conflict, but am aloso versatile enough for technical conflict as well. &lt;/p&gt;\n\n&lt;p&gt;Whats the best way to make this transition? I have an opportunity at my present company to help the company be data driven (no analytics available besides canned reports) and am reading the data engineering toolkit. I am also honing my SQL. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11thrhl", "is_robot_indexable": true, "report_reasons": null, "author": "BigBoatThrowaway", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11thrhl/head_of_it_at_a_100person_sized_company_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11thrhl/head_of_it_at_a_100person_sized_company_that/", "subreddit_subscribers": 93363, "created_utc": 1679029707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know of a good solution for a self-hosted columnar DB? I have a small Linux machine that is hosting a Postgres server. Setting it up and loading in some test data was seamless, but it hasn't been able to scale with the types of analytical queries I want to run. \n\nI've looked into the citus extension, but unfortunately it doesn't support ARM architecture (which is what the linux server has).", "author_fullname": "t2_oyypyadu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Columnar Databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tgpmi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679026372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of a good solution for a self-hosted columnar DB? I have a small Linux machine that is hosting a Postgres server. Setting it up and loading in some test data was seamless, but it hasn&amp;#39;t been able to scale with the types of analytical queries I want to run. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked into the citus extension, but unfortunately it doesn&amp;#39;t support ARM architecture (which is what the linux server has).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tgpmi", "is_robot_indexable": true, "report_reasons": null, "author": "ringoefc", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tgpmi/open_source_columnar_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tgpmi/open_source_columnar_databases/", "subreddit_subscribers": 93363, "created_utc": 1679026372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My day to day job mostly uses GBQ and dbt. So, I was doing a little bit of research on orchestration tools. I was looking at AirFlow initially. It looked complicated (at least the installation part)\n\nSo, I installed Prefect to my local machine and connected it to Prefect cloud. And I was experimenting.\n\nSo, this is what I did using Prefect.\n- Extract data from our production DB. I used pandas to load the tables. The table has about 2000 rows with about 100 columns.\n- There were some sensitive information. I hashed it as a transformation stage. And converted all the fields into string (if not it throws an error when I load it into GBQ)\n- Loaded it to GBQ\n\nI have a few questions here. Is Prefect overkill for my use case? What could have done better? \n\nLet\u2019s say if I have to update the tables in GBQ every 7 days. Is it possible to schedule it on Prefect? For scheduling part, do we need to have a VM running 24/7? \n\nWould appreciate your feedbacks. Not gonna lie, when the job was successful. It felt like an achievement.\n\nThank you.", "author_fullname": "t2_emzh9atv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created my first Prefect workflow.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sve4i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678976150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My day to day job mostly uses GBQ and dbt. So, I was doing a little bit of research on orchestration tools. I was looking at AirFlow initially. It looked complicated (at least the installation part)&lt;/p&gt;\n\n&lt;p&gt;So, I installed Prefect to my local machine and connected it to Prefect cloud. And I was experimenting.&lt;/p&gt;\n\n&lt;p&gt;So, this is what I did using Prefect.\n- Extract data from our production DB. I used pandas to load the tables. The table has about 2000 rows with about 100 columns.\n- There were some sensitive information. I hashed it as a transformation stage. And converted all the fields into string (if not it throws an error when I load it into GBQ)\n- Loaded it to GBQ&lt;/p&gt;\n\n&lt;p&gt;I have a few questions here. Is Prefect overkill for my use case? What could have done better? &lt;/p&gt;\n\n&lt;p&gt;Let\u2019s say if I have to update the tables in GBQ every 7 days. Is it possible to schedule it on Prefect? For scheduling part, do we need to have a VM running 24/7? &lt;/p&gt;\n\n&lt;p&gt;Would appreciate your feedbacks. Not gonna lie, when the job was successful. It felt like an achievement.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sve4i", "is_robot_indexable": true, "report_reasons": null, "author": "MaintenanceSad6825", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sve4i/i_created_my_first_prefect_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sve4i/i_created_my_first_prefect_workflow/", "subreddit_subscribers": 93363, "created_utc": 1678976150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a tool like Airbyte but for unstructured files? \n\nSomething that transfers between different file systems or object storage systems like S3 but not imposing a structured schema is what I'm looking for. The replication, extract/load and auth functiona are really what I want to achieve - no transform functionality is necessary.\n\nLike S3 to sftp, https to filesystem, etc.", "author_fullname": "t2_4i8pl31g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tool like Airbyte but for unstructured data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sucai", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678974579.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678973665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a tool like Airbyte but for unstructured files? &lt;/p&gt;\n\n&lt;p&gt;Something that transfers between different file systems or object storage systems like S3 but not imposing a structured schema is what I&amp;#39;m looking for. The replication, extract/load and auth functiona are really what I want to achieve - no transform functionality is necessary.&lt;/p&gt;\n\n&lt;p&gt;Like S3 to sftp, https to filesystem, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sucai", "is_robot_indexable": true, "report_reasons": null, "author": "knowledgebass", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sucai/tool_like_airbyte_but_for_unstructured_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sucai/tool_like_airbyte_but_for_unstructured_data/", "subreddit_subscribers": 93363, "created_utc": 1678973665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there\n\nI'm currently working on problem - I have a zoo of orchestration tools, like airflow, dbt etc\n\nand this tool need to synchronise in some cases - triggers, chain of control.\n\n&amp;#x200B;\n\nI think about Kafka with topics, dedicated to tools, or some kind of pg instance with events\n\nDoes anyone solved problem like this?", "author_fullname": "t2_89j762yi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration of orchestrators", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tmcjp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679046020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on problem - I have a zoo of orchestration tools, like airflow, dbt etc&lt;/p&gt;\n\n&lt;p&gt;and this tool need to synchronise in some cases - triggers, chain of control.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I think about Kafka with topics, dedicated to tools, or some kind of pg instance with events&lt;/p&gt;\n\n&lt;p&gt;Does anyone solved problem like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tmcjp", "is_robot_indexable": true, "report_reasons": null, "author": "tehdima", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tmcjp/orchestration_of_orchestrators/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tmcjp/orchestration_of_orchestrators/", "subreddit_subscribers": 93363, "created_utc": 1679046020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all--\n\nI'm learning DAGs and want to create some data ingress pipelines to test my Airflow knowledge. \n\nDoes anyone have experience with social media and some good data points to consume from Twitter and Facebook, say, for this type of project? It could be specific to a user or users, or general like popular trends or something.\n\nThanks for the ideas!", "author_fullname": "t2_3r56zv6e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Social Media Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t2wna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678992985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all--&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m learning DAGs and want to create some data ingress pipelines to test my Airflow knowledge. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with social media and some good data points to consume from Twitter and Facebook, say, for this type of project? It could be specific to a user or users, or general like popular trends or something.&lt;/p&gt;\n\n&lt;p&gt;Thanks for the ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t2wna", "is_robot_indexable": true, "report_reasons": null, "author": "qa_anaaq", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t2wna/creating_social_media_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t2wna/creating_social_media_data_pipelines/", "subreddit_subscribers": 93363, "created_utc": 1678992985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all I'm just in the middle of architecting a system where an important requirement is going to be storing information on \"where\" it went. Key info we'll need to store is\n\n\\- Where it went\n\n\\- Schema + Evolution\n\n\\- Arbitrary information about the size and shape of the dataset.\n\n\\- Accessible programatically/can have an API put in front of it.\n\n\\- Works with Datasets stored In AWS that might be in the Multi-TB/PB range.\n\nI'm obviously looking at Hive but at the moment we aren't a JVM company and that requires us to start worrying about Java world  as well (although granted some of our workloads are in Databricks and I've worked in Scala before so it's probably manageable). Are there any less cumbersome options for storing information about datasets around nowadays?\n\nThus far most of the options are things like [https://datahubproject.io/](https://datahubproject.io/) where I'm a little dubious about what it does and it's a little vague on if it's just storing metadata or doing transformations.\n\nI am working with a proprietary data processing system so ideally if it can be extended to cover that as one of our metadata sources too this would also be great if not a show-stopper. ", "author_fullname": "t2_9u69ulzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Metastores/Data Catalogs that aren't Hive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sws79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678979543.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all I&amp;#39;m just in the middle of architecting a system where an important requirement is going to be storing information on &amp;quot;where&amp;quot; it went. Key info we&amp;#39;ll need to store is&lt;/p&gt;\n\n&lt;p&gt;- Where it went&lt;/p&gt;\n\n&lt;p&gt;- Schema + Evolution&lt;/p&gt;\n\n&lt;p&gt;- Arbitrary information about the size and shape of the dataset.&lt;/p&gt;\n\n&lt;p&gt;- Accessible programatically/can have an API put in front of it.&lt;/p&gt;\n\n&lt;p&gt;- Works with Datasets stored In AWS that might be in the Multi-TB/PB range.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m obviously looking at Hive but at the moment we aren&amp;#39;t a JVM company and that requires us to start worrying about Java world  as well (although granted some of our workloads are in Databricks and I&amp;#39;ve worked in Scala before so it&amp;#39;s probably manageable). Are there any less cumbersome options for storing information about datasets around nowadays?&lt;/p&gt;\n\n&lt;p&gt;Thus far most of the options are things like &lt;a href=\"https://datahubproject.io/\"&gt;https://datahubproject.io/&lt;/a&gt; where I&amp;#39;m a little dubious about what it does and it&amp;#39;s a little vague on if it&amp;#39;s just storing metadata or doing transformations.&lt;/p&gt;\n\n&lt;p&gt;I am working with a proprietary data processing system so ideally if it can be extended to cover that as one of our metadata sources too this would also be great if not a show-stopper. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sws79", "is_robot_indexable": true, "report_reasons": null, "author": "tdatas", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sws79/metastoresdata_catalogs_that_arent_hive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sws79/metastoresdata_catalogs_that_arent_hive/", "subreddit_subscribers": 93363, "created_utc": 1678979359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my job I've inherited a Synapse Workspace with a dedicated SQL Pool serverless.\nOn this serverless we do have a SQL Server 2012 and I was wondering if there was a way to run some health checks (like you run scandisk on your memory).\n\nDoes someone know how to achieve it? Or which guidelines should be used?", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you check if your DWH is not corrupted?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11su26h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678973007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my job I&amp;#39;ve inherited a Synapse Workspace with a dedicated SQL Pool serverless.\nOn this serverless we do have a SQL Server 2012 and I was wondering if there was a way to run some health checks (like you run scandisk on your memory).&lt;/p&gt;\n\n&lt;p&gt;Does someone know how to achieve it? Or which guidelines should be used?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11su26h", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11su26h/how_do_you_check_if_your_dwh_is_not_corrupted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11su26h/how_do_you_check_if_your_dwh_is_not_corrupted/", "subreddit_subscribers": 93363, "created_utc": 1678973007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have captured some change data from a MySql database, serialized it with Avro converter and stored it in a Kafka topic. Now I am trying to read back that Avro serialized data from Kafka. I can easily get the serialized data but now I need to deserialize it. How do I do that? \n\nNote: I am using Pyspark to write my spark code. And I am creating a readingStream to stream the data in real-time from the Kafka topic. \n\nI found some tools for Spark(scala libraries) but the same solution for Pyspark is a bit complicated.\n\nI think using Abris [https://github.com/AbsaOSS/ABRiS](https://github.com/AbsaOSS/ABRiS) is a good choice as it supports for the python version of spark but I cannot seem to properly understand this scala written library's integration in Pyspark(Python). It's a bit complicated as I have not used non-python written libraries in Python. \n\nDoes someone have another better alternative for this use case? Another tool maybe. Or can anybody please make me understand this Abris integration for Pyspark. Thank you! [https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md](https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md)", "author_fullname": "t2_asyk1tug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tool do I use to serialize/deserialize Avro messages stored in a Kafka topic with schema registered in the schema-registry using Pyspark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11tp0si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679054328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have captured some change data from a MySql database, serialized it with Avro converter and stored it in a Kafka topic. Now I am trying to read back that Avro serialized data from Kafka. I can easily get the serialized data but now I need to deserialize it. How do I do that? &lt;/p&gt;\n\n&lt;p&gt;Note: I am using Pyspark to write my spark code. And I am creating a readingStream to stream the data in real-time from the Kafka topic. &lt;/p&gt;\n\n&lt;p&gt;I found some tools for Spark(scala libraries) but the same solution for Pyspark is a bit complicated.&lt;/p&gt;\n\n&lt;p&gt;I think using Abris &lt;a href=\"https://github.com/AbsaOSS/ABRiS\"&gt;https://github.com/AbsaOSS/ABRiS&lt;/a&gt; is a good choice as it supports for the python version of spark but I cannot seem to properly understand this scala written library&amp;#39;s integration in Pyspark(Python). It&amp;#39;s a bit complicated as I have not used non-python written libraries in Python. &lt;/p&gt;\n\n&lt;p&gt;Does someone have another better alternative for this use case? Another tool maybe. Or can anybody please make me understand this Abris integration for Pyspark. Thank you! &lt;a href=\"https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md\"&gt;https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?auto=webp&amp;v=enabled&amp;s=387171c784b9c3f45187c8d9f24b3674cbc1bb52", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1207a3960e5e55a82a8218b1b8b8abad78f49d69", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b6ac0dfd8920ccd177bef60a16308a959182d86", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d1dfd547b9ef352820ceb8894e689487d85430a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06f4b989a61c0df8e9a7938d2f45578cda6d7b10", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54d89f9fd5feabc2bb26800df2db77ff757a8d2d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa6dbe4194ff44d5c41148ee6bba5f28a9249f59", "width": 1080, "height": 540}], "variants": {}, "id": "HE64KdT2pM2hXXrwojeHuMQKclceURZ0rGPldjhg8rE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tp0si", "is_robot_indexable": true, "report_reasons": null, "author": "unixparadox", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tp0si/what_tool_do_i_use_to_serializedeserialize_avro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tp0si/what_tool_do_i_use_to_serializedeserialize_avro/", "subreddit_subscribers": 93363, "created_utc": 1679054328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What tech conferences do you look forward to in 2023.\n\nMyself - AWS Reinvent, Databricks summit", "author_fullname": "t2_j1zb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tech conferences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tnl1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679050001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What tech conferences do you look forward to in 2023.&lt;/p&gt;\n\n&lt;p&gt;Myself - AWS Reinvent, Databricks summit&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tnl1r", "is_robot_indexable": true, "report_reasons": null, "author": "abhi5025", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tnl1r/tech_conferences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tnl1r/tech_conferences/", "subreddit_subscribers": 93363, "created_utc": 1679050001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6hz5qk0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Protecting Your Pipeline From Malformed JSON/XML With Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11tdp0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ST2DKKJRZzU4dei-s8xcwgl9n-J2ZL9WrC5APVpUY5A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679017871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "phdata.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.phdata.io/blog/protecting-your-pipeline-from-malformed-json-xml-with-snowflake/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?auto=webp&amp;v=enabled&amp;s=166741237f2ff9f126f3c68ab2eaf5f27a936e86", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61d81a79c634171f1c879ed732c8fdb220db7f68", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a73d0d5d310a5266e637a82e544d19baf038c13", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f91cda5eb0a8ec12e1449e6590b4a26b18201956", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7df6bd85fb0d4bd72286c99a00d29059cd015f7b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f475471aa9ae20d58baee70c4eb75701a3816b28", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f8361f5d6e89e2e1066297a2e4352a15a77699b", "width": 1080, "height": 565}], "variants": {}, "id": "MDtCGjZfX66jP1QvdxkxHn3ZJDw5pZJcWoGOOSe5NdU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tdp0e", "is_robot_indexable": true, "report_reasons": null, "author": "OptimizedGradient", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tdp0e/protecting_your_pipeline_from_malformed_jsonxml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.phdata.io/blog/protecting-your-pipeline-from-malformed-json-xml-with-snowflake/", "subreddit_subscribers": 93363, "created_utc": 1679017871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working on a generic framework to write data to Confluent kafka via pyspark from a relational database \nI have a config table where I enter what table from OLTP to send to what kafka topic (it\u2019s always one to one)\nI then loop in each entry in the config table to write to kafka topics via pyspark \nIs there a way to write simultaneously to all kafka topics instead of looping \nAnd do you see any problems with this approach? \nI just want to know how others are doing this in their organization", "author_fullname": "t2_7pvpke89", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to write to multiple topics parallelly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t90ca", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679007070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working on a generic framework to write data to Confluent kafka via pyspark from a relational database \nI have a config table where I enter what table from OLTP to send to what kafka topic (it\u2019s always one to one)\nI then loop in each entry in the config table to write to kafka topics via pyspark \nIs there a way to write simultaneously to all kafka topics instead of looping \nAnd do you see any problems with this approach? \nI just want to know how others are doing this in their organization&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t90ca", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Channel9", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t90ca/how_to_write_to_multiple_topics_parallelly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t90ca/how_to_write_to_multiple_topics_parallelly/", "subreddit_subscribers": 93363, "created_utc": 1679007070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.youtube.com/watch?v=grTslxZIvYE](https://www.youtube.com/watch?v=grTslxZIvYE)", "author_fullname": "t2_vlp8q84d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Video] GitOps with CNDI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t6wwx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679002248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=grTslxZIvYE\"&gt;https://www.youtube.com/watch?v=grTslxZIvYE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?auto=webp&amp;v=enabled&amp;s=87378b018d849b5c67cb1c7c69ab1490b6aa8fb6", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e47b74ba97087b2b7baac6b3ff95eb1af8692e16", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c4ee588a07e25513a169f0e933936d56931cb37", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0bccbeb4ebae519c4707118099faba916361927", "width": 320, "height": 240}], "variants": {}, "id": "3JT3y1GjPSMCoHAar5uh1PiFhEZ_12wFlRuQbpF4aRE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11t6wwx", "is_robot_indexable": true, "report_reasons": null, "author": "SheldonMackay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t6wwx/video_gitops_with_cndi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t6wwx/video_gitops_with_cndi/", "subreddit_subscribers": 93363, "created_utc": 1679002248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to profile (mainly get % of nulls) several tables (and several columns in these tables) in the DWH (postgres) and thinking of the \"best\"/most efficient solution. There are data catalog that offers some simple validations, then there are specific data quality tools like great expectations or deequ, but - how these things perform on several hundreds of GB? I am afraid the profiling would be too slow (and unfortunately I do not have time budget to try our a lot of tools) so trying to learn what would be the most efficient approach here. Is there maybe some built-in metadata table in postgres that would make exploration of null values faster?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The most efficient way to profile WH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4zum", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678997829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to profile (mainly get % of nulls) several tables (and several columns in these tables) in the DWH (postgres) and thinking of the &amp;quot;best&amp;quot;/most efficient solution. There are data catalog that offers some simple validations, then there are specific data quality tools like great expectations or deequ, but - how these things perform on several hundreds of GB? I am afraid the profiling would be too slow (and unfortunately I do not have time budget to try our a lot of tools) so trying to learn what would be the most efficient approach here. Is there maybe some built-in metadata table in postgres that would make exploration of null values faster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t4zum", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4zum/the_most_efficient_way_to_profile_wh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t4zum/the_most_efficient_way_to_profile_wh/", "subreddit_subscribers": 93363, "created_utc": 1678997829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand I can create great expecations for the entire column (e.g. count of nulls) but I would need to validate % of nulls by category (by other column). Is it possible to do so with great expectations? And if not is there any alternative tool that can do it?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Great expecations - category wise expectations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4vv1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678997571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand I can create great expecations for the entire column (e.g. count of nulls) but I would need to validate % of nulls by category (by other column). Is it possible to do so with great expectations? And if not is there any alternative tool that can do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t4vv1", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4vv1/great_expecations_category_wise_expectations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t4vv1/great_expecations_category_wise_expectations/", "subreddit_subscribers": 93363, "created_utc": 1678997571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Typing this because I feel a little bit down... I got a couple interview questions on SQL like\n\n+ What's the difference between a CTE and a view?\n\nAnd I couldn't give a satisfactory answer, but I've been doing SQL for something close to 2 years now. Now I know a couple differences but then the best I can do is search on the internet \"intermediate &lt;technology&gt; interview questions\" and hope for the next tech interview to cover what I just read about X technology a couple days ago?\n\nI have a total of 2 YOE (a little bit than that) and my role has always been implementation, I understand why these kind of questions get asked but at the same time I feel frustrated thinking these questions should be aimed at people who will fill a tech lead role. Does this make sense?\n\nAnd finally, the question in the title, how did you get to knowing the answers to those interview questions? Just searching on the internet until it's carved in your memory? Certifications? \n\nThanks!", "author_fullname": "t2_6dbtcwt9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How did you get the knowledge needed to answer interview questions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t31u7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678993298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Typing this because I feel a little bit down... I got a couple interview questions on SQL like&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What&amp;#39;s the difference between a CTE and a view?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;And I couldn&amp;#39;t give a satisfactory answer, but I&amp;#39;ve been doing SQL for something close to 2 years now. Now I know a couple differences but then the best I can do is search on the internet &amp;quot;intermediate &amp;lt;technology&amp;gt; interview questions&amp;quot; and hope for the next tech interview to cover what I just read about X technology a couple days ago?&lt;/p&gt;\n\n&lt;p&gt;I have a total of 2 YOE (a little bit than that) and my role has always been implementation, I understand why these kind of questions get asked but at the same time I feel frustrated thinking these questions should be aimed at people who will fill a tech lead role. Does this make sense?&lt;/p&gt;\n\n&lt;p&gt;And finally, the question in the title, how did you get to knowing the answers to those interview questions? Just searching on the internet until it&amp;#39;s carved in your memory? Certifications? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t31u7", "is_robot_indexable": true, "report_reasons": null, "author": "ebboch", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t31u7/how_did_you_get_the_knowledge_needed_to_answer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t31u7/how_did_you_get_the_knowledge_needed_to_answer/", "subreddit_subscribers": 93363, "created_utc": 1678993298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There has been a ton of buzz around AI automation tools and how they will affect the job market. I would like to start a discussion around the inevitable change to skills and what direction they will take in the next 5-10 years...\n\n* I believe that jobs will increasingly be focused on higher-level architectural decisions and not so much getting into the nitty gritty since code generation tools are getting increasingly sophisticated and there is no reason to think that won't continue...\n   * This can essentially be boiled down to more Senior type roles, but at the same time-you need junior/mid people to get experience to replace the senior roles as they inevitably retire/quit\n* I think it can be safe to say that the main limitation for any software/tech to be created is time/budget...\n   * Would we then say that if efficiency is dramatically increased, then the # of projects would increase dramatically as well? \n   * And therefore the jobs available would shift to a more administrative/supervisory role, but there would be just as many or more jobs?\n\nThis video below (at the linked timestamp, only about the last 6 mins of the video) explains well what I foresee as the future:\n\n[https://youtu.be/brAwZ5l\\_fuQ?t=626](https://youtu.be/brAwZ5l_fuQ?t=626)\n\nEssentially, I see over time, all programming jobs be **decreasingly technical** as code generation tools become inevitably more sophisticated, but **increasingly soft skill and communication oriented**\\--however, still at similar or more # of jobs...\n\nWhat do you all think of my prognosis?", "author_fullname": "t2_8chdw7c4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Future skills for Data/Software Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t1uyj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678990613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There has been a ton of buzz around AI automation tools and how they will affect the job market. I would like to start a discussion around the inevitable change to skills and what direction they will take in the next 5-10 years...&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I believe that jobs will increasingly be focused on higher-level architectural decisions and not so much getting into the nitty gritty since code generation tools are getting increasingly sophisticated and there is no reason to think that won&amp;#39;t continue...\n\n&lt;ul&gt;\n&lt;li&gt;This can essentially be boiled down to more Senior type roles, but at the same time-you need junior/mid people to get experience to replace the senior roles as they inevitably retire/quit&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;I think it can be safe to say that the main limitation for any software/tech to be created is time/budget...\n\n&lt;ul&gt;\n&lt;li&gt;Would we then say that if efficiency is dramatically increased, then the # of projects would increase dramatically as well? &lt;/li&gt;\n&lt;li&gt;And therefore the jobs available would shift to a more administrative/supervisory role, but there would be just as many or more jobs?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This video below (at the linked timestamp, only about the last 6 mins of the video) explains well what I foresee as the future:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/brAwZ5l_fuQ?t=626\"&gt;https://youtu.be/brAwZ5l_fuQ?t=626&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Essentially, I see over time, all programming jobs be &lt;strong&gt;decreasingly technical&lt;/strong&gt; as code generation tools become inevitably more sophisticated, but &lt;strong&gt;increasingly soft skill and communication oriented&lt;/strong&gt;--however, still at similar or more # of jobs...&lt;/p&gt;\n\n&lt;p&gt;What do you all think of my prognosis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iHmNDIO6M2jByk6BGzE3XTp2KkA0TFzliN8WEazFfaQ.jpg?auto=webp&amp;v=enabled&amp;s=04c0cfc33be89e72dcaaee261323f9d8d2a0b8eb", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/iHmNDIO6M2jByk6BGzE3XTp2KkA0TFzliN8WEazFfaQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02bbc39968a4a5c0d87b9c65a0e740d10c92de9c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/iHmNDIO6M2jByk6BGzE3XTp2KkA0TFzliN8WEazFfaQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c64b19ddf31eb8bef505ecd4aeda48ff1a20aae5", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/iHmNDIO6M2jByk6BGzE3XTp2KkA0TFzliN8WEazFfaQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a280ac8511e1e3a0cca8dd5dde8fcbaad89795ff", "width": 320, "height": 240}], "variants": {}, "id": "cMMcewgb0PyFU06EC1RGs3JSQzECcVXzdJLqJZFMM2A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t1uyj", "is_robot_indexable": true, "report_reasons": null, "author": "pdxtechnologist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t1uyj/future_skills_for_datasoftware_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t1uyj/future_skills_for_datasoftware_engineering/", "subreddit_subscribers": 93363, "created_utc": 1678990613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why you might not even need a data platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_11sz5li", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cXGLlbuS0Rs6HLENqVxmbwrMfpsEW-85nBoDLafyKEg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678984722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "getdozer.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://getdozer.io/blog/why-you-might-not-need-a-data-platform", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?auto=webp&amp;v=enabled&amp;s=79536e18af301dba74e5c29e6718a2ae65d51da9", "width": 6848, "height": 3438}, "resolutions": [{"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3447de0d00b869f45c48aefd87f4d37eb81257c9", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e22d6500d7e64c02415b75cf0f8b427bb853f5ac", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ee1b3b80081f099248e2252ac19ab373261e8fd", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26f2e2eadcdff278a987872ae6fea6876541f61d", "width": 640, "height": 321}, {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=261163137064f5471f818767eed97f3cc7f33951", "width": 960, "height": 481}, {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acd84e6f8183473fd1ab6b2a9373908b49df9deb", "width": 1080, "height": 542}], "variants": {}, "id": "2XxngX9EhUhZzOGE9yT7YlH9c52y5S7afU_8wIpBxMU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11sz5li", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sz5li/why_you_might_not_even_need_a_data_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://getdozer.io/blog/why-you-might-not-need-a-data-platform", "subreddit_subscribers": 93363, "created_utc": 1678984722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All,\n\nWhat are the best replacement tools for SSIS, which we used to execute snowflake procedures by creating SSIS packages; one that comes to mind is Airflow. Are there other tools available to perform the same task the SSIS does?\n\nThank you", "author_fullname": "t2_7qs0ir3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSIS Replacement for production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11swovi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;What are the best replacement tools for SSIS, which we used to execute snowflake procedures by creating SSIS packages; one that comes to mind is Airflow. Are there other tools available to perform the same task the SSIS does?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11swovi", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Archer3356", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11swovi/ssis_replacement_for_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11swovi/ssis_replacement_for_production/", "subreddit_subscribers": 93363, "created_utc": 1678979140.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}