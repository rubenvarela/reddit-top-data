{"kind": "Listing", "data": {"after": "t3_11tttqg", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vxxrqrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tracking the Fake Stars Market with Dagster, BigQuery and dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4j09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KuVo6LkTRmwrRlRoeKwFt7dkpiuezOmsmBagrxsDfbk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678996759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/fake-stars", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?auto=webp&amp;v=enabled&amp;s=6feffd3ff633a036f59a08f8594eb3f065bbe3b1", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e360f3ccc3f8c501c3fd34b0f17d0904a271132", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f629d6c060d739672ab5769feef32945ed0b547", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ce2426c05b366dc0e7390f6941fdcf369417314", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3282917840b4991ab1509ce8ba7daf2e8acba140", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70e65fb12076a711151ddba54cd9469adc644038", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b91cbdc9fd1f26d26ae4c64763b11a5f8453325a", "width": 1080, "height": 567}], "variants": {}, "id": "eKvtnMcXWrAKAAoUL_SszTvxbrdxPwsxAJNSYYV7yNQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11t4j09", "is_robot_indexable": true, "report_reasons": null, "author": "MrMosBiggestFan", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4j09/tracking_the_fake_stars_market_with_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/fake-stars", "subreddit_subscribers": 93381, "created_utc": 1678996759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title", "author_fullname": "t2_c6w52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "do you think Zhamak has any idea how much time has now been wasted on orgs discussing if we should 'do a data mesh'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t1e0u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678989600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t1e0u", "is_robot_indexable": true, "report_reasons": null, "author": "EmergenL", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t1e0u/do_you_think_zhamak_has_any_idea_how_much_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t1e0u/do_you_think_zhamak_has_any_idea_how_much_time/", "subreddit_subscribers": 93381, "created_utc": 1678989600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What tech conferences do you look forward to in 2023.\n\nMyself - AWS Reinvent, Databricks summit", "author_fullname": "t2_j1zb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tech conferences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tnl1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679050001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What tech conferences do you look forward to in 2023.&lt;/p&gt;\n\n&lt;p&gt;Myself - AWS Reinvent, Databricks summit&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tnl1r", "is_robot_indexable": true, "report_reasons": null, "author": "abhi5025", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tnl1r/tech_conferences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tnl1r/tech_conferences/", "subreddit_subscribers": 93381, "created_utc": 1679050001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering! Charles here from [Prequel](https://prequel.co). We just launched the ability for companies to import data from their customer\u2019s data warehouse or database, and we wanted to share a little bit more about it with the community.\n\nIf you just want to see how it works, here is [a demo of the product we recorded.](https://www.loom.com/share/4724fb62583e41a9ba1a636fc8ea92f1)\n\n# Quick background on us:\n\nWe help companies integrate with their customer\u2019s data warehouse or database. We\u2019ve been busy helping companies export data to their customers \u2013 we\u2019re currently syncing over 40bn rows per month on behalf of companies. But folks kept on asking us if we could help them import data from their customers too. They wanted the ability to offer a 1st-party reverse ETL to their customers, similar to the 1st-party ETL capability we already helped them offer. So we built that product, and here we are.\n\n# Why would people want to import data? \n\nThere are actually plenty of use-cases here. Imagine a usage-based billing company that needs to get a daily pull from its customers of all the billing events that happened, so that they can generate relevant invoices. Or a fraud detection company who needs to get the latest transaction data from its customers so it can appropriately mark fraudulent ones.\n\nThere\u2019s no great way to import customer data currently. Typically, people solve this one of two ways today. One is they import data via CSV. This works well enough, but it requires ongoing work on the part of the customer: they need to put a CSV together, and upload it to the right place on a daily/weekly/monthly basis. This is painful and time-consuming, especially for data that needs to be continuously imported. Another one is companies make the customer write custom code to feed data to their API. This requires the customer to do a bunch of solutions engineering work just to get started using the product \u2013 which is a suboptimal onboarding experience.\n\nSo instead, we let the customer connect their database or data warehouse and we pull data directly from there, on an ongoing basis. They select which tables to import (and potentially map some columns to required fields), and that\u2019s it. The setup for your customer only takes 5 minutes, and requires no ongoing work. We feel like that\u2019s the kind of experience every company should provide when onboarding a new customer.\n\n# How do we do it?\n\nImporting all this data continuously is non-trivial, but thankfully we can actually reuse 95% of the infrastructure we built for data exports. It turns out our core transfer logic remains pretty much exactly the same, and all we had to do was ship new CRUD endpoints in our API layer to let users configure their source/destination. For those interested in our stack, we run a GoLang backend and Typescript/React frontend on k8s. We are also huge fans of [DuckDB](https://duckdb.org/) and use it heavily.\n\nIn terms of technical design, the most challenging decisions we have to make are around making database\u2019s type-systems play nicely with each other (kind of an evergreen problem really). For imports, we allow the data recipient to specify whether they want to receive this data as JSON blob, or as a nicely typed table. If they choose the latter, they specify exactly which columns they\u2019re expecting, as well as what type guarantees those should uphold. We\u2019re also working on the ability to feed that data directly into an API endpoint, and adding post-ingestion validation logic.\n\nWe know that security and privacy are paramount here. We're SOC 2 Type II certified, and we go through annual white-box pentests to make sure that all our code is up to snuff. We never store any of the data anywhere on our servers. Finally, we offer on-prem deployments, so data never even has to touch our servers if our customers don't want it to.\n\nWe\u2019re really stoked to be sharing this with the community. We\u2019ll be hanging out here for most of the day, but you can also reach us at hn (at) prequel.co if you have any questions!", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We just launched the ability for companies to import data from their customers' data warehouse or database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tta5t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679064618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;! Charles here from &lt;a href=\"https://prequel.co\"&gt;Prequel&lt;/a&gt;. We just launched the ability for companies to import data from their customer\u2019s data warehouse or database, and we wanted to share a little bit more about it with the community.&lt;/p&gt;\n\n&lt;p&gt;If you just want to see how it works, here is &lt;a href=\"https://www.loom.com/share/4724fb62583e41a9ba1a636fc8ea92f1\"&gt;a demo of the product we recorded.&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Quick background on us:&lt;/h1&gt;\n\n&lt;p&gt;We help companies integrate with their customer\u2019s data warehouse or database. We\u2019ve been busy helping companies export data to their customers \u2013 we\u2019re currently syncing over 40bn rows per month on behalf of companies. But folks kept on asking us if we could help them import data from their customers too. They wanted the ability to offer a 1st-party reverse ETL to their customers, similar to the 1st-party ETL capability we already helped them offer. So we built that product, and here we are.&lt;/p&gt;\n\n&lt;h1&gt;Why would people want to import data?&lt;/h1&gt;\n\n&lt;p&gt;There are actually plenty of use-cases here. Imagine a usage-based billing company that needs to get a daily pull from its customers of all the billing events that happened, so that they can generate relevant invoices. Or a fraud detection company who needs to get the latest transaction data from its customers so it can appropriately mark fraudulent ones.&lt;/p&gt;\n\n&lt;p&gt;There\u2019s no great way to import customer data currently. Typically, people solve this one of two ways today. One is they import data via CSV. This works well enough, but it requires ongoing work on the part of the customer: they need to put a CSV together, and upload it to the right place on a daily/weekly/monthly basis. This is painful and time-consuming, especially for data that needs to be continuously imported. Another one is companies make the customer write custom code to feed data to their API. This requires the customer to do a bunch of solutions engineering work just to get started using the product \u2013 which is a suboptimal onboarding experience.&lt;/p&gt;\n\n&lt;p&gt;So instead, we let the customer connect their database or data warehouse and we pull data directly from there, on an ongoing basis. They select which tables to import (and potentially map some columns to required fields), and that\u2019s it. The setup for your customer only takes 5 minutes, and requires no ongoing work. We feel like that\u2019s the kind of experience every company should provide when onboarding a new customer.&lt;/p&gt;\n\n&lt;h1&gt;How do we do it?&lt;/h1&gt;\n\n&lt;p&gt;Importing all this data continuously is non-trivial, but thankfully we can actually reuse 95% of the infrastructure we built for data exports. It turns out our core transfer logic remains pretty much exactly the same, and all we had to do was ship new CRUD endpoints in our API layer to let users configure their source/destination. For those interested in our stack, we run a GoLang backend and Typescript/React frontend on k8s. We are also huge fans of &lt;a href=\"https://duckdb.org/\"&gt;DuckDB&lt;/a&gt; and use it heavily.&lt;/p&gt;\n\n&lt;p&gt;In terms of technical design, the most challenging decisions we have to make are around making database\u2019s type-systems play nicely with each other (kind of an evergreen problem really). For imports, we allow the data recipient to specify whether they want to receive this data as JSON blob, or as a nicely typed table. If they choose the latter, they specify exactly which columns they\u2019re expecting, as well as what type guarantees those should uphold. We\u2019re also working on the ability to feed that data directly into an API endpoint, and adding post-ingestion validation logic.&lt;/p&gt;\n\n&lt;p&gt;We know that security and privacy are paramount here. We&amp;#39;re SOC 2 Type II certified, and we go through annual white-box pentests to make sure that all our code is up to snuff. We never store any of the data anywhere on our servers. Finally, we offer on-prem deployments, so data never even has to touch our servers if our customers don&amp;#39;t want it to.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re really stoked to be sharing this with the community. We\u2019ll be hanging out here for most of the day, but you can also reach us at hn (at) prequel.co if you have any questions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?auto=webp&amp;v=enabled&amp;s=d5b56dad1f1822b9f19af65e2aa45cd5ff12f52a", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7af403957eba2378208dd9fdfa22dcd36dfaa79a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd19aef9f45b31f083cf1c61c066542a12deebc2", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d956de44bd557e0dfd53c6fdbd68f2174847634", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=993eda00924d64b8d49e3f3b9c66855280149f1d", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d85dcbd3a24ee7ce05e9687e1431e70555a4e42", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c46cfcd352042bd90a6f21262a7edf2d06553f1f", "width": 1080, "height": 564}], "variants": {}, "id": "fzZ14hH15ZjeFPargglIyO_mdUAjmMfgFHhDZlS4gp0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tta5t", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11tta5t/we_just_launched_the_ability_for_companies_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tta5t/we_just_launched_the_ability_for_companies_to/", "subreddit_subscribers": 93381, "created_utc": 1679064618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At a former company, I had the chance to start building dashboards and thoroughly enjoyed it. I had a great boss who gave me a lot of opportunity to grow. Long story short, the company went bad and my boss referred me for a head of IT position at another firm, even though my experience overall in IT is quite short. \n\nI land the job, and have learned a TON. How to make value propositions and lead a department &amp; projects to name a few. I already take a leadership role in a non-profit Im involved in, so it hasnt been too steep of a learning curve personally. Ive done fairly well. \n\nHowever, I still have this itch to grow from a junior DE to a principal/Data Architect, functioning as someone who mentors junior DE\u2019s and has some strong technical chops. I definetly feel comfortable dealing with people-conflict, but am aloso versatile enough for technical conflict as well. \n\nWhats the best way to make this transition? I have an opportunity at my present company to help the company be data driven (no analytics available besides canned reports) and am reading the data engineering toolkit. I am also honing my SQL. Thanks!", "author_fullname": "t2_6x7d7af4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Head of IT at a 100-person sized company that always wanted to grow into an Individual Contributor as a DE. How can I make that transition?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11thrhl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679029887.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679029707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At a former company, I had the chance to start building dashboards and thoroughly enjoyed it. I had a great boss who gave me a lot of opportunity to grow. Long story short, the company went bad and my boss referred me for a head of IT position at another firm, even though my experience overall in IT is quite short. &lt;/p&gt;\n\n&lt;p&gt;I land the job, and have learned a TON. How to make value propositions and lead a department &amp;amp; projects to name a few. I already take a leadership role in a non-profit Im involved in, so it hasnt been too steep of a learning curve personally. Ive done fairly well. &lt;/p&gt;\n\n&lt;p&gt;However, I still have this itch to grow from a junior DE to a principal/Data Architect, functioning as someone who mentors junior DE\u2019s and has some strong technical chops. I definetly feel comfortable dealing with people-conflict, but am aloso versatile enough for technical conflict as well. &lt;/p&gt;\n\n&lt;p&gt;Whats the best way to make this transition? I have an opportunity at my present company to help the company be data driven (no analytics available besides canned reports) and am reading the data engineering toolkit. I am also honing my SQL. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11thrhl", "is_robot_indexable": true, "report_reasons": null, "author": "BigBoatThrowaway", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11thrhl/head_of_it_at_a_100person_sized_company_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11thrhl/head_of_it_at_a_100person_sized_company_that/", "subreddit_subscribers": 93381, "created_utc": 1679029707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In case you\u2019re not aware, AWS snowmobile is a service where AWS will park up a 45ft shipping container full of SSDs in order to transfer your data\u2014 up to 100 PB of it \u2014 to S3.\n\nI\u2019m curious if anyone here\u2019s actually used this service?", "author_fullname": "t2_4wbqkds2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ever used AWS Snowmobile?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t3ic9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678994370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In case you\u2019re not aware, AWS snowmobile is a service where AWS will park up a 45ft shipping container full of SSDs in order to transfer your data\u2014 up to 100 PB of it \u2014 to S3.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m curious if anyone here\u2019s actually used this service?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t3ic9", "is_robot_indexable": true, "report_reasons": null, "author": "bolivlake", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t3ic9/ever_used_aws_snowmobile/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t3ic9/ever_used_aws_snowmobile/", "subreddit_subscribers": 93381, "created_utc": 1678994370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a working prototype of Python code in a Jupyter notebook that downloads data onto my local machine.\n\nHow do I set this script up such that it automatically runs on a schedule and downloads data to AWS S3? I\u2019ve read AWS Glue, Lambda and maybe an EC2 instance but not sure which is the \u201cbest\u201d (cheapest + reliable) approach. Any thoughts?\n\nWe are using Snowflake as our data warehouse.", "author_fullname": "t2_55wqn55d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best AWS service to run a Jupyter-like Python script calling an API to download data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tqzbe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679060436.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679059355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a working prototype of Python code in a Jupyter notebook that downloads data onto my local machine.&lt;/p&gt;\n\n&lt;p&gt;How do I set this script up such that it automatically runs on a schedule and downloads data to AWS S3? I\u2019ve read AWS Glue, Lambda and maybe an EC2 instance but not sure which is the \u201cbest\u201d (cheapest + reliable) approach. Any thoughts?&lt;/p&gt;\n\n&lt;p&gt;We are using Snowflake as our data warehouse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tqzbe", "is_robot_indexable": true, "report_reasons": null, "author": "ahkd13", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tqzbe/whats_the_best_aws_service_to_run_a_jupyterlike/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tqzbe/whats_the_best_aws_service_to_run_a_jupyterlike/", "subreddit_subscribers": 93381, "created_utc": 1679059355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am aware of various data catalogs such as DataHub, Amundsen and so on that offer data lineage capability but is there any tool offering *solely* the lineage without all the stuff around? (I am thinking of building \"custom data catalog\" but I would need some \"help\" with the lineage)", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Standalone lineage tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4onj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678997107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am aware of various data catalogs such as DataHub, Amundsen and so on that offer data lineage capability but is there any tool offering &lt;em&gt;solely&lt;/em&gt; the lineage without all the stuff around? (I am thinking of building &amp;quot;custom data catalog&amp;quot; but I would need some &amp;quot;help&amp;quot; with the lineage)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t4onj", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4onj/standalone_lineage_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t4onj/standalone_lineage_tool/", "subreddit_subscribers": 93381, "created_utc": 1678997107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently export the data into excel and load as hive tables manually. I\u2019m wondering if anyone has automated this process.", "author_fullname": "t2_5jxm4ghg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone sourced sharepoint data dynamically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tlf2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679042825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently export the data into excel and load as hive tables manually. I\u2019m wondering if anyone has automated this process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tlf2v", "is_robot_indexable": true, "report_reasons": null, "author": "tentative_guy22", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tlf2v/has_anyone_sourced_sharepoint_data_dynamically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tlf2v/has_anyone_sourced_sharepoint_data_dynamically/", "subreddit_subscribers": 93381, "created_utc": 1679042825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I fall within the former and have for several years between companies, as most of my work has been centered on building out a data warehouse environment and enabling the business to use their data. Recently have wondered if this type of experience is more common than the extract-load pipeline version?\n\nThe motivation of the question is to figure out where to take my career and how to spend my time on which topics to learn - for example, should I spend some time learning databricks or spark on my own, knowing most of my experience has been in GCP or Azure? Should I spend time on information retrieval? Or on information architecture as it applies to data/analytics? Do I get deeper into dbt?\n\nWhat I am trying to understand is how to navigate DE going forward, and how to best spend my time when thinking of skills/education.", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are most data engineers primarily working on improving the data warehouse compared to building extract-load pipelines, or vice versa? Will there be a trend pushing data engineers closer to analytics/business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tshrc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679062970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I fall within the former and have for several years between companies, as most of my work has been centered on building out a data warehouse environment and enabling the business to use their data. Recently have wondered if this type of experience is more common than the extract-load pipeline version?&lt;/p&gt;\n\n&lt;p&gt;The motivation of the question is to figure out where to take my career and how to spend my time on which topics to learn - for example, should I spend some time learning databricks or spark on my own, knowing most of my experience has been in GCP or Azure? Should I spend time on information retrieval? Or on information architecture as it applies to data/analytics? Do I get deeper into dbt?&lt;/p&gt;\n\n&lt;p&gt;What I am trying to understand is how to navigate DE going forward, and how to best spend my time when thinking of skills/education.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11tshrc", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tshrc/are_most_data_engineers_primarily_working_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tshrc/are_most_data_engineers_primarily_working_on/", "subreddit_subscribers": 93381, "created_utc": 1679062970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_975og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing with dbt-expectations and how to avoid alert fatigue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11trc1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qOk7ZR-1fAoom1qSxB83YO7rqs7eOwFyKfzzrUhz1tQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679060203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datafold.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datafold.com/blog/dbt-expectations", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?auto=webp&amp;v=enabled&amp;s=c49849243878ba462683b0b4d4dee06adb9af3b9", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed8a1748063d4fffc583fd73b8adfde3609e259d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4075abbdcf21588a6af720cfdae720cc4500e41f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ce6dd9e92a77a87ea1c1dd544deddc952437fda", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=993310536b4b2afc57f0a5c450aa2dc2a07244b7", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f4942e3eac2eaf16aab802c8dc889bb0b7290a7", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e60326815512cc375e694a12638e80e4d922539", "width": 1080, "height": 607}], "variants": {}, "id": "76ppABcWi_cIcQgXA-2DergYkIbUsDWRlR1HHq_SJek"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11trc1j", "is_robot_indexable": true, "report_reasons": null, "author": "arimbr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11trc1j/testing_with_dbtexpectations_and_how_to_avoid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datafold.com/blog/dbt-expectations", "subreddit_subscribers": 93381, "created_utc": 1679060203.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know of a good solution for a self-hosted columnar DB? I have a small Linux machine that is hosting a Postgres server. Setting it up and loading in some test data was seamless, but it hasn't been able to scale with the types of analytical queries I want to run. \n\nI've looked into the citus extension, but unfortunately it doesn't support ARM architecture (which is what the linux server has).", "author_fullname": "t2_oyypyadu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Columnar Databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tgpmi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679026372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of a good solution for a self-hosted columnar DB? I have a small Linux machine that is hosting a Postgres server. Setting it up and loading in some test data was seamless, but it hasn&amp;#39;t been able to scale with the types of analytical queries I want to run. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked into the citus extension, but unfortunately it doesn&amp;#39;t support ARM architecture (which is what the linux server has).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tgpmi", "is_robot_indexable": true, "report_reasons": null, "author": "ringoefc", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tgpmi/open_source_columnar_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tgpmi/open_source_columnar_databases/", "subreddit_subscribers": 93381, "created_utc": 1679026372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nThere are a lot of ways to pass credentials to boto3 for aws.  I'm curious what's the most secure one to use? What do you guys use in production?\n\nThanks!", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to pass credentials to boto3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tuflm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679067129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;There are a lot of ways to pass credentials to boto3 for aws.  I&amp;#39;m curious what&amp;#39;s the most secure one to use? What do you guys use in production?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tuflm", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tuflm/best_way_to_pass_credentials_to_boto3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tuflm/best_way_to_pass_credentials_to_boto3/", "subreddit_subscribers": 93381, "created_utc": 1679067129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started searching for a new job as a junior/mid career data engineer about 2 weeks ago. Posting my results so far in case it's helpful for anyone. \n\n* 75 Applications\n* 20 rejections(mostly from senior roles that I wasn't qualified for)\n* 7 Interviews\n\nI also opened my Linkedin profile to actively searching, but at least 90% of messages are not really worthwhile opportunities. I feel like managing the recruiter emails can be time consuming. \n\nBase salary ranges between 120 -140k. \n\nI'm finding that most of the companies that were \"cool\" high growth tech companies that reached +1b revenue aren't hiring. I sent out some feeler applications back in October 2022, and got a lot more interest from these types of companies(Square, Zendesk, etc.) Most of the interviews I've gotten are either for Series A - C startups with less than 250 employees, or large corps that are in stagnant industries. \n\nCurious to hear other people's experiences. Or if any of you more experienced data engineers are holding off from moving jobs in this current job market.", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Search Statistics After 75 Applications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tszr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679064095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started searching for a new job as a junior/mid career data engineer about 2 weeks ago. Posting my results so far in case it&amp;#39;s helpful for anyone. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;75 Applications&lt;/li&gt;\n&lt;li&gt;20 rejections(mostly from senior roles that I wasn&amp;#39;t qualified for)&lt;/li&gt;\n&lt;li&gt;7 Interviews&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I also opened my Linkedin profile to actively searching, but at least 90% of messages are not really worthwhile opportunities. I feel like managing the recruiter emails can be time consuming. &lt;/p&gt;\n\n&lt;p&gt;Base salary ranges between 120 -140k. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m finding that most of the companies that were &amp;quot;cool&amp;quot; high growth tech companies that reached +1b revenue aren&amp;#39;t hiring. I sent out some feeler applications back in October 2022, and got a lot more interest from these types of companies(Square, Zendesk, etc.) Most of the interviews I&amp;#39;ve gotten are either for Series A - C startups with less than 250 employees, or large corps that are in stagnant industries. &lt;/p&gt;\n\n&lt;p&gt;Curious to hear other people&amp;#39;s experiences. Or if any of you more experienced data engineers are holding off from moving jobs in this current job market.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tszr6", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11tszr6/job_search_statistics_after_75_applications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tszr6/job_search_statistics_after_75_applications/", "subreddit_subscribers": 93381, "created_utc": 1679064095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If data lineage is the answer, what is the question?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "name": "t3_11tqb21", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_TItfra7pyByofbcpBLC06ZOlCYe2J0y526nFSSnIWw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679057700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/alvin-ai/if-data-lineage-is-the-answer-what-is-the-question-bad7f5f44fb5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?auto=webp&amp;v=enabled&amp;s=d27bfd253e22b985287197b107b744b6550b031c", "width": 1200, "height": 696}, "resolutions": [{"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc01f40a9c10113f1d59d4d24e9deacf656416f2", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc62a9d964b4d98d461bbf6480e9824f39a2c665", "width": 216, "height": 125}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cd3d26376ad33414283bbe83b7ff8db5970c4ab", "width": 320, "height": 185}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02916f59b8dbfdeb7a82aad9b880b28b96c35da7", "width": 640, "height": 371}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=849d6b7a9ecbcc3fece2986d2781d3fdd6ba49f9", "width": 960, "height": 556}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34c1fe8d94099f6cc31ef32dbd91d34a8cc26926", "width": 1080, "height": 626}], "variants": {}, "id": "_L6DDNGNDIlJSC0W9Hx_1Lonz-C-2itmin-V2GuRmtU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tqb21", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tqb21/if_data_lineage_is_the_answer_what_is_the_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/alvin-ai/if-data-lineage-is-the-answer-what-is-the-question-bad7f5f44fb5", "subreddit_subscribers": 93381, "created_utc": 1679057700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have captured some change data from a MySql database, serialized it with Avro converter and stored it in a Kafka topic. Now I am trying to read back that Avro serialized data from Kafka. I can easily get the serialized data but now I need to deserialize it. How do I do that? \n\nNote: I am using Pyspark to write my spark code. And I am creating a readingStream to stream the data in real-time from the Kafka topic. \n\nI found some tools for Spark(scala libraries) but the same solution for Pyspark is a bit complicated.\n\nI think using Abris [https://github.com/AbsaOSS/ABRiS](https://github.com/AbsaOSS/ABRiS) is a good choice as it supports for the python version of spark but I cannot seem to properly understand this scala written library's integration in Pyspark(Python). It's a bit complicated as I have not used non-python written libraries in Python. \n\nDoes someone have another better alternative for this use case? Another tool maybe. Or can anybody please make me understand this Abris integration for Pyspark. Thank you! [https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md](https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md)", "author_fullname": "t2_asyk1tug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tool do I use to serialize/deserialize Avro messages stored in a Kafka topic with schema registered in the schema-registry using Pyspark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tp0si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679054328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have captured some change data from a MySql database, serialized it with Avro converter and stored it in a Kafka topic. Now I am trying to read back that Avro serialized data from Kafka. I can easily get the serialized data but now I need to deserialize it. How do I do that? &lt;/p&gt;\n\n&lt;p&gt;Note: I am using Pyspark to write my spark code. And I am creating a readingStream to stream the data in real-time from the Kafka topic. &lt;/p&gt;\n\n&lt;p&gt;I found some tools for Spark(scala libraries) but the same solution for Pyspark is a bit complicated.&lt;/p&gt;\n\n&lt;p&gt;I think using Abris &lt;a href=\"https://github.com/AbsaOSS/ABRiS\"&gt;https://github.com/AbsaOSS/ABRiS&lt;/a&gt; is a good choice as it supports for the python version of spark but I cannot seem to properly understand this scala written library&amp;#39;s integration in Pyspark(Python). It&amp;#39;s a bit complicated as I have not used non-python written libraries in Python. &lt;/p&gt;\n\n&lt;p&gt;Does someone have another better alternative for this use case? Another tool maybe. Or can anybody please make me understand this Abris integration for Pyspark. Thank you! &lt;a href=\"https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md\"&gt;https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?auto=webp&amp;v=enabled&amp;s=387171c784b9c3f45187c8d9f24b3674cbc1bb52", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1207a3960e5e55a82a8218b1b8b8abad78f49d69", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b6ac0dfd8920ccd177bef60a16308a959182d86", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d1dfd547b9ef352820ceb8894e689487d85430a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06f4b989a61c0df8e9a7938d2f45578cda6d7b10", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54d89f9fd5feabc2bb26800df2db77ff757a8d2d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa6dbe4194ff44d5c41148ee6bba5f28a9249f59", "width": 1080, "height": 540}], "variants": {}, "id": "HE64KdT2pM2hXXrwojeHuMQKclceURZ0rGPldjhg8rE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tp0si", "is_robot_indexable": true, "report_reasons": null, "author": "unixparadox", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tp0si/what_tool_do_i_use_to_serializedeserialize_avro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tp0si/what_tool_do_i_use_to_serializedeserialize_avro/", "subreddit_subscribers": 93381, "created_utc": 1679054328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there\n\nI'm currently working on problem - I have a zoo of orchestration tools, like airflow, dbt etc\n\nand this tool need to synchronise in some cases - triggers, chain of control.\n\n&amp;#x200B;\n\nI think about Kafka with topics, dedicated to tools, or some kind of pg instance with events\n\nDoes anyone solved problem like this?", "author_fullname": "t2_89j762yi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration of orchestrators", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tmcjp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679046020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on problem - I have a zoo of orchestration tools, like airflow, dbt etc&lt;/p&gt;\n\n&lt;p&gt;and this tool need to synchronise in some cases - triggers, chain of control.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I think about Kafka with topics, dedicated to tools, or some kind of pg instance with events&lt;/p&gt;\n\n&lt;p&gt;Does anyone solved problem like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tmcjp", "is_robot_indexable": true, "report_reasons": null, "author": "tehdima", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tmcjp/orchestration_of_orchestrators/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tmcjp/orchestration_of_orchestrators/", "subreddit_subscribers": 93381, "created_utc": 1679046020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all--\n\nI'm learning DAGs and want to create some data ingress pipelines to test my Airflow knowledge. \n\nDoes anyone have experience with social media and some good data points to consume from Twitter and Facebook, say, for this type of project? It could be specific to a user or users, or general like popular trends or something.\n\nThanks for the ideas!", "author_fullname": "t2_3r56zv6e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Social Media Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t2wna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678992985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all--&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m learning DAGs and want to create some data ingress pipelines to test my Airflow knowledge. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with social media and some good data points to consume from Twitter and Facebook, say, for this type of project? It could be specific to a user or users, or general like popular trends or something.&lt;/p&gt;\n\n&lt;p&gt;Thanks for the ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t2wna", "is_robot_indexable": true, "report_reasons": null, "author": "qa_anaaq", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t2wna/creating_social_media_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t2wna/creating_social_media_data_pipelines/", "subreddit_subscribers": 93381, "created_utc": 1678992985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am reading messages from Kafka and processing it in pyspark.\n\n In some of the messages, there are some keys corresponding to which there are 0's and 1's. \n\nNow I want to convert these 0's and 1's to False and True. The challenge here I should only convert those columns for whose the datatype in the schema in boolean. There is no way of knowing which columns may have the datatype as BooleanType since this is streaming job and new columns are added everyday.\n\nThe main challenge here is, I need to do the conversion even before parsing the Kafka Message because as soon as I parse those messages using a schema, then column values become NULL (bcoz the columns have 0's and 1's and when parsed using BooleanType, they become NULL)\n\nCan anyone help me or point me in the right direction?", "author_fullname": "t2_188qz428", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Convert some specific columns that have 0 and 1 values in Kafka messages to False and True in PySpark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11tvzd9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679070506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reading messages from Kafka and processing it in pyspark.&lt;/p&gt;\n\n&lt;p&gt;In some of the messages, there are some keys corresponding to which there are 0&amp;#39;s and 1&amp;#39;s. &lt;/p&gt;\n\n&lt;p&gt;Now I want to convert these 0&amp;#39;s and 1&amp;#39;s to False and True. The challenge here I should only convert those columns for whose the datatype in the schema in boolean. There is no way of knowing which columns may have the datatype as BooleanType since this is streaming job and new columns are added everyday.&lt;/p&gt;\n\n&lt;p&gt;The main challenge here is, I need to do the conversion even before parsing the Kafka Message because as soon as I parse those messages using a schema, then column values become NULL (bcoz the columns have 0&amp;#39;s and 1&amp;#39;s and when parsed using BooleanType, they become NULL)&lt;/p&gt;\n\n&lt;p&gt;Can anyone help me or point me in the right direction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tvzd9", "is_robot_indexable": true, "report_reasons": null, "author": "swarup_i_am", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tvzd9/convert_some_specific_columns_that_have_0_and_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tvzd9/convert_some_specific_columns_that_have_0_and_1/", "subreddit_subscribers": 93381, "created_utc": 1679070506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_rx4wrtdi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6 Effective Tips to Hire Data Engineers Remotely: Best Practices for Data Engineer Interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": false, "name": "t3_11ttur3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/M8YcMpTR9W5derfEZRy8cozpAg3f7L-jtUvckudcy1A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679065859.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "borderlessmind.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.borderlessmind.com/blog/6-effective-tips-hire-data-engineers-remotely-best-practices-data-engineer-interviews/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?auto=webp&amp;v=enabled&amp;s=e9c11445bf920365a113c07a7b54a4b61a939578", "width": 1200, "height": 763}, "resolutions": [{"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f30bae4bd9fbedbca0ea052606d035953b9439f", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13f976c64d9d547309a154e83b1d436cdcfb564e", "width": 216, "height": 137}, {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ce10e60a910542e594f58788d9e51245ba5784d", "width": 320, "height": 203}, {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49374afb293b8d303f1e9fcc3c7dae8482eff380", "width": 640, "height": 406}, {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8359c4e75539d929fe4a84ab029f5760e414830d", "width": 960, "height": 610}, {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae0113b3ed683420a8485c1dee38b267fb825cf5", "width": 1080, "height": 686}], "variants": {}, "id": "1q-X_hWaphUhvnt06YINXEhG2ASNieB0KxNR6hHCl6c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11ttur3", "is_robot_indexable": true, "report_reasons": null, "author": "AccomplishedRice2031", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ttur3/6_effective_tips_to_hire_data_engineers_remotely/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.borderlessmind.com/blog/6-effective-tips-hire-data-engineers-remotely-best-practices-data-engineer-interviews/", "subreddit_subscribers": 93381, "created_utc": 1679065859.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am migrating R code from on-prem to Databricks. I learned this week that I can't use readRDS() of .rds object in S3 like reading a CSV. \n\nAny ideas? I could try to convert to a CSV but these don't have the same shape. \n\nCan I read from DBFS?", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can you read R .rds files in Databricks/Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tsww1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679063967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am migrating R code from on-prem to Databricks. I learned this week that I can&amp;#39;t use readRDS() of .rds object in S3 like reading a CSV. &lt;/p&gt;\n\n&lt;p&gt;Any ideas? I could try to convert to a CSV but these don&amp;#39;t have the same shape. &lt;/p&gt;\n\n&lt;p&gt;Can I read from DBFS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tsww1", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tsww1/how_can_you_read_r_rds_files_in_databricksspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tsww1/how_can_you_read_r_rds_files_in_databricksspark/", "subreddit_subscribers": 93381, "created_utc": 1679063967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need help. I am trying to migrate data from GCP BigQuery to AWS S3 by using Glue as the data integration tool. The Glue job will run daily and will only get the data of today and yesterday. However, there must be a better way. The problem with this approach is that this will introduce duplicates to the data, not to mention, additional processing time for the Glue job as well. This can easily be resolved by using DISTINCT by the data consumers. However, is there any means to retrieve only the deltas from the BigQuery table. Or is there a good approach to do this?", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BigQuery to S3 using Glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11trua2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679061432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help. I am trying to migrate data from GCP BigQuery to AWS S3 by using Glue as the data integration tool. The Glue job will run daily and will only get the data of today and yesterday. However, there must be a better way. The problem with this approach is that this will introduce duplicates to the data, not to mention, additional processing time for the Glue job as well. This can easily be resolved by using DISTINCT by the data consumers. However, is there any means to retrieve only the deltas from the BigQuery table. Or is there a good approach to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11trua2", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11trua2/bigquery_to_s3_using_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11trua2/bigquery_to_s3_using_glue/", "subreddit_subscribers": 93381, "created_utc": 1679061432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6hz5qk0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Protecting Your Pipeline From Malformed JSON/XML With Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11tdp0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ST2DKKJRZzU4dei-s8xcwgl9n-J2ZL9WrC5APVpUY5A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679017871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "phdata.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.phdata.io/blog/protecting-your-pipeline-from-malformed-json-xml-with-snowflake/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?auto=webp&amp;v=enabled&amp;s=166741237f2ff9f126f3c68ab2eaf5f27a936e86", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61d81a79c634171f1c879ed732c8fdb220db7f68", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a73d0d5d310a5266e637a82e544d19baf038c13", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f91cda5eb0a8ec12e1449e6590b4a26b18201956", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7df6bd85fb0d4bd72286c99a00d29059cd015f7b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f475471aa9ae20d58baee70c4eb75701a3816b28", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f8361f5d6e89e2e1066297a2e4352a15a77699b", "width": 1080, "height": 565}], "variants": {}, "id": "MDtCGjZfX66jP1QvdxkxHn3ZJDw5pZJcWoGOOSe5NdU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tdp0e", "is_robot_indexable": true, "report_reasons": null, "author": "OptimizedGradient", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tdp0e/protecting_your_pipeline_from_malformed_jsonxml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.phdata.io/blog/protecting-your-pipeline-from-malformed-json-xml-with-snowflake/", "subreddit_subscribers": 93381, "created_utc": 1679017871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m using Matillion ETL and snowflake. I have various data sources ljke DBs and APIs and files owned by other teams within the business.\n\nI have a raw data layer in snowflake where all the source data lands from matillion, then layers on top of that (cleansed, modelled, presentation layer).\n\nCurrent strategy is to merge/upsert incoming data into the raw layer so it only holds the latest version of each record when source data gets updated.\n\nIs that a typical pattern? Would it be better to store each version of every record in raw, then filter them out downstream to only select the latest record? And how and when do you apply that filter?", "author_fullname": "t2_4y2g3lh3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "History in the Raw Data Layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11tws9j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679072220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m using Matillion ETL and snowflake. I have various data sources ljke DBs and APIs and files owned by other teams within the business.&lt;/p&gt;\n\n&lt;p&gt;I have a raw data layer in snowflake where all the source data lands from matillion, then layers on top of that (cleansed, modelled, presentation layer).&lt;/p&gt;\n\n&lt;p&gt;Current strategy is to merge/upsert incoming data into the raw layer so it only holds the latest version of each record when source data gets updated.&lt;/p&gt;\n\n&lt;p&gt;Is that a typical pattern? Would it be better to store each version of every record in raw, then filter them out downstream to only select the latest record? And how and when do you apply that filter?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tws9j", "is_robot_indexable": true, "report_reasons": null, "author": "Right-Requirement895", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tws9j/history_in_the_raw_data_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tws9j/history_in_the_raw_data_layer/", "subreddit_subscribers": 93381, "created_utc": 1679072220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uypc8pwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DSA For The Rest Of Us - Into to Linked Lists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_11tttqg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rKlkVJkBs0cBD-Q7iMYOQ73dQ7s2D3AcpqWxhp854Xs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679065801.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/dsa-for-the-rest-of-us-part-1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7yugdQ_-MklYqSk8s_SoQKrHaIl7C_wkVlNJQ36r2Vg.jpg?auto=webp&amp;v=enabled&amp;s=e37daeab152f8b973578fcfb39a74cbb21592eff", "width": 900, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7yugdQ_-MklYqSk8s_SoQKrHaIl7C_wkVlNJQ36r2Vg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35c8e44e454dd93a169ddbcb302921eb3c80f73f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/7yugdQ_-MklYqSk8s_SoQKrHaIl7C_wkVlNJQ36r2Vg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b6c40e34bb1c5a915927fa4b2c395cb43705721", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/7yugdQ_-MklYqSk8s_SoQKrHaIl7C_wkVlNJQ36r2Vg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9bee06a0484b34f5b0d87ff60c4ae1b076e0d93c", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/7yugdQ_-MklYqSk8s_SoQKrHaIl7C_wkVlNJQ36r2Vg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf688c6b7a02561694a2eb32ecc739614cda10b2", "width": 640, "height": 426}], "variants": {}, "id": "cKBOkB2CV-NZpjD3ua5g11XaHAPE1Zvk14BH-IOqxZI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tttqg", "is_robot_indexable": true, "report_reasons": null, "author": "DarkClear3881", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tttqg/dsa_for_the_rest_of_us_into_to_linked_lists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/dsa-for-the-rest-of-us-part-1", "subreddit_subscribers": 93381, "created_utc": 1679065801.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}