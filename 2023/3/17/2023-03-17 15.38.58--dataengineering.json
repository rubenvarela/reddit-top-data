{"kind": "Listing", "data": {"after": "t3_11t6wwx", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vxxrqrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tracking the Fake Stars Market with Dagster, BigQuery and dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4j09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KuVo6LkTRmwrRlRoeKwFt7dkpiuezOmsmBagrxsDfbk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678996759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/fake-stars", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?auto=webp&amp;v=enabled&amp;s=6feffd3ff633a036f59a08f8594eb3f065bbe3b1", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e360f3ccc3f8c501c3fd34b0f17d0904a271132", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f629d6c060d739672ab5769feef32945ed0b547", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ce2426c05b366dc0e7390f6941fdcf369417314", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3282917840b4991ab1509ce8ba7daf2e8acba140", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70e65fb12076a711151ddba54cd9469adc644038", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b91cbdc9fd1f26d26ae4c64763b11a5f8453325a", "width": 1080, "height": 567}], "variants": {}, "id": "eKvtnMcXWrAKAAoUL_SszTvxbrdxPwsxAJNSYYV7yNQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11t4j09", "is_robot_indexable": true, "report_reasons": null, "author": "MrMosBiggestFan", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4j09/tracking_the_fake_stars_market_with_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/fake-stars", "subreddit_subscribers": 93373, "created_utc": 1678996759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title", "author_fullname": "t2_c6w52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "do you think Zhamak has any idea how much time has now been wasted on orgs discussing if we should 'do a data mesh'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t1e0u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678989600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t1e0u", "is_robot_indexable": true, "report_reasons": null, "author": "EmergenL", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t1e0u/do_you_think_zhamak_has_any_idea_how_much_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t1e0u/do_you_think_zhamak_has_any_idea_how_much_time/", "subreddit_subscribers": 93373, "created_utc": 1678989600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all. I recently started a position at a manufacturer where the entire IT and data process is done through SAP. They are trying to slowly get themselves off of SAP and hired me due to my experience in more modern ETL/Data platforms.\nI was just wondering if anyone here had experience in modernizing an SAP platform and the best tools for the job. Thanks", "author_fullname": "t2_6ra3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with SAP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11swnax", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all. I recently started a position at a manufacturer where the entire IT and data process is done through SAP. They are trying to slowly get themselves off of SAP and hired me due to my experience in more modern ETL/Data platforms.\nI was just wondering if anyone here had experience in modernizing an SAP platform and the best tools for the job. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11swnax", "is_robot_indexable": true, "report_reasons": null, "author": "deemerritt", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11swnax/working_with_sap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11swnax/working_with_sap/", "subreddit_subscribers": 93373, "created_utc": 1678979041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In case you\u2019re not aware, AWS snowmobile is a service where AWS will park up a 45ft shipping container full of SSDs in order to transfer your data\u2014 up to 100 PB of it \u2014 to S3.\n\nI\u2019m curious if anyone here\u2019s actually used this service?", "author_fullname": "t2_4wbqkds2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ever used AWS Snowmobile?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t3ic9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678994370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In case you\u2019re not aware, AWS snowmobile is a service where AWS will park up a 45ft shipping container full of SSDs in order to transfer your data\u2014 up to 100 PB of it \u2014 to S3.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m curious if anyone here\u2019s actually used this service?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t3ic9", "is_robot_indexable": true, "report_reasons": null, "author": "bolivlake", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t3ic9/ever_used_aws_snowmobile/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t3ic9/ever_used_aws_snowmobile/", "subreddit_subscribers": 93373, "created_utc": 1678994370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What tech conferences do you look forward to in 2023.\n\nMyself - AWS Reinvent, Databricks summit", "author_fullname": "t2_j1zb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tech conferences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tnl1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679050001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What tech conferences do you look forward to in 2023.&lt;/p&gt;\n\n&lt;p&gt;Myself - AWS Reinvent, Databricks summit&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tnl1r", "is_robot_indexable": true, "report_reasons": null, "author": "abhi5025", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tnl1r/tech_conferences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tnl1r/tech_conferences/", "subreddit_subscribers": 93373, "created_utc": 1679050001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am aware of various data catalogs such as DataHub, Amundsen and so on that offer data lineage capability but is there any tool offering *solely* the lineage without all the stuff around? (I am thinking of building \"custom data catalog\" but I would need some \"help\" with the lineage)", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Standalone lineage tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4onj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678997107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am aware of various data catalogs such as DataHub, Amundsen and so on that offer data lineage capability but is there any tool offering &lt;em&gt;solely&lt;/em&gt; the lineage without all the stuff around? (I am thinking of building &amp;quot;custom data catalog&amp;quot; but I would need some &amp;quot;help&amp;quot; with the lineage)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t4onj", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4onj/standalone_lineage_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t4onj/standalone_lineage_tool/", "subreddit_subscribers": 93373, "created_utc": 1678997107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At a former company, I had the chance to start building dashboards and thoroughly enjoyed it. I had a great boss who gave me a lot of opportunity to grow. Long story short, the company went bad and my boss referred me for a head of IT position at another firm, even though my experience overall in IT is quite short. \n\nI land the job, and have learned a TON. How to make value propositions and lead a department &amp; projects to name a few. I already take a leadership role in a non-profit Im involved in, so it hasnt been too steep of a learning curve personally. Ive done fairly well. \n\nHowever, I still have this itch to grow from a junior DE to a principal/Data Architect, functioning as someone who mentors junior DE\u2019s and has some strong technical chops. I definetly feel comfortable dealing with people-conflict, but am aloso versatile enough for technical conflict as well. \n\nWhats the best way to make this transition? I have an opportunity at my present company to help the company be data driven (no analytics available besides canned reports) and am reading the data engineering toolkit. I am also honing my SQL. Thanks!", "author_fullname": "t2_6x7d7af4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Head of IT at a 100-person sized company that always wanted to grow into an Individual Contributor as a DE. How can I make that transition?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11thrhl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679029887.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679029707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At a former company, I had the chance to start building dashboards and thoroughly enjoyed it. I had a great boss who gave me a lot of opportunity to grow. Long story short, the company went bad and my boss referred me for a head of IT position at another firm, even though my experience overall in IT is quite short. &lt;/p&gt;\n\n&lt;p&gt;I land the job, and have learned a TON. How to make value propositions and lead a department &amp;amp; projects to name a few. I already take a leadership role in a non-profit Im involved in, so it hasnt been too steep of a learning curve personally. Ive done fairly well. &lt;/p&gt;\n\n&lt;p&gt;However, I still have this itch to grow from a junior DE to a principal/Data Architect, functioning as someone who mentors junior DE\u2019s and has some strong technical chops. I definetly feel comfortable dealing with people-conflict, but am aloso versatile enough for technical conflict as well. &lt;/p&gt;\n\n&lt;p&gt;Whats the best way to make this transition? I have an opportunity at my present company to help the company be data driven (no analytics available besides canned reports) and am reading the data engineering toolkit. I am also honing my SQL. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11thrhl", "is_robot_indexable": true, "report_reasons": null, "author": "BigBoatThrowaway", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11thrhl/head_of_it_at_a_100person_sized_company_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11thrhl/head_of_it_at_a_100person_sized_company_that/", "subreddit_subscribers": 93373, "created_utc": 1679029707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently export the data into excel and load as hive tables manually. I\u2019m wondering if anyone has automated this process.", "author_fullname": "t2_5jxm4ghg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone sourced sharepoint data dynamically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tlf2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679042825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently export the data into excel and load as hive tables manually. I\u2019m wondering if anyone has automated this process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tlf2v", "is_robot_indexable": true, "report_reasons": null, "author": "tentative_guy22", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tlf2v/has_anyone_sourced_sharepoint_data_dynamically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tlf2v/has_anyone_sourced_sharepoint_data_dynamically/", "subreddit_subscribers": 93373, "created_utc": 1679042825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering! Charles here from [Prequel](https://prequel.co). We just launched the ability for companies to import data from their customer\u2019s data warehouse or database, and we wanted to share a little bit more about it with the community.\n\nIf you just want to see how it works, here is [a demo of the product we recorded.](https://www.loom.com/share/4724fb62583e41a9ba1a636fc8ea92f1)\n\n# Quick background on us:\n\nWe help companies integrate with their customer\u2019s data warehouse or database. We\u2019ve been busy helping companies export data to their customers \u2013 we\u2019re currently syncing over 40bn rows per month on behalf of companies. But folks kept on asking us if we could help them import data from their customers too. They wanted the ability to offer a 1st-party reverse ETL to their customers, similar to the 1st-party ETL capability we already helped them offer. So we built that product, and here we are.\n\n# Why would people want to import data? \n\nThere are actually plenty of use-cases here. Imagine a usage-based billing company that needs to get a daily pull from its customers of all the billing events that happened, so that they can generate relevant invoices. Or a fraud detection company who needs to get the latest transaction data from its customers so it can appropriately mark fraudulent ones.\n\nThere\u2019s no great way to import customer data currently. Typically, people solve this one of two ways today. One is they import data via CSV. This works well enough, but it requires ongoing work on the part of the customer: they need to put a CSV together, and upload it to the right place on a daily/weekly/monthly basis. This is painful and time-consuming, especially for data that needs to be continuously imported. Another one is companies make the customer write custom code to feed data to their API. This requires the customer to do a bunch of solutions engineering work just to get started using the product \u2013 which is a suboptimal onboarding experience.\n\nSo instead, we let the customer connect their database or data warehouse and we pull data directly from there, on an ongoing basis. They select which tables to import (and potentially map some columns to required fields), and that\u2019s it. The setup for your customer only takes 5 minutes, and requires no ongoing work. We feel like that\u2019s the kind of experience every company should provide when onboarding a new customer.\n\n# How do we do it?\n\nImporting all this data continuously is non-trivial, but thankfully we can actually reuse 95% of the infrastructure we built for data exports. It turns out our core transfer logic remains pretty much exactly the same, and all we had to do was ship new CRUD endpoints in our API layer to let users configure their source/destination. For those interested in our stack, we run a GoLang backend and Typescript/React frontend on k8s. We are also huge fans of [DuckDB](https://duckdb.org/) and use it heavily.\n\nIn terms of technical design, the most challenging decisions we have to make are around making database\u2019s type-systems play nicely with each other (kind of an evergreen problem really). For imports, we allow the data recipient to specify whether they want to receive this data as JSON blob, or as a nicely typed table. If they choose the latter, they specify exactly which columns they\u2019re expecting, as well as what type guarantees those should uphold. We\u2019re also working on the ability to feed that data directly into an API endpoint, and adding post-ingestion validation logic.\n\nWe know that security and privacy are paramount here. We're SOC 2 Type II certified, and we go through annual white-box pentests to make sure that all our code is up to snuff. We never store any of the data anywhere on our servers. Finally, we offer on-prem deployments, so data never even has to touch our servers if our customers don't want it to.\n\nWe\u2019re really stoked to be sharing this with the community. We\u2019ll be hanging out here for most of the day, but you can also reach us at hn (at) prequel.co if you have any questions!", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We just launched the ability for companies to import data from their customers' data warehouse or database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11tta5t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679064618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;! Charles here from &lt;a href=\"https://prequel.co\"&gt;Prequel&lt;/a&gt;. We just launched the ability for companies to import data from their customer\u2019s data warehouse or database, and we wanted to share a little bit more about it with the community.&lt;/p&gt;\n\n&lt;p&gt;If you just want to see how it works, here is &lt;a href=\"https://www.loom.com/share/4724fb62583e41a9ba1a636fc8ea92f1\"&gt;a demo of the product we recorded.&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Quick background on us:&lt;/h1&gt;\n\n&lt;p&gt;We help companies integrate with their customer\u2019s data warehouse or database. We\u2019ve been busy helping companies export data to their customers \u2013 we\u2019re currently syncing over 40bn rows per month on behalf of companies. But folks kept on asking us if we could help them import data from their customers too. They wanted the ability to offer a 1st-party reverse ETL to their customers, similar to the 1st-party ETL capability we already helped them offer. So we built that product, and here we are.&lt;/p&gt;\n\n&lt;h1&gt;Why would people want to import data?&lt;/h1&gt;\n\n&lt;p&gt;There are actually plenty of use-cases here. Imagine a usage-based billing company that needs to get a daily pull from its customers of all the billing events that happened, so that they can generate relevant invoices. Or a fraud detection company who needs to get the latest transaction data from its customers so it can appropriately mark fraudulent ones.&lt;/p&gt;\n\n&lt;p&gt;There\u2019s no great way to import customer data currently. Typically, people solve this one of two ways today. One is they import data via CSV. This works well enough, but it requires ongoing work on the part of the customer: they need to put a CSV together, and upload it to the right place on a daily/weekly/monthly basis. This is painful and time-consuming, especially for data that needs to be continuously imported. Another one is companies make the customer write custom code to feed data to their API. This requires the customer to do a bunch of solutions engineering work just to get started using the product \u2013 which is a suboptimal onboarding experience.&lt;/p&gt;\n\n&lt;p&gt;So instead, we let the customer connect their database or data warehouse and we pull data directly from there, on an ongoing basis. They select which tables to import (and potentially map some columns to required fields), and that\u2019s it. The setup for your customer only takes 5 minutes, and requires no ongoing work. We feel like that\u2019s the kind of experience every company should provide when onboarding a new customer.&lt;/p&gt;\n\n&lt;h1&gt;How do we do it?&lt;/h1&gt;\n\n&lt;p&gt;Importing all this data continuously is non-trivial, but thankfully we can actually reuse 95% of the infrastructure we built for data exports. It turns out our core transfer logic remains pretty much exactly the same, and all we had to do was ship new CRUD endpoints in our API layer to let users configure their source/destination. For those interested in our stack, we run a GoLang backend and Typescript/React frontend on k8s. We are also huge fans of &lt;a href=\"https://duckdb.org/\"&gt;DuckDB&lt;/a&gt; and use it heavily.&lt;/p&gt;\n\n&lt;p&gt;In terms of technical design, the most challenging decisions we have to make are around making database\u2019s type-systems play nicely with each other (kind of an evergreen problem really). For imports, we allow the data recipient to specify whether they want to receive this data as JSON blob, or as a nicely typed table. If they choose the latter, they specify exactly which columns they\u2019re expecting, as well as what type guarantees those should uphold. We\u2019re also working on the ability to feed that data directly into an API endpoint, and adding post-ingestion validation logic.&lt;/p&gt;\n\n&lt;p&gt;We know that security and privacy are paramount here. We&amp;#39;re SOC 2 Type II certified, and we go through annual white-box pentests to make sure that all our code is up to snuff. We never store any of the data anywhere on our servers. Finally, we offer on-prem deployments, so data never even has to touch our servers if our customers don&amp;#39;t want it to.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re really stoked to be sharing this with the community. We\u2019ll be hanging out here for most of the day, but you can also reach us at hn (at) prequel.co if you have any questions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?auto=webp&amp;v=enabled&amp;s=d5b56dad1f1822b9f19af65e2aa45cd5ff12f52a", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7af403957eba2378208dd9fdfa22dcd36dfaa79a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd19aef9f45b31f083cf1c61c066542a12deebc2", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d956de44bd557e0dfd53c6fdbd68f2174847634", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=993eda00924d64b8d49e3f3b9c66855280149f1d", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d85dcbd3a24ee7ce05e9687e1431e70555a4e42", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c46cfcd352042bd90a6f21262a7edf2d06553f1f", "width": 1080, "height": 564}], "variants": {}, "id": "fzZ14hH15ZjeFPargglIyO_mdUAjmMfgFHhDZlS4gp0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tta5t", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11tta5t/we_just_launched_the_ability_for_companies_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tta5t/we_just_launched_the_ability_for_companies_to/", "subreddit_subscribers": 93373, "created_utc": 1679064618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know of a good solution for a self-hosted columnar DB? I have a small Linux machine that is hosting a Postgres server. Setting it up and loading in some test data was seamless, but it hasn't been able to scale with the types of analytical queries I want to run. \n\nI've looked into the citus extension, but unfortunately it doesn't support ARM architecture (which is what the linux server has).", "author_fullname": "t2_oyypyadu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Columnar Databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tgpmi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679026372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of a good solution for a self-hosted columnar DB? I have a small Linux machine that is hosting a Postgres server. Setting it up and loading in some test data was seamless, but it hasn&amp;#39;t been able to scale with the types of analytical queries I want to run. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked into the citus extension, but unfortunately it doesn&amp;#39;t support ARM architecture (which is what the linux server has).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tgpmi", "is_robot_indexable": true, "report_reasons": null, "author": "ringoefc", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tgpmi/open_source_columnar_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tgpmi/open_source_columnar_databases/", "subreddit_subscribers": 93373, "created_utc": 1679026372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there\n\nI'm currently working on problem - I have a zoo of orchestration tools, like airflow, dbt etc\n\nand this tool need to synchronise in some cases - triggers, chain of control.\n\n&amp;#x200B;\n\nI think about Kafka with topics, dedicated to tools, or some kind of pg instance with events\n\nDoes anyone solved problem like this?", "author_fullname": "t2_89j762yi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration of orchestrators", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tmcjp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679046020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on problem - I have a zoo of orchestration tools, like airflow, dbt etc&lt;/p&gt;\n\n&lt;p&gt;and this tool need to synchronise in some cases - triggers, chain of control.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I think about Kafka with topics, dedicated to tools, or some kind of pg instance with events&lt;/p&gt;\n\n&lt;p&gt;Does anyone solved problem like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tmcjp", "is_robot_indexable": true, "report_reasons": null, "author": "tehdima", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tmcjp/orchestration_of_orchestrators/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tmcjp/orchestration_of_orchestrators/", "subreddit_subscribers": 93373, "created_utc": 1679046020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all--\n\nI'm learning DAGs and want to create some data ingress pipelines to test my Airflow knowledge. \n\nDoes anyone have experience with social media and some good data points to consume from Twitter and Facebook, say, for this type of project? It could be specific to a user or users, or general like popular trends or something.\n\nThanks for the ideas!", "author_fullname": "t2_3r56zv6e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Social Media Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t2wna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678992985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all--&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m learning DAGs and want to create some data ingress pipelines to test my Airflow knowledge. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with social media and some good data points to consume from Twitter and Facebook, say, for this type of project? It could be specific to a user or users, or general like popular trends or something.&lt;/p&gt;\n\n&lt;p&gt;Thanks for the ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t2wna", "is_robot_indexable": true, "report_reasons": null, "author": "qa_anaaq", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t2wna/creating_social_media_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t2wna/creating_social_media_data_pipelines/", "subreddit_subscribers": 93373, "created_utc": 1678992985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started searching for a new job as a junior/mid career data engineer about 2 weeks ago. Posting my results so far in case it's helpful for anyone. \n\n* 75 Applications\n* 20 rejections(mostly from senior roles that I wasn't qualified for)\n* 7 Interviews\n\nI also opened my Linkedin profile to actively searching, but at least 90% of messages are not really worthwhile opportunities. I feel like managing the recruiter emails can be time consuming. \n\nBase salary ranges between 120 -140k. \n\nI'm finding that most of the companies that were \"cool\" high growth tech companies that reached +1b revenue aren't hiring. I sent out some feeler applications back in October 2022, and got a lot more interest from these types of companies(Square, Zendesk, etc.) Most of the interviews I've gotten are either for Series A - C startups with less than 250 employees, or large corps that are in stagnant industries. \n\nCurious to hear other people's experiences. Or if any of you more experienced data engineers are holding off from moving jobs in this current job market.", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Search Statistics After 75 Applications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11tszr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679064095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started searching for a new job as a junior/mid career data engineer about 2 weeks ago. Posting my results so far in case it&amp;#39;s helpful for anyone. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;75 Applications&lt;/li&gt;\n&lt;li&gt;20 rejections(mostly from senior roles that I wasn&amp;#39;t qualified for)&lt;/li&gt;\n&lt;li&gt;7 Interviews&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I also opened my Linkedin profile to actively searching, but at least 90% of messages are not really worthwhile opportunities. I feel like managing the recruiter emails can be time consuming. &lt;/p&gt;\n\n&lt;p&gt;Base salary ranges between 120 -140k. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m finding that most of the companies that were &amp;quot;cool&amp;quot; high growth tech companies that reached +1b revenue aren&amp;#39;t hiring. I sent out some feeler applications back in October 2022, and got a lot more interest from these types of companies(Square, Zendesk, etc.) Most of the interviews I&amp;#39;ve gotten are either for Series A - C startups with less than 250 employees, or large corps that are in stagnant industries. &lt;/p&gt;\n\n&lt;p&gt;Curious to hear other people&amp;#39;s experiences. Or if any of you more experienced data engineers are holding off from moving jobs in this current job market.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tszr6", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11tszr6/job_search_statistics_after_75_applications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tszr6/job_search_statistics_after_75_applications/", "subreddit_subscribers": 93373, "created_utc": 1679064095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I fall within the former and have for several years between companies, as most of my work has been centered on building out a data warehouse environment and enabling the business to use their data. Recently have wondered if this type of experience is more common than the extract-load pipeline version?\n\nThe motivation of the question is to figure out where to take my career and how to spend my time on which topics to learn - for example, should I spend some time learning databricks or spark on my own, knowing most of my experience has been in GCP or Azure? Should I spend time on information retrieval? Or on information architecture as it applies to data/analytics? Do I get deeper into dbt?\n\nWhat I am trying to understand is how to navigate DE going forward, and how to best spend my time when thinking of skills/education.", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are most data engineers primarily working on improving the data warehouse compared to building extract-load pipelines, or vice versa? Will there be a trend pushing data engineers closer to analytics/business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11tshrc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679062970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I fall within the former and have for several years between companies, as most of my work has been centered on building out a data warehouse environment and enabling the business to use their data. Recently have wondered if this type of experience is more common than the extract-load pipeline version?&lt;/p&gt;\n\n&lt;p&gt;The motivation of the question is to figure out where to take my career and how to spend my time on which topics to learn - for example, should I spend some time learning databricks or spark on my own, knowing most of my experience has been in GCP or Azure? Should I spend time on information retrieval? Or on information architecture as it applies to data/analytics? Do I get deeper into dbt?&lt;/p&gt;\n\n&lt;p&gt;What I am trying to understand is how to navigate DE going forward, and how to best spend my time when thinking of skills/education.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11tshrc", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tshrc/are_most_data_engineers_primarily_working_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tshrc/are_most_data_engineers_primarily_working_on/", "subreddit_subscribers": 93373, "created_utc": 1679062970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_975og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing with dbt-expectations and how to avoid alert fatigue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11trc1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qOk7ZR-1fAoom1qSxB83YO7rqs7eOwFyKfzzrUhz1tQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679060203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datafold.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datafold.com/blog/dbt-expectations", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?auto=webp&amp;v=enabled&amp;s=c49849243878ba462683b0b4d4dee06adb9af3b9", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed8a1748063d4fffc583fd73b8adfde3609e259d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4075abbdcf21588a6af720cfdae720cc4500e41f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ce6dd9e92a77a87ea1c1dd544deddc952437fda", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=993310536b4b2afc57f0a5c450aa2dc2a07244b7", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f4942e3eac2eaf16aab802c8dc889bb0b7290a7", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e60326815512cc375e694a12638e80e4d922539", "width": 1080, "height": 607}], "variants": {}, "id": "76ppABcWi_cIcQgXA-2DergYkIbUsDWRlR1HHq_SJek"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11trc1j", "is_robot_indexable": true, "report_reasons": null, "author": "arimbr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11trc1j/testing_with_dbtexpectations_and_how_to_avoid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datafold.com/blog/dbt-expectations", "subreddit_subscribers": 93373, "created_utc": 1679060203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a working prototype of Python code in a Jupyter notebook that downloads data onto my local machine.\n\nHow do I set this script up such that it automatically runs on a schedule and downloads data to AWS S3? I\u2019ve read AWS Glue, Lambda and maybe an EC2 instance but not sure which is the \u201cbest\u201d (cheapest + reliable) approach. Any thoughts?\n\nWe are using Snowflake as our data warehouse.", "author_fullname": "t2_55wqn55d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best AWS service to run a Jupyter-like Python script calling an API to download data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tqzbe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679060436.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679059355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a working prototype of Python code in a Jupyter notebook that downloads data onto my local machine.&lt;/p&gt;\n\n&lt;p&gt;How do I set this script up such that it automatically runs on a schedule and downloads data to AWS S3? I\u2019ve read AWS Glue, Lambda and maybe an EC2 instance but not sure which is the \u201cbest\u201d (cheapest + reliable) approach. Any thoughts?&lt;/p&gt;\n\n&lt;p&gt;We are using Snowflake as our data warehouse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tqzbe", "is_robot_indexable": true, "report_reasons": null, "author": "ahkd13", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tqzbe/whats_the_best_aws_service_to_run_a_jupyterlike/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tqzbe/whats_the_best_aws_service_to_run_a_jupyterlike/", "subreddit_subscribers": 93373, "created_utc": 1679059355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If data lineage is the answer, what is the question?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "name": "t3_11tqb21", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_TItfra7pyByofbcpBLC06ZOlCYe2J0y526nFSSnIWw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679057700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/alvin-ai/if-data-lineage-is-the-answer-what-is-the-question-bad7f5f44fb5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?auto=webp&amp;v=enabled&amp;s=d27bfd253e22b985287197b107b744b6550b031c", "width": 1200, "height": 696}, "resolutions": [{"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc01f40a9c10113f1d59d4d24e9deacf656416f2", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc62a9d964b4d98d461bbf6480e9824f39a2c665", "width": 216, "height": 125}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cd3d26376ad33414283bbe83b7ff8db5970c4ab", "width": 320, "height": 185}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02916f59b8dbfdeb7a82aad9b880b28b96c35da7", "width": 640, "height": 371}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=849d6b7a9ecbcc3fece2986d2781d3fdd6ba49f9", "width": 960, "height": 556}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34c1fe8d94099f6cc31ef32dbd91d34a8cc26926", "width": 1080, "height": 626}], "variants": {}, "id": "_L6DDNGNDIlJSC0W9Hx_1Lonz-C-2itmin-V2GuRmtU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tqb21", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tqb21/if_data_lineage_is_the_answer_what_is_the_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/alvin-ai/if-data-lineage-is-the-answer-what-is-the-question-bad7f5f44fb5", "subreddit_subscribers": 93373, "created_utc": 1679057700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have captured some change data from a MySql database, serialized it with Avro converter and stored it in a Kafka topic. Now I am trying to read back that Avro serialized data from Kafka. I can easily get the serialized data but now I need to deserialize it. How do I do that? \n\nNote: I am using Pyspark to write my spark code. And I am creating a readingStream to stream the data in real-time from the Kafka topic. \n\nI found some tools for Spark(scala libraries) but the same solution for Pyspark is a bit complicated.\n\nI think using Abris [https://github.com/AbsaOSS/ABRiS](https://github.com/AbsaOSS/ABRiS) is a good choice as it supports for the python version of spark but I cannot seem to properly understand this scala written library's integration in Pyspark(Python). It's a bit complicated as I have not used non-python written libraries in Python. \n\nDoes someone have another better alternative for this use case? Another tool maybe. Or can anybody please make me understand this Abris integration for Pyspark. Thank you! [https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md](https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md)", "author_fullname": "t2_asyk1tug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tool do I use to serialize/deserialize Avro messages stored in a Kafka topic with schema registered in the schema-registry using Pyspark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tp0si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679054328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have captured some change data from a MySql database, serialized it with Avro converter and stored it in a Kafka topic. Now I am trying to read back that Avro serialized data from Kafka. I can easily get the serialized data but now I need to deserialize it. How do I do that? &lt;/p&gt;\n\n&lt;p&gt;Note: I am using Pyspark to write my spark code. And I am creating a readingStream to stream the data in real-time from the Kafka topic. &lt;/p&gt;\n\n&lt;p&gt;I found some tools for Spark(scala libraries) but the same solution for Pyspark is a bit complicated.&lt;/p&gt;\n\n&lt;p&gt;I think using Abris &lt;a href=\"https://github.com/AbsaOSS/ABRiS\"&gt;https://github.com/AbsaOSS/ABRiS&lt;/a&gt; is a good choice as it supports for the python version of spark but I cannot seem to properly understand this scala written library&amp;#39;s integration in Pyspark(Python). It&amp;#39;s a bit complicated as I have not used non-python written libraries in Python. &lt;/p&gt;\n\n&lt;p&gt;Does someone have another better alternative for this use case? Another tool maybe. Or can anybody please make me understand this Abris integration for Pyspark. Thank you! &lt;a href=\"https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md\"&gt;https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?auto=webp&amp;v=enabled&amp;s=387171c784b9c3f45187c8d9f24b3674cbc1bb52", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1207a3960e5e55a82a8218b1b8b8abad78f49d69", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b6ac0dfd8920ccd177bef60a16308a959182d86", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d1dfd547b9ef352820ceb8894e689487d85430a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06f4b989a61c0df8e9a7938d2f45578cda6d7b10", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54d89f9fd5feabc2bb26800df2db77ff757a8d2d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa6dbe4194ff44d5c41148ee6bba5f28a9249f59", "width": 1080, "height": 540}], "variants": {}, "id": "HE64KdT2pM2hXXrwojeHuMQKclceURZ0rGPldjhg8rE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tp0si", "is_robot_indexable": true, "report_reasons": null, "author": "unixparadox", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tp0si/what_tool_do_i_use_to_serializedeserialize_avro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tp0si/what_tool_do_i_use_to_serializedeserialize_avro/", "subreddit_subscribers": 93373, "created_utc": 1679054328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6hz5qk0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Protecting Your Pipeline From Malformed JSON/XML With Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11tdp0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ST2DKKJRZzU4dei-s8xcwgl9n-J2ZL9WrC5APVpUY5A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679017871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "phdata.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.phdata.io/blog/protecting-your-pipeline-from-malformed-json-xml-with-snowflake/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?auto=webp&amp;v=enabled&amp;s=166741237f2ff9f126f3c68ab2eaf5f27a936e86", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61d81a79c634171f1c879ed732c8fdb220db7f68", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a73d0d5d310a5266e637a82e544d19baf038c13", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f91cda5eb0a8ec12e1449e6590b4a26b18201956", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7df6bd85fb0d4bd72286c99a00d29059cd015f7b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f475471aa9ae20d58baee70c4eb75701a3816b28", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f8361f5d6e89e2e1066297a2e4352a15a77699b", "width": 1080, "height": 565}], "variants": {}, "id": "MDtCGjZfX66jP1QvdxkxHn3ZJDw5pZJcWoGOOSe5NdU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tdp0e", "is_robot_indexable": true, "report_reasons": null, "author": "OptimizedGradient", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tdp0e/protecting_your_pipeline_from_malformed_jsonxml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.phdata.io/blog/protecting-your-pipeline-from-malformed-json-xml-with-snowflake/", "subreddit_subscribers": 93373, "created_utc": 1679017871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all I'm just in the middle of architecting a system where an important requirement is going to be storing information on \"where\" it went. Key info we'll need to store is\n\n\\- Where it went\n\n\\- Schema + Evolution\n\n\\- Arbitrary information about the size and shape of the dataset.\n\n\\- Accessible programatically/can have an API put in front of it.\n\n\\- Works with Datasets stored In AWS that might be in the Multi-TB/PB range.\n\nI'm obviously looking at Hive but at the moment we aren't a JVM company and that requires us to start worrying about Java world  as well (although granted some of our workloads are in Databricks and I've worked in Scala before so it's probably manageable). Are there any less cumbersome options for storing information about datasets around nowadays?\n\nThus far most of the options are things like [https://datahubproject.io/](https://datahubproject.io/) where I'm a little dubious about what it does and it's a little vague on if it's just storing metadata or doing transformations.\n\nI am working with a proprietary data processing system so ideally if it can be extended to cover that as one of our metadata sources too this would also be great if not a show-stopper. ", "author_fullname": "t2_9u69ulzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Metastores/Data Catalogs that aren't Hive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sws79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678979543.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all I&amp;#39;m just in the middle of architecting a system where an important requirement is going to be storing information on &amp;quot;where&amp;quot; it went. Key info we&amp;#39;ll need to store is&lt;/p&gt;\n\n&lt;p&gt;- Where it went&lt;/p&gt;\n\n&lt;p&gt;- Schema + Evolution&lt;/p&gt;\n\n&lt;p&gt;- Arbitrary information about the size and shape of the dataset.&lt;/p&gt;\n\n&lt;p&gt;- Accessible programatically/can have an API put in front of it.&lt;/p&gt;\n\n&lt;p&gt;- Works with Datasets stored In AWS that might be in the Multi-TB/PB range.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m obviously looking at Hive but at the moment we aren&amp;#39;t a JVM company and that requires us to start worrying about Java world  as well (although granted some of our workloads are in Databricks and I&amp;#39;ve worked in Scala before so it&amp;#39;s probably manageable). Are there any less cumbersome options for storing information about datasets around nowadays?&lt;/p&gt;\n\n&lt;p&gt;Thus far most of the options are things like &lt;a href=\"https://datahubproject.io/\"&gt;https://datahubproject.io/&lt;/a&gt; where I&amp;#39;m a little dubious about what it does and it&amp;#39;s a little vague on if it&amp;#39;s just storing metadata or doing transformations.&lt;/p&gt;\n\n&lt;p&gt;I am working with a proprietary data processing system so ideally if it can be extended to cover that as one of our metadata sources too this would also be great if not a show-stopper. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sws79", "is_robot_indexable": true, "report_reasons": null, "author": "tdatas", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sws79/metastoresdata_catalogs_that_arent_hive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sws79/metastoresdata_catalogs_that_arent_hive/", "subreddit_subscribers": 93373, "created_utc": 1678979359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am migrating R code from on-prem to Databricks. I learned this week that I can't use readRDS() of .rds object in S3 like reading a CSV. \n\nAny ideas? I could try to convert to a CSV but these don't have the same shape. \n\nCan I read from DBFS?", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can you read R .rds files in Databricks/Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11tsww1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679063967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am migrating R code from on-prem to Databricks. I learned this week that I can&amp;#39;t use readRDS() of .rds object in S3 like reading a CSV. &lt;/p&gt;\n\n&lt;p&gt;Any ideas? I could try to convert to a CSV but these don&amp;#39;t have the same shape. &lt;/p&gt;\n\n&lt;p&gt;Can I read from DBFS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tsww1", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tsww1/how_can_you_read_r_rds_files_in_databricksspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tsww1/how_can_you_read_r_rds_files_in_databricksspark/", "subreddit_subscribers": 93373, "created_utc": 1679063967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\nI am working on building a property management software, and I wanted to know how could I migrate data from one software to another to make customer onboarding easier. \n\nIs there a third party tool to help with this process? If so how would it work? \n\nThank you.", "author_fullname": "t2_m39vpgyd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to migrate data from one software to another?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11ts4oz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679062106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I am working on building a property management software, and I wanted to know how could I migrate data from one software to another to make customer onboarding easier. &lt;/p&gt;\n\n&lt;p&gt;Is there a third party tool to help with this process? If so how would it work? &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11ts4oz", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial-Art-9322", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ts4oz/how_to_migrate_data_from_one_software_to_another/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ts4oz/how_to_migrate_data_from_one_software_to_another/", "subreddit_subscribers": 93373, "created_utc": 1679062106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need help. I am trying to migrate data from GCP BigQuery to AWS S3 by using Glue as the data integration tool. The Glue job will run daily and will only get the data of today and yesterday. However, there must be a better way. The problem with this approach is that this will introduce duplicates to the data, not to mention, additional processing time for the Glue job as well. This can easily be resolved by using DISTINCT by the data consumers. However, is there any means to retrieve only the deltas from the BigQuery table. Or is there a good approach to do this?", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BigQuery to S3 using Glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11trua2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679061432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help. I am trying to migrate data from GCP BigQuery to AWS S3 by using Glue as the data integration tool. The Glue job will run daily and will only get the data of today and yesterday. However, there must be a better way. The problem with this approach is that this will introduce duplicates to the data, not to mention, additional processing time for the Glue job as well. This can easily be resolved by using DISTINCT by the data consumers. However, is there any means to retrieve only the deltas from the BigQuery table. Or is there a good approach to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11trua2", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11trua2/bigquery_to_s3_using_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11trua2/bigquery_to_s3_using_glue/", "subreddit_subscribers": 93373, "created_utc": 1679061432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working on a generic framework to write data to Confluent kafka via pyspark from a relational database \nI have a config table where I enter what table from OLTP to send to what kafka topic (it\u2019s always one to one)\nI then loop in each entry in the config table to write to kafka topics via pyspark \nIs there a way to write simultaneously to all kafka topics instead of looping \nAnd do you see any problems with this approach? \nI just want to know how others are doing this in their organization", "author_fullname": "t2_7pvpke89", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to write to multiple topics parallelly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t90ca", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679007070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working on a generic framework to write data to Confluent kafka via pyspark from a relational database \nI have a config table where I enter what table from OLTP to send to what kafka topic (it\u2019s always one to one)\nI then loop in each entry in the config table to write to kafka topics via pyspark \nIs there a way to write simultaneously to all kafka topics instead of looping \nAnd do you see any problems with this approach? \nI just want to know how others are doing this in their organization&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t90ca", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Channel9", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t90ca/how_to_write_to_multiple_topics_parallelly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t90ca/how_to_write_to_multiple_topics_parallelly/", "subreddit_subscribers": 93373, "created_utc": 1679007070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.youtube.com/watch?v=grTslxZIvYE](https://www.youtube.com/watch?v=grTslxZIvYE)", "author_fullname": "t2_vlp8q84d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Video] GitOps with CNDI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t6wwx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679002248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=grTslxZIvYE\"&gt;https://www.youtube.com/watch?v=grTslxZIvYE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?auto=webp&amp;v=enabled&amp;s=87378b018d849b5c67cb1c7c69ab1490b6aa8fb6", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e47b74ba97087b2b7baac6b3ff95eb1af8692e16", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c4ee588a07e25513a169f0e933936d56931cb37", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0bccbeb4ebae519c4707118099faba916361927", "width": 320, "height": 240}], "variants": {}, "id": "3JT3y1GjPSMCoHAar5uh1PiFhEZ_12wFlRuQbpF4aRE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11t6wwx", "is_robot_indexable": true, "report_reasons": null, "author": "SheldonMackay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t6wwx/video_gitops_with_cndi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t6wwx/video_gitops_with_cndi/", "subreddit_subscribers": 93373, "created_utc": 1679002248.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}