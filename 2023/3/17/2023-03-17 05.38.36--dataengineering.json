{"kind": "Listing", "data": {"after": "t3_11tdp0e", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I see this sentiment a lot in this sub. Seems to be mainly from analytics engineer types who are focused on data modeling inside the warehouse. The reasoning is normally along the lines of \"tools like fivetran and airbyte have connectors for everything, so no need to write integrations for anything, we can just deploy those and get on with the real work\"\n\nWhile on some level this is true, i really do feel like its missing a big part of the picture. For one it doesn't consider streaming and real time data but that's a debate for another time. The big problem with the above vision for me is it's overly focused on just lifting and shifting data from OLTP prod systems, with no consideration for things like data quality, schema validation, schema evolution, data contracts etc. To me it's overly coupled to the specific OLTP technologies used. Shouldn't we be looking to see OLTP systems wrapped in an interface that doesn't rely on internal implementation details?\n\nCurious to hear other people's thoughts on this as it's something that's been bothering me lately.", "author_fullname": "t2_18xb5x05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"ingestion is a solved problem\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sq68k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 67, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 67, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678962424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see this sentiment a lot in this sub. Seems to be mainly from analytics engineer types who are focused on data modeling inside the warehouse. The reasoning is normally along the lines of &amp;quot;tools like fivetran and airbyte have connectors for everything, so no need to write integrations for anything, we can just deploy those and get on with the real work&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;While on some level this is true, i really do feel like its missing a big part of the picture. For one it doesn&amp;#39;t consider streaming and real time data but that&amp;#39;s a debate for another time. The big problem with the above vision for me is it&amp;#39;s overly focused on just lifting and shifting data from OLTP prod systems, with no consideration for things like data quality, schema validation, schema evolution, data contracts etc. To me it&amp;#39;s overly coupled to the specific OLTP technologies used. Shouldn&amp;#39;t we be looking to see OLTP systems wrapped in an interface that doesn&amp;#39;t rely on internal implementation details?&lt;/p&gt;\n\n&lt;p&gt;Curious to hear other people&amp;#39;s thoughts on this as it&amp;#39;s something that&amp;#39;s been bothering me lately.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sq68k", "is_robot_indexable": true, "report_reasons": null, "author": "the-data-scientist", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sq68k/ingestion_is_a_solved_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sq68k/ingestion_is_a_solved_problem/", "subreddit_subscribers": 93335, "created_utc": 1678962424.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vxxrqrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tracking the Fake Stars Market with Dagster, BigQuery and dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4j09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KuVo6LkTRmwrRlRoeKwFt7dkpiuezOmsmBagrxsDfbk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678996759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/fake-stars", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?auto=webp&amp;v=enabled&amp;s=6feffd3ff633a036f59a08f8594eb3f065bbe3b1", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e360f3ccc3f8c501c3fd34b0f17d0904a271132", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f629d6c060d739672ab5769feef32945ed0b547", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ce2426c05b366dc0e7390f6941fdcf369417314", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3282917840b4991ab1509ce8ba7daf2e8acba140", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70e65fb12076a711151ddba54cd9469adc644038", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/42HvhMX5zTUPW9n_2K4SWQb-CPZWTfGzO4yMjnEfamc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b91cbdc9fd1f26d26ae4c64763b11a5f8453325a", "width": 1080, "height": 567}], "variants": {}, "id": "eKvtnMcXWrAKAAoUL_SszTvxbrdxPwsxAJNSYYV7yNQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11t4j09", "is_robot_indexable": true, "report_reasons": null, "author": "MrMosBiggestFan", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4j09/tracking_the_fake_stars_market_with_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/fake-stars", "subreddit_subscribers": 93335, "created_utc": 1678996759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering what the community has used for moving petabytes (say 100+) of data from a clustered of databases to another (lets say from Mysql &amp; Postgres to snowflake)?\n\n&amp;#x200B;\n\nin my old life I'd use standard Microsoft tech using BCP wrapped in Powershell, honestly I was quite divorced from the whole process since it was cookie cutter templates that my old team developed.\n\nWhat I've seen so far -\n\n&amp;#x200B;\n\n* JDBC connectors to incrementally load data - works well if you only have a few TBs of data and can switch to a CDC approach to avoid reloading in future.\n* BCP/ Bulk flat file exports to Blob / S3 then stage that into Snowflake / Synapse / BQ / Redshift.\n* off-the shelf solutions for cloud owned databases (Aurora RDS -&gt; Redshift, Datafactory(SQL Server) -&gt; Synapse / Databricks).\n* EL tools like Airbyte, Fivetran, Stitch?\n\n&amp;#x200B;\n\nwith regards to EL tools - are they using some magic to lift and shift massive amounts of data - or is it all JDBC with throttling as you don't want to kill the databases? Looking at the source code for Airbyte I know it's a incremental batched process to persist data into memory and move it to an external location to stage.\n\nhappy to hear some stories from the more experience folk.", "author_fullname": "t2_7iuhjtv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving Petabytes of Data from DBs to a DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11spizb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678962352.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678960288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering what the community has used for moving petabytes (say 100+) of data from a clustered of databases to another (lets say from Mysql &amp;amp; Postgres to snowflake)?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;in my old life I&amp;#39;d use standard Microsoft tech using BCP wrapped in Powershell, honestly I was quite divorced from the whole process since it was cookie cutter templates that my old team developed.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve seen so far -&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;JDBC connectors to incrementally load data - works well if you only have a few TBs of data and can switch to a CDC approach to avoid reloading in future.&lt;/li&gt;\n&lt;li&gt;BCP/ Bulk flat file exports to Blob / S3 then stage that into Snowflake / Synapse / BQ / Redshift.&lt;/li&gt;\n&lt;li&gt;off-the shelf solutions for cloud owned databases (Aurora RDS -&amp;gt; Redshift, Datafactory(SQL Server) -&amp;gt; Synapse / Databricks).&lt;/li&gt;\n&lt;li&gt;EL tools like Airbyte, Fivetran, Stitch?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;with regards to EL tools - are they using some magic to lift and shift massive amounts of data - or is it all JDBC with throttling as you don&amp;#39;t want to kill the databases? Looking at the source code for Airbyte I know it&amp;#39;s a incremental batched process to persist data into memory and move it to an external location to stage.&lt;/p&gt;\n\n&lt;p&gt;happy to hear some stories from the more experience folk.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11spizb", "is_robot_indexable": true, "report_reasons": null, "author": "Omar_88", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11spizb/moving_petabytes_of_data_from_dbs_to_a_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11spizb/moving_petabytes_of_data_from_dbs_to_a_dwh/", "subreddit_subscribers": 93335, "created_utc": 1678960288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title", "author_fullname": "t2_c6w52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "do you think Zhamak has any idea how much time has now been wasted on orgs discussing if we should 'do a data mesh'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t1e0u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678989600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t1e0u", "is_robot_indexable": true, "report_reasons": null, "author": "EmergenL", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t1e0u/do_you_think_zhamak_has_any_idea_how_much_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t1e0u/do_you_think_zhamak_has_any_idea_how_much_time/", "subreddit_subscribers": 93335, "created_utc": 1678989600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In case you\u2019re not aware, AWS snowmobile is a service where AWS will park up a 45ft shipping container full of SSDs in order to transfer your data\u2014 up to 100 PB of it \u2014 to S3.\n\nI\u2019m curious if anyone here\u2019s actually used this service?", "author_fullname": "t2_4wbqkds2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ever used AWS Snowmobile?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t3ic9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678994370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In case you\u2019re not aware, AWS snowmobile is a service where AWS will park up a 45ft shipping container full of SSDs in order to transfer your data\u2014 up to 100 PB of it \u2014 to S3.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m curious if anyone here\u2019s actually used this service?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t3ic9", "is_robot_indexable": true, "report_reasons": null, "author": "bolivlake", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t3ic9/ever_used_aws_snowmobile/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t3ic9/ever_used_aws_snowmobile/", "subreddit_subscribers": 93335, "created_utc": 1678994370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all. I recently started a position at a manufacturer where the entire IT and data process is done through SAP. They are trying to slowly get themselves off of SAP and hired me due to my experience in more modern ETL/Data platforms.\nI was just wondering if anyone here had experience in modernizing an SAP platform and the best tools for the job. Thanks", "author_fullname": "t2_6ra3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with SAP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11swnax", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all. I recently started a position at a manufacturer where the entire IT and data process is done through SAP. They are trying to slowly get themselves off of SAP and hired me due to my experience in more modern ETL/Data platforms.\nI was just wondering if anyone here had experience in modernizing an SAP platform and the best tools for the job. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11swnax", "is_robot_indexable": true, "report_reasons": null, "author": "deemerritt", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11swnax/working_with_sap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11swnax/working_with_sap/", "subreddit_subscribers": 93335, "created_utc": 1678979041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am aware of various data catalogs such as DataHub, Amundsen and so on that offer data lineage capability but is there any tool offering *solely* the lineage without all the stuff around? (I am thinking of building \"custom data catalog\" but I would need some \"help\" with the lineage)", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Standalone lineage tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4onj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678997107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am aware of various data catalogs such as DataHub, Amundsen and so on that offer data lineage capability but is there any tool offering &lt;em&gt;solely&lt;/em&gt; the lineage without all the stuff around? (I am thinking of building &amp;quot;custom data catalog&amp;quot; but I would need some &amp;quot;help&amp;quot; with the lineage)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t4onj", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4onj/standalone_lineage_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t4onj/standalone_lineage_tool/", "subreddit_subscribers": 93335, "created_utc": 1678997107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My day to day job mostly uses GBQ and dbt. So, I was doing a little bit of research on orchestration tools. I was looking at AirFlow initially. It looked complicated (at least the installation part)\n\nSo, I installed Prefect to my local machine and connected it to Prefect cloud. And I was experimenting.\n\nSo, this is what I did using Prefect.\n- Extract data from our production DB. I used pandas to load the tables. The table has about 2000 rows with about 100 columns.\n- There were some sensitive information. I hashed it as a transformation stage. And converted all the fields into string (if not it throws an error when I load it into GBQ)\n- Loaded it to GBQ\n\nI have a few questions here. Is Prefect overkill for my use case? What could have done better? \n\nLet\u2019s say if I have to update the tables in GBQ every 7 days. Is it possible to schedule it on Prefect? For scheduling part, do we need to have a VM running 24/7? \n\nWould appreciate your feedbacks. Not gonna lie, when the job was successful. It felt like an achievement.\n\nThank you.", "author_fullname": "t2_emzh9atv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created my first Prefect workflow.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sve4i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678976150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My day to day job mostly uses GBQ and dbt. So, I was doing a little bit of research on orchestration tools. I was looking at AirFlow initially. It looked complicated (at least the installation part)&lt;/p&gt;\n\n&lt;p&gt;So, I installed Prefect to my local machine and connected it to Prefect cloud. And I was experimenting.&lt;/p&gt;\n\n&lt;p&gt;So, this is what I did using Prefect.\n- Extract data from our production DB. I used pandas to load the tables. The table has about 2000 rows with about 100 columns.\n- There were some sensitive information. I hashed it as a transformation stage. And converted all the fields into string (if not it throws an error when I load it into GBQ)\n- Loaded it to GBQ&lt;/p&gt;\n\n&lt;p&gt;I have a few questions here. Is Prefect overkill for my use case? What could have done better? &lt;/p&gt;\n\n&lt;p&gt;Let\u2019s say if I have to update the tables in GBQ every 7 days. Is it possible to schedule it on Prefect? For scheduling part, do we need to have a VM running 24/7? &lt;/p&gt;\n\n&lt;p&gt;Would appreciate your feedbacks. Not gonna lie, when the job was successful. It felt like an achievement.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sve4i", "is_robot_indexable": true, "report_reasons": null, "author": "MaintenanceSad6825", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sve4i/i_created_my_first_prefect_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sve4i/i_created_my_first_prefect_workflow/", "subreddit_subscribers": 93335, "created_utc": 1678976150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a tool like Airbyte but for unstructured files? \n\nSomething that transfers between different file systems or object storage systems like S3 but not imposing a structured schema is what I'm looking for. The replication, extract/load and auth functiona are really what I want to achieve - no transform functionality is necessary.\n\nLike S3 to sftp, https to filesystem, etc.", "author_fullname": "t2_4i8pl31g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tool like Airbyte but for unstructured data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sucai", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678974579.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678973665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a tool like Airbyte but for unstructured files? &lt;/p&gt;\n\n&lt;p&gt;Something that transfers between different file systems or object storage systems like S3 but not imposing a structured schema is what I&amp;#39;m looking for. The replication, extract/load and auth functiona are really what I want to achieve - no transform functionality is necessary.&lt;/p&gt;\n\n&lt;p&gt;Like S3 to sftp, https to filesystem, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sucai", "is_robot_indexable": true, "report_reasons": null, "author": "knowledgebass", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sucai/tool_like_airbyte_but_for_unstructured_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sucai/tool_like_airbyte_but_for_unstructured_data/", "subreddit_subscribers": 93335, "created_utc": 1678973665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all--\n\nI'm learning DAGs and want to create some data ingress pipelines to test my Airflow knowledge. \n\nDoes anyone have experience with social media and some good data points to consume from Twitter and Facebook, say, for this type of project? It could be specific to a user or users, or general like popular trends or something.\n\nThanks for the ideas!", "author_fullname": "t2_3r56zv6e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Social Media Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t2wna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678992985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all--&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m learning DAGs and want to create some data ingress pipelines to test my Airflow knowledge. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with social media and some good data points to consume from Twitter and Facebook, say, for this type of project? It could be specific to a user or users, or general like popular trends or something.&lt;/p&gt;\n\n&lt;p&gt;Thanks for the ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t2wna", "is_robot_indexable": true, "report_reasons": null, "author": "qa_anaaq", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t2wna/creating_social_media_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t2wna/creating_social_media_data_pipelines/", "subreddit_subscribers": 93335, "created_utc": 1678992985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There has been a ton of buzz around AI automation tools and how they will affect the job market. I would like to start a discussion around the inevitable change to skills and what direction they will take in the next 5-10 years...\n\n* I believe that jobs will increasingly be focused on higher-level architectural decisions and not so much getting into the nitty gritty since code generation tools are getting increasingly sophisticated and there is no reason to think that won't continue...\n   * This can essentially be boiled down to more Senior type roles, but at the same time-you need junior/mid people to get experience to replace the senior roles as they inevitably retire/quit\n* I think it can be safe to say that the main limitation for any software/tech to be created is time/budget...\n   * Would we then say that if efficiency is dramatically increased, then the # of projects would increase dramatically as well? \n   * And therefore the jobs available would shift to a more administrative/supervisory role, but there would be just as many or more jobs?\n\nThis video below (at the linked timestamp, only about the last 6 mins of the video) explains well what I foresee as the future:\n\n[https://youtu.be/brAwZ5l\\_fuQ?t=626](https://youtu.be/brAwZ5l_fuQ?t=626)\n\nEssentially, I see over time, all programming jobs be **decreasingly technical** as code generation tools become inevitably more sophisticated, but **increasingly soft skill and communication oriented**\\--however, still at similar or more # of jobs...\n\nWhat do you all think of my prognosis?", "author_fullname": "t2_8chdw7c4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Future skills for Data/Software Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t1uyj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678990613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There has been a ton of buzz around AI automation tools and how they will affect the job market. I would like to start a discussion around the inevitable change to skills and what direction they will take in the next 5-10 years...&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I believe that jobs will increasingly be focused on higher-level architectural decisions and not so much getting into the nitty gritty since code generation tools are getting increasingly sophisticated and there is no reason to think that won&amp;#39;t continue...\n\n&lt;ul&gt;\n&lt;li&gt;This can essentially be boiled down to more Senior type roles, but at the same time-you need junior/mid people to get experience to replace the senior roles as they inevitably retire/quit&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;I think it can be safe to say that the main limitation for any software/tech to be created is time/budget...\n\n&lt;ul&gt;\n&lt;li&gt;Would we then say that if efficiency is dramatically increased, then the # of projects would increase dramatically as well? &lt;/li&gt;\n&lt;li&gt;And therefore the jobs available would shift to a more administrative/supervisory role, but there would be just as many or more jobs?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This video below (at the linked timestamp, only about the last 6 mins of the video) explains well what I foresee as the future:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/brAwZ5l_fuQ?t=626\"&gt;https://youtu.be/brAwZ5l_fuQ?t=626&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Essentially, I see over time, all programming jobs be &lt;strong&gt;decreasingly technical&lt;/strong&gt; as code generation tools become inevitably more sophisticated, but &lt;strong&gt;increasingly soft skill and communication oriented&lt;/strong&gt;--however, still at similar or more # of jobs...&lt;/p&gt;\n\n&lt;p&gt;What do you all think of my prognosis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iHmNDIO6M2jByk6BGzE3XTp2KkA0TFzliN8WEazFfaQ.jpg?auto=webp&amp;v=enabled&amp;s=04c0cfc33be89e72dcaaee261323f9d8d2a0b8eb", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/iHmNDIO6M2jByk6BGzE3XTp2KkA0TFzliN8WEazFfaQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02bbc39968a4a5c0d87b9c65a0e740d10c92de9c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/iHmNDIO6M2jByk6BGzE3XTp2KkA0TFzliN8WEazFfaQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c64b19ddf31eb8bef505ecd4aeda48ff1a20aae5", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/iHmNDIO6M2jByk6BGzE3XTp2KkA0TFzliN8WEazFfaQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a280ac8511e1e3a0cca8dd5dde8fcbaad89795ff", "width": 320, "height": 240}], "variants": {}, "id": "cMMcewgb0PyFU06EC1RGs3JSQzECcVXzdJLqJZFMM2A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t1uyj", "is_robot_indexable": true, "report_reasons": null, "author": "pdxtechnologist", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t1uyj/future_skills_for_datasoftware_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t1uyj/future_skills_for_datasoftware_engineering/", "subreddit_subscribers": 93335, "created_utc": 1678990613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all I'm just in the middle of architecting a system where an important requirement is going to be storing information on \"where\" it went. Key info we'll need to store is\n\n\\- Where it went\n\n\\- Schema + Evolution\n\n\\- Arbitrary information about the size and shape of the dataset.\n\n\\- Accessible programatically/can have an API put in front of it.\n\n\\- Works with Datasets stored In AWS that might be in the Multi-TB/PB range.\n\nI'm obviously looking at Hive but at the moment we aren't a JVM company and that requires us to start worrying about Java world  as well (although granted some of our workloads are in Databricks and I've worked in Scala before so it's probably manageable). Are there any less cumbersome options for storing information about datasets around nowadays?\n\nThus far most of the options are things like [https://datahubproject.io/](https://datahubproject.io/) where I'm a little dubious about what it does and it's a little vague on if it's just storing metadata or doing transformations.\n\nI am working with a proprietary data processing system so ideally if it can be extended to cover that as one of our metadata sources too this would also be great if not a show-stopper. ", "author_fullname": "t2_9u69ulzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Metastores/Data Catalogs that aren't Hive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sws79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678979543.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all I&amp;#39;m just in the middle of architecting a system where an important requirement is going to be storing information on &amp;quot;where&amp;quot; it went. Key info we&amp;#39;ll need to store is&lt;/p&gt;\n\n&lt;p&gt;- Where it went&lt;/p&gt;\n\n&lt;p&gt;- Schema + Evolution&lt;/p&gt;\n\n&lt;p&gt;- Arbitrary information about the size and shape of the dataset.&lt;/p&gt;\n\n&lt;p&gt;- Accessible programatically/can have an API put in front of it.&lt;/p&gt;\n\n&lt;p&gt;- Works with Datasets stored In AWS that might be in the Multi-TB/PB range.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m obviously looking at Hive but at the moment we aren&amp;#39;t a JVM company and that requires us to start worrying about Java world  as well (although granted some of our workloads are in Databricks and I&amp;#39;ve worked in Scala before so it&amp;#39;s probably manageable). Are there any less cumbersome options for storing information about datasets around nowadays?&lt;/p&gt;\n\n&lt;p&gt;Thus far most of the options are things like &lt;a href=\"https://datahubproject.io/\"&gt;https://datahubproject.io/&lt;/a&gt; where I&amp;#39;m a little dubious about what it does and it&amp;#39;s a little vague on if it&amp;#39;s just storing metadata or doing transformations.&lt;/p&gt;\n\n&lt;p&gt;I am working with a proprietary data processing system so ideally if it can be extended to cover that as one of our metadata sources too this would also be great if not a show-stopper. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sws79", "is_robot_indexable": true, "report_reasons": null, "author": "tdatas", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sws79/metastoresdata_catalogs_that_arent_hive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sws79/metastoresdata_catalogs_that_arent_hive/", "subreddit_subscribers": 93335, "created_utc": 1678979359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my job I've inherited a Synapse Workspace with a dedicated SQL Pool serverless.\nOn this serverless we do have a SQL Server 2012 and I was wondering if there was a way to run some health checks (like you run scandisk on your memory).\n\nDoes someone know how to achieve it? Or which guidelines should be used?", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you check if your DWH is not corrupted?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11su26h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678973007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my job I&amp;#39;ve inherited a Synapse Workspace with a dedicated SQL Pool serverless.\nOn this serverless we do have a SQL Server 2012 and I was wondering if there was a way to run some health checks (like you run scandisk on your memory).&lt;/p&gt;\n\n&lt;p&gt;Does someone know how to achieve it? Or which guidelines should be used?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11su26h", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11su26h/how_do_you_check_if_your_dwh_is_not_corrupted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11su26h/how_do_you_check_if_your_dwh_is_not_corrupted/", "subreddit_subscribers": 93335, "created_utc": 1678973007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are on-premises RTU'S which are currently using the medina protocol and Palo Alto gateway. I would like to build a data pipeline to transfer the data from on-premises to AWS cloud through VPN. \n\nAdditional info, there is a postgres database running in an ec2 instance in which the data is being stored currently. \n\nWhat can I do to achieve this?", "author_fullname": "t2_6wgyvyl5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DATA pipeline from on premise RTU's to AWS Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11smn5v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678953699.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678949594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are on-premises RTU&amp;#39;S which are currently using the medina protocol and Palo Alto gateway. I would like to build a data pipeline to transfer the data from on-premises to AWS cloud through VPN. &lt;/p&gt;\n\n&lt;p&gt;Additional info, there is a postgres database running in an ec2 instance in which the data is being stored currently. &lt;/p&gt;\n\n&lt;p&gt;What can I do to achieve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11smn5v", "is_robot_indexable": true, "report_reasons": null, "author": "aimonster7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11smn5v/data_pipeline_from_on_premise_rtus_to_aws_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11smn5v/data_pipeline_from_on_premise_rtus_to_aws_cloud/", "subreddit_subscribers": 93335, "created_utc": 1678949594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know of a good solution for a self-hosted columnar DB? I have a small Linux machine that is hosting a Postgres server. Setting it up and loading in some test data was seamless, but it hasn't been able to scale with the types of analytical queries I want to run. \n\nI've looked into the citus extension, but unfortunately it doesn't support ARM architecture (which is what the linux server has).", "author_fullname": "t2_oyypyadu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Columnar Databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11tgpmi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679026372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of a good solution for a self-hosted columnar DB? I have a small Linux machine that is hosting a Postgres server. Setting it up and loading in some test data was seamless, but it hasn&amp;#39;t been able to scale with the types of analytical queries I want to run. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked into the citus extension, but unfortunately it doesn&amp;#39;t support ARM architecture (which is what the linux server has).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tgpmi", "is_robot_indexable": true, "report_reasons": null, "author": "ringoefc", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tgpmi/open_source_columnar_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tgpmi/open_source_columnar_databases/", "subreddit_subscribers": 93335, "created_utc": 1679026372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working on a generic framework to write data to Confluent kafka via pyspark from a relational database \nI have a config table where I enter what table from OLTP to send to what kafka topic (it\u2019s always one to one)\nI then loop in each entry in the config table to write to kafka topics via pyspark \nIs there a way to write simultaneously to all kafka topics instead of looping \nAnd do you see any problems with this approach? \nI just want to know how others are doing this in their organization", "author_fullname": "t2_7pvpke89", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to write to multiple topics parallelly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t90ca", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679007070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working on a generic framework to write data to Confluent kafka via pyspark from a relational database \nI have a config table where I enter what table from OLTP to send to what kafka topic (it\u2019s always one to one)\nI then loop in each entry in the config table to write to kafka topics via pyspark \nIs there a way to write simultaneously to all kafka topics instead of looping \nAnd do you see any problems with this approach? \nI just want to know how others are doing this in their organization&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t90ca", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Channel9", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t90ca/how_to_write_to_multiple_topics_parallelly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t90ca/how_to_write_to_multiple_topics_parallelly/", "subreddit_subscribers": 93335, "created_utc": 1679007070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.youtube.com/watch?v=grTslxZIvYE](https://www.youtube.com/watch?v=grTslxZIvYE)", "author_fullname": "t2_vlp8q84d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Video] GitOps with CNDI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t6wwx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679002248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=grTslxZIvYE\"&gt;https://www.youtube.com/watch?v=grTslxZIvYE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?auto=webp&amp;v=enabled&amp;s=87378b018d849b5c67cb1c7c69ab1490b6aa8fb6", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e47b74ba97087b2b7baac6b3ff95eb1af8692e16", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c4ee588a07e25513a169f0e933936d56931cb37", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/m1ysVx-DlYBV6jtfTlXbUGvLeZWc0cL9rScFGs_8mx0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0bccbeb4ebae519c4707118099faba916361927", "width": 320, "height": 240}], "variants": {}, "id": "3JT3y1GjPSMCoHAar5uh1PiFhEZ_12wFlRuQbpF4aRE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11t6wwx", "is_robot_indexable": true, "report_reasons": null, "author": "SheldonMackay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t6wwx/video_gitops_with_cndi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t6wwx/video_gitops_with_cndi/", "subreddit_subscribers": 93335, "created_utc": 1679002248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to profile (mainly get % of nulls) several tables (and several columns in these tables) in the DWH (postgres) and thinking of the \"best\"/most efficient solution. There are data catalog that offers some simple validations, then there are specific data quality tools like great expectations or deequ, but - how these things perform on several hundreds of GB? I am afraid the profiling would be too slow (and unfortunately I do not have time budget to try our a lot of tools) so trying to learn what would be the most efficient approach here. Is there maybe some built-in metadata table in postgres that would make exploration of null values faster?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The most efficient way to profile WH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4zum", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678997829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to profile (mainly get % of nulls) several tables (and several columns in these tables) in the DWH (postgres) and thinking of the &amp;quot;best&amp;quot;/most efficient solution. There are data catalog that offers some simple validations, then there are specific data quality tools like great expectations or deequ, but - how these things perform on several hundreds of GB? I am afraid the profiling would be too slow (and unfortunately I do not have time budget to try our a lot of tools) so trying to learn what would be the most efficient approach here. Is there maybe some built-in metadata table in postgres that would make exploration of null values faster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t4zum", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4zum/the_most_efficient_way_to_profile_wh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t4zum/the_most_efficient_way_to_profile_wh/", "subreddit_subscribers": 93335, "created_utc": 1678997829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand I can create great expecations for the entire column (e.g. count of nulls) but I would need to validate % of nulls by category (by other column). Is it possible to do so with great expectations? And if not is there any alternative tool that can do it?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Great expecations - category wise expectations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t4vv1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678997571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand I can create great expecations for the entire column (e.g. count of nulls) but I would need to validate % of nulls by category (by other column). Is it possible to do so with great expectations? And if not is there any alternative tool that can do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t4vv1", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t4vv1/great_expecations_category_wise_expectations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t4vv1/great_expecations_category_wise_expectations/", "subreddit_subscribers": 93335, "created_utc": 1678997571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Typing this because I feel a little bit down... I got a couple interview questions on SQL like\n\n+ What's the difference between a CTE and a view?\n\nAnd I couldn't give a satisfactory answer, but I've been doing SQL for something close to 2 years now. Now I know a couple differences but then the best I can do is search on the internet \"intermediate &lt;technology&gt; interview questions\" and hope for the next tech interview to cover what I just read about X technology a couple days ago?\n\nI have a total of 2 YOE (a little bit than that) and my role has always been implementation, I understand why these kind of questions get asked but at the same time I feel frustrated thinking these questions should be aimed at people who will fill a tech lead role. Does this make sense?\n\nAnd finally, the question in the title, how did you get to knowing the answers to those interview questions? Just searching on the internet until it's carved in your memory? Certifications? \n\nThanks!", "author_fullname": "t2_6dbtcwt9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How did you get the knowledge needed to answer interview questions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11t31u7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678993298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Typing this because I feel a little bit down... I got a couple interview questions on SQL like&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What&amp;#39;s the difference between a CTE and a view?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;And I couldn&amp;#39;t give a satisfactory answer, but I&amp;#39;ve been doing SQL for something close to 2 years now. Now I know a couple differences but then the best I can do is search on the internet &amp;quot;intermediate &amp;lt;technology&amp;gt; interview questions&amp;quot; and hope for the next tech interview to cover what I just read about X technology a couple days ago?&lt;/p&gt;\n\n&lt;p&gt;I have a total of 2 YOE (a little bit than that) and my role has always been implementation, I understand why these kind of questions get asked but at the same time I feel frustrated thinking these questions should be aimed at people who will fill a tech lead role. Does this make sense?&lt;/p&gt;\n\n&lt;p&gt;And finally, the question in the title, how did you get to knowing the answers to those interview questions? Just searching on the internet until it&amp;#39;s carved in your memory? Certifications? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t31u7", "is_robot_indexable": true, "report_reasons": null, "author": "ebboch", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t31u7/how_did_you_get_the_knowledge_needed_to_answer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t31u7/how_did_you_get_the_knowledge_needed_to_answer/", "subreddit_subscribers": 93335, "created_utc": 1678993298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All,\n\nWhat are the best replacement tools for SSIS, which we used to execute snowflake procedures by creating SSIS packages; one that comes to mind is Airflow. Are there other tools available to perform the same task the SSIS does?\n\nThank you", "author_fullname": "t2_7qs0ir3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSIS Replacement for production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11swovi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;What are the best replacement tools for SSIS, which we used to execute snowflake procedures by creating SSIS packages; one that comes to mind is Airflow. Are there other tools available to perform the same task the SSIS does?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11swovi", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Archer3356", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11swovi/ssis_replacement_for_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11swovi/ssis_replacement_for_production/", "subreddit_subscribers": 93335, "created_utc": 1678979140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Calling all BI developers! I want to learn from your experiences in creating and maintaining dashboards. If you're involved in the development of business intelligence dashboards, we invite you to take our survey on best practices for ensuring the continuity of your dashboards. Your valuable insights can help inform our understanding of how to create effective and sustainable BI solutions. Take the survey now and share your expertise with the community! I'll share the results in a week.\n\n[https://forms.office.com/e/h3ENF9LNzQ](https://forms.office.com/e/h3ENF9LNzQ)", "author_fullname": "t2_sjc8bva0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business Intelligence Dashboard Continuity/Maintenance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sszvk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678970354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Calling all BI developers! I want to learn from your experiences in creating and maintaining dashboards. If you&amp;#39;re involved in the development of business intelligence dashboards, we invite you to take our survey on best practices for ensuring the continuity of your dashboards. Your valuable insights can help inform our understanding of how to create effective and sustainable BI solutions. Take the survey now and share your expertise with the community! I&amp;#39;ll share the results in a week.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forms.office.com/e/h3ENF9LNzQ\"&gt;https://forms.office.com/e/h3ENF9LNzQ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11sszvk", "is_robot_indexable": true, "report_reasons": null, "author": "Total_Telephone_1260", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sszvk/business_intelligence_dashboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sszvk/business_intelligence_dashboard/", "subreddit_subscribers": 93335, "created_utc": 1678970354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to come up with a modern blueprint for an on-premises data lake and data warehouse.\n\nI was thinking to use pyspark and use spark as compute engine.\n\nFor storage, I am looking at Apache Iceberg (as an alternative to postgresql, mongodb etc). \n\nHow can Iceberg best be setup in an on-premises solution as a data cluster?\n\nAre there any limitations or alternatives to Iceberg?", "author_fullname": "t2_k7zel", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Iceberg as storage for on-premise data store (cluster)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ssjny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679030794.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678969219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to come up with a modern blueprint for an on-premises data lake and data warehouse.&lt;/p&gt;\n\n&lt;p&gt;I was thinking to use pyspark and use spark as compute engine.&lt;/p&gt;\n\n&lt;p&gt;For storage, I am looking at Apache Iceberg (as an alternative to postgresql, mongodb etc). &lt;/p&gt;\n\n&lt;p&gt;How can Iceberg best be setup in an on-premises solution as a data cluster?&lt;/p&gt;\n\n&lt;p&gt;Are there any limitations or alternatives to Iceberg?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11ssjny", "is_robot_indexable": true, "report_reasons": null, "author": "hgaronfolo", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ssjny/apache_iceberg_as_storage_for_onpremise_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ssjny/apache_iceberg_as_storage_for_onpremise_data/", "subreddit_subscribers": 93335, "created_utc": 1678969219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, noobie aspiring DE, currently working as a tech lead for a service-based company so hour logging is our bread and butter kinda. So we have an internal tool that tracks work logs and costs from different projects in Jira and reports it on the slack app, but it's horrendously slow. I was recently studying Kafka and how it has very low latency. So can I consider using Kafka for the above tool to make it better? If so, can you point me in the direction of where I can start?", "author_fullname": "t2_7lo4eaq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questioning my approach with Kafka!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11slyx7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678947202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, noobie aspiring DE, currently working as a tech lead for a service-based company so hour logging is our bread and butter kinda. So we have an internal tool that tracks work logs and costs from different projects in Jira and reports it on the slack app, but it&amp;#39;s horrendously slow. I was recently studying Kafka and how it has very low latency. So can I consider using Kafka for the above tool to make it better? If so, can you point me in the direction of where I can start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11slyx7", "is_robot_indexable": true, "report_reasons": null, "author": "The_late_comer18", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11slyx7/questioning_my_approach_with_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11slyx7/questioning_my_approach_with_kafka/", "subreddit_subscribers": 93335, "created_utc": 1678947202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6hz5qk0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Protecting Your Pipeline From Malformed JSON/XML With Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11tdp0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ST2DKKJRZzU4dei-s8xcwgl9n-J2ZL9WrC5APVpUY5A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679017871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "phdata.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.phdata.io/blog/protecting-your-pipeline-from-malformed-json-xml-with-snowflake/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?auto=webp&amp;v=enabled&amp;s=166741237f2ff9f126f3c68ab2eaf5f27a936e86", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61d81a79c634171f1c879ed732c8fdb220db7f68", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a73d0d5d310a5266e637a82e544d19baf038c13", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f91cda5eb0a8ec12e1449e6590b4a26b18201956", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7df6bd85fb0d4bd72286c99a00d29059cd015f7b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f475471aa9ae20d58baee70c4eb75701a3816b28", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/elyxYDb58kXNMH14UJtIcGunnlV4V_Q0pRx2ljmusS0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f8361f5d6e89e2e1066297a2e4352a15a77699b", "width": 1080, "height": 565}], "variants": {}, "id": "MDtCGjZfX66jP1QvdxkxHn3ZJDw5pZJcWoGOOSe5NdU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tdp0e", "is_robot_indexable": true, "report_reasons": null, "author": "OptimizedGradient", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tdp0e/protecting_your_pipeline_from_malformed_jsonxml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.phdata.io/blog/protecting-your-pipeline-from-malformed-json-xml-with-snowflake/", "subreddit_subscribers": 93335, "created_utc": 1679017871.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}