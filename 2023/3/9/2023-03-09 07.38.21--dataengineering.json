{"kind": "Listing", "data": {"after": "t3_11luuwy", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't know about you, but I have plenty of data engineering horror stories to share. I'd love to hear the one that still gives you shivers.\n\n  \n**Here's my highlight:**  \n\n\n* I'm at a 500 people medium sized company, 300-400$m revenue. But we're growing strong, at 10-20% each year. Ingest data via python, dbt for transformations, storage in PostgreSQL and Tableau as BI tool on top of it.\n* We've been pushing the adoption of Tableau, and the company is eating it up, they love it. They are all over the dashboards.\n* What we're particular proud of is our **\"north star metric**\" dashboard, showing our new key metric, based on a recent business pivot.\n* In our most important customer segment, it looks like it's starting to grow **exponentially**! Everyone is excited!\n\nSuddenly an important manager calls me up\n\n\"*hey, something is wrong with the north star. The dashboard looked completely different yesterday! Our exponential growth is gone! Surely there is something wrong, please fix it by this evening. Tomorrow is the board meeting and I'm presenting the exponential growth.\"*\n\n  \nTook us some time to understand this one... Apparently, ALL data changed, the complete metric in this customer segment broke in, not just for today, but also for yesterday, the day before, and so on...\n\n  \nAfter some research, we realized a huge problem: The biggest customer in that segment left a few months ago, and filed a \"deletion request\". The upstream team responsible for this followed through, and basically \"detached the relevant data from the customer account\". \n\n  \nSo there we were. We didn't even know about this process, and had no chance to recover. The manager was left without his exponential growth. \n\n  \n*Aftermath: So what we did from then on is to turn on snapshotting of important data sources (using dbt). After being really unhappy, the manager was still convinced of the underlying exponential growth which shouldn't be reliant on one big customer, but the situation felt terrible. And I'm quite happy that only data from basically one customer went down the drain.*\n\n\\----\n\n  \nHow about you? Do you have a horror story to share?", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got a data engineering horror story, what is yours?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lvm8z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 153, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 153, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678279973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know about you, but I have plenty of data engineering horror stories to share. I&amp;#39;d love to hear the one that still gives you shivers.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here&amp;#39;s my highlight:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;m at a 500 people medium sized company, 300-400$m revenue. But we&amp;#39;re growing strong, at 10-20% each year. Ingest data via python, dbt for transformations, storage in PostgreSQL and Tableau as BI tool on top of it.&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve been pushing the adoption of Tableau, and the company is eating it up, they love it. They are all over the dashboards.&lt;/li&gt;\n&lt;li&gt;What we&amp;#39;re particular proud of is our &lt;strong&gt;&amp;quot;north star metric&lt;/strong&gt;&amp;quot; dashboard, showing our new key metric, based on a recent business pivot.&lt;/li&gt;\n&lt;li&gt;In our most important customer segment, it looks like it&amp;#39;s starting to grow &lt;strong&gt;exponentially&lt;/strong&gt;! Everyone is excited!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Suddenly an important manager calls me up&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&lt;em&gt;hey, something is wrong with the north star. The dashboard looked completely different yesterday! Our exponential growth is gone! Surely there is something wrong, please fix it by this evening. Tomorrow is the board meeting and I&amp;#39;m presenting the exponential growth.&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Took us some time to understand this one... Apparently, ALL data changed, the complete metric in this customer segment broke in, not just for today, but also for yesterday, the day before, and so on...&lt;/p&gt;\n\n&lt;p&gt;After some research, we realized a huge problem: The biggest customer in that segment left a few months ago, and filed a &amp;quot;deletion request&amp;quot;. The upstream team responsible for this followed through, and basically &amp;quot;detached the relevant data from the customer account&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;So there we were. We didn&amp;#39;t even know about this process, and had no chance to recover. The manager was left without his exponential growth. &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Aftermath: So what we did from then on is to turn on snapshotting of important data sources (using dbt). After being really unhappy, the manager was still convinced of the underlying exponential growth which shouldn&amp;#39;t be reliant on one big customer, but the situation felt terrible. And I&amp;#39;m quite happy that only data from basically one customer went down the drain.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;----&lt;/p&gt;\n\n&lt;p&gt;How about you? Do you have a horror story to share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11lvm8z", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 76, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lvm8z/i_got_a_data_engineering_horror_story_what_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lvm8z/i_got_a_data_engineering_horror_story_what_is/", "subreddit_subscribers": 92349, "created_utc": 1678279973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/](https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/)", "author_fullname": "t2_3pb20mxg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Kestra, the open source data orchestration and scheduling platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lra9t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678265803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/\"&gt;https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lra9t", "is_robot_indexable": true, "report_reasons": null, "author": "loicmathieu", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lra9t/introduction_to_kestra_the_open_source_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lra9t/introduction_to_kestra_the_open_source_data/", "subreddit_subscribers": 92349, "created_utc": 1678265803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I'm in the Seattle area and was offered an analytics engineer role since I previously interned at the place last summer. Basically what we do is that we work in our Snowflake database and clean our data and make data enrichments. Technologies that we primarily use are SQL, dbt, Airflow, and a bit of Python for UDFs. The job is extremely easy when I was an intern, but I wanted to work as a SWE instead. I feel like this position is more similar to a data analyst position.\n\nI got an offer of 104k total comp which was too low for me. I replied to negotiate for around 130k instead but the recruiter basically said they feel like the offer they gave me was fair and to respond with my final decision in 24 hours.\n\nI want to decline, I feel like an analytics engineer position is something that will not make me learn anything and I want to do software instead. The job is really easy and the compensation is a bit too low, given I have a background in CS and Math and plan to graduate this quarter.\n\nAlso in the interview, one of the data engineering managers was shit-talking my team for 45 minutes saying how he would gut the whole thing and start from scratch. Didn't give me a good taste to be honest as he feels like there are too many problems with my team's current product they have. The data science manager also brought up similar issues.\n\nAm I shooting myself in the foot by declining given the current market? I haven't been interviewing at other places but I probably will have to since I don't have anything lined up. I'm not a big fan of the work as well. I just want anyone's advice to give me since I have 24 hours left. I can respond with more information as well", "author_fullname": "t2_qeq05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Offered an Analytics Engineering Position and want to decline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m9ak2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678312180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;m in the Seattle area and was offered an analytics engineer role since I previously interned at the place last summer. Basically what we do is that we work in our Snowflake database and clean our data and make data enrichments. Technologies that we primarily use are SQL, dbt, Airflow, and a bit of Python for UDFs. The job is extremely easy when I was an intern, but I wanted to work as a SWE instead. I feel like this position is more similar to a data analyst position.&lt;/p&gt;\n\n&lt;p&gt;I got an offer of 104k total comp which was too low for me. I replied to negotiate for around 130k instead but the recruiter basically said they feel like the offer they gave me was fair and to respond with my final decision in 24 hours.&lt;/p&gt;\n\n&lt;p&gt;I want to decline, I feel like an analytics engineer position is something that will not make me learn anything and I want to do software instead. The job is really easy and the compensation is a bit too low, given I have a background in CS and Math and plan to graduate this quarter.&lt;/p&gt;\n\n&lt;p&gt;Also in the interview, one of the data engineering managers was shit-talking my team for 45 minutes saying how he would gut the whole thing and start from scratch. Didn&amp;#39;t give me a good taste to be honest as he feels like there are too many problems with my team&amp;#39;s current product they have. The data science manager also brought up similar issues.&lt;/p&gt;\n\n&lt;p&gt;Am I shooting myself in the foot by declining given the current market? I haven&amp;#39;t been interviewing at other places but I probably will have to since I don&amp;#39;t have anything lined up. I&amp;#39;m not a big fan of the work as well. I just want anyone&amp;#39;s advice to give me since I have 24 hours left. I can respond with more information as well&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11m9ak2", "is_robot_indexable": true, "report_reasons": null, "author": "therealhm2", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m9ak2/offered_an_analytics_engineering_position_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m9ak2/offered_an_analytics_engineering_position_and/", "subreddit_subscribers": 92349, "created_utc": 1678312180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nWe are drawing from our Data Warehouse to do a report. The data is not yet large but is expected to be soon. However, the query work we are doing has become quite complex. We are coding it all up in Glue/Spark. But it begs the question: are we doing things wrong?\n\nHere's some other options:\n\n* prepare as much data manipulation into Views in the Data Warehouse\n* actually materialize some of the required data into tables via new ETLs\n* just keep programming it all in Spark\n* rely on BI tool to get the job done.\n\nHow would you achieve a report that requires many joins and filters on lots of data?\n\nBonus Question: In the context of big data, do you rely on your BI tool for such kinds of \"heavy\" reports?", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How/Where to make complex Reports?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m9t6g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678313395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;We are drawing from our Data Warehouse to do a report. The data is not yet large but is expected to be soon. However, the query work we are doing has become quite complex. We are coding it all up in Glue/Spark. But it begs the question: are we doing things wrong?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s some other options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;prepare as much data manipulation into Views in the Data Warehouse&lt;/li&gt;\n&lt;li&gt;actually materialize some of the required data into tables via new ETLs&lt;/li&gt;\n&lt;li&gt;just keep programming it all in Spark&lt;/li&gt;\n&lt;li&gt;rely on BI tool to get the job done.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How would you achieve a report that requires many joins and filters on lots of data?&lt;/p&gt;\n\n&lt;p&gt;Bonus Question: In the context of big data, do you rely on your BI tool for such kinds of &amp;quot;heavy&amp;quot; reports?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11m9t6g", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m9t6g/howwhere_to_make_complex_reports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m9t6g/howwhere_to_make_complex_reports/", "subreddit_subscribers": 92349, "created_utc": 1678313395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently working on creating reports for business areas and each data engineer is developing their own data marts similar to this architecture - https://www.researchgate.net/figure/Kimballs-data-warehouse-architecture-7_fig2_312486486\n\nEach business area (Finance, HR, Sales etc) have their own data mart, could be multiple fact tables and then the developers create -the reports. The organisation will end up with 70-100 fact tables, I assumed this was the way it should be done.\n\nMy colleague advised that this is the 'old way' of working, he is a data architect who has implemented a one version of the truth and a single data model which consists of all information of all levels of granularity - Finance, Customer, HR, Sales etc. This makes life easy for the developers and allows you to present your KPI's in one single dashboard.\n\nI was stumped, I didn't think it was possible and sounds to good to be true. I have many questions but I would like to know your thoughts on this? Any advice? Experiences with implementing something similar in the past.", "author_fullname": "t2_12rfbr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am fairly new to Data Warehousing and need some advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lyuir", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678288412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently working on creating reports for business areas and each data engineer is developing their own data marts similar to this architecture - &lt;a href=\"https://www.researchgate.net/figure/Kimballs-data-warehouse-architecture-7_fig2_312486486\"&gt;https://www.researchgate.net/figure/Kimballs-data-warehouse-architecture-7_fig2_312486486&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Each business area (Finance, HR, Sales etc) have their own data mart, could be multiple fact tables and then the developers create -the reports. The organisation will end up with 70-100 fact tables, I assumed this was the way it should be done.&lt;/p&gt;\n\n&lt;p&gt;My colleague advised that this is the &amp;#39;old way&amp;#39; of working, he is a data architect who has implemented a one version of the truth and a single data model which consists of all information of all levels of granularity - Finance, Customer, HR, Sales etc. This makes life easy for the developers and allows you to present your KPI&amp;#39;s in one single dashboard.&lt;/p&gt;\n\n&lt;p&gt;I was stumped, I didn&amp;#39;t think it was possible and sounds to good to be true. I have many questions but I would like to know your thoughts on this? Any advice? Experiences with implementing something similar in the past.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lyuir", "is_robot_indexable": true, "report_reasons": null, "author": "That_Sweet_Science", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lyuir/i_am_fairly_new_to_data_warehousing_and_need_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lyuir/i_am_fairly_new_to_data_warehousing_and_need_some/", "subreddit_subscribers": 92349, "created_utc": 1678288412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151](https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151)\n\nWhat do you think of this? I think its really awesome they are going forwards using open source scheduler tools in ADF.\n\nAnyone had already experiences with it? However it is still depending on Data Factory", "author_fullname": "t2_tzseb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managed Airflow in Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lup54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678280042.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678277255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151\"&gt;https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you think of this? I think its really awesome they are going forwards using open source scheduler tools in ADF.&lt;/p&gt;\n\n&lt;p&gt;Anyone had already experiences with it? However it is still depending on Data Factory&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?auto=webp&amp;v=enabled&amp;s=377641832eecbd621b310ab9dd16d381993c449f", "width": 2288, "height": 1448}, "resolutions": [{"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc4ffca7b91875a7fa3134c0d86d66dcd449c179", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75220a5e620ee298b7b2d8329044b879daca4586", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f764681c03c53d1229c8ff8e3799ea7bec9e4899", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f124a2fdd04ffd44d572a4e01089662b94a5f067", "width": 640, "height": 405}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7d3af51b0e95fbe753302b6f4c7969990339dc6", "width": 960, "height": 607}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a177ca1e48aaead34281cb5f3e145a59154b95e", "width": 1080, "height": 683}], "variants": {}, "id": "S0ZC01nTKv9edIZMquYzX-iG3I-MSj166ECzeJEKDn8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11lup54", "is_robot_indexable": true, "report_reasons": null, "author": "Zouzzi", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lup54/managed_airflow_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lup54/managed_airflow_in_azure/", "subreddit_subscribers": 92349, "created_utc": 1678277255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Obviously I have a dag for data extraction, dag for transformations and dag for \u201caggregations\u201d.\n\nNow, all dags are created manually - i. e. when we add new data source we add new task to the dag and we have to define the dependencies manually as well. \n\nHow common is such practice? Meaning - I am thinking of generating the dags from some yaml template but not sure it would make things easier. The main reason I am considering it is because of the need to manually define the dependencies in the dag which can lead to human errors. \n\nBtw. for the \u201caggregations\u201d it should be pretty easy to generate such dag as this data is in the RDBMS and its tables can be defined using SQLAlchemy or dbt. But how about the \u201cmiddle\u201d step which just takes raw data and standardizes it in the datalake to parquet format? \ud83e\udd14", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow dags generation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lxu63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678285898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously I have a dag for data extraction, dag for transformations and dag for \u201caggregations\u201d.&lt;/p&gt;\n\n&lt;p&gt;Now, all dags are created manually - i. e. when we add new data source we add new task to the dag and we have to define the dependencies manually as well. &lt;/p&gt;\n\n&lt;p&gt;How common is such practice? Meaning - I am thinking of generating the dags from some yaml template but not sure it would make things easier. The main reason I am considering it is because of the need to manually define the dependencies in the dag which can lead to human errors. &lt;/p&gt;\n\n&lt;p&gt;Btw. for the \u201caggregations\u201d it should be pretty easy to generate such dag as this data is in the RDBMS and its tables can be defined using SQLAlchemy or dbt. But how about the \u201cmiddle\u201d step which just takes raw data and standardizes it in the datalake to parquet format? \ud83e\udd14&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11lxu63", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lxu63/airflow_dags_generation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lxu63/airflow_dags_generation/", "subreddit_subscribers": 92349, "created_utc": 1678285898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I am running into an issue with Redshift and would appreciate any suggestions.\n\nI am creating some views on redshift using dbt. One of the in particular controls the row level access per user, so all models are ultimately joined with it. But it still is just a view definition, there are no tables in the mix.\n\nEvery single time dbt tries to update this view, if there are any queries at that time that involve that view (which is often, since this is a client facing database), it deadlocks.\nBy that I mean - dbt is not able to re-create the view, no one is able to query it, and this situations persists until dbt's process is manually killed.\n\nIt seems to me that even if the view is under use, as long as there are a few seconds of downtime (which there are), then dbt should do its job and no deadlock should occur.\n\nAny ideas? It's driving me crazy!\n\nThanks", "author_fullname": "t2_8xg3h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "question - dbt and redshift deadlock?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lqmmr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678263507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I am running into an issue with Redshift and would appreciate any suggestions.&lt;/p&gt;\n\n&lt;p&gt;I am creating some views on redshift using dbt. One of the in particular controls the row level access per user, so all models are ultimately joined with it. But it still is just a view definition, there are no tables in the mix.&lt;/p&gt;\n\n&lt;p&gt;Every single time dbt tries to update this view, if there are any queries at that time that involve that view (which is often, since this is a client facing database), it deadlocks.\nBy that I mean - dbt is not able to re-create the view, no one is able to query it, and this situations persists until dbt&amp;#39;s process is manually killed.&lt;/p&gt;\n\n&lt;p&gt;It seems to me that even if the view is under use, as long as there are a few seconds of downtime (which there are), then dbt should do its job and no deadlock should occur.&lt;/p&gt;\n\n&lt;p&gt;Any ideas? It&amp;#39;s driving me crazy!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lqmmr", "is_robot_indexable": true, "report_reasons": null, "author": "Fredbull", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lqmmr/question_dbt_and_redshift_deadlock/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lqmmr/question_dbt_and_redshift_deadlock/", "subreddit_subscribers": 92349, "created_utc": 1678263507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am starting a new job next week and I will be responsible for a lot more that I have in the past as a Power Bi Developer.  One of the requirements is to make sure that the database are in 2NF or 3NF.\n\nI understand what 1NF, 2NF and 3NF and how to change the database to be 2NF or 3NF.\n\nDoes anyone know of a good python package, SQL script or even a good check list to help identify the normalization status of a database?", "author_fullname": "t2_c25i55k3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database normalization 1NF, 2NF, 3NF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mhov6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678333552.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678333323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am starting a new job next week and I will be responsible for a lot more that I have in the past as a Power Bi Developer.  One of the requirements is to make sure that the database are in 2NF or 3NF.&lt;/p&gt;\n\n&lt;p&gt;I understand what 1NF, 2NF and 3NF and how to change the database to be 2NF or 3NF.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a good python package, SQL script or even a good check list to help identify the normalization status of a database?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11mhov6", "is_robot_indexable": true, "report_reasons": null, "author": "moltra_1", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mhov6/database_normalization_1nf_2nf_3nf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mhov6/database_normalization_1nf_2nf_3nf/", "subreddit_subscribers": 92349, "created_utc": 1678333323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For context I did my bachelors and masters in electrical engineering from TMU and I took 1 course on data engineering which got me really interested. Unfortunately I was too late to take more courses on data/ML cause i finished the requirements for the MEng and just graduated last year. I only learnt basic sql/gcp and how to ingest data in hive/Hadoop and use elastic search and kibana in that course. I want to apply for data engineering roles and basically get into this field but I don\u2019t even know how to operate GitHub or have any projects. There are way too many resources online and a tonne of free courses but I want to enroll in a program which can motivate me more and actually make build projects. The ones I am interested in currently is the data science certificate at UofT and the Big data at McMaster University. I must add I have little to no work experience. I just graduated and the thought of pouring money to another useless degree is not appealing to me. But if the certificate or bootcamp actually gets me a job in data engineering I\u2019d be more than happy to invest. I honestly need help and guidance if anyone went through the same thing. Any recommendations on what I should do now is more than welcome and is greatly appreciated (pls I beg)", "author_fullname": "t2_cqcj4ctw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big data engineering certificates/courses in Toronto to start out and build portfolio?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11md525", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678321169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context I did my bachelors and masters in electrical engineering from TMU and I took 1 course on data engineering which got me really interested. Unfortunately I was too late to take more courses on data/ML cause i finished the requirements for the MEng and just graduated last year. I only learnt basic sql/gcp and how to ingest data in hive/Hadoop and use elastic search and kibana in that course. I want to apply for data engineering roles and basically get into this field but I don\u2019t even know how to operate GitHub or have any projects. There are way too many resources online and a tonne of free courses but I want to enroll in a program which can motivate me more and actually make build projects. The ones I am interested in currently is the data science certificate at UofT and the Big data at McMaster University. I must add I have little to no work experience. I just graduated and the thought of pouring money to another useless degree is not appealing to me. But if the certificate or bootcamp actually gets me a job in data engineering I\u2019d be more than happy to invest. I honestly need help and guidance if anyone went through the same thing. Any recommendations on what I should do now is more than welcome and is greatly appreciated (pls I beg)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11md525", "is_robot_indexable": true, "report_reasons": null, "author": "anonthrowaways728181", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11md525/big_data_engineering_certificatescourses_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11md525/big_data_engineering_certificatescourses_in/", "subreddit_subscribers": 92349, "created_utc": 1678321169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Got a use case which requires to deploy multiple resources across multiple cloud projects with terraform.\n\nIs it better off to have one service account across all deployments or separate each with a distinct service account.\n\nI understand there are trade offs on both approaches and might not be one right solution, but is there maybe more secure to just protect one service account key instead of 50?", "author_fullname": "t2_fbkuq4f9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "one service account to rule them all, or not", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m07eb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678291595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got a use case which requires to deploy multiple resources across multiple cloud projects with terraform.&lt;/p&gt;\n\n&lt;p&gt;Is it better off to have one service account across all deployments or separate each with a distinct service account.&lt;/p&gt;\n\n&lt;p&gt;I understand there are trade offs on both approaches and might not be one right solution, but is there maybe more secure to just protect one service account key instead of 50?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11m07eb", "is_robot_indexable": true, "report_reasons": null, "author": "one_but_ninja", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m07eb/one_service_account_to_rule_them_all_or_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m07eb/one_service_account_to_rule_them_all_or_not/", "subreddit_subscribers": 92349, "created_utc": 1678291595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5pxn0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Teams Survey 2023 Results", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lzsrn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1678290680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jesse-anderson.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.jesse-anderson.com/2023/03/data-teams-survey-2023-results/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Mentor | Jesse Anderson", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lzsrn", "is_robot_indexable": true, "report_reasons": null, "author": "eljefe6a", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11lzsrn/data_teams_survey_2023_results/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.jesse-anderson.com/2023/03/data-teams-survey-2023-results/", "subreddit_subscribers": 92349, "created_utc": 1678290680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Alright so Databricks allows for Python and PySpark programming. \n\nIf i code in only Python, does databricks distribute the computation across nodes using Spark? Or would that only occur if I use PySpark?\n\nI\u2019ve been using databricks for a month now and didn\u2019t think to program in PySpark and have been programming in Python but now I\u2019m afraid I\u2019m not getting the full benefit of Databricks.", "author_fullname": "t2_6mct0oth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: Programming in Python vs PySpark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mj5cw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678337616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alright so Databricks allows for Python and PySpark programming. &lt;/p&gt;\n\n&lt;p&gt;If i code in only Python, does databricks distribute the computation across nodes using Spark? Or would that only occur if I use PySpark?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been using databricks for a month now and didn\u2019t think to program in PySpark and have been programming in Python but now I\u2019m afraid I\u2019m not getting the full benefit of Databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11mj5cw", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Baseball89", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mj5cw/databricks_programming_in_python_vs_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mj5cw/databricks_programming_in_python_vs_pyspark/", "subreddit_subscribers": 92349, "created_utc": 1678337616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have what seems like a pretty vanilla DE project: Take an OLTP system hosted on AWS and build out a system that supports ad hoc OLAP queries by analysts (bonus points if they can continue to use Tableau or Metabase on it) and feature extraction and model training by the ML team. Data volume is currently ~500GB/500M rows with an evolving schema and projected to grow 5x-10x in the next few years. The data is largely append-only but some of it is updateable. There is a desire to pull in some higher-volume, unstructured external data for enrichment as well.\n\nMy general thinking is:\n\nData lake with something along the lines of Iceberg to help manage the raw data since it handles schema evolution, point-in-time query capabilities, and the possibility of multiple query engines if analysts and the ML team end up with different tools. \n\nStream OLTP updates to the data lake. This is an area where I have the least expertise. Something along the lines of AWS Glue seems appropriate.\n\nA horizontally-scalable query engine such as Presto that will allow analysts to do ad hoc scans over relatively large volumes of data. It looks like Tableau and Metabase plug into it as well.\n\nI've done a fair bit of work at multiple layers in setups like these, but haven't ever put one together from end-to-end and I'm looking for some sanity checks and/or gaping holes in my reasoning -- major components that are missing or tools that may be overkill for TB-scale data volumes.", "author_fullname": "t2_6p9p15m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Greenfield DE project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m97iu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678311977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have what seems like a pretty vanilla DE project: Take an OLTP system hosted on AWS and build out a system that supports ad hoc OLAP queries by analysts (bonus points if they can continue to use Tableau or Metabase on it) and feature extraction and model training by the ML team. Data volume is currently ~500GB/500M rows with an evolving schema and projected to grow 5x-10x in the next few years. The data is largely append-only but some of it is updateable. There is a desire to pull in some higher-volume, unstructured external data for enrichment as well.&lt;/p&gt;\n\n&lt;p&gt;My general thinking is:&lt;/p&gt;\n\n&lt;p&gt;Data lake with something along the lines of Iceberg to help manage the raw data since it handles schema evolution, point-in-time query capabilities, and the possibility of multiple query engines if analysts and the ML team end up with different tools. &lt;/p&gt;\n\n&lt;p&gt;Stream OLTP updates to the data lake. This is an area where I have the least expertise. Something along the lines of AWS Glue seems appropriate.&lt;/p&gt;\n\n&lt;p&gt;A horizontally-scalable query engine such as Presto that will allow analysts to do ad hoc scans over relatively large volumes of data. It looks like Tableau and Metabase plug into it as well.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done a fair bit of work at multiple layers in setups like these, but haven&amp;#39;t ever put one together from end-to-end and I&amp;#39;m looking for some sanity checks and/or gaping holes in my reasoning -- major components that are missing or tools that may be overkill for TB-scale data volumes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11m97iu", "is_robot_indexable": true, "report_reasons": null, "author": "Db_Wrangler_1905", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m97iu/greenfield_de_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m97iu/greenfield_de_project/", "subreddit_subscribers": 92349, "created_utc": 1678311977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, while researching and learning about data warehouses, I will always only find a super minimal example to demonstrate a star schema with like 5 tables total in the warehouse. Like that's cool, but what if I have hundreds of tables (50+ facts &amp; 100+ dimensions) coming from 10+ databases and APIs???? I figure this is much more what you deal with in the real world.\n\nSo to my questions are:\n\n1. How do you guys organize all of this in your warehouse? Schema per Source?\n2. Are you using a data lake before the warehouse to bring all of the sources together in a single location?\n3. How do you manage the inserts, updates, and deletes for all of these tables in the warehouse?\n4. What does your reporting layer look like on top of the warehouse tables?\n5. Are you using something like DBT to transform the raw data (normalized form) into a denormalized form?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are your warehouses organized?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m0wtv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678293216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, while researching and learning about data warehouses, I will always only find a super minimal example to demonstrate a star schema with like 5 tables total in the warehouse. Like that&amp;#39;s cool, but what if I have hundreds of tables (50+ facts &amp;amp; 100+ dimensions) coming from 10+ databases and APIs???? I figure this is much more what you deal with in the real world.&lt;/p&gt;\n\n&lt;p&gt;So to my questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How do you guys organize all of this in your warehouse? Schema per Source?&lt;/li&gt;\n&lt;li&gt;Are you using a data lake before the warehouse to bring all of the sources together in a single location?&lt;/li&gt;\n&lt;li&gt;How do you manage the inserts, updates, and deletes for all of these tables in the warehouse?&lt;/li&gt;\n&lt;li&gt;What does your reporting layer look like on top of the warehouse tables?&lt;/li&gt;\n&lt;li&gt;Are you using something like DBT to transform the raw data (normalized form) into a denormalized form?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11m0wtv", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m0wtv/how_are_your_warehouses_organized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m0wtv/how_are_your_warehouses_organized/", "subreddit_subscribers": 92349, "created_utc": 1678293216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a conference you're looking forward to that deliver a lot of Data Engineering concepts or is centered on DE as a whole?", "author_fullname": "t2_pp8loa0k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Conferences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mbxsq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678318255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a conference you&amp;#39;re looking forward to that deliver a lot of Data Engineering concepts or is centered on DE as a whole?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11mbxsq", "is_robot_indexable": true, "report_reasons": null, "author": "de_epi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11mbxsq/de_conferences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mbxsq/de_conferences/", "subreddit_subscribers": 92349, "created_utc": 1678318255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a dataset with a million rows, and I'm using Pyspark for transformations to model data to meet data science needs, though Pyspark has most transformations, there are situations like I'm in now, where I need to use a third-party library function on my data which are not optimized for Pyspark  ( currently I'm tasked to find Jaro Winkler distance matrix ), running this function on about 8000 records is taking around 6 mins of time but it would take a lot of time for a million records.   \n\n\n1. How do you handle the execution of a third-party function on a huge dataset .? \n2. I'm on AWS and feel that I should use  [Python shell jobs in AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html) to run the Jaro Winkler function for about two days to get a matrix for the whole dataset,  am I right in this regard.?", "author_fullname": "t2_6psngyc6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running a third-party-library function on a Huge dataset that is not optimized for Pyspark.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mbvwq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678318133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a dataset with a million rows, and I&amp;#39;m using Pyspark for transformations to model data to meet data science needs, though Pyspark has most transformations, there are situations like I&amp;#39;m in now, where I need to use a third-party library function on my data which are not optimized for Pyspark  ( currently I&amp;#39;m tasked to find Jaro Winkler distance matrix ), running this function on about 8000 records is taking around 6 mins of time but it would take a lot of time for a million records.   &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How do you handle the execution of a third-party function on a huge dataset .? &lt;/li&gt;\n&lt;li&gt;I&amp;#39;m on AWS and feel that I should use  &lt;a href=\"https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html\"&gt;Python shell jobs in AWS Glue&lt;/a&gt; to run the Jaro Winkler function for about two days to get a matrix for the whole dataset,  am I right in this regard.?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11mbvwq", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo_51799", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mbvwq/running_a_thirdpartylibrary_function_on_a_huge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mbvwq/running_a_thirdpartylibrary_function_on_a_huge/", "subreddit_subscribers": 92349, "created_utc": 1678318133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am building a data pipeline and dashboard app that I want to deploy online (fastapi + postgresql + svelte/vue). It will read all data from postgresql, do computations (numpy / pandas), store the output back into postgresql, and then the frontend app will communicate with the api to render the dashboard.\n\nBut what is the best way to deploy postgres in production? My platform will start small, but I'd like to have the infrastructure to scale quickly when / if the platform takes off.\n\nI am a data engineer and programmer and I used to do everything myself, but with kubernets, docker things got more complicated:\n\n* use kubernetes to have postgresql, api (fastapi), and data pipeline in separate dockers and communicate with each other over network?\n* vm for postgresql?\n* \"bare metal\" (dedicated server) for postgresql?\n\nI've read pros and cons for each, I really don't know what the best way is to do this.\n\nDoes anybody has experience with this?", "author_fullname": "t2_15vo19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "from files to db in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lrzi6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678268273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building a data pipeline and dashboard app that I want to deploy online (fastapi + postgresql + svelte/vue). It will read all data from postgresql, do computations (numpy / pandas), store the output back into postgresql, and then the frontend app will communicate with the api to render the dashboard.&lt;/p&gt;\n\n&lt;p&gt;But what is the best way to deploy postgres in production? My platform will start small, but I&amp;#39;d like to have the infrastructure to scale quickly when / if the platform takes off.&lt;/p&gt;\n\n&lt;p&gt;I am a data engineer and programmer and I used to do everything myself, but with kubernets, docker things got more complicated:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;use kubernetes to have postgresql, api (fastapi), and data pipeline in separate dockers and communicate with each other over network?&lt;/li&gt;\n&lt;li&gt;vm for postgresql?&lt;/li&gt;\n&lt;li&gt;&amp;quot;bare metal&amp;quot; (dedicated server) for postgresql?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve read pros and cons for each, I really don&amp;#39;t know what the best way is to do this.&lt;/p&gt;\n\n&lt;p&gt;Does anybody has experience with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lrzi6", "is_robot_indexable": true, "report_reasons": null, "author": "iLLucionist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lrzi6/from_files_to_db_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lrzi6/from_files_to_db_in_production/", "subreddit_subscribers": 92349, "created_utc": 1678268273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9u69ulzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I use JIRA as a database? (NSFL)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11mkywc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1678343441.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/SoftwareEngineering/comments/11mehjg/jira_as_a_database/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "11mkywc", "is_robot_indexable": true, "report_reasons": null, "author": "tdatas", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mkywc/should_i_use_jira_as_a_database_nsfl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/SoftwareEngineering/comments/11mehjg/jira_as_a_database/", "subreddit_subscribers": 92349, "created_utc": 1678343441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand the separate use cases for batch and streaming. Batch for analytical/aggregate reporting data over a set period of time. Streaming for more immediate feedback on unbounded data. My question is about the reconciliation that happens between these two layers? Why would you need to reconcile when you already have the batch data that is most accurate?  Wouldn't the batch data suffice?", "author_fullname": "t2_nmk5l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about lambda architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11mkle2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678342197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand the separate use cases for batch and streaming. Batch for analytical/aggregate reporting data over a set period of time. Streaming for more immediate feedback on unbounded data. My question is about the reconciliation that happens between these two layers? Why would you need to reconcile when you already have the batch data that is most accurate?  Wouldn&amp;#39;t the batch data suffice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11mkle2", "is_robot_indexable": true, "report_reasons": null, "author": "getboy97", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mkle2/question_about_lambda_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mkle2/question_about_lambda_architecture/", "subreddit_subscribers": 92349, "created_utc": 1678342197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short I\u2019ve spent a decent amount of time figuring out how I could deploy dependencies for my (Python) lambda function. I kind of started a never ending loop with this topic, first I zipped them, but the zipped file was larger than 50MB so I did some refactoring, so that it won\u2019t go over 50MB. To deploy with docker is only an option in LocalStackPro, same with Lambda Layers and referencing to real AWS lambda. So I downloaded the wheels I thought I needed, but now I think it\u2019s the architecture. I\u2019m getting errors on missing files, even thought about hidden files, but there is none in the wheels I have checked. Is there something else I am missing? Is it not ARM64 and on X86? Or how do I know which architecture I need?", "author_fullname": "t2_se9vugql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lambda dependencies on LocalStack without Pro version?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m458m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678300520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I\u2019ve spent a decent amount of time figuring out how I could deploy dependencies for my (Python) lambda function. I kind of started a never ending loop with this topic, first I zipped them, but the zipped file was larger than 50MB so I did some refactoring, so that it won\u2019t go over 50MB. To deploy with docker is only an option in LocalStackPro, same with Lambda Layers and referencing to real AWS lambda. So I downloaded the wheels I thought I needed, but now I think it\u2019s the architecture. I\u2019m getting errors on missing files, even thought about hidden files, but there is none in the wheels I have checked. Is there something else I am missing? Is it not ARM64 and on X86? Or how do I know which architecture I need?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11m458m", "is_robot_indexable": true, "report_reasons": null, "author": "it-dummy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m458m/lambda_dependencies_on_localstack_without_pro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m458m/lambda_dependencies_on_localstack_without_pro/", "subreddit_subscribers": 92349, "created_utc": 1678300520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For context, we have a table which is populated with either forecasted or observed data.  When observed data becomes available we overwrite the forecasted data. The observed data does not come in at a regular, predictable cadence. My question is how would you handle this with dbt and are there any best practices surrounding this type of problem?", "author_fullname": "t2_wkhd5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on handling corrections in data with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m311g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678297989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, we have a table which is populated with either forecasted or observed data.  When observed data becomes available we overwrite the forecasted data. The observed data does not come in at a regular, predictable cadence. My question is how would you handle this with dbt and are there any best practices surrounding this type of problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11m311g", "is_robot_indexable": true, "report_reasons": null, "author": "getcoldlikeminnesota", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m311g/advice_on_handling_corrections_in_data_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m311g/advice_on_handling_corrections_in_data_with_dbt/", "subreddit_subscribers": 92349, "created_utc": 1678297989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that data analysis is probably the main path to reaching DE. But are there alternative approaches to a DE job as well, if you add in professional certifications from either Google, MS, and/or AWS? I am saying this in the regard that the alternative role places some emphasis in SQL. If not, what are other alternative roles besides data analysis that can lead en eventual role in DE?", "author_fullname": "t2_shlqver7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm an engineering grad and have enough credentials skill-wise to work as an industrial engineer. from what i have observed, this job demands knowledge in sql. That being said, is taking any role that has some emphasis on sql eligible enough to make the transition to higher level data roles like DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m1iuf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678294589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that data analysis is probably the main path to reaching DE. But are there alternative approaches to a DE job as well, if you add in professional certifications from either Google, MS, and/or AWS? I am saying this in the regard that the alternative role places some emphasis in SQL. If not, what are other alternative roles besides data analysis that can lead en eventual role in DE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11m1iuf", "is_robot_indexable": true, "report_reasons": null, "author": "Such-Assist4895", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m1iuf/im_an_engineering_grad_and_have_enough/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m1iuf/im_an_engineering_grad_and_have_enough/", "subreddit_subscribers": 92349, "created_utc": 1678294589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a personal project that I could use some advice on, and I'm not a DE so please pardon my naivety. I'm trying to figure out if there's possibly a better database for what I'm doing, or maybe my pipeline itself is just sub-par.\n\nThe service in question has the following I/O properties:\n\n* Receives data from multiple websocket streams and normalizes it into a database\n* Provides a GraphQL API for querying the data as well as outbound websocket streams\n\nThe data is market data from exchanges, which is mostly non-relational tabular numeric data, with a need for multi-column indexing, something like this:\n\n    time: datetime  \\\n    source: string   &gt; these 3 combine for the index\n    symbol: string  /\n    \n    (... a bunch of numeric columns ... )\n\nI'm emphasizing the indexing because that's my current hangup. Currently, this is implemented as a NodeJS service backed by a MongoDB instance. Currently there's only about 20mil records, but each incoming websocket receives data about every 200ms or so and I've currently got a handful of them spun up with a desire to add more. The read performance is fine, but write performance is a serious bottleneck, since every write causes a reindexing. My only workaround at the moment is to just cache data in memory and write in batch mode a few thousand records at a time.  This works ok, but is less than ideal since I also need the data to be available immediately through the API, and this requires some ugly stitching of queried data with in-memory data (and presents a lot of issues if I decide to scale out the service horizontally).\n\nI'm trying to figure out if there's potentially a more suitable database for this. I was looking at Cassandra, which I understand has excellent write performance. However, this is a single server for now, and I was curious if a single Cassandra node would really provide any better performance - in other words, is the engine itself better optimized for this or would I not really see any benefits unless I scaled it?  Is there another database that you'd recommend that might be better optimized for this problem?\n\nI would prefer something self-hosted rather than cloud-based. I also prefer NoSQL so I don't have to deal with schema/migrations, but if something like Postgres is really the way to go then so be it.", "author_fullname": "t2_w3le7elz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on improving write performance in my data service, possibly replacing MongoDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lvi81", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678279659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a personal project that I could use some advice on, and I&amp;#39;m not a DE so please pardon my naivety. I&amp;#39;m trying to figure out if there&amp;#39;s possibly a better database for what I&amp;#39;m doing, or maybe my pipeline itself is just sub-par.&lt;/p&gt;\n\n&lt;p&gt;The service in question has the following I/O properties:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Receives data from multiple websocket streams and normalizes it into a database&lt;/li&gt;\n&lt;li&gt;Provides a GraphQL API for querying the data as well as outbound websocket streams&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The data is market data from exchanges, which is mostly non-relational tabular numeric data, with a need for multi-column indexing, something like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;time: datetime  \\\nsource: string   &amp;gt; these 3 combine for the index\nsymbol: string  /\n\n(... a bunch of numeric columns ... )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m emphasizing the indexing because that&amp;#39;s my current hangup. Currently, this is implemented as a NodeJS service backed by a MongoDB instance. Currently there&amp;#39;s only about 20mil records, but each incoming websocket receives data about every 200ms or so and I&amp;#39;ve currently got a handful of them spun up with a desire to add more. The read performance is fine, but write performance is a serious bottleneck, since every write causes a reindexing. My only workaround at the moment is to just cache data in memory and write in batch mode a few thousand records at a time.  This works ok, but is less than ideal since I also need the data to be available immediately through the API, and this requires some ugly stitching of queried data with in-memory data (and presents a lot of issues if I decide to scale out the service horizontally).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to figure out if there&amp;#39;s potentially a more suitable database for this. I was looking at Cassandra, which I understand has excellent write performance. However, this is a single server for now, and I was curious if a single Cassandra node would really provide any better performance - in other words, is the engine itself better optimized for this or would I not really see any benefits unless I scaled it?  Is there another database that you&amp;#39;d recommend that might be better optimized for this problem?&lt;/p&gt;\n\n&lt;p&gt;I would prefer something self-hosted rather than cloud-based. I also prefer NoSQL so I don&amp;#39;t have to deal with schema/migrations, but if something like Postgres is really the way to go then so be it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lvi81", "is_robot_indexable": true, "report_reasons": null, "author": "8bitVagrant", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lvi81/looking_for_advice_on_improving_write_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lvi81/looking_for_advice_on_improving_write_performance/", "subreddit_subscribers": 92349, "created_utc": 1678279659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working as a senior DE in service company. Currently working on on-prim netezza warehouse migration to cloud. I have good hold of SSIS,SSRS,REDSHIFT,PANDAS PIPELINE,AWS S3. I am trying for a job change as salary is very low as compared to daily work. I am not getting a single call for interview, is it due to my age (40+) or anything else? I am currently learning pyspark even though it is not getting used in current project but will it help to get call/ land in new job?", "author_fullname": "t2_b7vvf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE openings for 16+YOE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11luuwy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678278396.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678277742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working as a senior DE in service company. Currently working on on-prim netezza warehouse migration to cloud. I have good hold of SSIS,SSRS,REDSHIFT,PANDAS PIPELINE,AWS S3. I am trying for a job change as salary is very low as compared to daily work. I am not getting a single call for interview, is it due to my age (40+) or anything else? I am currently learning pyspark even though it is not getting used in current project but will it help to get call/ land in new job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11luuwy", "is_robot_indexable": true, "report_reasons": null, "author": "planetabhi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11luuwy/de_openings_for_16yoe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11luuwy/de_openings_for_16yoe/", "subreddit_subscribers": 92349, "created_utc": 1678277742.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}