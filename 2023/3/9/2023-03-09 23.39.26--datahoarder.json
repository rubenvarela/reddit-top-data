{"kind": "Listing", "data": {"after": "t3_11n0jcx", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9o1tfd22", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Conflicted Need Advice: Buy or Not? Supermicro 24 Bay with 846TQ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mkfng10ocqma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/mkfng10ocqma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8ace1202d54d51f0c985fdbbb41635d7f425b81"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/mkfng10ocqma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3667e7c383ad71d90ed6c31dd32d5566faa296e3"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/mkfng10ocqma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94cccb09aa4e8870d87ba8b0b10ef23397167ba1"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/mkfng10ocqma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=daee372a245c104e40014713c58bf28020b2fb19"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/mkfng10ocqma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=01baaba95d8f8742673d00fae76249fb75aa6442"}], "s": {"y": 763, "x": 1017, "u": "https://preview.redd.it/mkfng10ocqma1.png?width=1017&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7da8f8664d9040947a852c9c18bf1dddd983af5a"}, "id": "mkfng10ocqma1"}, "m9g7d20ocqma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 80, "x": 108, "u": "https://preview.redd.it/m9g7d20ocqma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49630bfd388626603c31c1dc9abd2031d580f541"}, {"y": 161, "x": 216, "u": "https://preview.redd.it/m9g7d20ocqma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3e80afa4047e81ef99bfca0ed93e05158eb018b"}, {"y": 238, "x": 320, "u": "https://preview.redd.it/m9g7d20ocqma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70f50bb6f193efdb7a55b4467558ce694f171c45"}, {"y": 477, "x": 640, "u": "https://preview.redd.it/m9g7d20ocqma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b05922eb82e9385986a4aae7d8c4d7db6543fdcd"}, {"y": 716, "x": 960, "u": "https://preview.redd.it/m9g7d20ocqma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7c07dd0e219baa27a0462c861281c8764141f5d"}], "s": {"y": 764, "x": 1023, "u": "https://preview.redd.it/m9g7d20ocqma1.png?width=1023&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2b33438f27055375c248fde2d0e21950071e10e9"}, "id": "m9g7d20ocqma1"}, "egmet00ocqma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/egmet00ocqma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=740ce81fd2cd9911d72d141dfd3ba1af56682ed6"}, {"y": 163, "x": 216, "u": "https://preview.redd.it/egmet00ocqma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96a50574911971fa8a9150bf5360caad22879f84"}, {"y": 242, "x": 320, "u": "https://preview.redd.it/egmet00ocqma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c11a19359a52b261a3ac0b253f8e9c125d201d8f"}, {"y": 484, "x": 640, "u": "https://preview.redd.it/egmet00ocqma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bb9abf9433e7cb06f18094b3015605e94dbb7e4"}, {"y": 726, "x": 960, "u": "https://preview.redd.it/egmet00ocqma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ef04082efeb76e8e7538164d97a19d6f6f4045a"}], "s": {"y": 768, "x": 1015, "u": "https://preview.redd.it/egmet00ocqma1.png?width=1015&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b1ce69ba818127d474989b6388672d7c80a4b7a9"}, "id": "egmet00ocqma1"}}, "name": "t3_11muqm9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 205, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "mkfng10ocqma1", "id": 249037210}, {"media_id": "m9g7d20ocqma1", "id": 249037211}, {"media_id": "egmet00ocqma1", "id": 249037212}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 205, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/FNeqNGy7GDpnjreljRABs_o34H5fPLeHcF4PdrKcTCE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678374901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/11muqm9", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11muqm9", "is_robot_indexable": true, "report_reasons": null, "author": "IntelligentSlipUp", "discussion_type": null, "num_comments": 71, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11muqm9/conflicted_need_advice_buy_or_not_supermicro_24/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/11muqm9", "subreddit_subscribers": 672681, "created_utc": 1678374901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Lets say I have 5000+ files on a specific topic. However, they are all organized in different folders. Images in one folder, PDFs in another, zip files in another, etc.\n\nAny recommendations for how to find different files based on search criteria? Any (Local) file managers, data managers etc? Is there any easier way? After having thousands of files in terabytes of data it's getting difficult to find and categorize things. Any suggestions for local (not web based) solutions to storing and searching through thousands of files?", "author_fullname": "t2_o8wjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage and find large amount of files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mfn4d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678327685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say I have 5000+ files on a specific topic. However, they are all organized in different folders. Images in one folder, PDFs in another, zip files in another, etc.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations for how to find different files based on search criteria? Any (Local) file managers, data managers etc? Is there any easier way? After having thousands of files in terabytes of data it&amp;#39;s getting difficult to find and categorize things. Any suggestions for local (not web based) solutions to storing and searching through thousands of files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11mfn4d", "is_robot_indexable": true, "report_reasons": null, "author": "ReclusiveEagle", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11mfn4d/how_do_you_manage_and_find_large_amount_of_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11mfn4d/how_do_you_manage_and_find_large_amount_of_files/", "subreddit_subscribers": 672681, "created_utc": 1678327685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! Currently I'm running a 3-2-1 backup for my plex server + important documents on cryptomator.  I've got an external hardware RAID 1 array (I know I know) hooked up to my plex server (running Linux) for network access and I sync that data to an external drive on another machine which I also do Backblaze backups on regularly. I feel that in terms of hardware my data is pretty secure. However, my concern now is bitrot. I've seen it across the subreddit a few times and I've done a bunch of research to discover the best option and I just can't seem to find one. Most important to me is simply verifying checksums occasionally just to make sure there's nothing going on. I'd rather keep them on EXT4 if possible. I've tried hashdeep, but it kinda seems like an inconvenient solution. What are your favorite solutions for macOS, Linux or both? Thanks!", "author_fullname": "t2_o6qhgg79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best solution to prevent bitrot?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mj0f9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678337208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! Currently I&amp;#39;m running a 3-2-1 backup for my plex server + important documents on cryptomator.  I&amp;#39;ve got an external hardware RAID 1 array (I know I know) hooked up to my plex server (running Linux) for network access and I sync that data to an external drive on another machine which I also do Backblaze backups on regularly. I feel that in terms of hardware my data is pretty secure. However, my concern now is bitrot. I&amp;#39;ve seen it across the subreddit a few times and I&amp;#39;ve done a bunch of research to discover the best option and I just can&amp;#39;t seem to find one. Most important to me is simply verifying checksums occasionally just to make sure there&amp;#39;s nothing going on. I&amp;#39;d rather keep them on EXT4 if possible. I&amp;#39;ve tried hashdeep, but it kinda seems like an inconvenient solution. What are your favorite solutions for macOS, Linux or both? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11mj0f9", "is_robot_indexable": true, "report_reasons": null, "author": "Synvauct", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11mj0f9/best_solution_to_prevent_bitrot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11mj0f9/best_solution_to_prevent_bitrot/", "subreddit_subscribers": 672681, "created_utc": 1678337208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5jkqq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The SSD Edition: 2022 Drive Stats Review", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "name": "t3_11mxew2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": "", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4IxUiUYhHhKL9_AW5lsZ-f7Nq-EXEzyRNl9CPrRcttk.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678381278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "backblaze.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.backblaze.com/blog/ssd-edition-2022-drive-stats-review/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ABFF_qNAC8DCq0QqqZ6PX53wMfHa70WXWmT7timke58.jpg?auto=webp&amp;v=enabled&amp;s=25c9e83b229a89a2c8bcc38367fca2acdcdd7d4e", "width": 1440, "height": 820}, "resolutions": [{"url": "https://external-preview.redd.it/ABFF_qNAC8DCq0QqqZ6PX53wMfHa70WXWmT7timke58.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c566e45c8c87f80fc8a8412ab8b51bd228d13ad2", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/ABFF_qNAC8DCq0QqqZ6PX53wMfHa70WXWmT7timke58.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=819908bc07d03c61913267a0e08d1930f35c9c77", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/ABFF_qNAC8DCq0QqqZ6PX53wMfHa70WXWmT7timke58.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=611f79136e759a2930fa0acfcf3e56b072861295", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/ABFF_qNAC8DCq0QqqZ6PX53wMfHa70WXWmT7timke58.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ce3614845b1e419fcd6be68429a3e0391c2308a", "width": 640, "height": 364}, {"url": "https://external-preview.redd.it/ABFF_qNAC8DCq0QqqZ6PX53wMfHa70WXWmT7timke58.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdd2a11d1343a846ec6b5963d562c5e9f865e03d", "width": 960, "height": 546}, {"url": "https://external-preview.redd.it/ABFF_qNAC8DCq0QqqZ6PX53wMfHa70WXWmT7timke58.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95d40798e0fe999c9f823038a88ed4fdab2dd2d0", "width": 1080, "height": 615}], "variants": {}, "id": "XDsbeiEdIE1dMCZHdu9UFVBjkhnUTMoJ9EG_GaJwuC0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": " Yev from Backblaze", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11mxew2", "is_robot_indexable": true, "report_reasons": null, "author": "YevP", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11mxew2/the_ssd_edition_2022_drive_stats_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.backblaze.com/blog/ssd-edition-2022-drive-stats-review/", "subreddit_subscribers": 672681, "created_utc": 1678381278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 20-30ish year botany book that is rare I want to find some way I can scan in a professional way, so I can color in the black and white plant drawings to update the book and make it an ebook.  It\u2019s just I can\u2019t figure out how to get it scanned in a good way. It\u2019s a 950 page book.", "author_fullname": "t2_7xu9hhq9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best possible way to professionally scan a book to turn it into an ebook?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n449y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678396689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 20-30ish year botany book that is rare I want to find some way I can scan in a professional way, so I can color in the black and white plant drawings to update the book and make it an ebook.  It\u2019s just I can\u2019t figure out how to get it scanned in a good way. It\u2019s a 950 page book.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n449y", "is_robot_indexable": true, "report_reasons": null, "author": "Pher001", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n449y/best_possible_way_to_professionally_scan_a_book/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n449y/best_possible_way_to_professionally_scan_a_book/", "subreddit_subscribers": 672681, "created_utc": 1678396689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings r/datahoarder community,\n\n&amp;#x200B;\n\nI am currently in the market for a 10TB hard drive and I stumbled upon the ST10000NM018G being one of the few options available in India. Before making the purchase, I searched through the subreddit to see if anyone has had any experience with this model and found little information. \n\n&amp;#x200B;\n\nThe hard drive is selling for $240 which seems reasonable, but I am concerned about the reliability and longevity of this model. If anyone has any experience with the ST10000NM018G or has any other recommendations for a 10TB hard drive in India, I would greatly appreciate the input.\n\n&amp;#x200B;\n\nI apologize in advance for any grammatical errors or issues with my English. Thank you for taking the time to read my post and any help would be greatly appreciated.", "author_fullname": "t2_4k3hpg2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the ST10000NM018G a Good Hard Drive Option in India?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mneok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678352262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings &lt;a href=\"/r/datahoarder\"&gt;r/datahoarder&lt;/a&gt; community,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am currently in the market for a 10TB hard drive and I stumbled upon the ST10000NM018G being one of the few options available in India. Before making the purchase, I searched through the subreddit to see if anyone has had any experience with this model and found little information. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The hard drive is selling for $240 which seems reasonable, but I am concerned about the reliability and longevity of this model. If anyone has any experience with the ST10000NM018G or has any other recommendations for a 10TB hard drive in India, I would greatly appreciate the input.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I apologize in advance for any grammatical errors or issues with my English. Thank you for taking the time to read my post and any help would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11mneok", "is_robot_indexable": true, "report_reasons": null, "author": "_ilf04_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11mneok/is_the_st10000nm018g_a_good_hard_drive_option_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11mneok/is_the_st10000nm018g_a_good_hard_drive_option_in/", "subreddit_subscribers": 672681, "created_utc": 1678352262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone!\n\nSo after a possible data scare, I've decided to finally get my butt into gear with having actual backups of things, and have been looking around into some cloud based storage options and found iDrive, which fits most of my needs. I want something that I can use on multiple computers/phones and use to back up everything from folders on my computer to my phone's photos and files. I also want to eventually store everything on a NAS when I get one, not just cloud storage, so it seems to fit the bill.\n\nThe only thing I'm not sure about is the cloud drive feature. It's advertised as something like dropbox/google drive, in which I can use as a separate real time synced storage, which is great, as I have a bunch of movies/shows/etc from Plex that I'd like to have synced (rather than backed up). The only issue is all my media are in existing folders like X:\\Movies, X:\\Shows etc, and with iDrive you have to choose a single folder to use for cloud sync, but it also makes the root folder it looks in be named something like \"X:\\Cloud-Data-username\"\n\nI was wondering if anyone knew of any tools that would allow me to basically \"copy\" my existing movies/shows as have them in a fake location under X:\\Cloud-Data, so that everything can still be synced into iDrive's cloud storage, but I won't have to rename every folder (since Plex would hate me forever if I do). Sorry if copy isn't the right word here, I'm not 100% sure what would be correct?\n\nSorry if this is a simple question or there's a tool that the community already knows about - I'm still very new to all this and am trying to figure out what the best way to do everything is :). Alternatively if there's a better way to do this, I'm all ears - while I like iDrive so far, I haven't committed to anything yet\n\nThanks in advanced, and feel free to ask any questions!", "author_fullname": "t2_9arjy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tools to help use an existing folder as a cloud drive folder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mkzl7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678343506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;So after a possible data scare, I&amp;#39;ve decided to finally get my butt into gear with having actual backups of things, and have been looking around into some cloud based storage options and found iDrive, which fits most of my needs. I want something that I can use on multiple computers/phones and use to back up everything from folders on my computer to my phone&amp;#39;s photos and files. I also want to eventually store everything on a NAS when I get one, not just cloud storage, so it seems to fit the bill.&lt;/p&gt;\n\n&lt;p&gt;The only thing I&amp;#39;m not sure about is the cloud drive feature. It&amp;#39;s advertised as something like dropbox/google drive, in which I can use as a separate real time synced storage, which is great, as I have a bunch of movies/shows/etc from Plex that I&amp;#39;d like to have synced (rather than backed up). The only issue is all my media are in existing folders like X:\\Movies, X:\\Shows etc, and with iDrive you have to choose a single folder to use for cloud sync, but it also makes the root folder it looks in be named something like &amp;quot;X:\\Cloud-Data-username&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone knew of any tools that would allow me to basically &amp;quot;copy&amp;quot; my existing movies/shows as have them in a fake location under X:\\Cloud-Data, so that everything can still be synced into iDrive&amp;#39;s cloud storage, but I won&amp;#39;t have to rename every folder (since Plex would hate me forever if I do). Sorry if copy isn&amp;#39;t the right word here, I&amp;#39;m not 100% sure what would be correct?&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is a simple question or there&amp;#39;s a tool that the community already knows about - I&amp;#39;m still very new to all this and am trying to figure out what the best way to do everything is :). Alternatively if there&amp;#39;s a better way to do this, I&amp;#39;m all ears - while I like iDrive so far, I haven&amp;#39;t committed to anything yet&lt;/p&gt;\n\n&lt;p&gt;Thanks in advanced, and feel free to ask any questions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11mkzl7", "is_robot_indexable": true, "report_reasons": null, "author": "Opaquer", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11mkzl7/any_tools_to_help_use_an_existing_folder_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11mkzl7/any_tools_to_help_use_an_existing_folder_as_a/", "subreddit_subscribers": 672681, "created_utc": 1678343506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been working on a macbook for years. My filing system is schizophrenic and consists of:\n\n1 letting my desktop and downloads folders grow into monstrosities,\n\n2 every 3-6 months going through and deleting large files I don't need, while \"organizing\" the important files into new folders labeled appropriately\n\n3 repeating 1 and 2 for years and years\n\nThis has led to an immense cluster@#$@ of similarly named folders and files from over the years, many of which I can no longer remember or keep straight in my head.\n\nI desperately want to delete all of this crap from my laptop, but I'm terrified that I'll lose a critical file I may need down the road for legal reasons, etc.\n\nI started looking into \"time machine\" today and realized it's not a snapshot/archive utility at all, and is outright dangerous. If the external storage runs out of space, it will delete the oldest snapshots. This means if I clear out my source drive (my laptop), there's no guarantee it won't also delete the backed up files. This is stupid!\n\nSo I'm looking for other solutions. Here are my requirements:\n\n1. must create a snapshot / backup of all files on my laptop that will remain accessible and uncorrupted, even if the software used to create the files is no longer supported in the future\n2. will NOT delete old backups or files that are deleted from the source disk as subsequent backups are made\n3. will create a snapshot/backup that is generally searchable from a mac laptop if I plug in an external drive\n4. I do not care about encryption, as the external drive is in a safe location, and I've had encrypted backups fail on me in the past, and it makes searching a pain, as you must decrypt the entire backup to find anything.\n\n&amp;#x200B;\n\nI hope this makes sense. What would you recommend?", "author_fullname": "t2_5pvby8p2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives to \"time-machine\" for Mac that are real archive utilities and won't delete old files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mbv1u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678318076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a macbook for years. My filing system is schizophrenic and consists of:&lt;/p&gt;\n\n&lt;p&gt;1 letting my desktop and downloads folders grow into monstrosities,&lt;/p&gt;\n\n&lt;p&gt;2 every 3-6 months going through and deleting large files I don&amp;#39;t need, while &amp;quot;organizing&amp;quot; the important files into new folders labeled appropriately&lt;/p&gt;\n\n&lt;p&gt;3 repeating 1 and 2 for years and years&lt;/p&gt;\n\n&lt;p&gt;This has led to an immense cluster@#$@ of similarly named folders and files from over the years, many of which I can no longer remember or keep straight in my head.&lt;/p&gt;\n\n&lt;p&gt;I desperately want to delete all of this crap from my laptop, but I&amp;#39;m terrified that I&amp;#39;ll lose a critical file I may need down the road for legal reasons, etc.&lt;/p&gt;\n\n&lt;p&gt;I started looking into &amp;quot;time machine&amp;quot; today and realized it&amp;#39;s not a snapshot/archive utility at all, and is outright dangerous. If the external storage runs out of space, it will delete the oldest snapshots. This means if I clear out my source drive (my laptop), there&amp;#39;s no guarantee it won&amp;#39;t also delete the backed up files. This is stupid!&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m looking for other solutions. Here are my requirements:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;must create a snapshot / backup of all files on my laptop that will remain accessible and uncorrupted, even if the software used to create the files is no longer supported in the future&lt;/li&gt;\n&lt;li&gt;will NOT delete old backups or files that are deleted from the source disk as subsequent backups are made&lt;/li&gt;\n&lt;li&gt;will create a snapshot/backup that is generally searchable from a mac laptop if I plug in an external drive&lt;/li&gt;\n&lt;li&gt;I do not care about encryption, as the external drive is in a safe location, and I&amp;#39;ve had encrypted backups fail on me in the past, and it makes searching a pain, as you must decrypt the entire backup to find anything.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I hope this makes sense. What would you recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11mbv1u", "is_robot_indexable": true, "report_reasons": null, "author": "some_crypto_guy", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11mbv1u/alternatives_to_timemachine_for_mac_that_are_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11mbv1u/alternatives_to_timemachine_for_mac_that_are_real/", "subreddit_subscribers": 672681, "created_utc": 1678318076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Beginning my adventure into this data hoarding world. Got my hardware, all excited and then... What the hell is THAT noise!? I really have no idea. As soon as I got everything, hard drive went straight from its box into the DS920 so yeah uh. Idk. Please help :(\n\n[Edit: Idiot, forgot to include the video](https://reddit.com/link/11n0wvb/video/7iovf763lrma1/player)", "author_fullname": "t2_mb5hj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What the hell is this noise? 20TB Seagate, basically new, straight from box into DS920", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"7iovf763lrma1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/11n0wvb/asset/7iovf763lrma1/DASHPlaylist.mpd?a=1680997166%2CMTMzMGY4MzQ5ODJmMTY2NjUyY2ZiNzNlMzQ0Yzc4MGIxZmEyMWZjZjQ5NzQwYjg1NDg4Yzc1MWExYTc2OTNiMA%3D%3D&amp;v=1&amp;f=sd", "x": 1920, "y": 1080, "hlsUrl": "https://v.redd.it/link/11n0wvb/asset/7iovf763lrma1/HLSPlaylist.m3u8?a=1680997166%2CNjk4MTI2OWEzMWVjNzg2YzRkOWYyMzI1MjkwYThiMGUzYTA5NWY3YjE4YWI4ZTFjYWUwNGY1ZGM4NDQ3ZDJmMQ%3D%3D&amp;v=1&amp;f=sd", "id": "7iovf763lrma1", "isGif": false}}, "name": "t3_11n0wvb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678389867.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678389345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Beginning my adventure into this data hoarding world. Got my hardware, all excited and then... What the hell is THAT noise!? I really have no idea. As soon as I got everything, hard drive went straight from its box into the DS920 so yeah uh. Idk. Please help :(&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/11n0wvb/video/7iovf763lrma1/player\"&gt;Edit: Idiot, forgot to include the video&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n0wvb", "is_robot_indexable": true, "report_reasons": null, "author": "Ban_Hammered", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n0wvb/what_the_hell_is_this_noise_20tb_seagate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n0wvb/what_the_hell_is_this_noise_20tb_seagate/", "subreddit_subscribers": 672681, "created_utc": 1678389345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello\n\n**What is everyones thoughts on this product?**[ https://www.amazon.co.uk/dp/B09DYW25WB](https://www.amazon.co.uk/dp/B09DYW25WB) ?\n\n**It advertises \"Up to 80TB Large Capacity\" - What is the limitation of this device that stops it supporting more capacity? for example 5\\*20TB = 100TB total? would it just not work if I do that?**\n\n**Does anyone know the idle power consumption of it?** When no hard drives are spinning that is. I plan to use hdparm in linux to make my HDD to spindown after a few minutes of inactivity.  \n\n\nMany thanks", "author_fullname": "t2_9p2m2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ORICO 5Bay Hard Drive Enclosure - DS500U3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mvacj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678376233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What is everyones thoughts on this product?&lt;/strong&gt;&lt;a href=\"https://www.amazon.co.uk/dp/B09DYW25WB\"&gt; https://www.amazon.co.uk/dp/B09DYW25WB&lt;/a&gt; ?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;It advertises &amp;quot;Up to 80TB Large Capacity&amp;quot; - What is the limitation of this device that stops it supporting more capacity? for example 5*20TB = 100TB total? would it just not work if I do that?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Does anyone know the idle power consumption of it?&lt;/strong&gt; When no hard drives are spinning that is. I plan to use hdparm in linux to make my HDD to spindown after a few minutes of inactivity.  &lt;/p&gt;\n\n&lt;p&gt;Many thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11mvacj", "is_robot_indexable": true, "report_reasons": null, "author": "dewijones92", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11mvacj/orico_5bay_hard_drive_enclosure_ds500u3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11mvacj/orico_5bay_hard_drive_enclosure_ds500u3/", "subreddit_subscribers": 672681, "created_utc": 1678376233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings!\n\nThe question is about 2x 8TB Seagate Ironwolf HDDs that I have for storage. I made them RAID0-like by merging in single LV with LVM (I don't remember was it linear or striped). FS is ext4, unencrypted.\n\nI've noticed almost instant (within a day) increase in spin up time. Previously after spin down when I hit ls in a dir or started a video in Jellyfin it would take about 1-2 seconds to reply. Now it takes about 10-15 seconds. The difference is huge and I don't know what is the cause. \n\nSome ideas:\n1. LV is about &gt;50% full, content was ~10-40 GB files written linearly (pre-allocated) and rarely deleted. So my idea here is that FS wrote it all on the first drive and I get slow spin up because now it spins up the second slow drive.\n2. I've recently hit it with 700k files about 1 MB each, twice. Now they're deleted as I'm doing more processing on them at the moment at my workstation. Maybe it fragmented FS quite bad and I have something similar to idea no. 1? Or \"fragmented\" LVM in some way, so the performance was reduced?\n3. I update quarterly and my maintenance was exactly a week ago. OS is Fedora 37 Silverblue. I'm unsure whether it could be due to an update, cause I'm pretty sure I'd notice it almost immediately after an update.  Maybe they just reduced spin-down time and I feel long spin up only now?\n\nStrange parts:\n1. When I play a video on Jellyfin, I have no stutters or whatever. But when I open next video in a playlist, I'm waiting the delay. I guess it could easily cache that 1 GB video file in RAM. but...\n2. I had 500 GB BT share being downloaded from my server slowly (about 40 kbps per peer, about 20 peers), stored at that LV. Even when it was uploading continuously, I still experience the long delay. I doubt it would cache multiple ~random 5 GB files in RAM as I have only about 20 GB RAM free for cache.\n\nShould I be worried? I remember I've read somewhere that high spin up times might mean pre-fail state.\n\nEven reading smart data made a longer than usual delay, on both drives actually. But not 15 sec, just about 6 sec. [Smart data](https://paste.voronind.com/s?fec9aa3f9944b2c5#6Uhb3V98pJBaLuA185Du34K3hyK6Ek1XZDhVqMdV2dN5).\n\nYes, I have daily remote backups that I used to restore from in the past. \n\nThank you, have a great day!", "author_fullname": "t2_dleysue2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD spin up time increased unexpectedly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11n63it", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678401105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings!&lt;/p&gt;\n\n&lt;p&gt;The question is about 2x 8TB Seagate Ironwolf HDDs that I have for storage. I made them RAID0-like by merging in single LV with LVM (I don&amp;#39;t remember was it linear or striped). FS is ext4, unencrypted.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed almost instant (within a day) increase in spin up time. Previously after spin down when I hit ls in a dir or started a video in Jellyfin it would take about 1-2 seconds to reply. Now it takes about 10-15 seconds. The difference is huge and I don&amp;#39;t know what is the cause. &lt;/p&gt;\n\n&lt;p&gt;Some ideas:\n1. LV is about &amp;gt;50% full, content was ~10-40 GB files written linearly (pre-allocated) and rarely deleted. So my idea here is that FS wrote it all on the first drive and I get slow spin up because now it spins up the second slow drive.\n2. I&amp;#39;ve recently hit it with 700k files about 1 MB each, twice. Now they&amp;#39;re deleted as I&amp;#39;m doing more processing on them at the moment at my workstation. Maybe it fragmented FS quite bad and I have something similar to idea no. 1? Or &amp;quot;fragmented&amp;quot; LVM in some way, so the performance was reduced?\n3. I update quarterly and my maintenance was exactly a week ago. OS is Fedora 37 Silverblue. I&amp;#39;m unsure whether it could be due to an update, cause I&amp;#39;m pretty sure I&amp;#39;d notice it almost immediately after an update.  Maybe they just reduced spin-down time and I feel long spin up only now?&lt;/p&gt;\n\n&lt;p&gt;Strange parts:\n1. When I play a video on Jellyfin, I have no stutters or whatever. But when I open next video in a playlist, I&amp;#39;m waiting the delay. I guess it could easily cache that 1 GB video file in RAM. but...\n2. I had 500 GB BT share being downloaded from my server slowly (about 40 kbps per peer, about 20 peers), stored at that LV. Even when it was uploading continuously, I still experience the long delay. I doubt it would cache multiple ~random 5 GB files in RAM as I have only about 20 GB RAM free for cache.&lt;/p&gt;\n\n&lt;p&gt;Should I be worried? I remember I&amp;#39;ve read somewhere that high spin up times might mean pre-fail state.&lt;/p&gt;\n\n&lt;p&gt;Even reading smart data made a longer than usual delay, on both drives actually. But not 15 sec, just about 6 sec. &lt;a href=\"https://paste.voronind.com/s?fec9aa3f9944b2c5#6Uhb3V98pJBaLuA185Du34K3hyK6Ek1XZDhVqMdV2dN5\"&gt;Smart data&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Yes, I have daily remote backups that I used to restore from in the past. &lt;/p&gt;\n\n&lt;p&gt;Thank you, have a great day!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n63it", "is_robot_indexable": true, "report_reasons": null, "author": "cakee_ru", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n63it/hdd_spin_up_time_increased_unexpectedly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n63it/hdd_spin_up_time_increased_unexpectedly/", "subreddit_subscribers": 672681, "created_utc": 1678401105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!\nI'm trying to download this game installer from WayBack Machine: https://web.archive.org/web/*/http://cdndownload.91.com/dxc/20120316_dkonline_3115_client_gf.exe\n\nThe exe file is around 1,80GB big. The downloading from the archive starts fine but always fails at around 430MB. I tried to download it on my phone on a different network connection but still the same problem. The thing is I cannot find the game download anywhere else on the internet because it was discontinued in 2013, so that's why I came looking for it on WayBack Machine.\nDoes anyone know what the problem is? If the problem is on WayBack Machine's side, is there some kind of search thing on WayBack Machine where I can search for more downloads of this game or older/newer versions of it? There must be more available archived downloads of this game on WayBack Machine. Not just this single one.\nHave a nice day!", "author_fullname": "t2_wn1jt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help downloading a lost game from WayBack Machine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mxa5n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678380978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!\nI&amp;#39;m trying to download this game installer from WayBack Machine: &lt;a href=\"https://web.archive.org/web/*/http://cdndownload.91.com/dxc/20120316_dkonline_3115_client_gf.exe\"&gt;https://web.archive.org/web/*/http://cdndownload.91.com/dxc/20120316_dkonline_3115_client_gf.exe&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The exe file is around 1,80GB big. The downloading from the archive starts fine but always fails at around 430MB. I tried to download it on my phone on a different network connection but still the same problem. The thing is I cannot find the game download anywhere else on the internet because it was discontinued in 2013, so that&amp;#39;s why I came looking for it on WayBack Machine.\nDoes anyone know what the problem is? If the problem is on WayBack Machine&amp;#39;s side, is there some kind of search thing on WayBack Machine where I can search for more downloads of this game or older/newer versions of it? There must be more available archived downloads of this game on WayBack Machine. Not just this single one.\nHave a nice day!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11mxa5n", "is_robot_indexable": true, "report_reasons": null, "author": "StayPlay40K", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11mxa5n/help_downloading_a_lost_game_from_wayback_machine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11mxa5n/help_downloading_a_lost_game_from_wayback_machine/", "subreddit_subscribers": 672681, "created_utc": 1678380978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way of using EXIFtool or anything like that for renaming photos with street names using GPS data from exif info of the photo? I need photos to be categorized by street and number. Please help!", "author_fullname": "t2_1wuzfucl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing and renaming my photos after street name with EXIFtool.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mx86w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678380850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way of using EXIFtool or anything like that for renaming photos with street names using GPS data from exif info of the photo? I need photos to be categorized by street and number. Please help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11mx86w", "is_robot_indexable": true, "report_reasons": null, "author": "theandreineagu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11mx86w/organizing_and_renaming_my_photos_after_street/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11mx86w/organizing_and_renaming_my_photos_after_street/", "subreddit_subscribers": 672681, "created_utc": 1678380850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Data hoarders and archivists, take note: there's a powerful new tool in town that makes it easier than ever to preserve the internet's history. Meet Wayback, an open-source archiving utility that allows you to capture and store web pages for future reference.\n\nBut what sets Wayback apart from other archiving tools? For one, it's incredibly easy to use. Simply enter a URL into Wayback, and the tool will take care of the rest. It captures not just the HTML of the page, but also any linked CSS, JavaScript, and other assets, ensuring that the page is preserved as it was when it was captured.\n\nBut Wayback doesn't just capture individual pages \u2013 it also provides a way to browse the web as it was in the past. Thanks to its integration with the Internet Archive, Wayback allows you to search through billions of archived web pages, going back as far as 1996.\n\nFor data hoarders, this is a godsend. By using Wayback, you can ensure that valuable web content is preserved for future generations. Whether you're archiving a personal blog, a news site, or an entire web domain, Wayback makes it easy to capture and store web pages in a way that's both comprehensive and future-proof.\n\nBut what about sites that are difficult to capture, such as those with dynamic content or heavy use of JavaScript? Wayback has you covered there, too. It uses a headless browser to capture dynamic content, and can even be configured to wait for JavaScript to finish executing before taking a snapshot.\n\nOf course, Wayback isn't perfect \u2013 no archiving tool is. There are still some sites that can't be captured, and some content that may be lost over time. But as far as archiving tools go, Wayback is one of the best \u2013 and it's only getting better.\n\nSo if you're a data hoarder or archivist looking for a powerful, easy-to-use tool for preserving the internet's history, look no further than Wayback. Give it a try today, and give it star at https://github.com/wabarc/wayback.", "author_fullname": "t2_la2qylbp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Preserve the Internet: A New Way to Wayback Webpages", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ms1y3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678368129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data hoarders and archivists, take note: there&amp;#39;s a powerful new tool in town that makes it easier than ever to preserve the internet&amp;#39;s history. Meet Wayback, an open-source archiving utility that allows you to capture and store web pages for future reference.&lt;/p&gt;\n\n&lt;p&gt;But what sets Wayback apart from other archiving tools? For one, it&amp;#39;s incredibly easy to use. Simply enter a URL into Wayback, and the tool will take care of the rest. It captures not just the HTML of the page, but also any linked CSS, JavaScript, and other assets, ensuring that the page is preserved as it was when it was captured.&lt;/p&gt;\n\n&lt;p&gt;But Wayback doesn&amp;#39;t just capture individual pages \u2013 it also provides a way to browse the web as it was in the past. Thanks to its integration with the Internet Archive, Wayback allows you to search through billions of archived web pages, going back as far as 1996.&lt;/p&gt;\n\n&lt;p&gt;For data hoarders, this is a godsend. By using Wayback, you can ensure that valuable web content is preserved for future generations. Whether you&amp;#39;re archiving a personal blog, a news site, or an entire web domain, Wayback makes it easy to capture and store web pages in a way that&amp;#39;s both comprehensive and future-proof.&lt;/p&gt;\n\n&lt;p&gt;But what about sites that are difficult to capture, such as those with dynamic content or heavy use of JavaScript? Wayback has you covered there, too. It uses a headless browser to capture dynamic content, and can even be configured to wait for JavaScript to finish executing before taking a snapshot.&lt;/p&gt;\n\n&lt;p&gt;Of course, Wayback isn&amp;#39;t perfect \u2013 no archiving tool is. There are still some sites that can&amp;#39;t be captured, and some content that may be lost over time. But as far as archiving tools go, Wayback is one of the best \u2013 and it&amp;#39;s only getting better.&lt;/p&gt;\n\n&lt;p&gt;So if you&amp;#39;re a data hoarder or archivist looking for a powerful, easy-to-use tool for preserving the internet&amp;#39;s history, look no further than Wayback. Give it a try today, and give it star at &lt;a href=\"https://github.com/wabarc/wayback\"&gt;https://github.com/wabarc/wayback&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mJhGy3BfoSK5_c18UwdbowG8cSdq_Ierl2S1bYntwgc.jpg?auto=webp&amp;v=enabled&amp;s=32748605b82ee4f369eae29085ea8969a9d3dd04", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/mJhGy3BfoSK5_c18UwdbowG8cSdq_Ierl2S1bYntwgc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48752a8436b49624c1098c727c90e7791778c5ab", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/mJhGy3BfoSK5_c18UwdbowG8cSdq_Ierl2S1bYntwgc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea0170b8588e912f803cbadf696f793209642f2f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/mJhGy3BfoSK5_c18UwdbowG8cSdq_Ierl2S1bYntwgc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7bb88fdee6b0c42f00010d01cfa8b504a60871d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/mJhGy3BfoSK5_c18UwdbowG8cSdq_Ierl2S1bYntwgc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=095483c2eb939e7c6216f4e94b4ebaa0464e2e38", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/mJhGy3BfoSK5_c18UwdbowG8cSdq_Ierl2S1bYntwgc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6deb83a16160ef4dbdf458cb42f1d564bc4ab3de", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/mJhGy3BfoSK5_c18UwdbowG8cSdq_Ierl2S1bYntwgc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4dcc390eaaf70b292dee461c9973d47cd3b216e2", "width": 1080, "height": 540}], "variants": {}, "id": "WmbunmoEE_RBIyUxK-3w_t8snlr2GOprOqIdpd42-mA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ms1y3", "is_robot_indexable": true, "report_reasons": null, "author": "waybackarchiver", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ms1y3/preserve_the_internet_a_new_way_to_wayback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ms1y3/preserve_the_internet_a_new_way_to_wayback/", "subreddit_subscribers": 672681, "created_utc": 1678368129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! I'm an amateur data hoarder, and I'm having trouble figuring out how to access the data of an entire Weibo account. When I use a data scraper, gallery-dl, or manual scrolling, Sina Weibo stops short of the entire account and sends a \"there is no more content\" (at different points for mobile, desktop, and with the data scraper) long before the content actually ends. Does anyone know any methods-extensions,GitHub,websites,anything!- to be able to access the entirety of the accounts data?", "author_fullname": "t2_mek1y4er", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to scrape/view an entire Weibo account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11moi0l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678356471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I&amp;#39;m an amateur data hoarder, and I&amp;#39;m having trouble figuring out how to access the data of an entire Weibo account. When I use a data scraper, gallery-dl, or manual scrolling, Sina Weibo stops short of the entire account and sends a &amp;quot;there is no more content&amp;quot; (at different points for mobile, desktop, and with the data scraper) long before the content actually ends. Does anyone know any methods-extensions,GitHub,websites,anything!- to be able to access the entirety of the accounts data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11moi0l", "is_robot_indexable": true, "report_reasons": null, "author": "neo_theproletariat", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11moi0l/how_to_scrapeview_an_entire_weibo_account/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11moi0l/how_to_scrapeview_an_entire_weibo_account/", "subreddit_subscribers": 672681, "created_utc": 1678356471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've searched, I've asked Google, etc., but no one has given me an answer and I'm looking to fellow Google Workplace Admins for their experience.\n\nWe currently use Google Workspace Business Plus. We have the Archived users addon. But the Archived users addon has a hard limit of 50 archived users. We need to archive all past users, and it would be cheaper, and better managed, to archive users than to leave them in a suspended state. Google's documentation says that you can buy more archived users for all plan levels, but I know you can't for Google Workspace Business Plus because the subscription section doesn't let you add another batch of 50 licenses and in my calls with Google they said that I can't. But every time I tell them I would upgrade to Enterprise Standard or Plus if they can tell me whether I can add more than 50 archived users in those plan levels, they never get back to me.\n\nDoes anyone have experience with this? And could I add more than 50 archived users with Enterprise Standard or Plus?", "author_fullname": "t2_hqeza29r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Workplace Business Enterprise Archived Users Limits", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11n5wug", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678400702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve searched, I&amp;#39;ve asked Google, etc., but no one has given me an answer and I&amp;#39;m looking to fellow Google Workplace Admins for their experience.&lt;/p&gt;\n\n&lt;p&gt;We currently use Google Workspace Business Plus. We have the Archived users addon. But the Archived users addon has a hard limit of 50 archived users. We need to archive all past users, and it would be cheaper, and better managed, to archive users than to leave them in a suspended state. Google&amp;#39;s documentation says that you can buy more archived users for all plan levels, but I know you can&amp;#39;t for Google Workspace Business Plus because the subscription section doesn&amp;#39;t let you add another batch of 50 licenses and in my calls with Google they said that I can&amp;#39;t. But every time I tell them I would upgrade to Enterprise Standard or Plus if they can tell me whether I can add more than 50 archived users in those plan levels, they never get back to me.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with this? And could I add more than 50 archived users with Enterprise Standard or Plus?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n5wug", "is_robot_indexable": true, "report_reasons": null, "author": "Easy_Ambition_1072", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n5wug/google_workplace_business_enterprise_archived/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n5wug/google_workplace_business_enterprise_archived/", "subreddit_subscribers": 672681, "created_utc": 1678400702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a machine with pairs of hardrives in it functioning as a file warehouse and I am trying to find good solutions for backing up the data. The data is mostly tarballs that stay constant, but can be renamed, and occasionally deleted. I am hoping to find a system that is smart enough to automatically rename files in the backup instead of duplicating them, but also wont automatically delete files in the backup. The machine runs systems other than host the warehouse though so I am avoiding whole-os solutions if at all possible. Are there any known good solutions people are aware of or will I have to roll my own?", "author_fullname": "t2_qngs2dbp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backups on a multipurpose machine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11n5k0a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678399915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a machine with pairs of hardrives in it functioning as a file warehouse and I am trying to find good solutions for backing up the data. The data is mostly tarballs that stay constant, but can be renamed, and occasionally deleted. I am hoping to find a system that is smart enough to automatically rename files in the backup instead of duplicating them, but also wont automatically delete files in the backup. The machine runs systems other than host the warehouse though so I am avoiding whole-os solutions if at all possible. Are there any known good solutions people are aware of or will I have to roll my own?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n5k0a", "is_robot_indexable": true, "report_reasons": null, "author": "ByteArchivist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n5k0a/backups_on_a_multipurpose_machine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n5k0a/backups_on_a_multipurpose_machine/", "subreddit_subscribers": 672681, "created_utc": 1678399915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I rewrite now my old post and i hope its easiert to understand now : \n\n\\-&gt; I search for a system that covers all of my data handling needs, i want to work with it in a way that i can load stuff from it, and dump stuff onto it, all with a quick performance. I dont know how to describe that better... i just want a working and reliable NAS. \n\nSo im asking what kind of NAS you would recommend and also which hard drives to put into it... \n\n:) \n\n&amp;#x200B;\n\nAll the best !", "author_fullname": "t2_5462sezu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS - Network Attaches Storage - What brands / models of harddrives and what NAS to go with?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11n52bm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678398840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I rewrite now my old post and i hope its easiert to understand now : &lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; I search for a system that covers all of my data handling needs, i want to work with it in a way that i can load stuff from it, and dump stuff onto it, all with a quick performance. I dont know how to describe that better... i just want a working and reliable NAS. &lt;/p&gt;\n\n&lt;p&gt;So im asking what kind of NAS you would recommend and also which hard drives to put into it... &lt;/p&gt;\n\n&lt;p&gt;:) &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;All the best !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n52bm", "is_robot_indexable": true, "report_reasons": null, "author": "Witzmastah", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n52bm/nas_network_attaches_storage_what_brands_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n52bm/nas_network_attaches_storage_what_brands_models/", "subreddit_subscribers": 672681, "created_utc": 1678398840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am just curious as to whether this is a statistical attribute or an attribute that can indicate drive failure or motor problems?\n\nMy 41 day old 18tb easystore's Throughput Performance raw value - monitored daily with Hard Disk Sentinel - seems to switch between 107 and 108 day to day and has never been anything else but one of those two numbers... Today I checked and it is at 105. Is this anything to be concerned about?", "author_fullname": "t2_6auo27yj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What exactly does the S.M.A.R.T. attribute \"Throughput Performance\" signify?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n4kt9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678397731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am just curious as to whether this is a statistical attribute or an attribute that can indicate drive failure or motor problems?&lt;/p&gt;\n\n&lt;p&gt;My 41 day old 18tb easystore&amp;#39;s Throughput Performance raw value - monitored daily with Hard Disk Sentinel - seems to switch between 107 and 108 day to day and has never been anything else but one of those two numbers... Today I checked and it is at 105. Is this anything to be concerned about?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n4kt9", "is_robot_indexable": true, "report_reasons": null, "author": "fuckAraZobayan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n4kt9/what_exactly_does_the_smart_attribute_throughput/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n4kt9/what_exactly_does_the_smart_attribute_throughput/", "subreddit_subscribers": 672681, "created_utc": 1678397731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Prism Drive is offering 10TB of cloud storage for life for $90. Forget about it!\n\nSee:  [https://www.trustpilot.com/review/prismdrive.com](https://www.trustpilot.com/review/prismdrive.com) 1.6 stars\n\nWhat good is cloud storage if you can't upload files? Or maybe  89% of the reviewers are wrong? Nope.", "author_fullname": "t2_b2q4xmre", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Avoid Prism Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n4jy3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Lifetime Deal - Not!", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678397678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Prism Drive is offering 10TB of cloud storage for life for $90. Forget about it!&lt;/p&gt;\n\n&lt;p&gt;See:  &lt;a href=\"https://www.trustpilot.com/review/prismdrive.com\"&gt;https://www.trustpilot.com/review/prismdrive.com&lt;/a&gt; 1.6 stars&lt;/p&gt;\n\n&lt;p&gt;What good is cloud storage if you can&amp;#39;t upload files? Or maybe  89% of the reviewers are wrong? Nope.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Bj7tMI1iI5SOjqtzuspejjcta4H-XmjzOEtOW5DiQiI.jpg?auto=webp&amp;v=enabled&amp;s=878f33d6fef30f0543860223fb03c89a3208b207", "width": 1080, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/Bj7tMI1iI5SOjqtzuspejjcta4H-XmjzOEtOW5DiQiI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d39abc3c2955115e2e78497134a5b7d650ffc896", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Bj7tMI1iI5SOjqtzuspejjcta4H-XmjzOEtOW5DiQiI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=907b142ff4e4796e34653382f9557c8e233b4a7d", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Bj7tMI1iI5SOjqtzuspejjcta4H-XmjzOEtOW5DiQiI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=626eda2b1ccee07455e5c9118d0b1225a29755a0", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/Bj7tMI1iI5SOjqtzuspejjcta4H-XmjzOEtOW5DiQiI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10436f7fe56e86f33b10ce6120621a11f3461247", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/Bj7tMI1iI5SOjqtzuspejjcta4H-XmjzOEtOW5DiQiI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe9d7035ed5e30f76cea420e7d7c4e6357a5a44e", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/Bj7tMI1iI5SOjqtzuspejjcta4H-XmjzOEtOW5DiQiI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29e6a1d5d65f11a20797b3ee46d6ca751ca5871c", "width": 1080, "height": 1080}], "variants": {}, "id": "1UbPDgWYbagXW8uwxaggJzwX6ugY00nnrXQQVmqH3dY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11n4jy3", "is_robot_indexable": true, "report_reasons": null, "author": "Xeronolej", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n4jy3/avoid_prism_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n4jy3/avoid_prism_drive/", "subreddit_subscribers": 672681, "created_utc": 1678397678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently picked up a used 14TB WD Red Plus from an auction. The drive could not be detected on a test PC with Windows 10. Nothing shows on the Windows partition manager or BIOS. The drive powers on and spins. I have tried swapping the cables, different SATA ports and used Hard Disk Sentinel to try to initialize the drive but no dice. I checked the serial and the warranty is valid until 2028 but as the purchase wasn\u2019t from an authorized dealer, they probably won\u2019t service it. Still, I could register it on their website which accepts front and back photos of the hard drive in lieu of a receipt. Before I try this and do an RMA at my own expense, I want to verify the serial number actually corresponds to the drive itself and the label wasn\u2019t a swap job. As I can\u2019t initialize the drive and check through device manager, I need another way to verify. It is possible that the drive may have incurred fall damage during the return window although there are no scuff marks or even a single scratch to indicate that this is the case. \n\nAny ideas on how to best proceed?", "author_fullname": "t2_3xu6kqgm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Red Plus fails to initialize", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n4atx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678397110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently picked up a used 14TB WD Red Plus from an auction. The drive could not be detected on a test PC with Windows 10. Nothing shows on the Windows partition manager or BIOS. The drive powers on and spins. I have tried swapping the cables, different SATA ports and used Hard Disk Sentinel to try to initialize the drive but no dice. I checked the serial and the warranty is valid until 2028 but as the purchase wasn\u2019t from an authorized dealer, they probably won\u2019t service it. Still, I could register it on their website which accepts front and back photos of the hard drive in lieu of a receipt. Before I try this and do an RMA at my own expense, I want to verify the serial number actually corresponds to the drive itself and the label wasn\u2019t a swap job. As I can\u2019t initialize the drive and check through device manager, I need another way to verify. It is possible that the drive may have incurred fall damage during the return window although there are no scuff marks or even a single scratch to indicate that this is the case. &lt;/p&gt;\n\n&lt;p&gt;Any ideas on how to best proceed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n4atx", "is_robot_indexable": true, "report_reasons": null, "author": "Yantarlok", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n4atx/wd_red_plus_fails_to_initialize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n4atx/wd_red_plus_fails_to_initialize/", "subreddit_subscribers": 672681, "created_utc": 1678397110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 7-bay NAS that I built a RAID 6 array on. I recently replaced the drives with new ones and just stored my old drives away without wiping them. Can I take out my new drives and then re-install my old ones to take a look at all of my old data? Can I then reinsert all of my new drives back in the NAS once I'm done without affecting anything or losing any data? Thanks.", "author_fullname": "t2_p9vg3wjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I throw my old RAID drives back in my NAS? Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n2y6m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678394032.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 7-bay NAS that I built a RAID 6 array on. I recently replaced the drives with new ones and just stored my old drives away without wiping them. Can I take out my new drives and then re-install my old ones to take a look at all of my old data? Can I then reinsert all of my new drives back in the NAS once I&amp;#39;m done without affecting anything or losing any data? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n2y6m", "is_robot_indexable": true, "report_reasons": null, "author": "TCIE", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n2y6m/can_i_throw_my_old_raid_drives_back_in_my_nas_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n2y6m/can_i_throw_my_old_raid_drives_back_in_my_nas_help/", "subreddit_subscribers": 672681, "created_utc": 1678394032.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just as title says. I've got a bunch of drives to go though and check, and it'd be a lot easier if I can get something to use even in the field, rather than having to somehow free up my SATA ports at home. No listings I've dug through give any info on if they can let you read the drive's S.M.A.R.T. data, and the ones I've used block any checks since the docks have *their own* S.M.A.R.T. data.\n\nAny help or info is appreciated!", "author_fullname": "t2_w9mx6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for USB drive dock/dongle with S.M.A.R.T. \"passthrough\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n1dh2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678390381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just as title says. I&amp;#39;ve got a bunch of drives to go though and check, and it&amp;#39;d be a lot easier if I can get something to use even in the field, rather than having to somehow free up my SATA ports at home. No listings I&amp;#39;ve dug through give any info on if they can let you read the drive&amp;#39;s S.M.A.R.T. data, and the ones I&amp;#39;ve used block any checks since the docks have &lt;em&gt;their own&lt;/em&gt; S.M.A.R.T. data.&lt;/p&gt;\n\n&lt;p&gt;Any help or info is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n1dh2", "is_robot_indexable": true, "report_reasons": null, "author": "yamzee", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n1dh2/looking_for_usb_drive_dockdongle_with_smart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n1dh2/looking_for_usb_drive_dockdongle_with_smart/", "subreddit_subscribers": 672681, "created_utc": 1678390381.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I thought this [blog post/whitepaper](https://dropbox.tech/infrastructure/four-years-of-smr-storage-what-we-love-and-whats-next) from dropbox was interesting.  Conspicuously missing from the post is any mention of how using SMR has affected write performance, and only a cursory hand-waving about how they attempted to ameliorate the performance penalties, but an interesting overview of their experiences, and how SMR has helped them achieve much higher density, along with cost and power savings.", "author_fullname": "t2_8za3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The positive side of SMR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n0kca", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678388538.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought this &lt;a href=\"https://dropbox.tech/infrastructure/four-years-of-smr-storage-what-we-love-and-whats-next\"&gt;blog post/whitepaper&lt;/a&gt; from dropbox was interesting.  Conspicuously missing from the post is any mention of how using SMR has affected write performance, and only a cursory hand-waving about how they attempted to ameliorate the performance penalties, but an interesting overview of their experiences, and how SMR has helped them achieve much higher density, along with cost and power savings.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?auto=webp&amp;v=enabled&amp;s=b21c2418753b5628f18962d82d3d9a626e76eebc", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85709443795963dfeab8995e54970230fd8eefb1", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93c9d1523dd8f7432ed9eafd3c450696a44216db", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ba255c083eef77e0397f347c9d995542f19a05a", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23dcfc3cbf1fb0264f6085b55da28d3de3139329", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f7754ae0c9bc1e9d66cc67b1545e633656f1c66", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b541b658216c00c466fe1af6ffcb5a89dc44cbf3", "width": 1080, "height": 565}], "variants": {}, "id": "Fu1Qer5s91j2J3UBkhCf3H59eCu9Ane8MkOnvbEvgxs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11n0kca", "is_robot_indexable": true, "report_reasons": null, "author": "linuxturtle", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n0kca/the_positive_side_of_smr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n0kca/the_positive_side_of_smr/", "subreddit_subscribers": 672681, "created_utc": 1678388538.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had multiple 4TB HDD because my parents gifted them to me thinking \"you use pc here are hdd\", I really wanted to use them just because it's their gift. Made 3-2-1 backups of family photos and some important files, now what?\n\nMy budget is limited but I am open to invest something if it's worth it. I have basic knowledge but I don't really have that many files or need for perpetual backups or anything.\n\nImportant:\n\n* occasional 100-200GB photos backup from phones (iPhones and Android), manual\n* occasional 500GB backups of video editing projects and files, manual\n* best if easy to setup, even if this means losing on something else\n* \\&gt;20TB expandable\n\nAdditional:\n\n* private server where I can test code\n* auto-backup\n\nBudget:\n\n* 100-700$, I know it's low\n\nWhat now?\n\n* Docking for HDDs? How much can I rely on them?\n* NAS? I only know Synology for user friendliness, running on the network I'm not sure my mighty 60Mbps internet will be enough", "author_fullname": "t2_nm5xx7wx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First 16TB gone, can you advice a simple setup? (only used external HDDs)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n0jcx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678389475.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678388482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had multiple 4TB HDD because my parents gifted them to me thinking &amp;quot;you use pc here are hdd&amp;quot;, I really wanted to use them just because it&amp;#39;s their gift. Made 3-2-1 backups of family photos and some important files, now what?&lt;/p&gt;\n\n&lt;p&gt;My budget is limited but I am open to invest something if it&amp;#39;s worth it. I have basic knowledge but I don&amp;#39;t really have that many files or need for perpetual backups or anything.&lt;/p&gt;\n\n&lt;p&gt;Important:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;occasional 100-200GB photos backup from phones (iPhones and Android), manual&lt;/li&gt;\n&lt;li&gt;occasional 500GB backups of video editing projects and files, manual&lt;/li&gt;\n&lt;li&gt;best if easy to setup, even if this means losing on something else&lt;/li&gt;\n&lt;li&gt;&amp;gt;20TB expandable&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Additional:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;private server where I can test code&lt;/li&gt;\n&lt;li&gt;auto-backup&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Budget:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;100-700$, I know it&amp;#39;s low&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What now?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Docking for HDDs? How much can I rely on them?&lt;/li&gt;\n&lt;li&gt;NAS? I only know Synology for user friendliness, running on the network I&amp;#39;m not sure my mighty 60Mbps internet will be enough&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11n0jcx", "is_robot_indexable": true, "report_reasons": null, "author": "OkOpportunity1516", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11n0jcx/first_16tb_gone_can_you_advice_a_simple_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11n0jcx/first_16tb_gone_can_you_advice_a_simple_setup/", "subreddit_subscribers": 672681, "created_utc": 1678388482.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}