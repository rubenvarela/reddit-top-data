{"kind": "Listing", "data": {"after": "t3_11m458m", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't know about you, but I have plenty of data engineering horror stories to share. I'd love to hear the one that still gives you shivers.\n\n  \n**Here's my highlight:**  \n\n\n* I'm at a 500 people medium sized company, 300-400$m revenue. But we're growing strong, at 10-20% each year. Ingest data via python, dbt for transformations, storage in PostgreSQL and Tableau as BI tool on top of it.\n* We've been pushing the adoption of Tableau, and the company is eating it up, they love it. They are all over the dashboards.\n* What we're particular proud of is our **\"north star metric**\" dashboard, showing our new key metric, based on a recent business pivot.\n* In our most important customer segment, it looks like it's starting to grow **exponentially**! Everyone is excited!\n\nSuddenly an important manager calls me up\n\n\"*hey, something is wrong with the north star. The dashboard looked completely different yesterday! Our exponential growth is gone! Surely there is something wrong, please fix it by this evening. Tomorrow is the board meeting and I'm presenting the exponential growth.\"*\n\n  \nTook us some time to understand this one... Apparently, ALL data changed, the complete metric in this customer segment broke in, not just for today, but also for yesterday, the day before, and so on...\n\n  \nAfter some research, we realized a huge problem: The biggest customer in that segment left a few months ago, and filed a \"deletion request\". The upstream team responsible for this followed through, and basically \"detached the relevant data from the customer account\". \n\n  \nSo there we were. We didn't even know about this process, and had no chance to recover. The manager was left without his exponential growth. \n\n  \n*Aftermath: So what we did from then on is to turn on snapshotting of important data sources (using dbt). After being really unhappy, the manager was still convinced of the underlying exponential growth which shouldn't be reliant on one big customer, but the situation felt terrible. And I'm quite happy that only data from basically one customer went down the drain.*\n\n\\----\n\n  \nHow about you? Do you have a horror story to share?", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got a data engineering horror story, what is yours?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lvm8z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 155, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 155, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678279973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know about you, but I have plenty of data engineering horror stories to share. I&amp;#39;d love to hear the one that still gives you shivers.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here&amp;#39;s my highlight:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;m at a 500 people medium sized company, 300-400$m revenue. But we&amp;#39;re growing strong, at 10-20% each year. Ingest data via python, dbt for transformations, storage in PostgreSQL and Tableau as BI tool on top of it.&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve been pushing the adoption of Tableau, and the company is eating it up, they love it. They are all over the dashboards.&lt;/li&gt;\n&lt;li&gt;What we&amp;#39;re particular proud of is our &lt;strong&gt;&amp;quot;north star metric&lt;/strong&gt;&amp;quot; dashboard, showing our new key metric, based on a recent business pivot.&lt;/li&gt;\n&lt;li&gt;In our most important customer segment, it looks like it&amp;#39;s starting to grow &lt;strong&gt;exponentially&lt;/strong&gt;! Everyone is excited!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Suddenly an important manager calls me up&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&lt;em&gt;hey, something is wrong with the north star. The dashboard looked completely different yesterday! Our exponential growth is gone! Surely there is something wrong, please fix it by this evening. Tomorrow is the board meeting and I&amp;#39;m presenting the exponential growth.&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Took us some time to understand this one... Apparently, ALL data changed, the complete metric in this customer segment broke in, not just for today, but also for yesterday, the day before, and so on...&lt;/p&gt;\n\n&lt;p&gt;After some research, we realized a huge problem: The biggest customer in that segment left a few months ago, and filed a &amp;quot;deletion request&amp;quot;. The upstream team responsible for this followed through, and basically &amp;quot;detached the relevant data from the customer account&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;So there we were. We didn&amp;#39;t even know about this process, and had no chance to recover. The manager was left without his exponential growth. &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Aftermath: So what we did from then on is to turn on snapshotting of important data sources (using dbt). After being really unhappy, the manager was still convinced of the underlying exponential growth which shouldn&amp;#39;t be reliant on one big customer, but the situation felt terrible. And I&amp;#39;m quite happy that only data from basically one customer went down the drain.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;----&lt;/p&gt;\n\n&lt;p&gt;How about you? Do you have a horror story to share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11lvm8z", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 77, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lvm8z/i_got_a_data_engineering_horror_story_what_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lvm8z/i_got_a_data_engineering_horror_story_what_is/", "subreddit_subscribers": 92360, "created_utc": 1678279973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I'm in the Seattle area and was offered an analytics engineer role since I previously interned at the place last summer. Basically what we do is that we work in our Snowflake database and clean our data and make data enrichments. Technologies that we primarily use are SQL, dbt, Airflow, and a bit of Python for UDFs. The job is extremely easy when I was an intern, but I wanted to work as a SWE instead. I feel like this position is more similar to a data analyst position.\n\nI got an offer of 104k total comp which was too low for me. I replied to negotiate for around 130k instead but the recruiter basically said they feel like the offer they gave me was fair and to respond with my final decision in 24 hours.\n\nI want to decline, I feel like an analytics engineer position is something that will not make me learn anything and I want to do software instead. The job is really easy and the compensation is a bit too low, given I have a background in CS and Math and plan to graduate this quarter.\n\nAlso in the interview, one of the data engineering managers was shit-talking my team for 45 minutes saying how he would gut the whole thing and start from scratch. Didn't give me a good taste to be honest as he feels like there are too many problems with my team's current product they have. The data science manager also brought up similar issues.\n\nAm I shooting myself in the foot by declining given the current market? I haven't been interviewing at other places but I probably will have to since I don't have anything lined up. I'm not a big fan of the work as well. I just want anyone's advice to give me since I have 24 hours left. I can respond with more information as well", "author_fullname": "t2_qeq05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Offered an Analytics Engineering Position and want to decline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m9ak2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678312180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;m in the Seattle area and was offered an analytics engineer role since I previously interned at the place last summer. Basically what we do is that we work in our Snowflake database and clean our data and make data enrichments. Technologies that we primarily use are SQL, dbt, Airflow, and a bit of Python for UDFs. The job is extremely easy when I was an intern, but I wanted to work as a SWE instead. I feel like this position is more similar to a data analyst position.&lt;/p&gt;\n\n&lt;p&gt;I got an offer of 104k total comp which was too low for me. I replied to negotiate for around 130k instead but the recruiter basically said they feel like the offer they gave me was fair and to respond with my final decision in 24 hours.&lt;/p&gt;\n\n&lt;p&gt;I want to decline, I feel like an analytics engineer position is something that will not make me learn anything and I want to do software instead. The job is really easy and the compensation is a bit too low, given I have a background in CS and Math and plan to graduate this quarter.&lt;/p&gt;\n\n&lt;p&gt;Also in the interview, one of the data engineering managers was shit-talking my team for 45 minutes saying how he would gut the whole thing and start from scratch. Didn&amp;#39;t give me a good taste to be honest as he feels like there are too many problems with my team&amp;#39;s current product they have. The data science manager also brought up similar issues.&lt;/p&gt;\n\n&lt;p&gt;Am I shooting myself in the foot by declining given the current market? I haven&amp;#39;t been interviewing at other places but I probably will have to since I don&amp;#39;t have anything lined up. I&amp;#39;m not a big fan of the work as well. I just want anyone&amp;#39;s advice to give me since I have 24 hours left. I can respond with more information as well&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11m9ak2", "is_robot_indexable": true, "report_reasons": null, "author": "therealhm2", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m9ak2/offered_an_analytics_engineering_position_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m9ak2/offered_an_analytics_engineering_position_and/", "subreddit_subscribers": 92360, "created_utc": 1678312180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am starting a new job next week and I will be responsible for a lot more that I have in the past as a Power Bi Developer.  One of the requirements is to make sure that the database are in 2NF or 3NF.\n\nI understand what 1NF, 2NF and 3NF and how to change the database to be 2NF or 3NF.\n\nDoes anyone know of a good python package, SQL script or even a good check list to help identify the normalization status of a database?", "author_fullname": "t2_c25i55k3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database normalization 1NF, 2NF, 3NF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mhov6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678333552.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678333323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am starting a new job next week and I will be responsible for a lot more that I have in the past as a Power Bi Developer.  One of the requirements is to make sure that the database are in 2NF or 3NF.&lt;/p&gt;\n\n&lt;p&gt;I understand what 1NF, 2NF and 3NF and how to change the database to be 2NF or 3NF.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a good python package, SQL script or even a good check list to help identify the normalization status of a database?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11mhov6", "is_robot_indexable": true, "report_reasons": null, "author": "moltra_1", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mhov6/database_normalization_1nf_2nf_3nf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mhov6/database_normalization_1nf_2nf_3nf/", "subreddit_subscribers": 92360, "created_utc": 1678333323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nWe are drawing from our Data Warehouse to do a report. The data is not yet large but is expected to be soon. However, the query work we are doing has become quite complex. We are coding it all up in Glue/Spark. But it begs the question: are we doing things wrong?\n\nHere's some other options:\n\n* prepare as much data manipulation into Views in the Data Warehouse\n* actually materialize some of the required data into tables via new ETLs\n* just keep programming it all in Spark\n* rely on BI tool to get the job done.\n\nHow would you achieve a report that requires many joins and filters on lots of data?\n\nBonus Question: In the context of big data, do you rely on your BI tool for such kinds of \"heavy\" reports?", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How/Where to make complex Reports?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m9t6g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678313395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;We are drawing from our Data Warehouse to do a report. The data is not yet large but is expected to be soon. However, the query work we are doing has become quite complex. We are coding it all up in Glue/Spark. But it begs the question: are we doing things wrong?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s some other options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;prepare as much data manipulation into Views in the Data Warehouse&lt;/li&gt;\n&lt;li&gt;actually materialize some of the required data into tables via new ETLs&lt;/li&gt;\n&lt;li&gt;just keep programming it all in Spark&lt;/li&gt;\n&lt;li&gt;rely on BI tool to get the job done.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How would you achieve a report that requires many joins and filters on lots of data?&lt;/p&gt;\n\n&lt;p&gt;Bonus Question: In the context of big data, do you rely on your BI tool for such kinds of &amp;quot;heavy&amp;quot; reports?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11m9t6g", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m9t6g/howwhere_to_make_complex_reports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m9t6g/howwhere_to_make_complex_reports/", "subreddit_subscribers": 92360, "created_utc": 1678313395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently working on creating reports for business areas and each data engineer is developing their own data marts similar to this architecture - https://www.researchgate.net/figure/Kimballs-data-warehouse-architecture-7_fig2_312486486\n\nEach business area (Finance, HR, Sales etc) have their own data mart, could be multiple fact tables and then the developers create -the reports. The organisation will end up with 70-100 fact tables, I assumed this was the way it should be done.\n\nMy colleague advised that this is the 'old way' of working, he is a data architect who has implemented a one version of the truth and a single data model which consists of all information of all levels of granularity - Finance, Customer, HR, Sales etc. This makes life easy for the developers and allows you to present your KPI's in one single dashboard.\n\nI was stumped, I didn't think it was possible and sounds to good to be true. I have many questions but I would like to know your thoughts on this? Any advice? Experiences with implementing something similar in the past.", "author_fullname": "t2_12rfbr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am fairly new to Data Warehousing and need some advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lyuir", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678288412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently working on creating reports for business areas and each data engineer is developing their own data marts similar to this architecture - &lt;a href=\"https://www.researchgate.net/figure/Kimballs-data-warehouse-architecture-7_fig2_312486486\"&gt;https://www.researchgate.net/figure/Kimballs-data-warehouse-architecture-7_fig2_312486486&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Each business area (Finance, HR, Sales etc) have their own data mart, could be multiple fact tables and then the developers create -the reports. The organisation will end up with 70-100 fact tables, I assumed this was the way it should be done.&lt;/p&gt;\n\n&lt;p&gt;My colleague advised that this is the &amp;#39;old way&amp;#39; of working, he is a data architect who has implemented a one version of the truth and a single data model which consists of all information of all levels of granularity - Finance, Customer, HR, Sales etc. This makes life easy for the developers and allows you to present your KPI&amp;#39;s in one single dashboard.&lt;/p&gt;\n\n&lt;p&gt;I was stumped, I didn&amp;#39;t think it was possible and sounds to good to be true. I have many questions but I would like to know your thoughts on this? Any advice? Experiences with implementing something similar in the past.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lyuir", "is_robot_indexable": true, "report_reasons": null, "author": "That_Sweet_Science", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lyuir/i_am_fairly_new_to_data_warehousing_and_need_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lyuir/i_am_fairly_new_to_data_warehousing_and_need_some/", "subreddit_subscribers": 92360, "created_utc": 1678288412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151](https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151)\n\nWhat do you think of this? I think its really awesome they are going forwards using open source scheduler tools in ADF.\n\nAnyone had already experiences with it? However it is still depending on Data Factory", "author_fullname": "t2_tzseb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managed Airflow in Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lup54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678280042.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678277255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151\"&gt;https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you think of this? I think its really awesome they are going forwards using open source scheduler tools in ADF.&lt;/p&gt;\n\n&lt;p&gt;Anyone had already experiences with it? However it is still depending on Data Factory&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?auto=webp&amp;v=enabled&amp;s=377641832eecbd621b310ab9dd16d381993c449f", "width": 2288, "height": 1448}, "resolutions": [{"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc4ffca7b91875a7fa3134c0d86d66dcd449c179", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75220a5e620ee298b7b2d8329044b879daca4586", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f764681c03c53d1229c8ff8e3799ea7bec9e4899", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f124a2fdd04ffd44d572a4e01089662b94a5f067", "width": 640, "height": 405}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7d3af51b0e95fbe753302b6f4c7969990339dc6", "width": 960, "height": 607}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a177ca1e48aaead34281cb5f3e145a59154b95e", "width": 1080, "height": 683}], "variants": {}, "id": "S0ZC01nTKv9edIZMquYzX-iG3I-MSj166ECzeJEKDn8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11lup54", "is_robot_indexable": true, "report_reasons": null, "author": "Zouzzi", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lup54/managed_airflow_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lup54/managed_airflow_in_azure/", "subreddit_subscribers": 92360, "created_utc": 1678277255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Alright so Databricks allows for Python and PySpark programming. \n\nIf i code in only Python, does databricks distribute the computation across nodes using Spark? Or would that only occur if I use PySpark?\n\nI\u2019ve been using databricks for a month now and didn\u2019t think to program in PySpark and have been programming in Python but now I\u2019m afraid I\u2019m not getting the full benefit of Databricks.", "author_fullname": "t2_6mct0oth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: Programming in Python vs PySpark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mj5cw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678337616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alright so Databricks allows for Python and PySpark programming. &lt;/p&gt;\n\n&lt;p&gt;If i code in only Python, does databricks distribute the computation across nodes using Spark? Or would that only occur if I use PySpark?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been using databricks for a month now and didn\u2019t think to program in PySpark and have been programming in Python but now I\u2019m afraid I\u2019m not getting the full benefit of Databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11mj5cw", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Baseball89", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mj5cw/databricks_programming_in_python_vs_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mj5cw/databricks_programming_in_python_vs_pyspark/", "subreddit_subscribers": 92360, "created_utc": 1678337616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For context I did my bachelors and masters in electrical engineering from TMU and I took 1 course on data engineering which got me really interested. Unfortunately I was too late to take more courses on data/ML cause i finished the requirements for the MEng and just graduated last year. I only learnt basic sql/gcp and how to ingest data in hive/Hadoop and use elastic search and kibana in that course. I want to apply for data engineering roles and basically get into this field but I don\u2019t even know how to operate GitHub or have any projects. There are way too many resources online and a tonne of free courses but I want to enroll in a program which can motivate me more and actually make build projects. The ones I am interested in currently is the data science certificate at UofT and the Big data at McMaster University. I must add I have little to no work experience. I just graduated and the thought of pouring money to another useless degree is not appealing to me. But if the certificate or bootcamp actually gets me a job in data engineering I\u2019d be more than happy to invest. I honestly need help and guidance if anyone went through the same thing. Any recommendations on what I should do now is more than welcome and is greatly appreciated (pls I beg)", "author_fullname": "t2_cqcj4ctw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big data engineering certificates/courses in Toronto to start out and build portfolio?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11md525", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678321169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context I did my bachelors and masters in electrical engineering from TMU and I took 1 course on data engineering which got me really interested. Unfortunately I was too late to take more courses on data/ML cause i finished the requirements for the MEng and just graduated last year. I only learnt basic sql/gcp and how to ingest data in hive/Hadoop and use elastic search and kibana in that course. I want to apply for data engineering roles and basically get into this field but I don\u2019t even know how to operate GitHub or have any projects. There are way too many resources online and a tonne of free courses but I want to enroll in a program which can motivate me more and actually make build projects. The ones I am interested in currently is the data science certificate at UofT and the Big data at McMaster University. I must add I have little to no work experience. I just graduated and the thought of pouring money to another useless degree is not appealing to me. But if the certificate or bootcamp actually gets me a job in data engineering I\u2019d be more than happy to invest. I honestly need help and guidance if anyone went through the same thing. Any recommendations on what I should do now is more than welcome and is greatly appreciated (pls I beg)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11md525", "is_robot_indexable": true, "report_reasons": null, "author": "anonthrowaways728181", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11md525/big_data_engineering_certificatescourses_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11md525/big_data_engineering_certificatescourses_in/", "subreddit_subscribers": 92360, "created_utc": 1678321169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Obviously I have a dag for data extraction, dag for transformations and dag for \u201caggregations\u201d.\n\nNow, all dags are created manually - i. e. when we add new data source we add new task to the dag and we have to define the dependencies manually as well. \n\nHow common is such practice? Meaning - I am thinking of generating the dags from some yaml template but not sure it would make things easier. The main reason I am considering it is because of the need to manually define the dependencies in the dag which can lead to human errors. \n\nBtw. for the \u201caggregations\u201d it should be pretty easy to generate such dag as this data is in the RDBMS and its tables can be defined using SQLAlchemy or dbt. But how about the \u201cmiddle\u201d step which just takes raw data and standardizes it in the datalake to parquet format? \ud83e\udd14", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow dags generation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lxu63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678285898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously I have a dag for data extraction, dag for transformations and dag for \u201caggregations\u201d.&lt;/p&gt;\n\n&lt;p&gt;Now, all dags are created manually - i. e. when we add new data source we add new task to the dag and we have to define the dependencies manually as well. &lt;/p&gt;\n\n&lt;p&gt;How common is such practice? Meaning - I am thinking of generating the dags from some yaml template but not sure it would make things easier. The main reason I am considering it is because of the need to manually define the dependencies in the dag which can lead to human errors. &lt;/p&gt;\n\n&lt;p&gt;Btw. for the \u201caggregations\u201d it should be pretty easy to generate such dag as this data is in the RDBMS and its tables can be defined using SQLAlchemy or dbt. But how about the \u201cmiddle\u201d step which just takes raw data and standardizes it in the datalake to parquet format? \ud83e\udd14&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11lxu63", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lxu63/airflow_dags_generation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lxu63/airflow_dags_generation/", "subreddit_subscribers": 92360, "created_utc": 1678285898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Got a use case which requires to deploy multiple resources across multiple cloud projects with terraform.\n\nIs it better off to have one service account across all deployments or separate each with a distinct service account.\n\nI understand there are trade offs on both approaches and might not be one right solution, but is there maybe more secure to just protect one service account key instead of 50?", "author_fullname": "t2_fbkuq4f9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "one service account to rule them all, or not", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m07eb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678291595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got a use case which requires to deploy multiple resources across multiple cloud projects with terraform.&lt;/p&gt;\n\n&lt;p&gt;Is it better off to have one service account across all deployments or separate each with a distinct service account.&lt;/p&gt;\n\n&lt;p&gt;I understand there are trade offs on both approaches and might not be one right solution, but is there maybe more secure to just protect one service account key instead of 50?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11m07eb", "is_robot_indexable": true, "report_reasons": null, "author": "one_but_ninja", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m07eb/one_service_account_to_rule_them_all_or_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m07eb/one_service_account_to_rule_them_all_or_not/", "subreddit_subscribers": 92360, "created_utc": 1678291595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5pxn0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Teams Survey 2023 Results", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lzsrn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1678290680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jesse-anderson.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.jesse-anderson.com/2023/03/data-teams-survey-2023-results/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Mentor | Jesse Anderson", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11lzsrn", "is_robot_indexable": true, "report_reasons": null, "author": "eljefe6a", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11lzsrn/data_teams_survey_2023_results/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.jesse-anderson.com/2023/03/data-teams-survey-2023-results/", "subreddit_subscribers": 92360, "created_utc": 1678290680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have what seems like a pretty vanilla DE project: Take an OLTP system hosted on AWS and build out a system that supports ad hoc OLAP queries by analysts (bonus points if they can continue to use Tableau or Metabase on it) and feature extraction and model training by the ML team. Data volume is currently ~500GB/500M rows with an evolving schema and projected to grow 5x-10x in the next few years. The data is largely append-only but some of it is updateable. There is a desire to pull in some higher-volume, unstructured external data for enrichment as well.\n\nMy general thinking is:\n\nData lake with something along the lines of Iceberg to help manage the raw data since it handles schema evolution, point-in-time query capabilities, and the possibility of multiple query engines if analysts and the ML team end up with different tools. \n\nStream OLTP updates to the data lake. This is an area where I have the least expertise. Something along the lines of AWS Glue seems appropriate.\n\nA horizontally-scalable query engine such as Presto that will allow analysts to do ad hoc scans over relatively large volumes of data. It looks like Tableau and Metabase plug into it as well.\n\nI've done a fair bit of work at multiple layers in setups like these, but haven't ever put one together from end-to-end and I'm looking for some sanity checks and/or gaping holes in my reasoning -- major components that are missing or tools that may be overkill for TB-scale data volumes.", "author_fullname": "t2_6p9p15m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Greenfield DE project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m97iu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678311977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have what seems like a pretty vanilla DE project: Take an OLTP system hosted on AWS and build out a system that supports ad hoc OLAP queries by analysts (bonus points if they can continue to use Tableau or Metabase on it) and feature extraction and model training by the ML team. Data volume is currently ~500GB/500M rows with an evolving schema and projected to grow 5x-10x in the next few years. The data is largely append-only but some of it is updateable. There is a desire to pull in some higher-volume, unstructured external data for enrichment as well.&lt;/p&gt;\n\n&lt;p&gt;My general thinking is:&lt;/p&gt;\n\n&lt;p&gt;Data lake with something along the lines of Iceberg to help manage the raw data since it handles schema evolution, point-in-time query capabilities, and the possibility of multiple query engines if analysts and the ML team end up with different tools. &lt;/p&gt;\n\n&lt;p&gt;Stream OLTP updates to the data lake. This is an area where I have the least expertise. Something along the lines of AWS Glue seems appropriate.&lt;/p&gt;\n\n&lt;p&gt;A horizontally-scalable query engine such as Presto that will allow analysts to do ad hoc scans over relatively large volumes of data. It looks like Tableau and Metabase plug into it as well.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done a fair bit of work at multiple layers in setups like these, but haven&amp;#39;t ever put one together from end-to-end and I&amp;#39;m looking for some sanity checks and/or gaping holes in my reasoning -- major components that are missing or tools that may be overkill for TB-scale data volumes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11m97iu", "is_robot_indexable": true, "report_reasons": null, "author": "Db_Wrangler_1905", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m97iu/greenfield_de_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m97iu/greenfield_de_project/", "subreddit_subscribers": 92360, "created_utc": 1678311977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, while researching and learning about data warehouses, I will always only find a super minimal example to demonstrate a star schema with like 5 tables total in the warehouse. Like that's cool, but what if I have hundreds of tables (50+ facts &amp; 100+ dimensions) coming from 10+ databases and APIs???? I figure this is much more what you deal with in the real world.\n\nSo to my questions are:\n\n1. How do you guys organize all of this in your warehouse? Schema per Source?\n2. Are you using a data lake before the warehouse to bring all of the sources together in a single location?\n3. How do you manage the inserts, updates, and deletes for all of these tables in the warehouse?\n4. What does your reporting layer look like on top of the warehouse tables?\n5. Are you using something like DBT to transform the raw data (normalized form) into a denormalized form?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are your warehouses organized?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m0wtv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678293216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, while researching and learning about data warehouses, I will always only find a super minimal example to demonstrate a star schema with like 5 tables total in the warehouse. Like that&amp;#39;s cool, but what if I have hundreds of tables (50+ facts &amp;amp; 100+ dimensions) coming from 10+ databases and APIs???? I figure this is much more what you deal with in the real world.&lt;/p&gt;\n\n&lt;p&gt;So to my questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How do you guys organize all of this in your warehouse? Schema per Source?&lt;/li&gt;\n&lt;li&gt;Are you using a data lake before the warehouse to bring all of the sources together in a single location?&lt;/li&gt;\n&lt;li&gt;How do you manage the inserts, updates, and deletes for all of these tables in the warehouse?&lt;/li&gt;\n&lt;li&gt;What does your reporting layer look like on top of the warehouse tables?&lt;/li&gt;\n&lt;li&gt;Are you using something like DBT to transform the raw data (normalized form) into a denormalized form?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11m0wtv", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m0wtv/how_are_your_warehouses_organized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m0wtv/how_are_your_warehouses_organized/", "subreddit_subscribers": 92360, "created_utc": 1678293216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been tasked with doing a preliminary market scan for tools to integrate cloud-based data sources (mostly socials) like Facebook/Meta, Google, LinkedIn, etc.  We are currently using Azure Data Factory, which works fine for on-prem databases and even some SaaS products like Salesforce.  But for most cloud-based products, Microsoft offers a generic REST-based connector and not much else.\n\nSome options I'm looking at include Fivetran, Stitch, Hevo, Supermetrics, and CData... are there others worth looking at?\n\nWhat do you use?  And why?", "author_fullname": "t2_5txrt2ap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you use for cloud-based data source?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11mn1nn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678350830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been tasked with doing a preliminary market scan for tools to integrate cloud-based data sources (mostly socials) like Facebook/Meta, Google, LinkedIn, etc.  We are currently using Azure Data Factory, which works fine for on-prem databases and even some SaaS products like Salesforce.  But for most cloud-based products, Microsoft offers a generic REST-based connector and not much else.&lt;/p&gt;\n\n&lt;p&gt;Some options I&amp;#39;m looking at include Fivetran, Stitch, Hevo, Supermetrics, and CData... are there others worth looking at?&lt;/p&gt;\n\n&lt;p&gt;What do you use?  And why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11mn1nn", "is_robot_indexable": true, "report_reasons": null, "author": "tarzanboy76", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mn1nn/what_do_you_use_for_cloudbased_data_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mn1nn/what_do_you_use_for_cloudbased_data_source/", "subreddit_subscribers": 92360, "created_utc": 1678350830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9u69ulzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I use JIRA as a database? (NSFL)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mkywc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1678343441.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/SoftwareEngineering/comments/11mehjg/jira_as_a_database/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "11mkywc", "is_robot_indexable": true, "report_reasons": null, "author": "tdatas", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mkywc/should_i_use_jira_as_a_database_nsfl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/SoftwareEngineering/comments/11mehjg/jira_as_a_database/", "subreddit_subscribers": 92360, "created_utc": 1678343441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a conference you're looking forward to that deliver a lot of Data Engineering concepts or is centered on DE as a whole?", "author_fullname": "t2_pp8loa0k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Conferences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mbxsq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678318255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a conference you&amp;#39;re looking forward to that deliver a lot of Data Engineering concepts or is centered on DE as a whole?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11mbxsq", "is_robot_indexable": true, "report_reasons": null, "author": "de_epi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11mbxsq/de_conferences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mbxsq/de_conferences/", "subreddit_subscribers": 92360, "created_utc": 1678318255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a dataset with a million rows, and I'm using Pyspark for transformations to model data to meet data science needs, though Pyspark has most transformations, there are situations like I'm in now, where I need to use a third-party library function on my data which are not optimized for Pyspark  ( currently I'm tasked to find Jaro Winkler distance matrix ), running this function on about 8000 records is taking around 6 mins of time but it would take a lot of time for a million records.   \n\n\n1. How do you handle the execution of a third-party function on a huge dataset .? \n2. I'm on AWS and feel that I should use  [Python shell jobs in AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html) to run the Jaro Winkler function for about two days to get a matrix for the whole dataset,  am I right in this regard.?", "author_fullname": "t2_6psngyc6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running a third-party-library function on a Huge dataset that is not optimized for Pyspark.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mbvwq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678318133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a dataset with a million rows, and I&amp;#39;m using Pyspark for transformations to model data to meet data science needs, though Pyspark has most transformations, there are situations like I&amp;#39;m in now, where I need to use a third-party library function on my data which are not optimized for Pyspark  ( currently I&amp;#39;m tasked to find Jaro Winkler distance matrix ), running this function on about 8000 records is taking around 6 mins of time but it would take a lot of time for a million records.   &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How do you handle the execution of a third-party function on a huge dataset .? &lt;/li&gt;\n&lt;li&gt;I&amp;#39;m on AWS and feel that I should use  &lt;a href=\"https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html\"&gt;Python shell jobs in AWS Glue&lt;/a&gt; to run the Jaro Winkler function for about two days to get a matrix for the whole dataset,  am I right in this regard.?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11mbvwq", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo_51799", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mbvwq/running_a_thirdpartylibrary_function_on_a_huge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mbvwq/running_a_thirdpartylibrary_function_on_a_huge/", "subreddit_subscribers": 92360, "created_utc": 1678318133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am building a data pipeline and dashboard app that I want to deploy online (fastapi + postgresql + svelte/vue). It will read all data from postgresql, do computations (numpy / pandas), store the output back into postgresql, and then the frontend app will communicate with the api to render the dashboard.\n\nBut what is the best way to deploy postgres in production? My platform will start small, but I'd like to have the infrastructure to scale quickly when / if the platform takes off.\n\nI am a data engineer and programmer and I used to do everything myself, but with kubernets, docker things got more complicated:\n\n* use kubernetes to have postgresql, api (fastapi), and data pipeline in separate dockers and communicate with each other over network?\n* vm for postgresql?\n* \"bare metal\" (dedicated server) for postgresql?\n\nI've read pros and cons for each, I really don't know what the best way is to do this.\n\nDoes anybody has experience with this?", "author_fullname": "t2_15vo19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "from files to db in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11lrzi6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678268273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building a data pipeline and dashboard app that I want to deploy online (fastapi + postgresql + svelte/vue). It will read all data from postgresql, do computations (numpy / pandas), store the output back into postgresql, and then the frontend app will communicate with the api to render the dashboard.&lt;/p&gt;\n\n&lt;p&gt;But what is the best way to deploy postgres in production? My platform will start small, but I&amp;#39;d like to have the infrastructure to scale quickly when / if the platform takes off.&lt;/p&gt;\n\n&lt;p&gt;I am a data engineer and programmer and I used to do everything myself, but with kubernets, docker things got more complicated:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;use kubernetes to have postgresql, api (fastapi), and data pipeline in separate dockers and communicate with each other over network?&lt;/li&gt;\n&lt;li&gt;vm for postgresql?&lt;/li&gt;\n&lt;li&gt;&amp;quot;bare metal&amp;quot; (dedicated server) for postgresql?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve read pros and cons for each, I really don&amp;#39;t know what the best way is to do this.&lt;/p&gt;\n\n&lt;p&gt;Does anybody has experience with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11lrzi6", "is_robot_indexable": true, "report_reasons": null, "author": "iLLucionist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11lrzi6/from_files_to_db_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11lrzi6/from_files_to_db_in_production/", "subreddit_subscribers": 92360, "created_utc": 1678268273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wonder how many of you still do ETL with GUI tools like Oracle Data Integrator, SSIS, etc..\nI am DWH/ETL developer and for ETL we use ODI (for complicated things I write PL/SQL packages and procedures) I am worried that this way of doing ETL is dead and if I dont change job, I will have problem to find job in future. What do you guys think about it?", "author_fullname": "t2_kh4jiqla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GUI-based ETL developer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11mnc5w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678351975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wonder how many of you still do ETL with GUI tools like Oracle Data Integrator, SSIS, etc..\nI am DWH/ETL developer and for ETL we use ODI (for complicated things I write PL/SQL packages and procedures) I am worried that this way of doing ETL is dead and if I dont change job, I will have problem to find job in future. What do you guys think about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11mnc5w", "is_robot_indexable": true, "report_reasons": null, "author": "No_Pause7942", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mnc5w/guibased_etl_developer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mnc5w/guibased_etl_developer/", "subreddit_subscribers": 92360, "created_utc": 1678351975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,  \n\n\nI'd like to get a better understanding of data platforms and best practices for data problems. Basic things that data people understand like the 3V's of big data and different architecture styles. I'm trying to learn things like this to apply to our data situations at work and understand how to best solve them and where our problems lie.   \n\n\nI already have the SQL and Python skills but now Im trying to get better at solving data issues using best practices. Not through code but through platforms.  \n\n\nHope that makes sense,  \n\n\nThanks!", "author_fullname": "t2_7xwk8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn how to build solutions to solve data issues?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11mn0ys", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678350749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to get a better understanding of data platforms and best practices for data problems. Basic things that data people understand like the 3V&amp;#39;s of big data and different architecture styles. I&amp;#39;m trying to learn things like this to apply to our data situations at work and understand how to best solve them and where our problems lie.   &lt;/p&gt;\n\n&lt;p&gt;I already have the SQL and Python skills but now Im trying to get better at solving data issues using best practices. Not through code but through platforms.  &lt;/p&gt;\n\n&lt;p&gt;Hope that makes sense,  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11mn0ys", "is_robot_indexable": true, "report_reasons": null, "author": "Manyreason", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mn0ys/how_to_learn_how_to_build_solutions_to_solve_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mn0ys/how_to_learn_how_to_build_solutions_to_solve_data/", "subreddit_subscribers": 92360, "created_utc": 1678350749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I\u2019m a current college sophomore enrolled in computer engineering. I\u2019m on track to graduate on time for a regular 4 year degree. I intend to pursue data engineering as a career.  My current plan is\nsummer 24\u2019 - internship\nSpring 25\u2019 - Graduate\nSummer 25\u2019 - entry level job\nand then maybe after a year or two of working on becoming a data engineering and gaining industry experience begin looking for a data engineering position. I plan on starting this summer the early milestones on the data engineering map i\u2019ve found in this sub and other sources. I have a few questions. \n1. Does my internship particularly matter? Would getting a standard computer engineering or software engineering role be sufficient. I currently don\u2019t feel like I have enough experience to land data science internships. \n2. Would my entry level job particularly matter? Should I shoot for a junior data engineering role, or even a data science role? Or would the transition from software/computer engineering entry positions be fine?\n3. Should I lower my expectations? Is 2 years enough time to transition to a data engineer? Is it enough time to build my resume and skills to be hire-able ?\n\nAny advice or suggestions would be appreciated. Thanks", "author_fullname": "t2_2uzppvoh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requesting guidance: current undergrad", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11mmz19", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678350554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I\u2019m a current college sophomore enrolled in computer engineering. I\u2019m on track to graduate on time for a regular 4 year degree. I intend to pursue data engineering as a career.  My current plan is\nsummer 24\u2019 - internship\nSpring 25\u2019 - Graduate\nSummer 25\u2019 - entry level job\nand then maybe after a year or two of working on becoming a data engineering and gaining industry experience begin looking for a data engineering position. I plan on starting this summer the early milestones on the data engineering map i\u2019ve found in this sub and other sources. I have a few questions. \n1. Does my internship particularly matter? Would getting a standard computer engineering or software engineering role be sufficient. I currently don\u2019t feel like I have enough experience to land data science internships. \n2. Would my entry level job particularly matter? Should I shoot for a junior data engineering role, or even a data science role? Or would the transition from software/computer engineering entry positions be fine?\n3. Should I lower my expectations? Is 2 years enough time to transition to a data engineer? Is it enough time to build my resume and skills to be hire-able ?&lt;/p&gt;\n\n&lt;p&gt;Any advice or suggestions would be appreciated. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11mmz19", "is_robot_indexable": true, "report_reasons": null, "author": "FatNutsV2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mmz19/requesting_guidance_current_undergrad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mmz19/requesting_guidance_current_undergrad/", "subreddit_subscribers": 92360, "created_utc": 1678350554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a large Python app that I can run with different argumentrs, e.g. `python` [`app.py`](https://app.py/) `--algorithm 1` and `python` [`app.py`](https://app.py/) `--algorithm 2`. To date, I successfully dockerized my app in a large image (\\~750MB). Next, I want to time-schedule runs of those two different algorithms. My idea was to ovveride the CMD command of the docker image and make two different runs of the same image.\n\nHowever, I'm struggling to understand if everytime I have to run a container on the cloud, e.g. with Azure Container Instances, I have to pull the image from another server and pay for the storage/internet throughput, which in my case is huge to the fact that the image is large (\\~750MB) and the time schedule is strict, e.g. every 5 minutes.\n\nAm I missing something? Could you share your ideas or review mine?", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Review needed: is a Docker container a good candidate to refactor my app architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11mmy4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678350455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a large Python app that I can run with different argumentrs, e.g. &lt;code&gt;python&lt;/code&gt; &lt;a href=\"https://app.py/\"&gt;&lt;code&gt;app.py&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--algorithm 1&lt;/code&gt; and &lt;code&gt;python&lt;/code&gt; &lt;a href=\"https://app.py/\"&gt;&lt;code&gt;app.py&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--algorithm 2&lt;/code&gt;. To date, I successfully dockerized my app in a large image (~750MB). Next, I want to time-schedule runs of those two different algorithms. My idea was to ovveride the CMD command of the docker image and make two different runs of the same image.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m struggling to understand if everytime I have to run a container on the cloud, e.g. with Azure Container Instances, I have to pull the image from another server and pay for the storage/internet throughput, which in my case is huge to the fact that the image is large (~750MB) and the time schedule is strict, e.g. every 5 minutes.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something? Could you share your ideas or review mine?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11mmy4n", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mmy4n/review_needed_is_a_docker_container_a_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mmy4n/review_needed_is_a_docker_container_a_good/", "subreddit_subscribers": 92360, "created_utc": 1678350455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m curious whether anyone had played with Data Vault 2.0 DWH architecture in a real graph database such as Neo4j?\n\nGraph databases have nodes and edges. Data Vault has hubs and links. There is a certain similarity right?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Vault 2.0 in a graph database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mlwyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678346687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m curious whether anyone had played with Data Vault 2.0 DWH architecture in a real graph database such as Neo4j?&lt;/p&gt;\n\n&lt;p&gt;Graph databases have nodes and edges. Data Vault has hubs and links. There is a certain similarity right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11mlwyw", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mlwyw/data_vault_20_in_a_graph_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mlwyw/data_vault_20_in_a_graph_database/", "subreddit_subscribers": 92360, "created_utc": 1678346687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand the separate use cases for batch and streaming. Batch for analytical/aggregate reporting data over a set period of time. Streaming for more immediate feedback on unbounded data. My question is about the reconciliation that happens between these two layers? Why would you need to reconcile when you already have the batch data that is most accurate?  Wouldn't the batch data suffice?", "author_fullname": "t2_nmk5l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about lambda architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mkle2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678342197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand the separate use cases for batch and streaming. Batch for analytical/aggregate reporting data over a set period of time. Streaming for more immediate feedback on unbounded data. My question is about the reconciliation that happens between these two layers? Why would you need to reconcile when you already have the batch data that is most accurate?  Wouldn&amp;#39;t the batch data suffice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11mkle2", "is_robot_indexable": true, "report_reasons": null, "author": "getboy97", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mkle2/question_about_lambda_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mkle2/question_about_lambda_architecture/", "subreddit_subscribers": 92360, "created_utc": 1678342197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short I\u2019ve spent a decent amount of time figuring out how I could deploy dependencies for my (Python) lambda function. I kind of started a never ending loop with this topic, first I zipped them, but the zipped file was larger than 50MB so I did some refactoring, so that it won\u2019t go over 50MB. To deploy with docker is only an option in LocalStackPro, same with Lambda Layers and referencing to real AWS lambda. So I downloaded the wheels I thought I needed, but now I think it\u2019s the architecture. I\u2019m getting errors on missing files, even thought about hidden files, but there is none in the wheels I have checked. Is there something else I am missing? Is it not ARM64 and on X86? Or how do I know which architecture I need?", "author_fullname": "t2_se9vugql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lambda dependencies on LocalStack without Pro version?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11m458m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678300520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I\u2019ve spent a decent amount of time figuring out how I could deploy dependencies for my (Python) lambda function. I kind of started a never ending loop with this topic, first I zipped them, but the zipped file was larger than 50MB so I did some refactoring, so that it won\u2019t go over 50MB. To deploy with docker is only an option in LocalStackPro, same with Lambda Layers and referencing to real AWS lambda. So I downloaded the wheels I thought I needed, but now I think it\u2019s the architecture. I\u2019m getting errors on missing files, even thought about hidden files, but there is none in the wheels I have checked. Is there something else I am missing? Is it not ARM64 and on X86? Or how do I know which architecture I need?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11m458m", "is_robot_indexable": true, "report_reasons": null, "author": "it-dummy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11m458m/lambda_dependencies_on_localstack_without_pro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11m458m/lambda_dependencies_on_localstack_without_pro/", "subreddit_subscribers": 92360, "created_utc": 1678300520.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}