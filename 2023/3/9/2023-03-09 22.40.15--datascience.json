{"kind": "Listing", "data": {"after": "t3_11mvn4z", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "&amp;#x200B;\n\n[Visualization of polarized echo chambers](https://preview.redd.it/45ac0obl3oma1.png?width=405&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f17e0082e619641616b6929ee6d19c5e7372dd5f)\n\nOn the one hand, this study is a fascinating data science use case because it shows that it seems possible to narrow down disinformation clusters in social networks purely based on user interaction (shares). And that without viewing the content!\n\n[https://www.researchgate.net/publication/368961767](https://www.researchgate.net/publication/368961767)\n\nThe paper presents a method to identify these problematic interaction patterns providing data science contributions to improve social media platforms.\n\nOn the other hand, this study also reveals worrying conditions: The study recorded and analyzed the German Twitter stream over two months. More than 6.7 million accounts and 75.5 million interactions (including 33 million retweets) were recorded. The analysis shows a reasonably stable \"red echo chamber\" that spreads disinformation and reinforces prejudice and polarization in the network. This echo chamber, which consists of about 66,000 accounts, focuses on only a few topics, such as Anti-Covid, Right-wing Populism, and pro-Russian narratives, which is probably reinforced by Kremlin-initiated troll accounts. In the same way, there is a \"blue chamber\", but its content range is much more multilayered, and its communication pattern is much more inconspicuous. Since Twitter provided only about 1% of the data stream, these echo chambers are likely to be much more prominent in reality.", "author_fullname": "t2_2dyqc8ht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This Data Science study identified a German echo chamber of 66K accounts mainly focused on Anti-Covid, Right-wing Populism, and pro-Russian narratives likely reinforced by Kremlin-orchestrated Troll-Accounts just by looking at retweets!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 65, "top_awarded_type": null, "hide_score": false, "media_metadata": {"45ac0obl3oma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/45ac0obl3oma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c6b8fe001d3e087e55fe26740041e5907851e83"}, {"y": 100, "x": 216, "u": "https://preview.redd.it/45ac0obl3oma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f79712ce496ecbd1e5bbe30ca918f4770eb0e29"}, {"y": 149, "x": 320, "u": "https://preview.redd.it/45ac0obl3oma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8857bb5b7b88bb568f25d34bce5b7d1c914be95"}], "s": {"y": 189, "x": 405, "u": "https://preview.redd.it/45ac0obl3oma1.png?width=405&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f17e0082e619641616b6929ee6d19c5e7372dd5f"}, "id": "45ac0obl3oma1"}}, "name": "t3_11mm7uc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 447, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 447, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sX5bEiGI5_RTAp0C-X3TAJ7YDSG8a3GcBy4P8IQ6raM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678347763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/45ac0obl3oma1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=f17e0082e619641616b6929ee6d19c5e7372dd5f\"&gt;Visualization of polarized echo chambers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On the one hand, this study is a fascinating data science use case because it shows that it seems possible to narrow down disinformation clusters in social networks purely based on user interaction (shares). And that without viewing the content!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.researchgate.net/publication/368961767\"&gt;https://www.researchgate.net/publication/368961767&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The paper presents a method to identify these problematic interaction patterns providing data science contributions to improve social media platforms.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, this study also reveals worrying conditions: The study recorded and analyzed the German Twitter stream over two months. More than 6.7 million accounts and 75.5 million interactions (including 33 million retweets) were recorded. The analysis shows a reasonably stable &amp;quot;red echo chamber&amp;quot; that spreads disinformation and reinforces prejudice and polarization in the network. This echo chamber, which consists of about 66,000 accounts, focuses on only a few topics, such as Anti-Covid, Right-wing Populism, and pro-Russian narratives, which is probably reinforced by Kremlin-initiated troll accounts. In the same way, there is a &amp;quot;blue chamber&amp;quot;, but its content range is much more multilayered, and its communication pattern is much more inconspicuous. Since Twitter provided only about 1% of the data stream, these echo chambers are likely to be much more prominent in reality.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mm7uc", "is_robot_indexable": true, "report_reasons": null, "author": "nkode", "discussion_type": null, "num_comments": 67, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mm7uc/this_data_science_study_identified_a_german_echo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mm7uc/this_data_science_study_identified_a_german_echo/", "subreddit_subscribers": 855514, "created_utc": 1678347763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This article is co-written by me and my colleague Kai Dai. We are both data platform engineers at Tencent Music (NYSE: TME), a music streaming service provider with a whopping 800 million monthly active users. To drop the number here is not to brag but to give a hint of the sea of data that my poor coworkers and I have to deal with everyday.\n\n# What We Use ClickHouse For?\n\nThe music library of Tencent Music contains data of all forms and types: recorded music, live music, audios, videos, etc. As data platform engineers, our job is to distill information from the data, based on which our teammates can make better decisions to support our users and musical partners.\n\nSpecifically, we do all-round analysis of the songs, lyrics, melodies, albums, and artists, turn all this information into data assets, and pass them to our internal data users for inventory counting, user profiling, metrics analysis, and group targeting.\n\nhttps://preview.redd.it/6ctox51ummma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=44303723bc4a50800f958061738af4040b640190\n\n&amp;#x200B;\n\nWe stored and processed most of our data in Tencent Data Warehouse (TDW), an offline data platform where we put the data into various tag and metric systems and then created flat tables centering each object (songs, artists, etc.).\n\nThen we imported the flat tables into ClickHouse for analysis and Elasticsearch for data searching and group targeting.\n\nAfter that, our data analysts used the data under the tags and metrics they needed to form datasets for different usage scenarios, during which they could create their own tags and metrics.\n\nThe data processing pipeline looked like this:\n\nhttps://preview.redd.it/4k18troxmmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=39a3235ede128a223072f8de89de00074139b8aa\n\n# The Problems with ClickHouse\n\nWhen working with the above pipeline, we encountered a few difficulties:\n\n1. **Partial Update**: Partial update of columns was not supported. Therefore, any latency from any one of the data sources could delay the creation of flat tables, and thus undermine data timeliness.\n2. **High storage cost**: Data under different tags and metrics was updated at different frequencies. As much as ClickHouse excelled in dealing with flat tables, it was a huge waste of storage resources to just pour all data into a flat table and partition it by day, not to mention the maintenance cost coming with it.\n3. **High maintenance cost**: Architecturally speaking, ClickHouse was characterized by the strong coupling of storage nodes and compute nodes. Its components were heavily interdependent, adding to the risks of cluster instability. Plus, for federated queries across ClickHouse and Elasticsearch, we had to take care of a huge amount of connection issues. That was just tedious.\n\n# Transition to Apache Doris\n\n[Apache Doris](https://github.com/apache/doris), a real-time analytical database, boasts a few features that are exactly what we needed in solving our problems:\n\n1. **Partial update**: Doris supports a wide variety of data models, among which the Aggregate Model supports real-time partial update of columns. Building on this, we can directly ingest raw data into Doris and create flat tables there. The ingestion goes like this: Firstly, we use Spark to load data into Kafka; then, any incremental data will be updated to Doris and Elasticsearch via Flink. Meanwhile, Flink will pre-aggregate the data so as to release burden on Doris and Elasticsearch.\n2. **Storage cost**: Doris supports multi-table join queries and federated queries across Hive, Iceberg, Hudi, MySQL, and Elasticsearch. This allows us to split the large flat tables into smaller ones and partition them by update frequency. The benefits of doing so include a relief of storage burden and an increase of query throughput.\n3. **Maintenance cost**: Doris is of simple architecture and is compatible with MySQL protocol. Deploying Doris only involves two processes (FE and BE) with no dependency on other systems, making it easy to operate and maintain. Also, Doris supports querying external ES data tables. It can easily interface with the metadata in ES and automatically map the table schema from ES so we can conduct queries on Elasticsearch data via Doris without grappling with complex connections.\n\nWhat\u2019s more, Doris supports multiple data ingestion methods, including batch import from remote storage such as HDFS and S3, data reads from MySQL binlog and Kafka, and real-time data synchronization or batch import from MySQL, Oracle, and PostgreSQL. It ensures service availability and data reliability through a consistency protocol and is capable of auto debugging. This is great news for our operators and maintainers.\n\nStatistically speaking, these features have cut our storage cost by 42% and development cost by 40%.\n\nDuring our usage of Doris, we have received lots of support from the open source Apache Doris community and timely help from the SelectDB team, which is now running a commercial version of Apache Doris.\n\nhttps://preview.redd.it/81bc89ozmmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cf2a398c37aecd8784007f081da735941e8b89dc\n\n# Further Improvement to Serve Our Needs\n\n## Introduce a Semantic Layer\n\nSpeaking of the datasets, on the bright side, our data analysts are given the liberty of redefining and combining the tags and metrics at their convenience. But on the dark side, high heterogeneity of the tag and metric systems leads to more difficulty in their usage and management.\n\nOur solution is to introduce a semantic layer in our data processing pipeline. The semantic layer is where all the technical terms are translated into more comprehensible concepts for our internal data users. In other words, we are turning the tags and metrics into first-class citizens for data definement and management.\n\nhttps://preview.redd.it/aidg3is0nmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8a807c4b1f643b44807cff93b44b7d72ea1712d4\n\n**Why would this help?**\n\nFor data analysts, all tags and metrics will be created and shared at the semantic layer so there will be less confusion and higher efficiency.\n\nFor data users, they no longer need to create their own datasets or figure out which one is applicable for each scenario but can simply conduct queries on their specified tagset and metricset.\n\n## Upgrade the Semantic Layer\n\nExplicitly defining the tags and metrics at the semantic layer was not enough. In order to build a standardized data processing system, our next goal was to ensure consistent definition of tags and metrics throughout the whole data processing pipeline.\n\nFor this sake, we made the semantic layer the heart of our data management system:\n\nhttps://preview.redd.it/um5s6x6mnmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=78bd381f8ab37c59fd36962c7cbe3141332f1fbe\n\n**How does it work?**\n\nAll computing logics in TDW will be defined at the semantic layer in the form of a single tag or metric.\n\nThe semantic layer receives logic queries from the application side, selects an engine accordingly, and generates SQL. Then it sends the SQL command to TDW for execution. Meanwhile, it might also send configuration and data ingestion tasks to Doris and decide which metrics and tags should be accelerated.\n\nIn this way, we have made the tags and metrics more manageable. A fly in the ointment is that since each tag and metric is individually defined, we are struggling with automating the generation of a valid SQL statement for the queries. If you have any idea about this, you are more than welcome to talk to us.\n\n# Give Full Play to Apache Doris\n\nAs you can see, Apache Doris has played a pivotal role in our solution. Optimizing the usage of Doris can largely improve our overall data processing efficiency. So in this part, we are going to share with you what we do with Doris to accelerate data ingestion and queries and reduce costs.\n\n## What We Want?\n\nhttps://preview.redd.it/x6h7kdinnmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a029a06e63fd7711e531b92e7bad3e1a060cc127\n\nCurrently, we have 800+ tags and 1300+ metrics derived from the 80+ source tables in TDW.\n\nWhen importing data from TDW to Doris, we hope to achieve:\n\n* **Real-time availability:** In addition to the traditional T+1 offline data ingestion, we require real-time tagging.\n* **Partial update**: Each source table generates data through its own ETL task at various paces and involves only part of the tags and metrics, so we require the support for partial update of columns.\n* **High performance**: We need a response time of only a few seconds in group targeting, analysis and reporting scenarios.\n* **Low costs**: We hope to reduce costs as much as possible.\n\n## What We Do?\n\n1. **Generate Flat Tables in Flink Instead of TDW**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/gvz11yponmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0aefada4c8afaf0bd701a15e27e832a6b6cebec7\n\nGenerating flat tables in TDW has a few downsides:\n\n* **High storage cost**: TDW has to maintain an extra flat table apart from the discrete 80+ source tables. That\u2019s huge redundancy.\n* **Low real-timeliness**: Any delay in the source tables will be augmented and retard the whole data link.\n* **High development cost**: To achieve real-timeliness would require extra development efforts and resources.\n\nOn the contrary, generating flat tables in Doris is much easier and less expensive. The process is as follows:\n\n* Use Spark to import new data into Kafka in an offline manner.\n* Use Flink to consume Kafka data.\n* Create a flat table via the primary key ID.\n* Import the flat table into Doris.\n\nAs is shown below, Flink has aggregated the five lines of data, of which \u201cID\u201d=1, into one line in Doris, reducing the data writing pressure on Doris.\n\nhttps://preview.redd.it/nbpzimvtnmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5e61a1f259bdf67a94eea049f5e777c8c3f49121\n\nThis can largely reduce storage costs since TDW no long has to maintain two copies of data and KafKa only needs to store the new data pending for ingestion. What\u2019s more, we can add whatever ETL logic we want into Flink and reuse lots of development logic for offline and real-time data ingestion.\n\n**2. Name the Columns Smartly**\n\nAs we mentioned, the Aggregate Model of Doris allows partial update of columns. Here we provide a simple introduction to other data models in Doris for your reference:\n\n**Unique Model**: This is applicable for scenarios requiring primary key uniqueness. It only keeps the latest data of the same primary key ID. (As far as we know, the Apache Doris community is planning to include partial update of columns in the Unique Model, too.)\n\n**Duplicate Model**: This model stores all original data exactly as it is without any pre-aggregation or deduplication.\n\nAfter determining the data model, we had to think about how to name the columns. Using the tags or metrics as column names was not a choice because:\n\nI. Our internal data users might need to rename the metrics or tags, but Doris 1.1.3 does not support modification of column names.\n\nII. Tags might be taken online and offline frequently. If that involves the adding and dropping of columns, it will be not only time-consuming but also detrimental to query performance.\n\nInstead, we do the following:\n\n* **For flexible renaming of tags and metrics**, we use MySQL tables to store the metadata (name, globally unique ID, status, etc.). Any change to the names will only happen in the metadata but will not affect the table schema in Doris. For example, if a song\\_nameis given an ID of 4, it will be stored with the column name of a4 in Doris. Then if the song\\_nameis involved in a query, it will be converted to a4 in SQL.\n* **For the onlining and offlining of tags**, we sort out the tags based on how frequently they are being used. The least used ones will be given an offline mark in their metadata. No new data will be put under the offline tags but the existing data under those tags will still be available.\n* **For real-time availability of newly added tags and metrics**, we prebuild a few ID columns in Doris tables based on the mapping of name IDs. These reserved ID columns will be allocated to the newly added tags and metrics. Thus, we can avoid table schema change and the consequent overheads. Our experience shows that only 10 minutes after the tags and metrics are added, the data under them can be available.\n\nNoteworthily, the recently released Doris 1.2.0 supports Light Schema Change, which means that to add or remove columns, you only need to modify the metadata in FE. Also, you can rename the columns in data tables as long as you have enabled Light Schema Change for the tables. This is a big trouble saver for us.\n\n**3. Optimize Date Writing**\n\nHere are a few practices that have reduced our daily offline data ingestion time by 75% and our CUMU compaction score from 600+ to 100.\n\n* Flink pre-aggregation: as is mentioned above.\n* Auto-sizing of writing batch: To reduce Flink resource usage, we enable the data in one Kafka Topic to be written into various Doris tables and realize the automatic alteration of batch size based on the data amount.\n* Optimization of Doris data writing: fine-tune the the sizes of tablets and buckets as well as the compaction parameters for each scenario:\n\n`max_XXXX_compaction_threadmax_cumulative_compaction_num_singleton_deltas`\n\n* Optimization of the BE commit logic: conduct regular caching of BE lists, commit them to the BE nodes batch by batch, and use finer load balancing granularity.\n\nhttps://preview.redd.it/wwcorjpwnmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6ed322b73c0c3755141a6185fd496164b826f472\n\n**4. Use Dori-on-ES in Queries**\n\nAbout 60% of our data queries involve group targeting. Group targeting is to find our target data by using a set of tags as filters. It poses a few requirements for our data processing architecture:\n\n* Group targeting related to APP users can involve very complicated logic. That means the system must support hundreds of tags as filters simultaneously.\n* Most group targeting scenarios only require the latest tag data. However, metric queries need to support historical data.\n* Data users might need to perform further aggregated analysis of metric data after group targeting.\n* Data users might also need to perform detailed queries on tags and metrics after group targeting.\n\nAfter consideration, we decided to adopt Doris-on-ES. Doris is where we store the metric data for each scenario as a partition table, while Elasticsearch stores all tag data. The Doris-on-ES solution combines the distributed query planning capability of Doris and the full-text search capability of Elasticsearch. The query pattern is as follows:\n\n`SELECT tag, agg(metric)FROM DorisWHERE id in (select id from Es where tagFilter)GROUP BY tag`\n\nAs is shown, the ID data located in Elasticsearch will be used in the sub-query in Doris for metric analysis.\n\nIn practice, we find that the query response time is related to the size of the target group. If the target group contains over one million objects, the query will take up to 60 seconds. If it is even larger, a timeout error might occur.\n\nAfter investigation, we identified our two biggest time wasters:\n\nI. When Doris BE pulls data from Elasticsearch (1024 lines at a time by default), for a target group of over one million objects, the network I/O overhead can be huge.\n\nII. After the data pulling, Doris BE needs to conduct Join operations with local metric tables via SHUFFLE/BROADCAST, which can cost a lot.\n\nhttps://preview.redd.it/sv0ecnvynmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e9ad986f5c0a647ca89694ec992ffc28d8eb3d68\n\nThus, we make the following optimizations:\n\n* Add a query session variable es\\_optimizethat specifies whether to enable optimization.\n* In data writing into ES, add a BK column to store the bucket number after the primary key ID is hashed. The algorithm is the same as the bucketing algorithm in Doris (CRC32).\n* Use Doris BE to generate a Bucket Join execution plan, dispatch the bucket number to BE ScanNode and push it down to ES.\n* Use ES to compress the queried data; turn multiple data fetch into one and reduce network I/O overhead.\n* Make sure that Doris BE only pulls the data of buckets related to the local metric tables and conducts local Join operations directly to avoid data shuffling between Doris BEs.\n\nhttps://preview.redd.it/b8tjmhq1omma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c6448ec12283376c968b71fbb18bc2f6f8a546be\n\nAs a result, we reduce the query response time for large group targeting from 60 seconds to a surprising 3.7 seconds.\n\nCommunity information shows that Doris is going to support inverted indexing since version 2.0.0, which is soon to be released. With this new version, we will be able to conduct full-text search on text types, equivalence or range filtering of texts, numbers, and datetime, and conveniently combine AND, OR, NOT logic in filtering since the inverted indexing supports array types. This new feature of Doris is expected to deliver 3\\~5 times better performance than Elasticsearch on the same task.\n\n**5. Refine the Management of Data**\n\nDoris\u2019 capability of cold and hot data separation provides the foundation of our cost reduction strategies in data processing.\n\n* Based on the TTL mechanism of Doris, we only store data of the current year in Doris and put the historical data before that in TDW for lower storage cost.\n* We vary the numbers of copies for different data partitions. For example, we set three copies for data of the recent three months, which is used frequently, one copy for data older than six months, and two copies for data in between.\n* Doris supports turning hot data into cold data so we only store data of the past seven days in SSD and transfer data older than that to HDD for less expensive storage.\n\n# Conclusion\n\nThank you for scrolling all the way down here and finishing this long read. We\u2019ve shared our cheers and tears, lessons learned, and a few practices that might be of some value to you during our transition from ClickHouse to Doris. We really appreciate the help from the Apache Doris community and the SelectDB team, but we might still be chasing them around for a while since we attempt to realize auto-identification of cold and hot data, pre-computation of frequently used tags/metrics, simplification of code logic using Materialized Views, and so on and so forth.", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tencent Data Engineer: Why We Go from ClickHouse to Apache Doris?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "media_metadata": {"b8tjmhq1omma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 77, "x": 108, "u": "https://preview.redd.it/b8tjmhq1omma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b37d044c0ba5f84a80ba27d68305c3b88ea560a1"}, {"y": 155, "x": 216, "u": "https://preview.redd.it/b8tjmhq1omma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d0c8394c532ae83dcaa1dd71364aef3549c23c40"}, {"y": 231, "x": 320, "u": "https://preview.redd.it/b8tjmhq1omma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b6c4268184aa24e5e4a650fb6c796784c262cf0"}, {"y": 462, "x": 640, "u": "https://preview.redd.it/b8tjmhq1omma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=80eef4e184777d75b3f9a7cd0d88de2738a1d807"}, {"y": 693, "x": 960, "u": "https://preview.redd.it/b8tjmhq1omma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63e77e5e52d30124f6dd33cf5153d790c8d9a726"}, {"y": 779, "x": 1080, "u": "https://preview.redd.it/b8tjmhq1omma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93876f540a97567d9d58aad74c9f1880e9744945"}], "s": {"y": 924, "x": 1280, "u": "https://preview.redd.it/b8tjmhq1omma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c6448ec12283376c968b71fbb18bc2f6f8a546be"}, "id": "b8tjmhq1omma1"}, "4k18troxmmma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/4k18troxmmma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e38628fade0b180a0be29cbeafcc186b402f5f79"}, {"y": 125, "x": 216, "u": "https://preview.redd.it/4k18troxmmma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7c3c70a1f909c77beae95739458160193788545"}, {"y": 185, "x": 320, "u": "https://preview.redd.it/4k18troxmmma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf363841e2d09774dcd0c350e20a9478a5818c46"}, {"y": 371, "x": 640, "u": "https://preview.redd.it/4k18troxmmma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b84ba6b5ae04fdffbec485824b826b48fd77133a"}, {"y": 557, "x": 960, "u": "https://preview.redd.it/4k18troxmmma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37f0ecc99bf52a4c11f296a50af59c9393d524f6"}, {"y": 626, "x": 1080, "u": "https://preview.redd.it/4k18troxmmma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af74a88846b3518febb07fc18cb363f9fe5887ac"}], "s": {"y": 743, "x": 1280, "u": "https://preview.redd.it/4k18troxmmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=39a3235ede128a223072f8de89de00074139b8aa"}, "id": "4k18troxmmma1"}, "um5s6x6mnmma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/um5s6x6mnmma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3dcb5efce180e3489b8aa934104a2b54935cc7c8"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/um5s6x6mnmma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4643e9314dbac7432eea7457f1782f705c409750"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/um5s6x6mnmma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ad53ba0f1b4231f2f3fc8ecef0eb347c9d2c4ca"}, {"y": 357, "x": 640, "u": "https://preview.redd.it/um5s6x6mnmma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=afc5fd70b73d1d6ca9d3e1c6f4531267c04cb908"}, {"y": 535, "x": 960, "u": "https://preview.redd.it/um5s6x6mnmma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a34544a0e2bfac68d75fedeb017f91ff98d0f36b"}, {"y": 602, "x": 1080, "u": "https://preview.redd.it/um5s6x6mnmma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83969c40fe2d21160504f1591b208f96188a65b7"}], "s": {"y": 714, "x": 1280, "u": "https://preview.redd.it/um5s6x6mnmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=78bd381f8ab37c59fd36962c7cbe3141332f1fbe"}, "id": "um5s6x6mnmma1"}, "wwcorjpwnmma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 43, "x": 108, "u": "https://preview.redd.it/wwcorjpwnmma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0094321fc89b5ef2b843258d9c6768be3a46b0d1"}, {"y": 86, "x": 216, "u": "https://preview.redd.it/wwcorjpwnmma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fe1f890860a794a1c43a8f4949e7f7650e125a1"}, {"y": 127, "x": 320, "u": "https://preview.redd.it/wwcorjpwnmma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be6d1166b52ee56d14d76faba5060fa4a375ed53"}, {"y": 255, "x": 640, "u": "https://preview.redd.it/wwcorjpwnmma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce714040da3b3c761b078cc27a2080054ac79f03"}, {"y": 383, "x": 960, "u": "https://preview.redd.it/wwcorjpwnmma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a4b7de5252eb24b6a1127b4e5533cb6da8a674c"}, {"y": 431, "x": 1080, "u": "https://preview.redd.it/wwcorjpwnmma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0dc8c6a5e2336f3e021dbb5e6e9f43cc1cb943e4"}], "s": {"y": 511, "x": 1280, "u": "https://preview.redd.it/wwcorjpwnmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6ed322b73c0c3755141a6185fd496164b826f472"}, "id": "wwcorjpwnmma1"}, "6ctox51ummma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 58, "x": 108, "u": "https://preview.redd.it/6ctox51ummma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c80ec9b20188c4941c9d75cb0d6d59826970873"}, {"y": 116, "x": 216, "u": "https://preview.redd.it/6ctox51ummma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed3e4bd2fea16756c34015debfbd8423a619db98"}, {"y": 173, "x": 320, "u": "https://preview.redd.it/6ctox51ummma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=208ccc16b699e3ef2a56056d7efa2be857543cf4"}, {"y": 346, "x": 640, "u": "https://preview.redd.it/6ctox51ummma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=026b75160e490b9de4c41640738678761085d20b"}, {"y": 519, "x": 960, "u": "https://preview.redd.it/6ctox51ummma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f64f97e9d7b8e929790f4daae5c9986aaee5a1de"}, {"y": 584, "x": 1080, "u": "https://preview.redd.it/6ctox51ummma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37f266f93588e9efb5c57ec837fe7e61977d32f9"}], "s": {"y": 693, "x": 1280, "u": "https://preview.redd.it/6ctox51ummma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=44303723bc4a50800f958061738af4040b640190"}, "id": "6ctox51ummma1"}, "nbpzimvtnmma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/nbpzimvtnmma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e2f060f9d9a7c9a38efaec7fc06fa6ba9d76a1f"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/nbpzimvtnmma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a6743682a828f85d382cc7835d808fb6889cde8b"}, {"y": 155, "x": 320, "u": "https://preview.redd.it/nbpzimvtnmma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f2ef36af7c2eb65b5d6708e17b5bf91873deeb6"}, {"y": 311, "x": 640, "u": "https://preview.redd.it/nbpzimvtnmma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e04941224237afd61a399b38e666412e9806c1c9"}, {"y": 466, "x": 960, "u": "https://preview.redd.it/nbpzimvtnmma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93df29dc1634b8c3a237bf292f1f9536c4d603dd"}, {"y": 524, "x": 1080, "u": "https://preview.redd.it/nbpzimvtnmma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cfdc1e430b917f1610fe0b8bf8f0da6bf0be82de"}], "s": {"y": 622, "x": 1280, "u": "https://preview.redd.it/nbpzimvtnmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5e61a1f259bdf67a94eea049f5e777c8c3f49121"}, "id": "nbpzimvtnmma1"}, "aidg3is0nmma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/aidg3is0nmma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2de2b6ef8fab93e8edb684540750d64d8eca0263"}, {"y": 125, "x": 216, "u": "https://preview.redd.it/aidg3is0nmma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2984261d2d71fa93f93941049f7ca428a9b6e18d"}, {"y": 185, "x": 320, "u": "https://preview.redd.it/aidg3is0nmma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=961803fb67dcd617bbb9f49766ce5cd6b31bfb2f"}, {"y": 371, "x": 640, "u": "https://preview.redd.it/aidg3is0nmma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4af0cd7b7f21dc2533e838f7553c65b2331b3d7f"}, {"y": 557, "x": 960, "u": "https://preview.redd.it/aidg3is0nmma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b44792ec90f99d8bc727ea1ab741186029799129"}, {"y": 626, "x": 1080, "u": "https://preview.redd.it/aidg3is0nmma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a60072529e63e8f1abd27cb13729b7b1baec60b3"}], "s": {"y": 743, "x": 1280, "u": "https://preview.redd.it/aidg3is0nmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8a807c4b1f643b44807cff93b44b7d72ea1712d4"}, "id": "aidg3is0nmma1"}, "gvz11yponmma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/gvz11yponmma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f70402a73c3feaa9b6be336da6c42d8e122f261"}, {"y": 95, "x": 216, "u": "https://preview.redd.it/gvz11yponmma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9ae13c16d7c653c468ac0cc345fa661857f5e45"}, {"y": 141, "x": 320, "u": "https://preview.redd.it/gvz11yponmma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec73fc5f17356042167e60fb4e681531c9783f76"}, {"y": 283, "x": 640, "u": "https://preview.redd.it/gvz11yponmma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2598df0c90406864a4c6d3fc7fbb1322d707098"}, {"y": 425, "x": 960, "u": "https://preview.redd.it/gvz11yponmma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de901be30512353d75710ce7f3be55b07ce072ce"}, {"y": 478, "x": 1080, "u": "https://preview.redd.it/gvz11yponmma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96e5dcab8264410c18137565851393b76f686551"}], "s": {"y": 567, "x": 1280, "u": "https://preview.redd.it/gvz11yponmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0aefada4c8afaf0bd701a15e27e832a6b6cebec7"}, "id": "gvz11yponmma1"}, "x6h7kdinnmma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/x6h7kdinnmma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c128e5091d3841c6bdd560cff3fde6a200008a5a"}, {"y": 74, "x": 216, "u": "https://preview.redd.it/x6h7kdinnmma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5fd6105d961bda3089f9d8ff4789cbb5c134be3"}, {"y": 111, "x": 320, "u": "https://preview.redd.it/x6h7kdinnmma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adc4d97e45f37732d817872d485b9c0a8916cd35"}, {"y": 222, "x": 640, "u": "https://preview.redd.it/x6h7kdinnmma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=704f038b98d30c2e7365af41323d38d76a8212b9"}, {"y": 333, "x": 960, "u": "https://preview.redd.it/x6h7kdinnmma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fab898f50adeed5740db11e8d85dd8eee770ff4"}, {"y": 374, "x": 1080, "u": "https://preview.redd.it/x6h7kdinnmma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70468ae0f733e0cf37888ee180ffda7c8ad5f9b6"}], "s": {"y": 444, "x": 1280, "u": "https://preview.redd.it/x6h7kdinnmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a029a06e63fd7711e531b92e7bad3e1a060cc127"}, "id": "x6h7kdinnmma1"}, "81bc89ozmmma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 61, "x": 108, "u": "https://preview.redd.it/81bc89ozmmma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0ac82b53bfc04a84e0aef4b802037fad900c6ee"}, {"y": 123, "x": 216, "u": "https://preview.redd.it/81bc89ozmmma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0da99db05f33ffed878984d3996780d5cc0294df"}, {"y": 183, "x": 320, "u": "https://preview.redd.it/81bc89ozmmma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae66b01d17fd0267a978afb387574b099bba2dae"}, {"y": 367, "x": 640, "u": "https://preview.redd.it/81bc89ozmmma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ee08dcbeee16e10393035b94b2b5a7212c40bfb"}, {"y": 550, "x": 960, "u": "https://preview.redd.it/81bc89ozmmma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41ad4a1ab2b1f6c33c0b0922de30ba7884108b3e"}, {"y": 619, "x": 1080, "u": "https://preview.redd.it/81bc89ozmmma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b39f544d858b90cd46a828b1f0e2891000eacc0"}], "s": {"y": 734, "x": 1280, "u": "https://preview.redd.it/81bc89ozmmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cf2a398c37aecd8784007f081da735941e8b89dc"}, "id": "81bc89ozmmma1"}, "sv0ecnvynmma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/sv0ecnvynmma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b26acd7831b5ecb3180e49e880c5b9c5e599b97e"}, {"y": 149, "x": 216, "u": "https://preview.redd.it/sv0ecnvynmma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fc95a6b6169b0441903b455245f0b1e3132cc8f"}, {"y": 220, "x": 320, "u": "https://preview.redd.it/sv0ecnvynmma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=42b77770b96f02e3b306dad9f36e490a9034b14f"}, {"y": 441, "x": 640, "u": "https://preview.redd.it/sv0ecnvynmma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df4c27a0fac4b8e1971af284849fd22cb14311e0"}, {"y": 662, "x": 960, "u": "https://preview.redd.it/sv0ecnvynmma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c670ff7437ebdd8603185745d0787e0fde479943"}, {"y": 745, "x": 1080, "u": "https://preview.redd.it/sv0ecnvynmma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b88f1ad3d0b90a0045dd7a8dfb18b5298f42308c"}], "s": {"y": 883, "x": 1280, "u": "https://preview.redd.it/sv0ecnvynmma1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e9ad986f5c0a647ca89694ec992ffc28d8eb3d68"}, "id": "sv0ecnvynmma1"}}, "name": "t3_11mgmf1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/uSDK_TrBd_ZPOeANWw3cuCrqVjVMYLgmEwTPkJKedK8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1678330365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This article is co-written by me and my colleague Kai Dai. We are both data platform engineers at Tencent Music (NYSE: TME), a music streaming service provider with a whopping 800 million monthly active users. To drop the number here is not to brag but to give a hint of the sea of data that my poor coworkers and I have to deal with everyday.&lt;/p&gt;\n\n&lt;h1&gt;What We Use ClickHouse For?&lt;/h1&gt;\n\n&lt;p&gt;The music library of Tencent Music contains data of all forms and types: recorded music, live music, audios, videos, etc. As data platform engineers, our job is to distill information from the data, based on which our teammates can make better decisions to support our users and musical partners.&lt;/p&gt;\n\n&lt;p&gt;Specifically, we do all-round analysis of the songs, lyrics, melodies, albums, and artists, turn all this information into data assets, and pass them to our internal data users for inventory counting, user profiling, metrics analysis, and group targeting.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6ctox51ummma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=44303723bc4a50800f958061738af4040b640190\"&gt;https://preview.redd.it/6ctox51ummma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=44303723bc4a50800f958061738af4040b640190&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;We stored and processed most of our data in Tencent Data Warehouse (TDW), an offline data platform where we put the data into various tag and metric systems and then created flat tables centering each object (songs, artists, etc.).&lt;/p&gt;\n\n&lt;p&gt;Then we imported the flat tables into ClickHouse for analysis and Elasticsearch for data searching and group targeting.&lt;/p&gt;\n\n&lt;p&gt;After that, our data analysts used the data under the tags and metrics they needed to form datasets for different usage scenarios, during which they could create their own tags and metrics.&lt;/p&gt;\n\n&lt;p&gt;The data processing pipeline looked like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4k18troxmmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=39a3235ede128a223072f8de89de00074139b8aa\"&gt;https://preview.redd.it/4k18troxmmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=39a3235ede128a223072f8de89de00074139b8aa&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;The Problems with ClickHouse&lt;/h1&gt;\n\n&lt;p&gt;When working with the above pipeline, we encountered a few difficulties:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Partial Update&lt;/strong&gt;: Partial update of columns was not supported. Therefore, any latency from any one of the data sources could delay the creation of flat tables, and thus undermine data timeliness.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High storage cost&lt;/strong&gt;: Data under different tags and metrics was updated at different frequencies. As much as ClickHouse excelled in dealing with flat tables, it was a huge waste of storage resources to just pour all data into a flat table and partition it by day, not to mention the maintenance cost coming with it.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High maintenance cost&lt;/strong&gt;: Architecturally speaking, ClickHouse was characterized by the strong coupling of storage nodes and compute nodes. Its components were heavily interdependent, adding to the risks of cluster instability. Plus, for federated queries across ClickHouse and Elasticsearch, we had to take care of a huge amount of connection issues. That was just tedious.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Transition to Apache Doris&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/apache/doris\"&gt;Apache Doris&lt;/a&gt;, a real-time analytical database, boasts a few features that are exactly what we needed in solving our problems:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Partial update&lt;/strong&gt;: Doris supports a wide variety of data models, among which the Aggregate Model supports real-time partial update of columns. Building on this, we can directly ingest raw data into Doris and create flat tables there. The ingestion goes like this: Firstly, we use Spark to load data into Kafka; then, any incremental data will be updated to Doris and Elasticsearch via Flink. Meanwhile, Flink will pre-aggregate the data so as to release burden on Doris and Elasticsearch.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Storage cost&lt;/strong&gt;: Doris supports multi-table join queries and federated queries across Hive, Iceberg, Hudi, MySQL, and Elasticsearch. This allows us to split the large flat tables into smaller ones and partition them by update frequency. The benefits of doing so include a relief of storage burden and an increase of query throughput.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Maintenance cost&lt;/strong&gt;: Doris is of simple architecture and is compatible with MySQL protocol. Deploying Doris only involves two processes (FE and BE) with no dependency on other systems, making it easy to operate and maintain. Also, Doris supports querying external ES data tables. It can easily interface with the metadata in ES and automatically map the table schema from ES so we can conduct queries on Elasticsearch data via Doris without grappling with complex connections.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What\u2019s more, Doris supports multiple data ingestion methods, including batch import from remote storage such as HDFS and S3, data reads from MySQL binlog and Kafka, and real-time data synchronization or batch import from MySQL, Oracle, and PostgreSQL. It ensures service availability and data reliability through a consistency protocol and is capable of auto debugging. This is great news for our operators and maintainers.&lt;/p&gt;\n\n&lt;p&gt;Statistically speaking, these features have cut our storage cost by 42% and development cost by 40%.&lt;/p&gt;\n\n&lt;p&gt;During our usage of Doris, we have received lots of support from the open source Apache Doris community and timely help from the SelectDB team, which is now running a commercial version of Apache Doris.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/81bc89ozmmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cf2a398c37aecd8784007f081da735941e8b89dc\"&gt;https://preview.redd.it/81bc89ozmmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cf2a398c37aecd8784007f081da735941e8b89dc&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Further Improvement to Serve Our Needs&lt;/h1&gt;\n\n&lt;h2&gt;Introduce a Semantic Layer&lt;/h2&gt;\n\n&lt;p&gt;Speaking of the datasets, on the bright side, our data analysts are given the liberty of redefining and combining the tags and metrics at their convenience. But on the dark side, high heterogeneity of the tag and metric systems leads to more difficulty in their usage and management.&lt;/p&gt;\n\n&lt;p&gt;Our solution is to introduce a semantic layer in our data processing pipeline. The semantic layer is where all the technical terms are translated into more comprehensible concepts for our internal data users. In other words, we are turning the tags and metrics into first-class citizens for data definement and management.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aidg3is0nmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8a807c4b1f643b44807cff93b44b7d72ea1712d4\"&gt;https://preview.redd.it/aidg3is0nmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8a807c4b1f643b44807cff93b44b7d72ea1712d4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why would this help?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For data analysts, all tags and metrics will be created and shared at the semantic layer so there will be less confusion and higher efficiency.&lt;/p&gt;\n\n&lt;p&gt;For data users, they no longer need to create their own datasets or figure out which one is applicable for each scenario but can simply conduct queries on their specified tagset and metricset.&lt;/p&gt;\n\n&lt;h2&gt;Upgrade the Semantic Layer&lt;/h2&gt;\n\n&lt;p&gt;Explicitly defining the tags and metrics at the semantic layer was not enough. In order to build a standardized data processing system, our next goal was to ensure consistent definition of tags and metrics throughout the whole data processing pipeline.&lt;/p&gt;\n\n&lt;p&gt;For this sake, we made the semantic layer the heart of our data management system:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/um5s6x6mnmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=78bd381f8ab37c59fd36962c7cbe3141332f1fbe\"&gt;https://preview.redd.it/um5s6x6mnmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=78bd381f8ab37c59fd36962c7cbe3141332f1fbe&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How does it work?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;All computing logics in TDW will be defined at the semantic layer in the form of a single tag or metric.&lt;/p&gt;\n\n&lt;p&gt;The semantic layer receives logic queries from the application side, selects an engine accordingly, and generates SQL. Then it sends the SQL command to TDW for execution. Meanwhile, it might also send configuration and data ingestion tasks to Doris and decide which metrics and tags should be accelerated.&lt;/p&gt;\n\n&lt;p&gt;In this way, we have made the tags and metrics more manageable. A fly in the ointment is that since each tag and metric is individually defined, we are struggling with automating the generation of a valid SQL statement for the queries. If you have any idea about this, you are more than welcome to talk to us.&lt;/p&gt;\n\n&lt;h1&gt;Give Full Play to Apache Doris&lt;/h1&gt;\n\n&lt;p&gt;As you can see, Apache Doris has played a pivotal role in our solution. Optimizing the usage of Doris can largely improve our overall data processing efficiency. So in this part, we are going to share with you what we do with Doris to accelerate data ingestion and queries and reduce costs.&lt;/p&gt;\n\n&lt;h2&gt;What We Want?&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/x6h7kdinnmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a029a06e63fd7711e531b92e7bad3e1a060cc127\"&gt;https://preview.redd.it/x6h7kdinnmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a029a06e63fd7711e531b92e7bad3e1a060cc127&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Currently, we have 800+ tags and 1300+ metrics derived from the 80+ source tables in TDW.&lt;/p&gt;\n\n&lt;p&gt;When importing data from TDW to Doris, we hope to achieve:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Real-time availability:&lt;/strong&gt; In addition to the traditional T+1 offline data ingestion, we require real-time tagging.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Partial update&lt;/strong&gt;: Each source table generates data through its own ETL task at various paces and involves only part of the tags and metrics, so we require the support for partial update of columns.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt;: We need a response time of only a few seconds in group targeting, analysis and reporting scenarios.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Low costs&lt;/strong&gt;: We hope to reduce costs as much as possible.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;What We Do?&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Generate Flat Tables in Flink Instead of TDW&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gvz11yponmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0aefada4c8afaf0bd701a15e27e832a6b6cebec7\"&gt;https://preview.redd.it/gvz11yponmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0aefada4c8afaf0bd701a15e27e832a6b6cebec7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Generating flat tables in TDW has a few downsides:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;High storage cost&lt;/strong&gt;: TDW has to maintain an extra flat table apart from the discrete 80+ source tables. That\u2019s huge redundancy.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Low real-timeliness&lt;/strong&gt;: Any delay in the source tables will be augmented and retard the whole data link.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High development cost&lt;/strong&gt;: To achieve real-timeliness would require extra development efforts and resources.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;On the contrary, generating flat tables in Doris is much easier and less expensive. The process is as follows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use Spark to import new data into Kafka in an offline manner.&lt;/li&gt;\n&lt;li&gt;Use Flink to consume Kafka data.&lt;/li&gt;\n&lt;li&gt;Create a flat table via the primary key ID.&lt;/li&gt;\n&lt;li&gt;Import the flat table into Doris.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As is shown below, Flink has aggregated the five lines of data, of which \u201cID\u201d=1, into one line in Doris, reducing the data writing pressure on Doris.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nbpzimvtnmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5e61a1f259bdf67a94eea049f5e777c8c3f49121\"&gt;https://preview.redd.it/nbpzimvtnmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5e61a1f259bdf67a94eea049f5e777c8c3f49121&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This can largely reduce storage costs since TDW no long has to maintain two copies of data and KafKa only needs to store the new data pending for ingestion. What\u2019s more, we can add whatever ETL logic we want into Flink and reuse lots of development logic for offline and real-time data ingestion.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Name the Columns Smartly&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;As we mentioned, the Aggregate Model of Doris allows partial update of columns. Here we provide a simple introduction to other data models in Doris for your reference:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Unique Model&lt;/strong&gt;: This is applicable for scenarios requiring primary key uniqueness. It only keeps the latest data of the same primary key ID. (As far as we know, the Apache Doris community is planning to include partial update of columns in the Unique Model, too.)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Duplicate Model&lt;/strong&gt;: This model stores all original data exactly as it is without any pre-aggregation or deduplication.&lt;/p&gt;\n\n&lt;p&gt;After determining the data model, we had to think about how to name the columns. Using the tags or metrics as column names was not a choice because:&lt;/p&gt;\n\n&lt;p&gt;I. Our internal data users might need to rename the metrics or tags, but Doris 1.1.3 does not support modification of column names.&lt;/p&gt;\n\n&lt;p&gt;II. Tags might be taken online and offline frequently. If that involves the adding and dropping of columns, it will be not only time-consuming but also detrimental to query performance.&lt;/p&gt;\n\n&lt;p&gt;Instead, we do the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;For flexible renaming of tags and metrics&lt;/strong&gt;, we use MySQL tables to store the metadata (name, globally unique ID, status, etc.). Any change to the names will only happen in the metadata but will not affect the table schema in Doris. For example, if a song_nameis given an ID of 4, it will be stored with the column name of a4 in Doris. Then if the song_nameis involved in a query, it will be converted to a4 in SQL.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;For the onlining and offlining of tags&lt;/strong&gt;, we sort out the tags based on how frequently they are being used. The least used ones will be given an offline mark in their metadata. No new data will be put under the offline tags but the existing data under those tags will still be available.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;For real-time availability of newly added tags and metrics&lt;/strong&gt;, we prebuild a few ID columns in Doris tables based on the mapping of name IDs. These reserved ID columns will be allocated to the newly added tags and metrics. Thus, we can avoid table schema change and the consequent overheads. Our experience shows that only 10 minutes after the tags and metrics are added, the data under them can be available.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Noteworthily, the recently released Doris 1.2.0 supports Light Schema Change, which means that to add or remove columns, you only need to modify the metadata in FE. Also, you can rename the columns in data tables as long as you have enabled Light Schema Change for the tables. This is a big trouble saver for us.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Optimize Date Writing&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Here are a few practices that have reduced our daily offline data ingestion time by 75% and our CUMU compaction score from 600+ to 100.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Flink pre-aggregation: as is mentioned above.&lt;/li&gt;\n&lt;li&gt;Auto-sizing of writing batch: To reduce Flink resource usage, we enable the data in one Kafka Topic to be written into various Doris tables and realize the automatic alteration of batch size based on the data amount.&lt;/li&gt;\n&lt;li&gt;Optimization of Doris data writing: fine-tune the the sizes of tablets and buckets as well as the compaction parameters for each scenario:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;code&gt;max_XXXX_compaction_threadmax_cumulative_compaction_num_singleton_deltas&lt;/code&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Optimization of the BE commit logic: conduct regular caching of BE lists, commit them to the BE nodes batch by batch, and use finer load balancing granularity.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wwcorjpwnmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6ed322b73c0c3755141a6185fd496164b826f472\"&gt;https://preview.redd.it/wwcorjpwnmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6ed322b73c0c3755141a6185fd496164b826f472&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4. Use Dori-on-ES in Queries&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;About 60% of our data queries involve group targeting. Group targeting is to find our target data by using a set of tags as filters. It poses a few requirements for our data processing architecture:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Group targeting related to APP users can involve very complicated logic. That means the system must support hundreds of tags as filters simultaneously.&lt;/li&gt;\n&lt;li&gt;Most group targeting scenarios only require the latest tag data. However, metric queries need to support historical data.&lt;/li&gt;\n&lt;li&gt;Data users might need to perform further aggregated analysis of metric data after group targeting.&lt;/li&gt;\n&lt;li&gt;Data users might also need to perform detailed queries on tags and metrics after group targeting.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;After consideration, we decided to adopt Doris-on-ES. Doris is where we store the metric data for each scenario as a partition table, while Elasticsearch stores all tag data. The Doris-on-ES solution combines the distributed query planning capability of Doris and the full-text search capability of Elasticsearch. The query pattern is as follows:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SELECT tag, agg(metric)FROM DorisWHERE id in (select id from Es where tagFilter)GROUP BY tag&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;As is shown, the ID data located in Elasticsearch will be used in the sub-query in Doris for metric analysis.&lt;/p&gt;\n\n&lt;p&gt;In practice, we find that the query response time is related to the size of the target group. If the target group contains over one million objects, the query will take up to 60 seconds. If it is even larger, a timeout error might occur.&lt;/p&gt;\n\n&lt;p&gt;After investigation, we identified our two biggest time wasters:&lt;/p&gt;\n\n&lt;p&gt;I. When Doris BE pulls data from Elasticsearch (1024 lines at a time by default), for a target group of over one million objects, the network I/O overhead can be huge.&lt;/p&gt;\n\n&lt;p&gt;II. After the data pulling, Doris BE needs to conduct Join operations with local metric tables via SHUFFLE/BROADCAST, which can cost a lot.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/sv0ecnvynmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e9ad986f5c0a647ca89694ec992ffc28d8eb3d68\"&gt;https://preview.redd.it/sv0ecnvynmma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e9ad986f5c0a647ca89694ec992ffc28d8eb3d68&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thus, we make the following optimizations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Add a query session variable es_optimizethat specifies whether to enable optimization.&lt;/li&gt;\n&lt;li&gt;In data writing into ES, add a BK column to store the bucket number after the primary key ID is hashed. The algorithm is the same as the bucketing algorithm in Doris (CRC32).&lt;/li&gt;\n&lt;li&gt;Use Doris BE to generate a Bucket Join execution plan, dispatch the bucket number to BE ScanNode and push it down to ES.&lt;/li&gt;\n&lt;li&gt;Use ES to compress the queried data; turn multiple data fetch into one and reduce network I/O overhead.&lt;/li&gt;\n&lt;li&gt;Make sure that Doris BE only pulls the data of buckets related to the local metric tables and conducts local Join operations directly to avoid data shuffling between Doris BEs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/b8tjmhq1omma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c6448ec12283376c968b71fbb18bc2f6f8a546be\"&gt;https://preview.redd.it/b8tjmhq1omma1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c6448ec12283376c968b71fbb18bc2f6f8a546be&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As a result, we reduce the query response time for large group targeting from 60 seconds to a surprising 3.7 seconds.&lt;/p&gt;\n\n&lt;p&gt;Community information shows that Doris is going to support inverted indexing since version 2.0.0, which is soon to be released. With this new version, we will be able to conduct full-text search on text types, equivalence or range filtering of texts, numbers, and datetime, and conveniently combine AND, OR, NOT logic in filtering since the inverted indexing supports array types. This new feature of Doris is expected to deliver 3~5 times better performance than Elasticsearch on the same task.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5. Refine the Management of Data&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Doris\u2019 capability of cold and hot data separation provides the foundation of our cost reduction strategies in data processing.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Based on the TTL mechanism of Doris, we only store data of the current year in Doris and put the historical data before that in TDW for lower storage cost.&lt;/li&gt;\n&lt;li&gt;We vary the numbers of copies for different data partitions. For example, we set three copies for data of the recent three months, which is used frequently, one copy for data older than six months, and two copies for data in between.&lt;/li&gt;\n&lt;li&gt;Doris supports turning hot data into cold data so we only store data of the past seven days in SSD and transfer data older than that to HDD for less expensive storage.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Conclusion&lt;/h1&gt;\n\n&lt;p&gt;Thank you for scrolling all the way down here and finishing this long read. We\u2019ve shared our cheers and tears, lessons learned, and a few practices that might be of some value to you during our transition from ClickHouse to Doris. We really appreciate the help from the Apache Doris community and the SelectDB team, but we might still be chasing them around for a while since we attempt to realize auto-identification of cold and hot data, pre-computation of frequently used tags/metrics, simplification of code logic using Materialized Views, and so on and so forth.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4Qzb7fL-c1RpnMEKXlO8dM7IJbIAGm2gx9hyUxiUyFw.jpg?auto=webp&amp;v=enabled&amp;s=c071e8ac9cf83732ad688809e2f3fab38ee0bd2d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4Qzb7fL-c1RpnMEKXlO8dM7IJbIAGm2gx9hyUxiUyFw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b604b240fa28732fe848c4e80c20c50167929f3b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4Qzb7fL-c1RpnMEKXlO8dM7IJbIAGm2gx9hyUxiUyFw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f05cdb2d842dbd4f3822715e740f740940df77ee", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4Qzb7fL-c1RpnMEKXlO8dM7IJbIAGm2gx9hyUxiUyFw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9deea94f4d5ba809a5291d996114fe6e3ed06c27", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4Qzb7fL-c1RpnMEKXlO8dM7IJbIAGm2gx9hyUxiUyFw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ac22eea9bd5ae96ed13e4ed5ac51fd2b14e55ca", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4Qzb7fL-c1RpnMEKXlO8dM7IJbIAGm2gx9hyUxiUyFw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b230e7428ca4b7057fa8b22f752cd660b8e7550d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4Qzb7fL-c1RpnMEKXlO8dM7IJbIAGm2gx9hyUxiUyFw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2d8086eb3a366a67434c9898d34ff81c016a9a9", "width": 1080, "height": 540}], "variants": {}, "id": "uzLEZZHMwZxZOkWwz6mLSxTjWIdjuDM2MmNkAeVnNJA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mgmf1", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mgmf1/tencent_data_engineer_why_we_go_from_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mgmf1/tencent_data_engineer_why_we_go_from_clickhouse/", "subreddit_subscribers": 855514, "created_utc": 1678330365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Here's an interview with Chris Deotte, Quadruple Kaggle Grandmaster at NVIDIA. \n\nIn this episode, Chris shares valuable insights on topics such as crafting a strong data science resume, achieving grandmaster status on Kaggle (even quadruple), working at NVIDIA, and how to approach current data science challenges. Learn more about Kaggle, the data science world, and NVIDIA through the fascinating story of Chris Deotte. (and win an RTX 4080 thanks to NVIDIA GTC collaboration!)\n\nListen to this week's episode on your favorite platform: \n\n[https://youtu.be/NjGnnG3evmE](https://youtu.be/NjGnnG3evmE)\n\n[https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690](https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690)\n\n[https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt](https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt)", "author_fullname": "t2_c14wpji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An interview with Chris Deotte, Quadruple Kaggle Grandmaster and Data Scientist at NVIDIA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mhlju", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678333053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s an interview with Chris Deotte, Quadruple Kaggle Grandmaster at NVIDIA. &lt;/p&gt;\n\n&lt;p&gt;In this episode, Chris shares valuable insights on topics such as crafting a strong data science resume, achieving grandmaster status on Kaggle (even quadruple), working at NVIDIA, and how to approach current data science challenges. Learn more about Kaggle, the data science world, and NVIDIA through the fascinating story of Chris Deotte. (and win an RTX 4080 thanks to NVIDIA GTC collaboration!)&lt;/p&gt;\n\n&lt;p&gt;Listen to this week&amp;#39;s episode on your favorite platform: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/NjGnnG3evmE\"&gt;https://youtu.be/NjGnnG3evmE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690\"&gt;https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt\"&gt;https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QxJKr_LjR-d7tcAYXMrRQlHXi-xte4UZ7w3cUdJ87oQ.jpg?auto=webp&amp;v=enabled&amp;s=ad217e9e22b59124cccaca919e7b15b6fc857a91", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/QxJKr_LjR-d7tcAYXMrRQlHXi-xte4UZ7w3cUdJ87oQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10242c52b7d287c934dd8b5522605ee7cdb6a0f7", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QxJKr_LjR-d7tcAYXMrRQlHXi-xte4UZ7w3cUdJ87oQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97716c0df33be5eac5d14335dd05fd87fff78ef1", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QxJKr_LjR-d7tcAYXMrRQlHXi-xte4UZ7w3cUdJ87oQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d0668d76a843f731972b74487c091737af85436", "width": 320, "height": 240}], "variants": {}, "id": "NkclBFNszCSm9L1egqxSalhI4ESIMbvPkls0EYHxLvI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mhlju", "is_robot_indexable": true, "report_reasons": null, "author": "OnlyProggingForFun", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mhlju/an_interview_with_chris_deotte_quadruple_kaggle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mhlju/an_interview_with_chris_deotte_quadruple_kaggle/", "subreddit_subscribers": 855514, "created_utc": 1678333053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m mentoring a junior data scientist on my team and we\u2019ve been going through the importance of exploratory data analysis. Most of the techniques I use are ones I\u2019ve just picked up or learned over time, but does anyone have any favorite resources (books, blog posts, videos, etc.) that put all these EDA best practices together? Like if you have x type of data, these are some recommended first steps for EDA? We\u2019re currently working on a lot of text data but more general EDA tips would be great as well.", "author_fullname": "t2_femmyhh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Favorite resources for EDA?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mz57q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678385283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m mentoring a junior data scientist on my team and we\u2019ve been going through the importance of exploratory data analysis. Most of the techniques I use are ones I\u2019ve just picked up or learned over time, but does anyone have any favorite resources (books, blog posts, videos, etc.) that put all these EDA best practices together? Like if you have x type of data, these are some recommended first steps for EDA? We\u2019re currently working on a lot of text data but more general EDA tips would be great as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mz57q", "is_robot_indexable": true, "report_reasons": null, "author": "umnosorry", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mz57q/favorite_resources_for_eda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mz57q/favorite_resources_for_eda/", "subreddit_subscribers": 855514, "created_utc": 1678385283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Making the transition from finance to data science, and it\u2019s been one of those days where I\u2019m wondering if it\u2019s worth the additional investment (I\u2019m early 40s). \n\nWould love to hear any specific examples people can share of how they got immense satisfaction from their data science role, either through the measurable impact of a particular project, or how they enjoyed a recent project they worked on.", "author_fullname": "t2_uux9jy2q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would love to hear examples of projects people have worked on that gave them a great sense of purpose?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mdvl2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "spoiler", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678323041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Making the transition from finance to data science, and it\u2019s been one of those days where I\u2019m wondering if it\u2019s worth the additional investment (I\u2019m early 40s). &lt;/p&gt;\n\n&lt;p&gt;Would love to hear any specific examples people can share of how they got immense satisfaction from their data science role, either through the measurable impact of a particular project, or how they enjoyed a recent project they worked on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": true, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mdvl2", "is_robot_indexable": true, "report_reasons": null, "author": "idiskfla", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mdvl2/would_love_to_hear_examples_of_projects_people/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mdvl2/would_love_to_hear_examples_of_projects_people/", "subreddit_subscribers": 855514, "created_utc": 1678323041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How much importance does recuruiters/interviewers give to Github portfolio for Data Science jobs?", "author_fullname": "t2_3it27rt0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Github Portfolio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mjsxa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678339641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How much importance does recuruiters/interviewers give to Github portfolio for Data Science jobs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mjsxa", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Opinion_5729", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mjsxa/github_portfolio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mjsxa/github_portfolio/", "subreddit_subscribers": 855514, "created_utc": 1678339641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I am working with a high-dimensional dataset that contains approximately 300 columns and 10,000 rows. The dataset is characterized by a significant number of columns with missing values. To gain a better understanding of the missing data patterns, I generated a nullity matrix, which revealed the following:   \n\n[nullity matrix](https://preview.redd.it/6bhzfkds2qma1.png?width=2074&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=68c05b771136c8e20f2b2a39d7bd0fbd15a3950a)\n\n After careful consideration, I have devised a plan to divide the data into two datasets. The first dataset will consist of the first 5000 rows, while the second dataset will encompass the remaining rows after columns with a high number of missing values have been dropped. It is worth noting that the test dataset exhibits the same pattern of missing values, and will therefore undergo the same treatment. Subsequently, I will train my model using the first dataset to predict the outcomes of the first portion of the test data, and then repeat the process using the second dataset for the remaining portion of the test data. Is this approach valid, or is there a concern that important information will be lost? Do you have any suggestions for alternative, more optimal solutions for this particular challenge?", "author_fullname": "t2_7uwwf65z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dealing with a lot of missing values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 55, "top_awarded_type": null, "hide_score": false, "media_metadata": {"6bhzfkds2qma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdb5bebc1d165d70a832076654f3a7abf1f76970"}, {"y": 85, "x": 216, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c89d13eb9dc5c2781ba7cb82e25d9105c46ff5d"}, {"y": 126, "x": 320, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89947b0fdead6015965d641b52eaa672a95a05c5"}, {"y": 252, "x": 640, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efb348a7397231cefb27c03771c0de886de6eb3a"}, {"y": 378, "x": 960, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d41d281c1a3be7b98132ebf631891c7e8203aaa5"}, {"y": 425, "x": 1080, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1bf1b4135b3ff68b23bf181e04612b2e2c08bc9"}], "s": {"y": 817, "x": 2074, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=2074&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=68c05b771136c8e20f2b2a39d7bd0fbd15a3950a"}, "id": "6bhzfkds2qma1"}}, "name": "t3_11mtoua", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5Ai5OJ2cloS7xUaRLt83UptWtJpolBjcb2l13ISbhLM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678372315.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a high-dimensional dataset that contains approximately 300 columns and 10,000 rows. The dataset is characterized by a significant number of columns with missing values. To gain a better understanding of the missing data patterns, I generated a nullity matrix, which revealed the following:   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6bhzfkds2qma1.png?width=2074&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=68c05b771136c8e20f2b2a39d7bd0fbd15a3950a\"&gt;nullity matrix&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After careful consideration, I have devised a plan to divide the data into two datasets. The first dataset will consist of the first 5000 rows, while the second dataset will encompass the remaining rows after columns with a high number of missing values have been dropped. It is worth noting that the test dataset exhibits the same pattern of missing values, and will therefore undergo the same treatment. Subsequently, I will train my model using the first dataset to predict the outcomes of the first portion of the test data, and then repeat the process using the second dataset for the remaining portion of the test data. Is this approach valid, or is there a concern that important information will be lost? Do you have any suggestions for alternative, more optimal solutions for this particular challenge?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mtoua", "is_robot_indexable": true, "report_reasons": null, "author": "Hamdi_bks", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mtoua/dealing_with_a_lot_of_missing_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mtoua/dealing_with_a_lot_of_missing_values/", "subreddit_subscribers": 855514, "created_utc": 1678372315.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "By areas, I mean CV, NLP, Time series, Audio classification, and others. Do you try to have some basic knowledge of every areas?", "author_fullname": "t2_tuwq1ase", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When you are building portfolio, are you doing projects from more areas, or do you focus on one?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mz2ca", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678385101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;By areas, I mean CV, NLP, Time series, Audio classification, and others. Do you try to have some basic knowledge of every areas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mz2ca", "is_robot_indexable": true, "report_reasons": null, "author": "No_Philosophy_8520", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mz2ca/when_you_are_building_portfolio_are_you_doing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mz2ca/when_you_are_building_portfolio_are_you_doing/", "subreddit_subscribers": 855514, "created_utc": 1678385101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So, I'm six years in working at a mid sized commercial real estate brokerage (nine years total experience as DS, three years as head of dept) and have been getting the impression things were not going well for a while. I got official word today that they will be moving me to a consulting role but in light of my time with the company are giving me three months before officially doing so. It seems like they're winding down the office I work in and I just don't work closely enough with the main office to justify my salary. I don't suspect the consulting position is going to pay the bills.\n\nCouple of questions for y'all. First, generally speaking, how is the market out there? I know there have been a lot of tech layoffs recently. \n\nSecond, I feel like with my experience I should be making more (currently at 165 w/ bonus) and think private equity is where the money is. So figure I should be targeting something in finance for this next role as a stepping stone. Other than CRE being somewhat related to finance, I have no experience in the field. What would you guys be doing to prep for the move? Are there any issues with that plan I'm not foreseeing? Is it problematic that the market right now is down?\n\nI should probably point out my current role is mostly data wrangling and descriptive statistics, so my ML and stats knowledge is pretty rusty. I'm also primarily an R guy and pretty light on Python. Already started a \"Python for R users\" course.", "author_fullname": "t2_9wge0haf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Three month notice on my job, haven't been on the market in a while... couple questions for the group.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mfcvc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678326938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;m six years in working at a mid sized commercial real estate brokerage (nine years total experience as DS, three years as head of dept) and have been getting the impression things were not going well for a while. I got official word today that they will be moving me to a consulting role but in light of my time with the company are giving me three months before officially doing so. It seems like they&amp;#39;re winding down the office I work in and I just don&amp;#39;t work closely enough with the main office to justify my salary. I don&amp;#39;t suspect the consulting position is going to pay the bills.&lt;/p&gt;\n\n&lt;p&gt;Couple of questions for y&amp;#39;all. First, generally speaking, how is the market out there? I know there have been a lot of tech layoffs recently. &lt;/p&gt;\n\n&lt;p&gt;Second, I feel like with my experience I should be making more (currently at 165 w/ bonus) and think private equity is where the money is. So figure I should be targeting something in finance for this next role as a stepping stone. Other than CRE being somewhat related to finance, I have no experience in the field. What would you guys be doing to prep for the move? Are there any issues with that plan I&amp;#39;m not foreseeing? Is it problematic that the market right now is down?&lt;/p&gt;\n\n&lt;p&gt;I should probably point out my current role is mostly data wrangling and descriptive statistics, so my ML and stats knowledge is pretty rusty. I&amp;#39;m also primarily an R guy and pretty light on Python. Already started a &amp;quot;Python for R users&amp;quot; course.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mfcvc", "is_robot_indexable": true, "report_reasons": null, "author": "Tamalelulu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mfcvc/three_month_notice_on_my_job_havent_been_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mfcvc/three_month_notice_on_my_job_havent_been_on_the/", "subreddit_subscribers": 855514, "created_utc": 1678326938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi r/datascience community,\n\nI have been invited to give a lecture on \"*data science in practice*\" for a class of MSc. business analytics students. While I plan on covering some technical aspects of the field, I believe that it might be worthwhile to focus more on the non-technical insights and tricks that one picks up after some time in industry. The things you don't learn on Kaggle.\n\nThis got me thinking: *What are the important non-technical lessons to you guys*? Perhaps it's the importance of effective communication skills, the ability to work collaboratively with colleagues from diverse backgrounds, or the value of prioritizing business needs over technical perfectionism.\n\nI'm hoping to compile a list of key takeaways to share with the class (and this subreddit), and your insights would be incredibly valuable. Whether you're a seasoned data scientist or just starting out, I'd love to hear your thoughts on this topic.\n\nTo start the conversation off, I have considered the following two points:\n\n\\- **How to choose DS projects:** For example, I find it helpful to score the potential project on \"feasibility\", \"scalability\", \"maintainability\" and \"business importance\". \n\n\\- **Understanding the differences in** ***data science maturity*** across sectors and organizations. For many organisations, the barriers to success with DS/ML/AI are strategic alignment, data availability/integrity etc. or executive sponsorship/buy-in - not the technical know-how.\n\nLooking forward to hearing your thoughts.\n\n\ud83d\udcf7\ud83d\udcf7", "author_fullname": "t2_26n50qtm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What non-technical lessons are important for aspiring data scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n1xgt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678391695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt; community,&lt;/p&gt;\n\n&lt;p&gt;I have been invited to give a lecture on &amp;quot;&lt;em&gt;data science in practice&lt;/em&gt;&amp;quot; for a class of MSc. business analytics students. While I plan on covering some technical aspects of the field, I believe that it might be worthwhile to focus more on the non-technical insights and tricks that one picks up after some time in industry. The things you don&amp;#39;t learn on Kaggle.&lt;/p&gt;\n\n&lt;p&gt;This got me thinking: &lt;em&gt;What are the important non-technical lessons to you guys&lt;/em&gt;? Perhaps it&amp;#39;s the importance of effective communication skills, the ability to work collaboratively with colleagues from diverse backgrounds, or the value of prioritizing business needs over technical perfectionism.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping to compile a list of key takeaways to share with the class (and this subreddit), and your insights would be incredibly valuable. Whether you&amp;#39;re a seasoned data scientist or just starting out, I&amp;#39;d love to hear your thoughts on this topic.&lt;/p&gt;\n\n&lt;p&gt;To start the conversation off, I have considered the following two points:&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;How to choose DS projects:&lt;/strong&gt; For example, I find it helpful to score the potential project on &amp;quot;feasibility&amp;quot;, &amp;quot;scalability&amp;quot;, &amp;quot;maintainability&amp;quot; and &amp;quot;business importance&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Understanding the differences in&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;data science maturity&lt;/em&gt;&lt;/strong&gt; across sectors and organizations. For many organisations, the barriers to success with DS/ML/AI are strategic alignment, data availability/integrity etc. or executive sponsorship/buy-in - not the technical know-how.&lt;/p&gt;\n\n&lt;p&gt;Looking forward to hearing your thoughts.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcf7\ud83d\udcf7&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n1xgt", "is_robot_indexable": true, "report_reasons": null, "author": "Academy-", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n1xgt/what_nontechnical_lessons_are_important_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n1xgt/what_nontechnical_lessons_are_important_for/", "subreddit_subscribers": 855514, "created_utc": 1678391695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone, \n\nI hope this is a right sub since it is also a bit of finance related but mostly about data science career. \n\nI need some advice and tips for what I have been doing wrong. A little of background, my undergrad degree was in Applied Mathematics. My goal at the time was data science, I learned R, STATA, a bit of Python in college but no experience (I did try, but couldn\u2019t find any internship). I had a hard time getting into data science when I graduated, I get it, it was hard when I had 0 experience. I have been a software developer for 4 years now, I absolutely not enjoying it and looking to get back to data science. The relevant skill I have now is SQL, and with my understanding of software development, I thought it would help as well. I applied from entry to mid level, I tailored my resume, trying to connect with people on LinkedIn and all that. I still have a hard time, most recruiters that got back to me told me that the only relevant skill I have is SQL and that they don\u2019t think it\u2019s a good fit so not referring to hiring manager. I\u2019m so discouraged and Idk what am I doing wrong. Is that just bc of the economy now? I did noticed a lot of position prefer candidates with a master. I was thinking of going back to school to get my master in Data Science while still keeping my full time job (my employer won\u2019t pay for it. It will cost me $20k total, I won\u2019t have to take out loan but will have to cut somewhere else of course. It will still be a big commitment with time and money. So, what would you do if you were in my situation? Any tips and advice for me? \n\nThank you all for answering.", "author_fullname": "t2_k37cqst5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for transitioning from Software Developer to Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n0yc8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678389438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;I hope this is a right sub since it is also a bit of finance related but mostly about data science career. &lt;/p&gt;\n\n&lt;p&gt;I need some advice and tips for what I have been doing wrong. A little of background, my undergrad degree was in Applied Mathematics. My goal at the time was data science, I learned R, STATA, a bit of Python in college but no experience (I did try, but couldn\u2019t find any internship). I had a hard time getting into data science when I graduated, I get it, it was hard when I had 0 experience. I have been a software developer for 4 years now, I absolutely not enjoying it and looking to get back to data science. The relevant skill I have now is SQL, and with my understanding of software development, I thought it would help as well. I applied from entry to mid level, I tailored my resume, trying to connect with people on LinkedIn and all that. I still have a hard time, most recruiters that got back to me told me that the only relevant skill I have is SQL and that they don\u2019t think it\u2019s a good fit so not referring to hiring manager. I\u2019m so discouraged and Idk what am I doing wrong. Is that just bc of the economy now? I did noticed a lot of position prefer candidates with a master. I was thinking of going back to school to get my master in Data Science while still keeping my full time job (my employer won\u2019t pay for it. It will cost me $20k total, I won\u2019t have to take out loan but will have to cut somewhere else of course. It will still be a big commitment with time and money. So, what would you do if you were in my situation? Any tips and advice for me? &lt;/p&gt;\n\n&lt;p&gt;Thank you all for answering.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n0yc8", "is_robot_indexable": true, "report_reasons": null, "author": "imalamebutt", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n0yc8/tips_for_transitioning_from_software_developer_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n0yc8/tips_for_transitioning_from_software_developer_to/", "subreddit_subscribers": 855514, "created_utc": 1678389438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_1zz7cefa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automating operations on column-based data across multiple files using the Codex Davinci model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11msd2v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/yl1xdh05upma1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/yl1xdh05upma1/DASH_96.mp4", "dash_url": "https://v.redd.it/yl1xdh05upma1/DASHPlaylist.mpd?a=1680993615%2CNTYyMDBhM2E1Yjc4MjdjY2YxZjdjMDlmZDVkMGIwMWIyNWM3Y2QwOWE0NWNkNDhmMzVhODc3M2NmMjA3YzEyMw%3D%3D&amp;v=1&amp;f=sd", "duration": 19, "hls_url": "https://v.redd.it/yl1xdh05upma1/HLSPlaylist.m3u8?a=1680993615%2CYmNiYTVhY2M0OTEwYmU2ZDY5NzU4YzJkYzg2NjFjZDkyMTE4N2IwNjgzYjY5OWJhYWYyM2YzNDg4MGYyMWM5NA%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/s3J_Y0ezmKHiH2pX98m0wBU4FKskQlwwJWUkTGBqnLc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678368970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/yl1xdh05upma1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a08af611da44165a74d1c8e4a6ba151b3cfe8225", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=173556aff19a963730bb5c13645b06e0c87d2888", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8c9e849efa9fd3666c8c68dcf92fb8a5ba962ea2", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7df886a3ceab54f2068538fe5828182fd267d7b0", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=32ef900b4ae0be5098352535f6183c1558a595c2", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5d00980b8129c1e403c574d5a1f4fd9296db4e19", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6b0e9f98d5bf740c90650d6f66966a5f320ba66a", "width": 1080, "height": 607}], "variants": {}, "id": "sD1ViAqbGBOUiDB-PrxLdzGJ96XYSJnGoymD9gu3uww"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11msd2v", "is_robot_indexable": true, "report_reasons": null, "author": "kijubikal", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11msd2v/automating_operations_on_columnbased_data_across/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/yl1xdh05upma1", "subreddit_subscribers": 855514, "created_utc": 1678368970.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/yl1xdh05upma1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/yl1xdh05upma1/DASH_96.mp4", "dash_url": "https://v.redd.it/yl1xdh05upma1/DASHPlaylist.mpd?a=1680993615%2CNTYyMDBhM2E1Yjc4MjdjY2YxZjdjMDlmZDVkMGIwMWIyNWM3Y2QwOWE0NWNkNDhmMzVhODc3M2NmMjA3YzEyMw%3D%3D&amp;v=1&amp;f=sd", "duration": 19, "hls_url": "https://v.redd.it/yl1xdh05upma1/HLSPlaylist.m3u8?a=1680993615%2CYmNiYTVhY2M0OTEwYmU2ZDY5NzU4YzJkYzg2NjFjZDkyMTE4N2IwNjgzYjY5OWJhYWYyM2YzNDg4MGYyMWM5NA%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello people,\n\nI look for reference books for cleaning time series datasets. can anyone help me with this request.\n\nthanks", "author_fullname": "t2_eikjje19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "time series data cleaning reference books", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mrkmz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678366811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello people,&lt;/p&gt;\n\n&lt;p&gt;I look for reference books for cleaning time series datasets. can anyone help me with this request.&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mrkmz", "is_robot_indexable": true, "report_reasons": null, "author": "jorgecormane", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mrkmz/time_series_data_cleaning_reference_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mrkmz/time_series_data_cleaning_reference_books/", "subreddit_subscribers": 855514, "created_utc": 1678366811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_kgkprqpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Leading Technology Event - Big Data &amp; AI World 8-9 March, 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 52, "top_awarded_type": null, "hide_score": false, "name": "t3_11mqftc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/05_goNF3kF_nE5zdGN6lwz4PVQTtvoB0uDyUhHCwjxo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678363345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dasca.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.dasca.org/newsroom/discover-the-intelligent-future-at-big-data-ai-world-2023", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?auto=webp&amp;v=enabled&amp;s=de57f865cc5e42243d5235a4715c71954269bf68", "width": 1600, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adefb8b91ad01ebd34517d171d938b01bf5b8c66", "width": 108, "height": 40}, {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=130ce02b62410f3ed6685924e4fad97aa13a0f04", "width": 216, "height": 81}, {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6ce1c7599c569b518259aecb7cb2d35a806b26b", "width": 320, "height": 120}, {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72ee3e8af1bf03361ac42916120b0d876e4d67ae", "width": 640, "height": 240}, {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=436adb0f6d9229cb591781aede5f372aab9eaaba", "width": 960, "height": 360}, {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f94cf3246648acceb7000a1bea76ef45d413850", "width": 1080, "height": 405}], "variants": {}, "id": "aFdnRqHZhCc_vlA3H683TQXP5YJ0qnajroSr9Kd4K0M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mqftc", "is_robot_indexable": true, "report_reasons": null, "author": "Emily-joe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mqftc/leading_technology_event_big_data_ai_world_89/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dasca.org/newsroom/discover-the-intelligent-future-at-big-data-ai-world-2023", "subreddit_subscribers": 855514, "created_utc": 1678363345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all!\n\nI'm currently working with time series data. My manager wants me to use a \"simple\" model that is explainable. He said to start off with tree models, so I went with XGBoost having seen it being used for time series. I'm new to time series though, so I'm a bit confused as to how some things work.\n\nMy question is, upon train/test split, do I have to use the tail end of the dataset for the test set?\n\nIt doesn't seem to me like that makes a huge amount of sense for an XGBoost. Does the XGBoost model really take into account the order of the data points?", "author_fullname": "t2_9a5zvrr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "XGBoost for time series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11moqft", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678357333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working with time series data. My manager wants me to use a &amp;quot;simple&amp;quot; model that is explainable. He said to start off with tree models, so I went with XGBoost having seen it being used for time series. I&amp;#39;m new to time series though, so I&amp;#39;m a bit confused as to how some things work.&lt;/p&gt;\n\n&lt;p&gt;My question is, upon train/test split, do I have to use the tail end of the dataset for the test set?&lt;/p&gt;\n\n&lt;p&gt;It doesn&amp;#39;t seem to me like that makes a huge amount of sense for an XGBoost. Does the XGBoost model really take into account the order of the data points?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11moqft", "is_robot_indexable": true, "report_reasons": null, "author": "No_Storm_1500", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11moqft/xgboost_for_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11moqft/xgboost_for_time_series/", "subreddit_subscribers": 855514, "created_utc": 1678357333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! \n\nI am currently working as data analyst. I've been in several different fields but healthcare related fields. If anyone has suggestions for a field(domain) that you think is going to be in demand, stable in job security, I would greatly appreciate your input.\n\nAfter some research, what I find unique is healthcare claims data analytics. This job seems to require specific domain knowledge related to diagnostics and billing codes. \n\nAny healthcare claims data analysts out there willing to share some more details on what your day looks like? Do you deal with something new day to day requiring exploration? \n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_78q5af98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Domain Recommendations. How is your life, healthcare claims data analysts out there?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mbxk1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678318240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! &lt;/p&gt;\n\n&lt;p&gt;I am currently working as data analyst. I&amp;#39;ve been in several different fields but healthcare related fields. If anyone has suggestions for a field(domain) that you think is going to be in demand, stable in job security, I would greatly appreciate your input.&lt;/p&gt;\n\n&lt;p&gt;After some research, what I find unique is healthcare claims data analytics. This job seems to require specific domain knowledge related to diagnostics and billing codes. &lt;/p&gt;\n\n&lt;p&gt;Any healthcare claims data analysts out there willing to share some more details on what your day looks like? Do you deal with something new day to day requiring exploration? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mbxk1", "is_robot_indexable": true, "report_reasons": null, "author": "Lion_Awkward", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mbxk1/domain_recommendations_how_is_your_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mbxk1/domain_recommendations_how_is_your_life/", "subreddit_subscribers": 855514, "created_utc": 1678318240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I am new to LLMs world. I am curious to know whether it is worth using LLMs for text classification problems or they are better suited for text generation use cases?", "author_fullname": "t2_aad9f5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it worth using LLMs like GPT-3 for text classification?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mad6m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678314657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am new to LLMs world. I am curious to know whether it is worth using LLMs for text classification problems or they are better suited for text generation use cases?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mad6m", "is_robot_indexable": true, "report_reasons": null, "author": "pgalgali", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mad6m/is_it_worth_using_llms_like_gpt3_for_text/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mad6m/is_it_worth_using_llms_like_gpt3_for_text/", "subreddit_subscribers": 855514, "created_utc": 1678314657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Looking for some free tools to get quick insights", "author_fullname": "t2_65chay6oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good tools for exploratory data analysis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11n52b0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678398839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some free tools to get quick insights&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n52b0", "is_robot_indexable": true, "report_reasons": null, "author": "Mona_Labs", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n52b0/what_are_some_good_tools_for_exploratory_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n52b0/what_are_some_good_tools_for_exploratory_data/", "subreddit_subscribers": 855514, "created_utc": 1678398839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Has anyone written a Roadmap of Udemy courses a self taught person could use to learn Data Science and that way be prepared for the job market?\n\nThank you for your input, used google, searched a few forums and have not been able to find one.", "author_fullname": "t2_3luaynwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Udemy Roadmap request", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11n4he4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678397522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone written a Roadmap of Udemy courses a self taught person could use to learn Data Science and that way be prepared for the job market?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your input, used google, searched a few forums and have not been able to find one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n4he4", "is_robot_indexable": true, "report_reasons": null, "author": "LightDarkCloud", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n4he4/udemy_roadmap_request/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n4he4/udemy_roadmap_request/", "subreddit_subscribers": 855514, "created_utc": 1678397522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "DS in telecom here. My company's departments tend to work in silos. Occasionally we will need to collab with other DS from other departments. They seems to be extremely open to receiving code from our department to help with their modeling, and extremely reserved to share their code to help us out in return. Same goes for analysts teams and junior analysts are left to just \"figure it out on their own\"\n\nI'm a newer DS, and I feel like even the code I produce is potentially better than everyone else's because they never kick anything back to me or recommend improvements to what I provide. \n\nWhich brings on the confusion. I can't be that good at this. \n\nBut if I put together a pipeline with a logistic text classifier model hitting 80-85% accuracy, and send it over, and never hear back. What gives? Any senior DS understand what's going on here?", "author_fullname": "t2_jsdhakf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do y'all collaborate code with other internal departments?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n2dmf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678392732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DS in telecom here. My company&amp;#39;s departments tend to work in silos. Occasionally we will need to collab with other DS from other departments. They seems to be extremely open to receiving code from our department to help with their modeling, and extremely reserved to share their code to help us out in return. Same goes for analysts teams and junior analysts are left to just &amp;quot;figure it out on their own&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a newer DS, and I feel like even the code I produce is potentially better than everyone else&amp;#39;s because they never kick anything back to me or recommend improvements to what I provide. &lt;/p&gt;\n\n&lt;p&gt;Which brings on the confusion. I can&amp;#39;t be that good at this. &lt;/p&gt;\n\n&lt;p&gt;But if I put together a pipeline with a logistic text classifier model hitting 80-85% accuracy, and send it over, and never hear back. What gives? Any senior DS understand what&amp;#39;s going on here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n2dmf", "is_robot_indexable": true, "report_reasons": null, "author": "WeAreSOL", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n2dmf/do_yall_collaborate_code_with_other_internal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n2dmf/do_yall_collaborate_code_with_other_internal/", "subreddit_subscribers": 855514, "created_utc": 1678392732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Not sure if \u201cheterogeneous\u201d is the right word here but I\u2019m doing KRR where features that appear to be more suited to a power/log transform, some more suited for centering to 0 mean and unit variance. If I\u2019m manually choosing \u201chow\u201d to encode each feature, maybe I should upgrade to a fancier model, like a a neural network with VAE? Curious to hear opinions on this.", "author_fullname": "t2_1k2rzmpm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mixed Feature Transformers for highly heterogeneous data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n1yol", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678391771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if \u201cheterogeneous\u201d is the right word here but I\u2019m doing KRR where features that appear to be more suited to a power/log transform, some more suited for centering to 0 mean and unit variance. If I\u2019m manually choosing \u201chow\u201d to encode each feature, maybe I should upgrade to a fancier model, like a a neural network with VAE? Curious to hear opinions on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n1yol", "is_robot_indexable": true, "report_reasons": null, "author": "vent2012", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n1yol/mixed_feature_transformers_for_highly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n1yol/mixed_feature_transformers_for_highly/", "subreddit_subscribers": 855514, "created_utc": 1678391771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone, I am a 12th grader in a data science class and I chose my project to be on how data science can be used to learn aspects of history. I'm wondering what you all think about this, I've read some articles that were beneficial but I was interested in what you all had to say.\n\nthank you!", "author_fullname": "t2_62ubafd4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can data science be used to learn aspects of history?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n0d4p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678388101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I am a 12th grader in a data science class and I chose my project to be on how data science can be used to learn aspects of history. I&amp;#39;m wondering what you all think about this, I&amp;#39;ve read some articles that were beneficial but I was interested in what you all had to say.&lt;/p&gt;\n\n&lt;p&gt;thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n0d4p", "is_robot_indexable": true, "report_reasons": null, "author": "smellykenma", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n0d4p/how_can_data_science_be_used_to_learn_aspects_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n0d4p/how_can_data_science_be_used_to_learn_aspects_of/", "subreddit_subscribers": 855514, "created_utc": 1678388101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The project I'm currently working on uses (relatively) small tables / DB sizes. We have a setup that is appropriate for our current size of team, size of data, ease of use to maintain, ease of use to extract, etc.\n\nWe are going to be expanding our data size by something like 4x-20x in the next year or two.\n\nWe want to find examples of companies doing similar data work to us, and seeing how they managed their data.\n\nOur main setup is:\n\n1. Raw data (in the thousands of records, with thousands of datapoints, so relatively small all things considered). This is not expanding much.\n2. Raw data processing pipelines. This is where the 4x-20x is coming from. We can edit the processing pipeline to give us different output features.\n3. Post processing Feature values. We have a database of post processing features so we don't need to process the raw data each time we do a model training run.\n\nThe setup as described above must be quite common - storing post processed feature values. Then combining different sets of features to do model training runs.\n\nAre there any good resources on how more established companies manage this setup?", "author_fullname": "t2_80ont", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog posts detailing ML companies data infrastructure / DB schema?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mzo1f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678386478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The project I&amp;#39;m currently working on uses (relatively) small tables / DB sizes. We have a setup that is appropriate for our current size of team, size of data, ease of use to maintain, ease of use to extract, etc.&lt;/p&gt;\n\n&lt;p&gt;We are going to be expanding our data size by something like 4x-20x in the next year or two.&lt;/p&gt;\n\n&lt;p&gt;We want to find examples of companies doing similar data work to us, and seeing how they managed their data.&lt;/p&gt;\n\n&lt;p&gt;Our main setup is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Raw data (in the thousands of records, with thousands of datapoints, so relatively small all things considered). This is not expanding much.&lt;/li&gt;\n&lt;li&gt;Raw data processing pipelines. This is where the 4x-20x is coming from. We can edit the processing pipeline to give us different output features.&lt;/li&gt;\n&lt;li&gt;Post processing Feature values. We have a database of post processing features so we don&amp;#39;t need to process the raw data each time we do a model training run.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The setup as described above must be quite common - storing post processed feature values. Then combining different sets of features to do model training runs.&lt;/p&gt;\n\n&lt;p&gt;Are there any good resources on how more established companies manage this setup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mzo1f", "is_robot_indexable": true, "report_reasons": null, "author": "Cwlrs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mzo1f/blog_posts_detailing_ml_companies_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mzo1f/blog_posts_detailing_ml_companies_data/", "subreddit_subscribers": 855514, "created_utc": 1678386478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Have you ever wanted to use handy scikit-learn functionalities with your neural networks, but couldn\u2019t because TensorFlow models are not compatible with the scikit-learn API?\n\nI\u2019m excited to introduce one-line wrappers for TensorFlow/Keras models that enable you to use TensorFlow models within scikit-learn workflows with features like Pipeline, GridSearch, and more.\n\n[Swap in one line of code to use keras\\/TF models with scikit-learn.](https://preview.redd.it/jebe5i446rma1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8afe3c365de5c2810bfb2deaea840e7447b59f3e)\n\nTransformers are extremely popular for modeling text nowadays with GPT3, ChatGPT, Bard, PaLM, FLAN excelling for conversational AI and other Transformers like T5 &amp; BERT excelling for text classification. Scikit-learn offers a broadly useful suite of features for classifier models, but these are hard to use with Transformers. However not if you use these wrappers we developed, which only require changing one line of code to make your existing Tensorflow/Keras model compatible with scikit-learn\u2019s rich ecosystem!\n\nAll you have to do is swap `keras.Model` \u2192 `KerasWrapperModel`, or `keras.Sequential` \u2192 `KerasSequentialWrapper`. The wrapper objects have all the same methods as their keras counterparts, plus you can use them with tons of awesome scikit-learn methods.\n\nYou can find a demo jupyter notebook and read more about the wrappers [here](https://cleanlab.ai/blog/transformer-sklearn/) :)", "author_fullname": "t2_s0qucgfk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Training Transformer Networks in Scikit-Learn?!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"jebe5i446rma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/jebe5i446rma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b54c29c6cb01e512465dcd303a36d2f7edf905e0"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/jebe5i446rma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d08d3f6b881fa8b74ffc2f95a820c62ada76729b"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/jebe5i446rma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e8b15bf5d72de07a2d23a804518f9ba1ab43ede"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/jebe5i446rma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ded45299e716979802c2c3aecd270d2412cf343"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/jebe5i446rma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=787b99c95bb4e9ae19f07aca494b01ff89c4914f"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/jebe5i446rma1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8afe3c365de5c2810bfb2deaea840e7447b59f3e"}, "id": "jebe5i446rma1"}}, "name": "t3_11mzcle", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/v1Fd8XNkRgEWk-CFj66FA0QCVOqelXOeu69e1-2kd78.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678385751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you ever wanted to use handy scikit-learn functionalities with your neural networks, but couldn\u2019t because TensorFlow models are not compatible with the scikit-learn API?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m excited to introduce one-line wrappers for TensorFlow/Keras models that enable you to use TensorFlow models within scikit-learn workflows with features like Pipeline, GridSearch, and more.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jebe5i446rma1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8afe3c365de5c2810bfb2deaea840e7447b59f3e\"&gt;Swap in one line of code to use keras/TF models with scikit-learn.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Transformers are extremely popular for modeling text nowadays with GPT3, ChatGPT, Bard, PaLM, FLAN excelling for conversational AI and other Transformers like T5 &amp;amp; BERT excelling for text classification. Scikit-learn offers a broadly useful suite of features for classifier models, but these are hard to use with Transformers. However not if you use these wrappers we developed, which only require changing one line of code to make your existing Tensorflow/Keras model compatible with scikit-learn\u2019s rich ecosystem!&lt;/p&gt;\n\n&lt;p&gt;All you have to do is swap &lt;code&gt;keras.Model&lt;/code&gt; \u2192 &lt;code&gt;KerasWrapperModel&lt;/code&gt;, or &lt;code&gt;keras.Sequential&lt;/code&gt; \u2192 &lt;code&gt;KerasSequentialWrapper&lt;/code&gt;. The wrapper objects have all the same methods as their keras counterparts, plus you can use them with tons of awesome scikit-learn methods.&lt;/p&gt;\n\n&lt;p&gt;You can find a demo jupyter notebook and read more about the wrappers &lt;a href=\"https://cleanlab.ai/blog/transformer-sklearn/\"&gt;here&lt;/a&gt; :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mzcle", "is_robot_indexable": true, "report_reasons": null, "author": "cmauck10", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mzcle/training_transformer_networks_in_scikitlearn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mzcle/training_transformer_networks_in_scikitlearn/", "subreddit_subscribers": 855514, "created_utc": 1678385751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nI have a question regarding encoding high cardinality categorical variables and learning embedding vectors.\n\nTo make the explanation easier, I will take the example of columns representing ingredients for a recipe. They are categorical variables:\n\nProtein (meat, fish, eggs, beans, etc.)\n\nGrains (wheat, rice, oat, etc.)\n\nSpices (cinammon, cumin, ginger, etc.)\n\nBecause some recipes contain more than 1 protein, or more than 1 spice, these columns appear multiple times (Spice\\_1, Spice\\_2, Spice\\_3, etc.)\n\nQuestion 1: Is it reasonable to label encode Spice\\_1, Spice\\_2, Spice\\_3 simulateneously? \n\nQuestion 2: My main goal here is to learn embeddings to represent all these categories. Typically, we can use embedding layers to learn the mapping between categories and a dense vector representation for each feature and its associated categories.\n\nIn my case, by following this approach, I end up with different embedding representation of the same spice, depending if it was in Spice\\_1 column, Spice\\_2 column, etc.\n\nWhat would be the best way to ensure that the embeddings learned for similar features (Spice\\_1, Spice\\_2, Spice\\_3) do match?\n\nPlease note that I am actually working with chemical codes, so pretrained language models cannot be leveraged.\n\nThank you for your suggestions!", "author_fullname": "t2_19qf9hu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] Learning consistent embeddings across multiple features?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mvn4z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678377096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a question regarding encoding high cardinality categorical variables and learning embedding vectors.&lt;/p&gt;\n\n&lt;p&gt;To make the explanation easier, I will take the example of columns representing ingredients for a recipe. They are categorical variables:&lt;/p&gt;\n\n&lt;p&gt;Protein (meat, fish, eggs, beans, etc.)&lt;/p&gt;\n\n&lt;p&gt;Grains (wheat, rice, oat, etc.)&lt;/p&gt;\n\n&lt;p&gt;Spices (cinammon, cumin, ginger, etc.)&lt;/p&gt;\n\n&lt;p&gt;Because some recipes contain more than 1 protein, or more than 1 spice, these columns appear multiple times (Spice_1, Spice_2, Spice_3, etc.)&lt;/p&gt;\n\n&lt;p&gt;Question 1: Is it reasonable to label encode Spice_1, Spice_2, Spice_3 simulateneously? &lt;/p&gt;\n\n&lt;p&gt;Question 2: My main goal here is to learn embeddings to represent all these categories. Typically, we can use embedding layers to learn the mapping between categories and a dense vector representation for each feature and its associated categories.&lt;/p&gt;\n\n&lt;p&gt;In my case, by following this approach, I end up with different embedding representation of the same spice, depending if it was in Spice_1 column, Spice_2 column, etc.&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to ensure that the embeddings learned for similar features (Spice_1, Spice_2, Spice_3) do match?&lt;/p&gt;\n\n&lt;p&gt;Please note that I am actually working with chemical codes, so pretrained language models cannot be leveraged.&lt;/p&gt;\n\n&lt;p&gt;Thank you for your suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mvn4z", "is_robot_indexable": true, "report_reasons": null, "author": "DreamyPen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mvn4z/d_learning_consistent_embeddings_across_multiple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mvn4z/d_learning_consistent_embeddings_across_multiple/", "subreddit_subscribers": 855514, "created_utc": 1678377096.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}