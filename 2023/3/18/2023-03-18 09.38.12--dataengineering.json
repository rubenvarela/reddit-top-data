{"kind": "Listing", "data": {"after": "t3_11uijzg", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I fall within the former and have for several years between companies, as most of my work has been centered on building out a data warehouse environment and enabling the business to use their data. Recently have wondered if this type of experience is more common than the extract-load pipeline version?\n\nThe motivation of the question is to figure out where to take my career and how to spend my time on which topics to learn - for example, should I spend some time learning databricks or spark on my own, knowing most of my experience has been in GCP or Azure? Should I spend time on information retrieval? Or on information architecture as it applies to data/analytics? Do I get deeper into dbt?\n\nWhat I am trying to understand is how to navigate DE going forward, and how to best spend my time when thinking of skills/education.", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are most data engineers primarily working on improving the data warehouse compared to building extract-load pipelines, or vice versa? Will there be a trend pushing data engineers closer to analytics/business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tshrc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679062970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I fall within the former and have for several years between companies, as most of my work has been centered on building out a data warehouse environment and enabling the business to use their data. Recently have wondered if this type of experience is more common than the extract-load pipeline version?&lt;/p&gt;\n\n&lt;p&gt;The motivation of the question is to figure out where to take my career and how to spend my time on which topics to learn - for example, should I spend some time learning databricks or spark on my own, knowing most of my experience has been in GCP or Azure? Should I spend time on information retrieval? Or on information architecture as it applies to data/analytics? Do I get deeper into dbt?&lt;/p&gt;\n\n&lt;p&gt;What I am trying to understand is how to navigate DE going forward, and how to best spend my time when thinking of skills/education.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11tshrc", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tshrc/are_most_data_engineers_primarily_working_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tshrc/are_most_data_engineers_primarily_working_on/", "subreddit_subscribers": 93425, "created_utc": 1679062970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a working prototype of Python code in a Jupyter notebook that downloads data onto my local machine.\n\nHow do I set this script up such that it automatically runs on a schedule and downloads data to AWS S3? I\u2019ve read AWS Glue, Lambda and maybe an EC2 instance but not sure which is the \u201cbest\u201d (cheapest + reliable) approach. Any thoughts?\n\nWe are using Snowflake as our data warehouse.", "author_fullname": "t2_55wqn55d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best AWS service to run a Jupyter-like Python script calling an API to download data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tqzbe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679060436.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679059355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a working prototype of Python code in a Jupyter notebook that downloads data onto my local machine.&lt;/p&gt;\n\n&lt;p&gt;How do I set this script up such that it automatically runs on a schedule and downloads data to AWS S3? I\u2019ve read AWS Glue, Lambda and maybe an EC2 instance but not sure which is the \u201cbest\u201d (cheapest + reliable) approach. Any thoughts?&lt;/p&gt;\n\n&lt;p&gt;We are using Snowflake as our data warehouse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tqzbe", "is_robot_indexable": true, "report_reasons": null, "author": "ahkd13", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tqzbe/whats_the_best_aws_service_to_run_a_jupyterlike/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tqzbe/whats_the_best_aws_service_to_run_a_jupyterlike/", "subreddit_subscribers": 93425, "created_utc": 1679059355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In today's data-driven world, Snowflake has emerged as a popular cloud-based data warehousing platform. However, managing costs and ensuring optimal performance can be a challenge.  \n\n\nIn this post, I will share my tactics of cutting my client's Snowflake costs by 65% and boosting performance by 50% that you can use too!  \n\n\nEfficient Data Loading:  \n\u200b\u200b\u200b\u200b\u200b\u200b\u200b1.1 Sort data on ingestion  \n1.2 Use the CSV (Gzipped) format  \n1.3 Use COPY INTO instead of INSERT  \n1.4 Use Snowpipe for continuous, real-time data ingestion  \n1.5 Use file formats optimized for Snowflake, such as Parquet or ORC  \n1.6 Use a separate staging area for data loading  \n\n\nOptimize Compute Resources:  \n2.1 Use Virtual Warehouses to scale compute resources on demand  \n2.2 Use Auto-Suspend to automatically suspend inactive warehouses  \n2.3 Use Auto-Resume to automatically resume suspended warehouses  \n2.4 Use Multi-Cluster Warehouses for high-concurrency workloads  \n2.5 Use Snowflake's Resource Monitors to allocate resources based on priorities  \n2.6 Use query control parameters to manage concurrency and query processing  \n2.7 Set automatic suspension and resizing  \n\n\nEfficient Script Writing:  \n3.1 Avoid temporary tables - Use common table expressions (CTEs) wherever possible  \n3.2 Use window functions instead of subqueries  \n3.3 Avoid using DISTINCT and GROUP BY together  \n3.5 Use the query profile to identify inefficient queries  \n3.6 Use the TIMEZONE function to avoid time zone conversion issues  \n\n\nOptimize Storage:  \n4.1 Use S3 buckets in the same geographic region  \n4.2 Keep file sizes between 60-100 MB to benefit from parallel loading  \n4.3 Avoid using materialized views except in specific use cases  \n4.4 Use Snowflake's Zero-Copy Cloning feature to create lightweight copies of data  \n4.5 Use the VARIANT data type for semi-structured data to reduce storage costs  \n4.6 Use Snowflake's Data Sharing feature to share data between accounts or regions  \n4.7 Use partitioning to improve query performance and reduce storage costs  \n4.8 Use object metadata to manage and track data usage and retention policies.  \n\n\nSome of the key takeaways in this post include optimizing data loading, compute resources, script writing, and storage, as well as adopting best practices for query management and governance.  \n\n\nBy following these tactics, you can also achieve significant cost savings and performance improvements in your Snowflake environment.  \n\n\nI hope you found these tactics helpful in optimizing your Snowflake costs and performance. If you did, please consider showing your support by liking and sharing this post. I'd love to hear your own tips too, so feel free to leave a comment. Also, join the Snowflake Mastery group for more tips and insights on Snowflake.\n\n[\\#snowflake](https://www.linkedin.com/feed/hashtag/?keywords=snowflake&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768) [\\#costoptimization](https://www.linkedin.com/feed/hashtag/?keywords=costoptimization&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768) [\\#performanceimprovement](https://www.linkedin.com/feed/hashtag/?keywords=performanceimprovement&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768) [\\#datawarehousing](https://www.linkedin.com/feed/hashtag/?keywords=datawarehousing&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768) [\\#queryoptimization](https://www.linkedin.com/feed/hashtag/?keywords=queryoptimization&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768) [\\#snowflakemastery](https://www.linkedin.com/feed/hashtag/?keywords=snowflakemastery&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768)", "author_fullname": "t2_5yzj730ib", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How I Cut My Client's Snowflake Cost by 65% and Boosted Performance by 50% - Learn My Tactics!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ufzi4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679119219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In today&amp;#39;s data-driven world, Snowflake has emerged as a popular cloud-based data warehousing platform. However, managing costs and ensuring optimal performance can be a challenge.  &lt;/p&gt;\n\n&lt;p&gt;In this post, I will share my tactics of cutting my client&amp;#39;s Snowflake costs by 65% and boosting performance by 50% that you can use too!  &lt;/p&gt;\n\n&lt;p&gt;Efficient Data Loading:&lt;br/&gt;\n\u200b\u200b\u200b\u200b\u200b\u200b\u200b1.1 Sort data on ingestion&lt;br/&gt;\n1.2 Use the CSV (Gzipped) format&lt;br/&gt;\n1.3 Use COPY INTO instead of INSERT&lt;br/&gt;\n1.4 Use Snowpipe for continuous, real-time data ingestion&lt;br/&gt;\n1.5 Use file formats optimized for Snowflake, such as Parquet or ORC&lt;br/&gt;\n1.6 Use a separate staging area for data loading  &lt;/p&gt;\n\n&lt;p&gt;Optimize Compute Resources:&lt;br/&gt;\n2.1 Use Virtual Warehouses to scale compute resources on demand&lt;br/&gt;\n2.2 Use Auto-Suspend to automatically suspend inactive warehouses&lt;br/&gt;\n2.3 Use Auto-Resume to automatically resume suspended warehouses&lt;br/&gt;\n2.4 Use Multi-Cluster Warehouses for high-concurrency workloads&lt;br/&gt;\n2.5 Use Snowflake&amp;#39;s Resource Monitors to allocate resources based on priorities&lt;br/&gt;\n2.6 Use query control parameters to manage concurrency and query processing&lt;br/&gt;\n2.7 Set automatic suspension and resizing  &lt;/p&gt;\n\n&lt;p&gt;Efficient Script Writing:&lt;br/&gt;\n3.1 Avoid temporary tables - Use common table expressions (CTEs) wherever possible&lt;br/&gt;\n3.2 Use window functions instead of subqueries&lt;br/&gt;\n3.3 Avoid using DISTINCT and GROUP BY together&lt;br/&gt;\n3.5 Use the query profile to identify inefficient queries&lt;br/&gt;\n3.6 Use the TIMEZONE function to avoid time zone conversion issues  &lt;/p&gt;\n\n&lt;p&gt;Optimize Storage:&lt;br/&gt;\n4.1 Use S3 buckets in the same geographic region&lt;br/&gt;\n4.2 Keep file sizes between 60-100 MB to benefit from parallel loading&lt;br/&gt;\n4.3 Avoid using materialized views except in specific use cases&lt;br/&gt;\n4.4 Use Snowflake&amp;#39;s Zero-Copy Cloning feature to create lightweight copies of data&lt;br/&gt;\n4.5 Use the VARIANT data type for semi-structured data to reduce storage costs&lt;br/&gt;\n4.6 Use Snowflake&amp;#39;s Data Sharing feature to share data between accounts or regions&lt;br/&gt;\n4.7 Use partitioning to improve query performance and reduce storage costs&lt;br/&gt;\n4.8 Use object metadata to manage and track data usage and retention policies.  &lt;/p&gt;\n\n&lt;p&gt;Some of the key takeaways in this post include optimizing data loading, compute resources, script writing, and storage, as well as adopting best practices for query management and governance.  &lt;/p&gt;\n\n&lt;p&gt;By following these tactics, you can also achieve significant cost savings and performance improvements in your Snowflake environment.  &lt;/p&gt;\n\n&lt;p&gt;I hope you found these tactics helpful in optimizing your Snowflake costs and performance. If you did, please consider showing your support by liking and sharing this post. I&amp;#39;d love to hear your own tips too, so feel free to leave a comment. Also, join the Snowflake Mastery group for more tips and insights on Snowflake.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflake&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768\"&gt;#snowflake&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=costoptimization&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768\"&gt;#costoptimization&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=performanceimprovement&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768\"&gt;#performanceimprovement&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datawarehousing&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768\"&gt;#datawarehousing&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=queryoptimization&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768\"&gt;#queryoptimization&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflakemastery&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7040667008199200768\"&gt;#snowflakemastery&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5ZI7oL3JTPPt59G0vTfOaQMHvka17QCAdFnF87leUeA.jpg?auto=webp&amp;v=enabled&amp;s=751b05e77b1c50dfc8477f4c599cb33affc7e2fc", "width": 64, "height": 64}, "resolutions": [], "variants": {}, "id": "QqSY3F9i2BgB-OdT_JpQr1vBqr2oq4spYNzkghHXwCM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11ufzi4", "is_robot_indexable": true, "report_reasons": null, "author": "mina_eros", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ufzi4/how_i_cut_my_clients_snowflake_cost_by_65_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ufzi4/how_i_cut_my_clients_snowflake_cost_by_65_and/", "subreddit_subscribers": 93425, "created_utc": 1679119219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In hive it is preferred that\n\n    SELECT * FROM smaller_table  s  JOIN bigger_table b ON  s.id = b.id  \n\nas it is much faster. Is this the same for SparkSQL?", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does order of table matter in Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11udgmw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679111034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In hive it is preferred that&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT * FROM smaller_table  s  JOIN bigger_table b ON  s.id = b.id  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;as it is much faster. Is this the same for SparkSQL?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11udgmw", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11udgmw/does_order_of_table_matter_in_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11udgmw/does_order_of_table_matter_in_spark/", "subreddit_subscribers": 93425, "created_utc": 1679111034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering! Charles here from [Prequel](https://prequel.co). We just launched the ability for companies to import data from their customer\u2019s data warehouse or database, and we wanted to share a little bit more about it with the community.\n\nIf you just want to see how it works, here is [a demo of the product we recorded.](https://www.loom.com/share/4724fb62583e41a9ba1a636fc8ea92f1)\n\n# Quick background on us:\n\nWe help companies integrate with their customer\u2019s data warehouse or database. We\u2019ve been busy helping companies export data to their customers \u2013 we\u2019re currently syncing over 40bn rows per month on behalf of companies. But folks kept on asking us if we could help them import data from their customers too. They wanted the ability to offer a 1st-party reverse ETL to their customers, similar to the 1st-party ETL capability we already helped them offer. So we built that product, and here we are.\n\n# Why would people want to import data? \n\nThere are actually plenty of use-cases here. Imagine a usage-based billing company that needs to get a daily pull from its customers of all the billing events that happened, so that they can generate relevant invoices. Or a fraud detection company who needs to get the latest transaction data from its customers so it can appropriately mark fraudulent ones.\n\nThere\u2019s no great way to import customer data currently. Typically, people solve this one of two ways today. One is they import data via CSV. This works well enough, but it requires ongoing work on the part of the customer: they need to put a CSV together, and upload it to the right place on a daily/weekly/monthly basis. This is painful and time-consuming, especially for data that needs to be continuously imported. Another one is companies make the customer write custom code to feed data to their API. This requires the customer to do a bunch of solutions engineering work just to get started using the product \u2013 which is a suboptimal onboarding experience.\n\nSo instead, we let the customer connect their database or data warehouse and we pull data directly from there, on an ongoing basis. They select which tables to import (and potentially map some columns to required fields), and that\u2019s it. The setup for your customer only takes 5 minutes, and requires no ongoing work. We feel like that\u2019s the kind of experience every company should provide when onboarding a new customer.\n\n# How do we do it?\n\nImporting all this data continuously is non-trivial, but thankfully we can actually reuse 95% of the infrastructure we built for data exports. It turns out our core transfer logic remains pretty much exactly the same, and all we had to do was ship new CRUD endpoints in our API layer to let users configure their source/destination. For those interested in our stack, we run a GoLang backend and Typescript/React frontend on k8s. We are also huge fans of [DuckDB](https://duckdb.org/) and use it heavily.\n\nIn terms of technical design, the most challenging decisions we have to make are around making database\u2019s type-systems play nicely with each other (kind of an evergreen problem really). For imports, we allow the data recipient to specify whether they want to receive this data as JSON blob, or as a nicely typed table. If they choose the latter, they specify exactly which columns they\u2019re expecting, as well as what type guarantees those should uphold. We\u2019re also working on the ability to feed that data directly into an API endpoint, and adding post-ingestion validation logic.\n\nWe know that security and privacy are paramount here. We're SOC 2 Type II certified, and we go through annual white-box pentests to make sure that all our code is up to snuff. We never store any of the data anywhere on our servers. Finally, we offer on-prem deployments, so data never even has to touch our servers if our customers don't want it to.\n\nWe\u2019re really stoked to be sharing this with the community. We\u2019ll be hanging out here for most of the day, but you can also reach us at hn (at) prequel.co if you have any questions!", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We just launched the ability for companies to import data from their customers' data warehouse or database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tta5t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679064618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;! Charles here from &lt;a href=\"https://prequel.co\"&gt;Prequel&lt;/a&gt;. We just launched the ability for companies to import data from their customer\u2019s data warehouse or database, and we wanted to share a little bit more about it with the community.&lt;/p&gt;\n\n&lt;p&gt;If you just want to see how it works, here is &lt;a href=\"https://www.loom.com/share/4724fb62583e41a9ba1a636fc8ea92f1\"&gt;a demo of the product we recorded.&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Quick background on us:&lt;/h1&gt;\n\n&lt;p&gt;We help companies integrate with their customer\u2019s data warehouse or database. We\u2019ve been busy helping companies export data to their customers \u2013 we\u2019re currently syncing over 40bn rows per month on behalf of companies. But folks kept on asking us if we could help them import data from their customers too. They wanted the ability to offer a 1st-party reverse ETL to their customers, similar to the 1st-party ETL capability we already helped them offer. So we built that product, and here we are.&lt;/p&gt;\n\n&lt;h1&gt;Why would people want to import data?&lt;/h1&gt;\n\n&lt;p&gt;There are actually plenty of use-cases here. Imagine a usage-based billing company that needs to get a daily pull from its customers of all the billing events that happened, so that they can generate relevant invoices. Or a fraud detection company who needs to get the latest transaction data from its customers so it can appropriately mark fraudulent ones.&lt;/p&gt;\n\n&lt;p&gt;There\u2019s no great way to import customer data currently. Typically, people solve this one of two ways today. One is they import data via CSV. This works well enough, but it requires ongoing work on the part of the customer: they need to put a CSV together, and upload it to the right place on a daily/weekly/monthly basis. This is painful and time-consuming, especially for data that needs to be continuously imported. Another one is companies make the customer write custom code to feed data to their API. This requires the customer to do a bunch of solutions engineering work just to get started using the product \u2013 which is a suboptimal onboarding experience.&lt;/p&gt;\n\n&lt;p&gt;So instead, we let the customer connect their database or data warehouse and we pull data directly from there, on an ongoing basis. They select which tables to import (and potentially map some columns to required fields), and that\u2019s it. The setup for your customer only takes 5 minutes, and requires no ongoing work. We feel like that\u2019s the kind of experience every company should provide when onboarding a new customer.&lt;/p&gt;\n\n&lt;h1&gt;How do we do it?&lt;/h1&gt;\n\n&lt;p&gt;Importing all this data continuously is non-trivial, but thankfully we can actually reuse 95% of the infrastructure we built for data exports. It turns out our core transfer logic remains pretty much exactly the same, and all we had to do was ship new CRUD endpoints in our API layer to let users configure their source/destination. For those interested in our stack, we run a GoLang backend and Typescript/React frontend on k8s. We are also huge fans of &lt;a href=\"https://duckdb.org/\"&gt;DuckDB&lt;/a&gt; and use it heavily.&lt;/p&gt;\n\n&lt;p&gt;In terms of technical design, the most challenging decisions we have to make are around making database\u2019s type-systems play nicely with each other (kind of an evergreen problem really). For imports, we allow the data recipient to specify whether they want to receive this data as JSON blob, or as a nicely typed table. If they choose the latter, they specify exactly which columns they\u2019re expecting, as well as what type guarantees those should uphold. We\u2019re also working on the ability to feed that data directly into an API endpoint, and adding post-ingestion validation logic.&lt;/p&gt;\n\n&lt;p&gt;We know that security and privacy are paramount here. We&amp;#39;re SOC 2 Type II certified, and we go through annual white-box pentests to make sure that all our code is up to snuff. We never store any of the data anywhere on our servers. Finally, we offer on-prem deployments, so data never even has to touch our servers if our customers don&amp;#39;t want it to.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re really stoked to be sharing this with the community. We\u2019ll be hanging out here for most of the day, but you can also reach us at hn (at) prequel.co if you have any questions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?auto=webp&amp;v=enabled&amp;s=d5b56dad1f1822b9f19af65e2aa45cd5ff12f52a", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7af403957eba2378208dd9fdfa22dcd36dfaa79a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd19aef9f45b31f083cf1c61c066542a12deebc2", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d956de44bd557e0dfd53c6fdbd68f2174847634", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=993eda00924d64b8d49e3f3b9c66855280149f1d", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d85dcbd3a24ee7ce05e9687e1431e70555a4e42", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/vpwbZ8qa3aeE3dzVM6sSX6NzOIv72mlgsaQ-i_T-uLo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c46cfcd352042bd90a6f21262a7edf2d06553f1f", "width": 1080, "height": 564}], "variants": {}, "id": "fzZ14hH15ZjeFPargglIyO_mdUAjmMfgFHhDZlS4gp0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tta5t", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11tta5t/we_just_launched_the_ability_for_companies_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tta5t/we_just_launched_the_ability_for_companies_to/", "subreddit_subscribers": 93425, "created_utc": 1679064618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If data lineage is the answer, what is the question?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "name": "t3_11tqb21", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_TItfra7pyByofbcpBLC06ZOlCYe2J0y526nFSSnIWw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679057700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/alvin-ai/if-data-lineage-is-the-answer-what-is-the-question-bad7f5f44fb5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?auto=webp&amp;v=enabled&amp;s=d27bfd253e22b985287197b107b744b6550b031c", "width": 1200, "height": 696}, "resolutions": [{"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc01f40a9c10113f1d59d4d24e9deacf656416f2", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc62a9d964b4d98d461bbf6480e9824f39a2c665", "width": 216, "height": 125}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cd3d26376ad33414283bbe83b7ff8db5970c4ab", "width": 320, "height": 185}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02916f59b8dbfdeb7a82aad9b880b28b96c35da7", "width": 640, "height": 371}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=849d6b7a9ecbcc3fece2986d2781d3fdd6ba49f9", "width": 960, "height": 556}, {"url": "https://external-preview.redd.it/d-hTghJe7md5AnLYqgKwHqiu3De_KfO7u8VlcA8aVDE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34c1fe8d94099f6cc31ef32dbd91d34a8cc26926", "width": 1080, "height": 626}], "variants": {}, "id": "_L6DDNGNDIlJSC0W9Hx_1Lonz-C-2itmin-V2GuRmtU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11tqb21", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tqb21/if_data_lineage_is_the_answer_what_is_the_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/alvin-ai/if-data-lineage-is-the-answer-what-is-the-question-bad7f5f44fb5", "subreddit_subscribers": 93425, "created_utc": 1679057700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What tech conferences do you look forward to in 2023.\n\nMyself - AWS Reinvent, Databricks summit", "author_fullname": "t2_j1zb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tech conferences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tnl1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679050001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What tech conferences do you look forward to in 2023.&lt;/p&gt;\n\n&lt;p&gt;Myself - AWS Reinvent, Databricks summit&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tnl1r", "is_robot_indexable": true, "report_reasons": null, "author": "abhi5025", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tnl1r/tech_conferences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tnl1r/tech_conferences/", "subreddit_subscribers": 93425, "created_utc": 1679050001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nThere are a lot of ways to pass credentials to boto3 for aws.  I'm curious what's the most secure one to use? What do you guys use in production?\n\nThanks!", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to pass credentials to boto3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tuflm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679067129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;There are a lot of ways to pass credentials to boto3 for aws.  I&amp;#39;m curious what&amp;#39;s the most secure one to use? What do you guys use in production?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tuflm", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tuflm/best_way_to_pass_credentials_to_boto3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tuflm/best_way_to_pass_credentials_to_boto3/", "subreddit_subscribers": 93425, "created_utc": 1679067129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started searching for a new job as a junior/mid career data engineer about 2 weeks ago. Posting my results so far in case it's helpful for anyone. \n\n* 75 Applications\n* 20 rejections(mostly from senior roles that I wasn't qualified for)\n* 7 Interviews\n\nI also opened my Linkedin profile to actively searching, but at least 90% of messages are not really worthwhile opportunities. I feel like managing the recruiter emails can be time consuming. \n\nBase salary ranges between 120 -140k. \n\nI'm finding that most of the companies that were \"cool\" high growth tech companies that reached +1b revenue aren't hiring. I sent out some feeler applications back in October 2022, and got a lot more interest from these types of companies(Square, Zendesk, etc.) Most of the interviews I've gotten are either for Series A - C startups with less than 250 employees, or large corps that are in stagnant industries. \n\nCurious to hear other people's experiences. Or if any of you more experienced data engineers are holding off from moving jobs in this current job market.", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Search Statistics After 75 Applications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tszr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679064095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started searching for a new job as a junior/mid career data engineer about 2 weeks ago. Posting my results so far in case it&amp;#39;s helpful for anyone. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;75 Applications&lt;/li&gt;\n&lt;li&gt;20 rejections(mostly from senior roles that I wasn&amp;#39;t qualified for)&lt;/li&gt;\n&lt;li&gt;7 Interviews&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I also opened my Linkedin profile to actively searching, but at least 90% of messages are not really worthwhile opportunities. I feel like managing the recruiter emails can be time consuming. &lt;/p&gt;\n\n&lt;p&gt;Base salary ranges between 120 -140k. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m finding that most of the companies that were &amp;quot;cool&amp;quot; high growth tech companies that reached +1b revenue aren&amp;#39;t hiring. I sent out some feeler applications back in October 2022, and got a lot more interest from these types of companies(Square, Zendesk, etc.) Most of the interviews I&amp;#39;ve gotten are either for Series A - C startups with less than 250 employees, or large corps that are in stagnant industries. &lt;/p&gt;\n\n&lt;p&gt;Curious to hear other people&amp;#39;s experiences. Or if any of you more experienced data engineers are holding off from moving jobs in this current job market.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tszr6", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11tszr6/job_search_statistics_after_75_applications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tszr6/job_search_statistics_after_75_applications/", "subreddit_subscribers": 93425, "created_utc": 1679064095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently, I am working as a DE in Canada, utilizing ADF, snowflake, python and Databricks.\n\nI have heard that many DEs in Canada work remotely for US companies, and I am keen to join their ranks. \nHowever, I am having a hard time finding a way to get there since I don\u2019t have the proper visa.\n\nWhile I understand that the job market may be a slow now, I\u2019m eager to prepare myself for future opportunities .\n\nHence, I would be grateful if you could share your journey to be able to work remotely in the US as a  DE from a different country.\n\nThank you!", "author_fullname": "t2_9fzxlzm8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am very interested in learning about your experiences.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u0l82", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679080193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently, I am working as a DE in Canada, utilizing ADF, snowflake, python and Databricks.&lt;/p&gt;\n\n&lt;p&gt;I have heard that many DEs in Canada work remotely for US companies, and I am keen to join their ranks. \nHowever, I am having a hard time finding a way to get there since I don\u2019t have the proper visa.&lt;/p&gt;\n\n&lt;p&gt;While I understand that the job market may be a slow now, I\u2019m eager to prepare myself for future opportunities .&lt;/p&gt;\n\n&lt;p&gt;Hence, I would be grateful if you could share your journey to be able to work remotely in the US as a  DE from a different country.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11u0l82", "is_robot_indexable": true, "report_reasons": null, "author": "kitkatsareyummy15", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11u0l82/i_am_very_interested_in_learning_about_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11u0l82/i_am_very_interested_in_learning_about_your/", "subreddit_subscribers": 93425, "created_utc": 1679080193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_975og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing with dbt-expectations and how to avoid alert fatigue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11trc1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qOk7ZR-1fAoom1qSxB83YO7rqs7eOwFyKfzzrUhz1tQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679060203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datafold.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datafold.com/blog/dbt-expectations", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?auto=webp&amp;v=enabled&amp;s=c49849243878ba462683b0b4d4dee06adb9af3b9", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed8a1748063d4fffc583fd73b8adfde3609e259d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4075abbdcf21588a6af720cfdae720cc4500e41f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ce6dd9e92a77a87ea1c1dd544deddc952437fda", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=993310536b4b2afc57f0a5c450aa2dc2a07244b7", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f4942e3eac2eaf16aab802c8dc889bb0b7290a7", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/daxSaWz1nmUooVPkK1wjArVFx9EU9XrLqOkBoljiM0k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e60326815512cc375e694a12638e80e4d922539", "width": 1080, "height": 607}], "variants": {}, "id": "76ppABcWi_cIcQgXA-2DergYkIbUsDWRlR1HHq_SJek"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11trc1j", "is_robot_indexable": true, "report_reasons": null, "author": "arimbr", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11trc1j/testing_with_dbtexpectations_and_how_to_avoid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datafold.com/blog/dbt-expectations", "subreddit_subscribers": 93425, "created_utc": 1679060203.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On average, what type of work consumes most of your time in your job as an engineer? Feel free to note if you feel you spend too much/too little on any given category.\n\n[View Poll](https://www.reddit.com/poll/11ubqwc)", "author_fullname": "t2_kdj5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you spend the majority of your time doing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ubqwc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679105970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On average, what type of work consumes most of your time in your job as an engineer? Feel free to note if you feel you spend too much/too little on any given category.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/11ubqwc\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11ubqwc", "is_robot_indexable": true, "report_reasons": null, "author": "Touvejs", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1679710770176, "options": [{"text": "ETL Development", "id": "22117207"}, {"text": "Infrastructure as Code Development", "id": "22117208"}, {"text": "Testing/bug fixes", "id": "22117209"}, {"text": "Designing/implementing Data Models or Database/Warehouse/Lake/Mart/Vault/etc", "id": "22117210"}, {"text": "Requirements Gathering/Stakeholder meetings", "id": "22117211"}, {"text": "Other (Add in Comments)", "id": "22117212"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 329, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ubqwc/what_do_you_spend_the_majority_of_your_time_doing/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/11ubqwc/what_do_you_spend_the_majority_of_your_time_doing/", "subreddit_subscribers": 93425, "created_utc": 1679105970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How many data warehouse architectures are there, and is there one that stands out with a good balance between auditability, performance, ease of use, with a preference for designs that can be built over time in an agile team?\n\nBy architecture, I should clarify I\u2019m not referring to medallion architecture or similar, but rather the design of tables and their relationships.\n\nExamples:\n- Inmon DWH\n- Kimball Star Schema\n- Linstead Data Vault (and Data Vault 2.0)\n- Ensemble Modelling\n- Anchor Modelling \n- OBT\n\nWhat others are there out there, and how do they rate?\n\nWe have our own approach, but it\u2019s showing issues, so want to survey the landscape and workout what direction we should head.\n\nEdit: so others don\u2019t go down the same path as us, this is our process:\n\n1. We land data into raw databases in Snowflake, then immediately snapshot them with dbt. This gives us static history. \n2. We bring through to our base schema a 1:1 table relationship to the raw table, with PII redacted and field names changed to \u2018business naming convention\u2019. \n3. We then build \u2018object\u2019 tables. These are source agnostic, include joins between sources, business rules etc. and importantly handle the time element, aligning multiple snapshots based on their valid to/valid from to get the full history of the \u2018object\u2019, be that a customer table, an application table, a team member table. This gives us a business view that ai would describe as an independent of source \u2018transactional table\u2019 with full type 2 history. If two systems that have been blended into one table have the same field, we coalesce the data with preference to what we believe should be the source system that is the primary source for that data.\n4. We build fact and dimension tables on top of our objects.\n\nThe problems are:\n- we probably shouldn\u2019t have changed field names to business names so eagerly, but rather left it until later in the process.\n- the object tables take time to calculate and build. We do a full rebuild each night because the different systems data updates at differing frequencies, and it\u2019s easier to truncate and reload than to do incremental.\n- the data is tightly coupled, so a failure in one upstream dataset can lead to a catastrophic failure of the whole load. Example: lead table in source fails to refresh, and it\u2019s joined to our customer object, which then gets skipped by dbt, which is used in our dim_customer, which is skipped, which basically links to every fact table, which then also get skipped. Not ideal!", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouse Architectures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uhfv8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679127226.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679124393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How many data warehouse architectures are there, and is there one that stands out with a good balance between auditability, performance, ease of use, with a preference for designs that can be built over time in an agile team?&lt;/p&gt;\n\n&lt;p&gt;By architecture, I should clarify I\u2019m not referring to medallion architecture or similar, but rather the design of tables and their relationships.&lt;/p&gt;\n\n&lt;p&gt;Examples:\n- Inmon DWH\n- Kimball Star Schema\n- Linstead Data Vault (and Data Vault 2.0)\n- Ensemble Modelling\n- Anchor Modelling \n- OBT&lt;/p&gt;\n\n&lt;p&gt;What others are there out there, and how do they rate?&lt;/p&gt;\n\n&lt;p&gt;We have our own approach, but it\u2019s showing issues, so want to survey the landscape and workout what direction we should head.&lt;/p&gt;\n\n&lt;p&gt;Edit: so others don\u2019t go down the same path as us, this is our process:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We land data into raw databases in Snowflake, then immediately snapshot them with dbt. This gives us static history. &lt;/li&gt;\n&lt;li&gt;We bring through to our base schema a 1:1 table relationship to the raw table, with PII redacted and field names changed to \u2018business naming convention\u2019. &lt;/li&gt;\n&lt;li&gt;We then build \u2018object\u2019 tables. These are source agnostic, include joins between sources, business rules etc. and importantly handle the time element, aligning multiple snapshots based on their valid to/valid from to get the full history of the \u2018object\u2019, be that a customer table, an application table, a team member table. This gives us a business view that ai would describe as an independent of source \u2018transactional table\u2019 with full type 2 history. If two systems that have been blended into one table have the same field, we coalesce the data with preference to what we believe should be the source system that is the primary source for that data.&lt;/li&gt;\n&lt;li&gt;We build fact and dimension tables on top of our objects.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The problems are:\n- we probably shouldn\u2019t have changed field names to business names so eagerly, but rather left it until later in the process.\n- the object tables take time to calculate and build. We do a full rebuild each night because the different systems data updates at differing frequencies, and it\u2019s easier to truncate and reload than to do incremental.\n- the data is tightly coupled, so a failure in one upstream dataset can lead to a catastrophic failure of the whole load. Example: lead table in source fails to refresh, and it\u2019s joined to our customer object, which then gets skipped by dbt, which is used in our dim_customer, which is skipped, which basically links to every fact table, which then also get skipped. Not ideal!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11uhfv8", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11uhfv8/data_warehouse_architectures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11uhfv8/data_warehouse_architectures/", "subreddit_subscribers": 93425, "created_utc": 1679124393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone running Starburst over their data lake (Delta lake file format generally) as the general query engine? If yes, just wanted a general idea of whether it's a good idea to do so? Is the performance is upto par? Is the cost is manageable?", "author_fullname": "t2_7fd7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake + Starburst patterns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uezcw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679115819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone running Starburst over their data lake (Delta lake file format generally) as the general query engine? If yes, just wanted a general idea of whether it&amp;#39;s a good idea to do so? Is the performance is upto par? Is the cost is manageable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11uezcw", "is_robot_indexable": true, "report_reasons": null, "author": "counterstruck", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11uezcw/delta_lake_starburst_patterns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11uezcw/delta_lake_starburst_patterns/", "subreddit_subscribers": 93425, "created_utc": 1679115819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that this might be better suited in some other subreddits, but since data engineers usually are close to the infrastructure, I was curious if anyone is running Nomad or has moved to it?", "author_fullname": "t2_73sd7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using Nomad over Kubernetes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u54m2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679090142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that this might be better suited in some other subreddits, but since data engineers usually are close to the infrastructure, I was curious if anyone is running Nomad or has moved to it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11u54m2", "is_robot_indexable": true, "report_reasons": null, "author": "intellidumb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11u54m2/anyone_using_nomad_over_kubernetes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11u54m2/anyone_using_nomad_over_kubernetes/", "subreddit_subscribers": 93425, "created_utc": 1679090142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have captured some change data from a MySql database, serialized it with Avro converter and stored it in a Kafka topic. Now I am trying to read back that Avro serialized data from Kafka. I can easily get the serialized data but now I need to deserialize it. How do I do that? \n\nNote: I am using Pyspark to write my spark code. And I am creating a readingStream to stream the data in real-time from the Kafka topic. \n\nI found some tools for Spark(scala libraries) but the same solution for Pyspark is a bit complicated.\n\nI think using Abris [https://github.com/AbsaOSS/ABRiS](https://github.com/AbsaOSS/ABRiS) is a good choice as it supports for the python version of spark but I cannot seem to properly understand this scala written library's integration in Pyspark(Python). It's a bit complicated as I have not used non-python written libraries in Python. \n\nDoes someone have another better alternative for this use case? Another tool maybe. Or can anybody please make me understand this Abris integration for Pyspark. Thank you! [https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md](https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md)", "author_fullname": "t2_asyk1tug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tool do I use to serialize/deserialize Avro messages stored in a Kafka topic with schema registered in the schema-registry using Pyspark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tp0si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679054328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have captured some change data from a MySql database, serialized it with Avro converter and stored it in a Kafka topic. Now I am trying to read back that Avro serialized data from Kafka. I can easily get the serialized data but now I need to deserialize it. How do I do that? &lt;/p&gt;\n\n&lt;p&gt;Note: I am using Pyspark to write my spark code. And I am creating a readingStream to stream the data in real-time from the Kafka topic. &lt;/p&gt;\n\n&lt;p&gt;I found some tools for Spark(scala libraries) but the same solution for Pyspark is a bit complicated.&lt;/p&gt;\n\n&lt;p&gt;I think using Abris &lt;a href=\"https://github.com/AbsaOSS/ABRiS\"&gt;https://github.com/AbsaOSS/ABRiS&lt;/a&gt; is a good choice as it supports for the python version of spark but I cannot seem to properly understand this scala written library&amp;#39;s integration in Pyspark(Python). It&amp;#39;s a bit complicated as I have not used non-python written libraries in Python. &lt;/p&gt;\n\n&lt;p&gt;Does someone have another better alternative for this use case? Another tool maybe. Or can anybody please make me understand this Abris integration for Pyspark. Thank you! &lt;a href=\"https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md\"&gt;https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?auto=webp&amp;v=enabled&amp;s=387171c784b9c3f45187c8d9f24b3674cbc1bb52", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1207a3960e5e55a82a8218b1b8b8abad78f49d69", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b6ac0dfd8920ccd177bef60a16308a959182d86", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d1dfd547b9ef352820ceb8894e689487d85430a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06f4b989a61c0df8e9a7938d2f45578cda6d7b10", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54d89f9fd5feabc2bb26800df2db77ff757a8d2d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4C5f7ZgwrgPqVmaoX8oRrknGtbUDYXR1K7mPoe_JQ7c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa6dbe4194ff44d5c41148ee6bba5f28a9249f59", "width": 1080, "height": 540}], "variants": {}, "id": "HE64KdT2pM2hXXrwojeHuMQKclceURZ0rGPldjhg8rE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11tp0si", "is_robot_indexable": true, "report_reasons": null, "author": "unixparadox", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tp0si/what_tool_do_i_use_to_serializedeserialize_avro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tp0si/what_tool_do_i_use_to_serializedeserialize_avro/", "subreddit_subscribers": 93425, "created_utc": 1679054328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there\n\nI'm currently working on problem - I have a zoo of orchestration tools, like airflow, dbt etc\n\nand this tool need to synchronise in some cases - triggers, chain of control.\n\n&amp;#x200B;\n\nI think about Kafka with topics, dedicated to tools, or some kind of pg instance with events\n\nDoes anyone solved problem like this?", "author_fullname": "t2_89j762yi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration of orchestrators", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tmcjp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679046020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on problem - I have a zoo of orchestration tools, like airflow, dbt etc&lt;/p&gt;\n\n&lt;p&gt;and this tool need to synchronise in some cases - triggers, chain of control.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I think about Kafka with topics, dedicated to tools, or some kind of pg instance with events&lt;/p&gt;\n\n&lt;p&gt;Does anyone solved problem like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tmcjp", "is_robot_indexable": true, "report_reasons": null, "author": "tehdima", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tmcjp/orchestration_of_orchestrators/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tmcjp/orchestration_of_orchestrators/", "subreddit_subscribers": 93425, "created_utc": 1679046020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title suggest I\u2019d love to know what books you felt made you become a better data / software engineer. Ones that helped you either advance your career or changed the way you thought about data / programming in general.", "author_fullname": "t2_8jkeh00n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Books that made you become a better engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11uiemx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679128038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title suggest I\u2019d love to know what books you felt made you become a better data / software engineer. Ones that helped you either advance your career or changed the way you thought about data / programming in general.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11uiemx", "is_robot_indexable": true, "report_reasons": null, "author": "GC_invests", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11uiemx/books_that_made_you_become_a_better_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11uiemx/books_that_made_you_become_a_better_engineer/", "subreddit_subscribers": 93425, "created_utc": 1679128038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have an IoT device that feeds data into a MySQL table. The most relevant columns in this table are:\n\n- The `value` column, which is a float column that just stores the quantitative measurement of interest.\n- The `datetime_created` column, which is a datetime field that stores the datetime this record was generated.\n\nWe have an application that queries this table, passing in the following 2 parameters, `date_start` and `date_end`.\n\nBoth of these parameters are dates. We then filter our table, and run aggregations on the filtered dataset including:\n\n1. Aggregating by hour, \n2. Running some statistical computation to calculate min/max/median within each hour, \n3. And after the above, we return the data to our frontend layer.\n\nLately, as this table has grown, we've found that queries take upwards of 30-40 seconds when the date range selected is huge.\n\nAre there ways we could be optimizing this at either the database layer or the application layer? Is our chosen method of storing this in a MySQL db just not optimized for our case?", "author_fullname": "t2_k01zajj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improving query times from a MySQL table with a datetime column that stores a time stream of occurrences about 2-3 minutes apart.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ue8ci", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679113429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have an IoT device that feeds data into a MySQL table. The most relevant columns in this table are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The &lt;code&gt;value&lt;/code&gt; column, which is a float column that just stores the quantitative measurement of interest.&lt;/li&gt;\n&lt;li&gt;The &lt;code&gt;datetime_created&lt;/code&gt; column, which is a datetime field that stores the datetime this record was generated.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We have an application that queries this table, passing in the following 2 parameters, &lt;code&gt;date_start&lt;/code&gt; and &lt;code&gt;date_end&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;Both of these parameters are dates. We then filter our table, and run aggregations on the filtered dataset including:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Aggregating by hour, &lt;/li&gt;\n&lt;li&gt;Running some statistical computation to calculate min/max/median within each hour, &lt;/li&gt;\n&lt;li&gt;And after the above, we return the data to our frontend layer.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Lately, as this table has grown, we&amp;#39;ve found that queries take upwards of 30-40 seconds when the date range selected is huge.&lt;/p&gt;\n\n&lt;p&gt;Are there ways we could be optimizing this at either the database layer or the application layer? Is our chosen method of storing this in a MySQL db just not optimized for our case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11ue8ci", "is_robot_indexable": true, "report_reasons": null, "author": "Lostwhispers05", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ue8ci/improving_query_times_from_a_mysql_table_with_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ue8ci/improving_query_times_from_a_mysql_table_with_a/", "subreddit_subscribers": 93425, "created_utc": 1679113429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m using Matillion ETL and snowflake. I have various data sources ljke DBs and APIs and files owned by other teams within the business.\n\nI have a raw data layer in snowflake where all the source data lands from matillion, then layers on top of that (cleansed, modelled, presentation layer).\n\nCurrent strategy is to merge/upsert incoming data into the raw layer so it only holds the latest version of each record when source data gets updated.\n\nIs that a typical pattern? Would it be better to store each version of every record in raw, then filter them out downstream to only select the latest record? And how and when do you apply that filter?", "author_fullname": "t2_4y2g3lh3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "History in the Raw Data Layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tws9j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679072220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m using Matillion ETL and snowflake. I have various data sources ljke DBs and APIs and files owned by other teams within the business.&lt;/p&gt;\n\n&lt;p&gt;I have a raw data layer in snowflake where all the source data lands from matillion, then layers on top of that (cleansed, modelled, presentation layer).&lt;/p&gt;\n\n&lt;p&gt;Current strategy is to merge/upsert incoming data into the raw layer so it only holds the latest version of each record when source data gets updated.&lt;/p&gt;\n\n&lt;p&gt;Is that a typical pattern? Would it be better to store each version of every record in raw, then filter them out downstream to only select the latest record? And how and when do you apply that filter?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tws9j", "is_robot_indexable": true, "report_reasons": null, "author": "Right-Requirement895", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tws9j/history_in_the_raw_data_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tws9j/history_in_the_raw_data_layer/", "subreddit_subscribers": 93425, "created_utc": 1679072220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am reading messages from Kafka and processing it in pyspark.\n\n In some of the messages, there are some keys corresponding to which there are 0's and 1's. \n\nNow I want to convert these 0's and 1's to False and True. The challenge here I should only convert those columns for whose the datatype in the schema in boolean. There is no way of knowing which columns may have the datatype as BooleanType since this is streaming job and new columns are added everyday.\n\nThe main challenge here is, I need to do the conversion even before parsing the Kafka Message because as soon as I parse those messages using a schema, then column values become NULL (bcoz the columns have 0's and 1's and when parsed using BooleanType, they become NULL)\n\nCan anyone help me or point me in the right direction?", "author_fullname": "t2_188qz428", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Convert some specific columns that have 0 and 1 values in Kafka messages to False and True in PySpark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tvzd9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679070506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reading messages from Kafka and processing it in pyspark.&lt;/p&gt;\n\n&lt;p&gt;In some of the messages, there are some keys corresponding to which there are 0&amp;#39;s and 1&amp;#39;s. &lt;/p&gt;\n\n&lt;p&gt;Now I want to convert these 0&amp;#39;s and 1&amp;#39;s to False and True. The challenge here I should only convert those columns for whose the datatype in the schema in boolean. There is no way of knowing which columns may have the datatype as BooleanType since this is streaming job and new columns are added everyday.&lt;/p&gt;\n\n&lt;p&gt;The main challenge here is, I need to do the conversion even before parsing the Kafka Message because as soon as I parse those messages using a schema, then column values become NULL (bcoz the columns have 0&amp;#39;s and 1&amp;#39;s and when parsed using BooleanType, they become NULL)&lt;/p&gt;\n\n&lt;p&gt;Can anyone help me or point me in the right direction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tvzd9", "is_robot_indexable": true, "report_reasons": null, "author": "swarup_i_am", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tvzd9/convert_some_specific_columns_that_have_0_and_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tvzd9/convert_some_specific_columns_that_have_0_and_1/", "subreddit_subscribers": 93425, "created_utc": 1679070506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_rx4wrtdi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6 Effective Tips to Hire Data Engineers Remotely: Best Practices for Data Engineer Interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": false, "name": "t3_11ttur3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/M8YcMpTR9W5derfEZRy8cozpAg3f7L-jtUvckudcy1A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679065859.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "borderlessmind.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.borderlessmind.com/blog/6-effective-tips-hire-data-engineers-remotely-best-practices-data-engineer-interviews/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?auto=webp&amp;v=enabled&amp;s=e9c11445bf920365a113c07a7b54a4b61a939578", "width": 1200, "height": 763}, "resolutions": [{"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f30bae4bd9fbedbca0ea052606d035953b9439f", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13f976c64d9d547309a154e83b1d436cdcfb564e", "width": 216, "height": 137}, {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ce10e60a910542e594f58788d9e51245ba5784d", "width": 320, "height": 203}, {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49374afb293b8d303f1e9fcc3c7dae8482eff380", "width": 640, "height": 406}, {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8359c4e75539d929fe4a84ab029f5760e414830d", "width": 960, "height": 610}, {"url": "https://external-preview.redd.it/ZZeRFpQyjSqO1tzjkCQ9AVFWVuvdJqDoWh5bcO1lbn4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae0113b3ed683420a8485c1dee38b267fb825cf5", "width": 1080, "height": 686}], "variants": {}, "id": "1q-X_hWaphUhvnt06YINXEhG2ASNieB0KxNR6hHCl6c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11ttur3", "is_robot_indexable": true, "report_reasons": null, "author": "AccomplishedRice2031", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ttur3/6_effective_tips_to_hire_data_engineers_remotely/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.borderlessmind.com/blog/6-effective-tips-hire-data-engineers-remotely-best-practices-data-engineer-interviews/", "subreddit_subscribers": 93425, "created_utc": 1679065859.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am migrating R code from on-prem to Databricks. I learned this week that I can't use readRDS() of .rds object in S3 like reading a CSV. \n\nAny ideas? I could try to convert to a CSV but these don't have the same shape. \n\nCan I read from DBFS?", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can you read R .rds files in Databricks/Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11tsww1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679063967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am migrating R code from on-prem to Databricks. I learned this week that I can&amp;#39;t use readRDS() of .rds object in S3 like reading a CSV. &lt;/p&gt;\n\n&lt;p&gt;Any ideas? I could try to convert to a CSV but these don&amp;#39;t have the same shape. &lt;/p&gt;\n\n&lt;p&gt;Can I read from DBFS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11tsww1", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11tsww1/how_can_you_read_r_rds_files_in_databricksspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11tsww1/how_can_you_read_r_rds_files_in_databricksspark/", "subreddit_subscribers": 93425, "created_utc": 1679063967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need help. I am trying to migrate data from GCP BigQuery to AWS S3 by using Glue as the data integration tool. The Glue job will run daily and will only get the data of today and yesterday. However, there must be a better way. The problem with this approach is that this will introduce duplicates to the data, not to mention, additional processing time for the Glue job as well. This can easily be resolved by using DISTINCT by the data consumers. However, is there any means to retrieve only the deltas from the BigQuery table. Or is there a good approach to do this?", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BigQuery to S3 using Glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11trua2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679061432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help. I am trying to migrate data from GCP BigQuery to AWS S3 by using Glue as the data integration tool. The Glue job will run daily and will only get the data of today and yesterday. However, there must be a better way. The problem with this approach is that this will introduce duplicates to the data, not to mention, additional processing time for the Glue job as well. This can easily be resolved by using DISTINCT by the data consumers. However, is there any means to retrieve only the deltas from the BigQuery table. Or is there a good approach to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11trua2", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11trua2/bigquery_to_s3_using_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11trua2/bigquery_to_s3_using_glue/", "subreddit_subscribers": 93425, "created_utc": 1679061432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Interviewing at a company that still uses SSIS and OLAP cubes. Everything is still done in SQL, no python nowhere. Opinion?", "author_fullname": "t2_r7bmwiee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSIS &amp; OLAP Cubes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11uijzg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679128623.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interviewing at a company that still uses SSIS and OLAP cubes. Everything is still done in SQL, no python nowhere. Opinion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "11uijzg", "is_robot_indexable": true, "report_reasons": null, "author": "va1kyrja-kara", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11uijzg/ssis_olap_cubes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11uijzg/ssis_olap_cubes/", "subreddit_subscribers": 93425, "created_utc": 1679128623.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}