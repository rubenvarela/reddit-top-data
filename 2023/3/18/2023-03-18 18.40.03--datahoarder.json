{"kind": "Listing", "data": {"after": "t3_11ucyot", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_uegnp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this ticking sound normal? I fell like the drive is failing slowly.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11ukbsr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 156, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/mwufgnac4hoa1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/mwufgnac4hoa1/DASH_96.mp4", "dash_url": "https://v.redd.it/mwufgnac4hoa1/DASHPlaylist.mpd?a=1681756803%2CNGU4N2JjZDVkYTk3YmQ3Yjg0ODg4MmIyZGQ3ZjkwOThmOTMwM2E1Yjc4ZmQyMjg3NDM4MmVkMDUwYjk2OGM3Mg%3D%3D&amp;v=1&amp;f=sd", "duration": 16, "hls_url": "https://v.redd.it/mwufgnac4hoa1/HLSPlaylist.m3u8?a=1681756803%2CMmRmMzYzYTczNjliMDUxM2I3NzM4N2IzMjdhMjg1MzAwZTA4Y2JkYWQzNWRkZTE4ZWQzNzc2Mjk4ZmMxMjkzMw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 156, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EvrgGIMo7XEA2e_xYCC2GBum8KLuZH8G1OEx-HZ14qw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679134808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/mwufgnac4hoa1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5fce8a121d34a69c4ea8c99736a9d3a846f4d8e4", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=bfaebbd27a240e39ff44e2e5cb8d3c17178386dc", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d46a8071b805744234bd8b2057cebb60110890a8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d57fe217791b51b6293ee6d9ec2fe7957ed30e08", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=67d1dcebc5e1ce942bd46d0e32724575f1edeb94", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0fc5332ac68de049b31e0f893e3d8cfba097c16e", "width": 960, "height": 540}], "variants": {}, "id": "l6IRE2oY_8wu1Ho9CejFerWMIKwLTBqzDymm5csVmhA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ukbsr", "is_robot_indexable": true, "report_reasons": null, "author": "CrazyYAY", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ukbsr/is_this_ticking_sound_normal_i_fell_like_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/mwufgnac4hoa1", "subreddit_subscribers": 673606, "created_utc": 1679134808.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/mwufgnac4hoa1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/mwufgnac4hoa1/DASH_96.mp4", "dash_url": "https://v.redd.it/mwufgnac4hoa1/DASHPlaylist.mpd?a=1681756803%2CNGU4N2JjZDVkYTk3YmQ3Yjg0ODg4MmIyZGQ3ZjkwOThmOTMwM2E1Yjc4ZmQyMjg3NDM4MmVkMDUwYjk2OGM3Mg%3D%3D&amp;v=1&amp;f=sd", "duration": 16, "hls_url": "https://v.redd.it/mwufgnac4hoa1/HLSPlaylist.m3u8?a=1681756803%2CMmRmMzYzYTczNjliMDUxM2I3NzM4N2IzMjdhMjg1MzAwZTA4Y2JkYWQzNWRkZTE4ZWQzNzc2Mjk4ZmMxMjkzMw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Due to a lack of data management skills and being afraid of deleting the wrong files I have too many duplicate files, especially images now. (I rather have duplicates than data loss!) Google Files is working very well for finding duplicate images and comparing them on Android. Does anyone know a good and cheap or free software, maybe even open-source, for Windows that can be used for finding and comparing duplicate images in chosen folders? I found some but they look like from Windows 2000 and I want to use something modern.", "author_fullname": "t2_2m2bms7e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for finding duplicate images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u1xyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679083105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Due to a lack of data management skills and being afraid of deleting the wrong files I have too many duplicate files, especially images now. (I rather have duplicates than data loss!) Google Files is working very well for finding duplicate images and comparing them on Android. Does anyone know a good and cheap or free software, maybe even open-source, for Windows that can be used for finding and comparing duplicate images in chosen folders? I found some but they look like from Windows 2000 and I want to use something modern.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11u1xyd", "is_robot_indexable": true, "report_reasons": null, "author": "Rationale-Glum-Power", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u1xyd/software_for_finding_duplicate_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u1xyd/software_for_finding_duplicate_images/", "subreddit_subscribers": 673606, "created_utc": 1679083105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I've been working on this script all week. I literally thought it would take a few hours and it's consumed every hour of this past week. \n\nSo I've made a script in powershell that uses yt-dlp to download the latest youtube videos from your subscriptions, creates a playlist from all the files in the resulting folder, and creates a notification showing the names of the channels from the latest downloads. \n\nNote, all of this can be modified fairly straightforward.\n\n1. Create folder to hold everything. &lt;mainFolder&gt;\n\n2. create &lt;powershellScriptName&gt;.ps1, &lt;vbsScriptName&gt;.vbs in `mainFolder`\n\n3. make sure `mainFolder` also includes yt-dlp.exe, ffmpeg.exe, ffprobe.exe (not 100% sure the last one is necessary)\n\n4. fill `powershellSciptName` with [this pasteBin](https://pastebin.com/zFPU7f1M)\n\n4. fill `vbsScriptName` with [this pasteBin](https://pastebin.com/QsgQrXd9)\n\n5. fill in the previous 2 files using the direction below\n\n**PowerShell script:**\n\nReplace the following:\n\n`&lt;browser&gt;` - use the browser you have logged into youtube, or you can follow [this comment](https://www.reddit.com/r/youtubedl/comments/rq0zms/ytdlp_how_do_i_use_the_cookie_function/hqac3ic/)\n\n`&lt;destinationDirectory&gt;` - where you want the files to finally end up\n\n`&lt;downloadDirectory&gt;` - where to initially download the files to\n\nThe following are my own options, feel free to adjust as you like\n\n `--match-filter \"!is_live &amp; !post_live &amp; !was_live\" ` - doesn't download any live videos \n\n`notificationTitle` - Change to whatever you want the notification to say\n\n`-o \"$downloadDir\\[%(channel)s] - %(title)s.%(ext)s\" :ytsubs://user/` - this is how the files will be organized and names formatted. Feel free to adjust to your liking. yt-dlp's github will help if you need guidance\n\nmoving the items is not mandatory - I like to download first to my C drive, then move them all to my NAS. Since I run this every five minutes, it doesn't matter.\n\n**vbsScript**\n\nreplace the following:\n\n`&lt;pathToMainScript&gt;` - this will be the absolute path to your powershell script.\n\n\n#####**Automating the script** \nThis was fairly frustrating because the powershell window would popup every 5 minutes, even if you set window to hidden in the arguments. That's why you make the vbs script, as it will actually run silently\n\n1. open Task Scheduler\n2. click the arow to expand the `Task Scheduler Library` in the lefthand directory\n3. It's advisable to create your own folder for your own tasks if you haven't already. Select the Task Scheduler Library. select `Action &gt; New Folder...` from the menu bar. Name how you like.\n4. With your new folder selected, select `Create Task` from the Action pane on the right hand side.\n5. Name however you like\n6. Go to triggers tab. This will be where you select your preferred interval. To run every 5 minutes, I've created 3 triggers. one that runs daily at 12:00:00am, one that runs on startup, and one that runs when the task is altered. On each of these I have it set to run every 5 minutes.\n7. Go to the Actions tab. This will be where you call the vbs script, which in turn calls the powershell script.\n8. under program/script, enter the following: `C:\\Windows\\System32\\wscript.exe`\n9. under add arguments enter `\"&lt;pathToVBScript&gt;\"`\n10. under Start In enter: `&lt;pathToMainFolder&gt;`\n11. Go to the settings tab. check `Run task as soon as possible after a scheduled start is missed` select `Queue a new instance` for the bottom option: `If the task is already running, then the following rule applies`\n12. hit OK, then select Run from the Action pane.\n\n\nThat's it! There's some jank but like I said, I've already spent way too long on this. Hopefully this helps you out! \n\nA couple improvements I'd like to make eventually (very open to help here):\n\n*  click on the notification to open the playlist - should open automatically in the m3u associated player.\n*  better file organization\n* make a gui to make it easier to run, and potentially convert from windows task scheduler task to a daemon or service with option to adjust frequency of checks\n* any of your suggestions!\n\nI'm still really new to this, so I'm happy to hear any suggestions for improvements!", "author_fullname": "t2_37kkn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Auto download latest youtube videos from your subscriptions, with options and notification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ualom", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679120460.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679102795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;ve been working on this script all week. I literally thought it would take a few hours and it&amp;#39;s consumed every hour of this past week. &lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve made a script in powershell that uses yt-dlp to download the latest youtube videos from your subscriptions, creates a playlist from all the files in the resulting folder, and creates a notification showing the names of the channels from the latest downloads. &lt;/p&gt;\n\n&lt;p&gt;Note, all of this can be modified fairly straightforward.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Create folder to hold everything. &amp;lt;mainFolder&amp;gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;create &amp;lt;powershellScriptName&amp;gt;.ps1, &amp;lt;vbsScriptName&amp;gt;.vbs in &lt;code&gt;mainFolder&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;make sure &lt;code&gt;mainFolder&lt;/code&gt; also includes yt-dlp.exe, ffmpeg.exe, ffprobe.exe (not 100% sure the last one is necessary)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;fill &lt;code&gt;powershellSciptName&lt;/code&gt; with &lt;a href=\"https://pastebin.com/zFPU7f1M\"&gt;this pasteBin&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;fill &lt;code&gt;vbsScriptName&lt;/code&gt; with &lt;a href=\"https://pastebin.com/QsgQrXd9\"&gt;this pasteBin&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;fill in the previous 2 files using the direction below&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;PowerShell script:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Replace the following:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;browser&amp;gt;&lt;/code&gt; - use the browser you have logged into youtube, or you can follow &lt;a href=\"https://www.reddit.com/r/youtubedl/comments/rq0zms/ytdlp_how_do_i_use_the_cookie_function/hqac3ic/\"&gt;this comment&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;destinationDirectory&amp;gt;&lt;/code&gt; - where you want the files to finally end up&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;downloadDirectory&amp;gt;&lt;/code&gt; - where to initially download the files to&lt;/p&gt;\n\n&lt;p&gt;The following are my own options, feel free to adjust as you like&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--match-filter &amp;quot;!is_live &amp;amp; !post_live &amp;amp; !was_live&amp;quot;&lt;/code&gt; - doesn&amp;#39;t download any live videos &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;notificationTitle&lt;/code&gt; - Change to whatever you want the notification to say&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;-o &amp;quot;$downloadDir\\[%(channel)s] - %(title)s.%(ext)s&amp;quot; :ytsubs://user/&lt;/code&gt; - this is how the files will be organized and names formatted. Feel free to adjust to your liking. yt-dlp&amp;#39;s github will help if you need guidance&lt;/p&gt;\n\n&lt;p&gt;moving the items is not mandatory - I like to download first to my C drive, then move them all to my NAS. Since I run this every five minutes, it doesn&amp;#39;t matter.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;vbsScript&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;replace the following:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;pathToMainScript&amp;gt;&lt;/code&gt; - this will be the absolute path to your powershell script.&lt;/p&gt;\n\n&lt;h5&gt;&lt;strong&gt;Automating the script&lt;/strong&gt;&lt;/h5&gt;\n\n&lt;p&gt;This was fairly frustrating because the powershell window would popup every 5 minutes, even if you set window to hidden in the arguments. That&amp;#39;s why you make the vbs script, as it will actually run silently&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;open Task Scheduler&lt;/li&gt;\n&lt;li&gt;click the arow to expand the &lt;code&gt;Task Scheduler Library&lt;/code&gt; in the lefthand directory&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s advisable to create your own folder for your own tasks if you haven&amp;#39;t already. Select the Task Scheduler Library. select &lt;code&gt;Action &amp;gt; New Folder...&lt;/code&gt; from the menu bar. Name how you like.&lt;/li&gt;\n&lt;li&gt;With your new folder selected, select &lt;code&gt;Create Task&lt;/code&gt; from the Action pane on the right hand side.&lt;/li&gt;\n&lt;li&gt;Name however you like&lt;/li&gt;\n&lt;li&gt;Go to triggers tab. This will be where you select your preferred interval. To run every 5 minutes, I&amp;#39;ve created 3 triggers. one that runs daily at 12:00:00am, one that runs on startup, and one that runs when the task is altered. On each of these I have it set to run every 5 minutes.&lt;/li&gt;\n&lt;li&gt;Go to the Actions tab. This will be where you call the vbs script, which in turn calls the powershell script.&lt;/li&gt;\n&lt;li&gt;under program/script, enter the following: &lt;code&gt;C:\\Windows\\System32\\wscript.exe&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;under add arguments enter &lt;code&gt;&amp;quot;&amp;lt;pathToVBScript&amp;gt;&amp;quot;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;under Start In enter: &lt;code&gt;&amp;lt;pathToMainFolder&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Go to the settings tab. check &lt;code&gt;Run task as soon as possible after a scheduled start is missed&lt;/code&gt; select &lt;code&gt;Queue a new instance&lt;/code&gt; for the bottom option: &lt;code&gt;If the task is already running, then the following rule applies&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;hit OK, then select Run from the Action pane.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That&amp;#39;s it! There&amp;#39;s some jank but like I said, I&amp;#39;ve already spent way too long on this. Hopefully this helps you out! &lt;/p&gt;\n\n&lt;p&gt;A couple improvements I&amp;#39;d like to make eventually (very open to help here):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; click on the notification to open the playlist - should open automatically in the m3u associated player.&lt;/li&gt;\n&lt;li&gt; better file organization&lt;/li&gt;\n&lt;li&gt;make a gui to make it easier to run, and potentially convert from windows task scheduler task to a daemon or service with option to adjust frequency of checks&lt;/li&gt;\n&lt;li&gt;any of your suggestions!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m still really new to this, so I&amp;#39;m happy to hear any suggestions for improvements!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;v=enabled&amp;s=decc328886393c0699bb01cf9d08b602f60525c8", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cda4e996a0e1c77c65ec3810a634071f4573481", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ualom", "is_robot_indexable": true, "report_reasons": null, "author": "MonkAndCanatella", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ualom/auto_download_latest_youtube_videos_from_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ualom/auto_download_latest_youtube_videos_from_your/", "subreddit_subscribers": 673606, "created_utc": 1679102795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_baej04p7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Locally cheapest 4TB pro NVMe TLC drive. What's the catch? People say it runs hot.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 127, "top_awarded_type": null, "hide_score": false, "name": "t3_11ud6wv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GhBwijbcDpwKQhZb5BddanrIdN42gijnIL2w6jUerLM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679110218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/x2hkh6cv2foa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?auto=webp&amp;v=enabled&amp;s=1460986d1696cf1ae8c3e86a3eef2fab3822bae4", "width": 940, "height": 855}, "resolutions": [{"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be8440724585a44a7efea6b65cb8d68f0c9c8b84", "width": 108, "height": 98}, {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=911e433563111cc4bfb63cd8500189bf038c343a", "width": 216, "height": 196}, {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb110f2ba1c09e2d287a5d85158053ed6f141127", "width": 320, "height": 291}, {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e30c04109c1d5317db258a670a3413f1ef312f2", "width": 640, "height": 582}], "variants": {}, "id": "NHDqbeaHaJPCSNVmOLmnr2oyYe1OwYnbFQ7G9Ugd9H0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11ud6wv", "is_robot_indexable": true, "report_reasons": null, "author": "voinian", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ud6wv/locally_cheapest_4tb_pro_nvme_tlc_drive_whats_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/x2hkh6cv2foa1.png", "subreddit_subscribers": 673606, "created_utc": 1679110218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A triple layer 100GB BD-R disc fits 33.3GB onto a single layer.  Anyone know why there aren't any single layer 33GB or dual layer 66GB BD-R discs?  Always been curious about this.", "author_fullname": "t2_mgmtm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Are There No 33GB or 66GB BD-R Discs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uecgk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679113799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A triple layer 100GB BD-R disc fits 33.3GB onto a single layer.  Anyone know why there aren&amp;#39;t any single layer 33GB or dual layer 66GB BD-R discs?  Always been curious about this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11uecgk", "is_robot_indexable": true, "report_reasons": null, "author": "HarryMuscle", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uecgk/why_are_there_no_33gb_or_66gb_bdr_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uecgk/why_are_there_no_33gb_or_66gb_bdr_discs/", "subreddit_subscribers": 673606, "created_utc": 1679113799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ucb1wzy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A major lawsuit against the nonprofit Internet Archive threatens the future of all libraries. Big publishers are suing to cut off libraries\u2019 ownership and control of digital books, opening new paths for censorship. Oral arguments are on March 20.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_11uumi6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 54, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 54, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3lyR3uZXSb9fF3pTRLEM_Kp7yQSDpSO5Vvd2Dm9FK-E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679161046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "battleforlibraries.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.battleforlibraries.com/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?auto=webp&amp;v=enabled&amp;s=d72a8ba1d5166bdc96f6b2f9b93c3bcd9936ef2e", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7de28828f340945b38242f8be460aa6ace453dc5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50d5ab55255f912cfd4ac7d600c914b3783a5603", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60d72111fbc6a41da63bada636e16309199bad1b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40b4c4c77aac15edba831bcdcef96328480dd781", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3272e6d1bdee77555e10f5cde91759575138f218", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1686e6d24136af4ca004ee03775b7c899d80f2ec", "width": 1080, "height": 567}], "variants": {}, "id": "C2YqM5Wf7oqn1V33WBtgm3CxoSFhDUc-wG6Hr9jQC7g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uumi6", "is_robot_indexable": true, "report_reasons": null, "author": "BananaBus43", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uumi6/a_major_lawsuit_against_the_nonprofit_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.battleforlibraries.com/", "subreddit_subscribers": 673606, "created_utc": 1679161046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is a bit of an unconventional ask that I haven't seen in other posts about Spotify downloads.  \n\nI pay for spotify premium (student), and I'm quite happy with it for the sheer number of hours I use it, so I really have no need for local copies currently. HOWEVER- I'd like to download *lists* of the songs in my playlists into some kind of parseable text file per-playlist, containing song name, artist, maybe release date/runtime, etc.  \n\nI've amassed about 1600 songs so far in various playlists with a lot of \"mood specialization\" for each list. Making list backups of just the song titles is good enough for me to be able to recreate the same playlists from alternate sources if Spotify is to ever kick the bucket. Does a tool like this exist? I tried the old \"CTRL-A/CTRL-C\" trick listed in posts a few years ago but Spotify now only provides direct Spotify links instead of the name and artist when you do that.", "author_fullname": "t2_grolz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading spotify playlists - but Just the Names?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ufysj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679119150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a bit of an unconventional ask that I haven&amp;#39;t seen in other posts about Spotify downloads.  &lt;/p&gt;\n\n&lt;p&gt;I pay for spotify premium (student), and I&amp;#39;m quite happy with it for the sheer number of hours I use it, so I really have no need for local copies currently. HOWEVER- I&amp;#39;d like to download &lt;em&gt;lists&lt;/em&gt; of the songs in my playlists into some kind of parseable text file per-playlist, containing song name, artist, maybe release date/runtime, etc.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve amassed about 1600 songs so far in various playlists with a lot of &amp;quot;mood specialization&amp;quot; for each list. Making list backups of just the song titles is good enough for me to be able to recreate the same playlists from alternate sources if Spotify is to ever kick the bucket. Does a tool like this exist? I tried the old &amp;quot;CTRL-A/CTRL-C&amp;quot; trick listed in posts a few years ago but Spotify now only provides direct Spotify links instead of the name and artist when you do that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "still think Betamax shoulda won ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ufysj", "is_robot_indexable": true, "report_reasons": null, "author": "empirebuilder1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11ufysj/downloading_spotify_playlists_but_just_the_names/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ufysj/downloading_spotify_playlists_but_just_the_names/", "subreddit_subscribers": 673606, "created_utc": 1679119150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all! \n\nI have [this](https://www.fractal-design.com/products/cases/define/define-7-xl/black-tg-dark-tint/) case, the fractal Define 7 XL. I got it really low cost used with no HDD racks. I'm wondering if anyone knows of something that would be compatible for the front of the case near the front intake fans for about 4-5 HDD's?\n\nThanks!", "author_fullname": "t2_igthflmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an internal case mount for HDD's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u9h6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679100000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! &lt;/p&gt;\n\n&lt;p&gt;I have &lt;a href=\"https://www.fractal-design.com/products/cases/define/define-7-xl/black-tg-dark-tint/\"&gt;this&lt;/a&gt; case, the fractal Define 7 XL. I got it really low cost used with no HDD racks. I&amp;#39;m wondering if anyone knows of something that would be compatible for the front of the case near the front intake fans for about 4-5 HDD&amp;#39;s?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?auto=webp&amp;v=enabled&amp;s=48ad768a2ef3270064417e446895481cc66bc164", "width": 2560, "height": 1810}, "resolutions": [{"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bfedd6360c8bbea98a53921264465aa41d5e444", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b07f44f5a2eba78790934483cb735a877578e403", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca7e4a8c36ec0861796e0b7acf41095d26e7d543", "width": 320, "height": 226}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2fc4ebe975020d0ebaff111075ac1aeab7252e9", "width": 640, "height": 452}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a8c7cc9b3420c41669a397a52b7ebd57ae155d5", "width": 960, "height": 678}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7177bb9403f795eec118516462ad239ef3cf4f8", "width": 1080, "height": 763}], "variants": {}, "id": "Bn8gSTySoqn4Q4oRa1BLtFVkcDdV-5abpUxWxfiR8_A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11u9h6h", "is_robot_indexable": true, "report_reasons": null, "author": "Novel_Excitement5693", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u9h6h/looking_for_an_internal_case_mount_for_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u9h6h/looking_for_an_internal_case_mount_for_hdds/", "subreddit_subscribers": 673606, "created_utc": 1679100000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_16cmrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: Hachette v. Internet Archive. Oral Arguments at Southern District of New York on Monday(2023-03-20 at 1PM ET OR 6 PM UTC). This hearing is happening via telephone. You can join via 1-888-363-4749, with access code 8140049.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 100, "top_awarded_type": null, "hide_score": true, "name": "t3_11utujv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0Qpq9etcpLnkqm670muYjirhW-oPHpkaup77c1Ui4xI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679159216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.archive.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.archive.org/2023/03/17/heres-how-to-participate-in-mondays-oral-arguments/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?auto=webp&amp;v=enabled&amp;s=e7232a685c447dbfd10e31e6fa562aebba34ee32", "width": 1000, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58e6e7603724de9b6bad1f33bc2f7c258e82b315", "width": 108, "height": 77}, {"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3c72a8b6e89f24e8f5ff96258e95c342849975c", "width": 216, "height": 155}, {"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2e7c884ca760a347c935db701b0aaef8478ae8c", "width": 320, "height": 230}, {"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a30468c80ebd064e80f36b9d91a6070466083a5", "width": 640, "height": 460}, {"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=264582fefb9f3dd77f1f3c93f4f4da1c9815aedf", "width": 960, "height": 691}], "variants": {}, "id": "iGhrtRYFD0Ku0C467L1P1D6b9k_N6WUieUKRTXoKgZw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11utujv", "is_robot_indexable": true, "report_reasons": null, "author": "rpollost", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11utujv/psa_hachette_v_internet_archive_oral_arguments_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.archive.org/2023/03/17/heres-how-to-participate-in-mondays-oral-arguments/", "subreddit_subscribers": 673606, "created_utc": 1679159216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently I have a 4TB Seagate Barracuda drive on my PC for general storage.\n\nI was thinking about replacing it with an IronWolf 8TB. Keep in mind this is not for NAS, but for inside my PC.\n\nIs this the way to go, or do you have other suggestions?", "author_fullname": "t2_37lnd1hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "8TB Drive Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u09bx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679079526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I have a 4TB Seagate Barracuda drive on my PC for general storage.&lt;/p&gt;\n\n&lt;p&gt;I was thinking about replacing it with an IronWolf 8TB. Keep in mind this is not for NAS, but for inside my PC.&lt;/p&gt;\n\n&lt;p&gt;Is this the way to go, or do you have other suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11u09bx", "is_robot_indexable": true, "report_reasons": null, "author": "JohnsonZ887", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u09bx/8tb_drive_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u09bx/8tb_drive_suggestions/", "subreddit_subscribers": 673606, "created_utc": 1679079526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What are some good tools to quickly and easily archive a forum like Tapatalk for example? : [https://www.tapatalk.com/](https://www.tapatalk.com/)", "author_fullname": "t2_7gl7ysbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best tools to archive a forum quickly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ujp39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679132792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some good tools to quickly and easily archive a forum like Tapatalk for example? : &lt;a href=\"https://www.tapatalk.com/\"&gt;https://www.tapatalk.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ujp39", "is_robot_indexable": true, "report_reasons": null, "author": "DarknessMoonlight", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ujp39/what_are_the_best_tools_to_archive_a_forum_quickly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ujp39/what_are_the_best_tools_to_archive_a_forum_quickly/", "subreddit_subscribers": 673606, "created_utc": 1679132792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm doing a little personal thing and I'm wondering if there is a way to scrape and download ALL images from website... not just a page, or a profile, THE WHOLE WEBSITE.I know it'd probably take a while to download all of it, and also probably I'd need a lot of TB for it. But I'm wondering anyway.\n\nMost tutorials I see show how to scrap it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).\n\n&amp;#x200B;\n\nI've been doing it manually so far, but it's taking a LONG time and also I don't wanna get carpal tunnel, so I got myself thinking 'there must be an easier way to do it!'.\n\nKnowing the internet as I do I also know that if I do scrape a website, no matter which website it is, I'm gonna end up getting loads of unwanted images, like graphic imagery and porn and... less then legal stuff I suppose. If there's a way to filter out those I'd also like to know, otherwise I'd just have to delete them out mannually.\n\nI'd also like to convert all images to .pngm but if not possible I'm willing to do it mannually.\n\nMost tutorials I see show how to scrape it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).", "author_fullname": "t2_65yetpx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to scrape ALL images from a website (not a single page, the whole website)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uq1z6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679150000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing a little personal thing and I&amp;#39;m wondering if there is a way to scrape and download ALL images from website... not just a page, or a profile, THE WHOLE WEBSITE.I know it&amp;#39;d probably take a while to download all of it, and also probably I&amp;#39;d need a lot of TB for it. But I&amp;#39;m wondering anyway.&lt;/p&gt;\n\n&lt;p&gt;Most tutorials I see show how to scrap it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been doing it manually so far, but it&amp;#39;s taking a LONG time and also I don&amp;#39;t wanna get carpal tunnel, so I got myself thinking &amp;#39;there must be an easier way to do it!&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Knowing the internet as I do I also know that if I do scrape a website, no matter which website it is, I&amp;#39;m gonna end up getting loads of unwanted images, like graphic imagery and porn and... less then legal stuff I suppose. If there&amp;#39;s a way to filter out those I&amp;#39;d also like to know, otherwise I&amp;#39;d just have to delete them out mannually.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d also like to convert all images to .pngm but if not possible I&amp;#39;m willing to do it mannually.&lt;/p&gt;\n\n&lt;p&gt;Most tutorials I see show how to scrape it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uq1z6", "is_robot_indexable": true, "report_reasons": null, "author": "abcbibi", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uq1z6/is_there_a_way_to_scrape_all_images_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uq1z6/is_there_a_way_to_scrape_all_images_from_a/", "subreddit_subscribers": 673606, "created_utc": 1679150000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I remember couple years ago an older build leaked. But MVG made [video](https://www.youtube.com/watch?v=1fwSR8gjrCk) with newer version that sound impressive. Have these been preserved yet?", "author_fullname": "t2_w6lyzny8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has Nintendo's official NSO GBA/GB emulators been preserved yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ual61", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679102755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember couple years ago an older build leaked. But MVG made &lt;a href=\"https://www.youtube.com/watch?v=1fwSR8gjrCk\"&gt;video&lt;/a&gt; with newer version that sound impressive. Have these been preserved yet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?auto=webp&amp;v=enabled&amp;s=a6b9844c7c72095912b81476a84e34b8abe06c63", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c04916d2bc6f2aab163bd7f2e61be73fff275a6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9abb6417309562656febad8fc2a317fe469bcbe6", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f073e41099e13b18258feafcf673dd1693d90b17", "width": 320, "height": 240}], "variants": {}, "id": "VTvIbMpG2MB-WJO9Ipc_obgXKuScYk-EBzz_frA02Oo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ual61", "is_robot_indexable": true, "report_reasons": null, "author": "JebryyathHS", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ual61/has_nintendos_official_nso_gbagb_emulators_been/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ual61/has_nintendos_official_nso_gbagb_emulators_been/", "subreddit_subscribers": 673606, "created_utc": 1679102755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Of the Micro SD card brands I've owned over the years, ALL but one of the Sandisk cards has failed. ALL of the Transcend, Lexar, Kingston, and Samsung cards are still going strong.\n\nOf the Sandisk flash drives I've owned, I've experienced ~50% failure rate on the newer drives (16GB and higher). I just had a (seldom used) Sandisk Ultra Flair 128GB fail last night.\n\n/rant", "author_fullname": "t2_cd00t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: Sandisk SD cards and flash drives suck for reliability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uq21w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679150005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Of the Micro SD card brands I&amp;#39;ve owned over the years, ALL but one of the Sandisk cards has failed. ALL of the Transcend, Lexar, Kingston, and Samsung cards are still going strong.&lt;/p&gt;\n\n&lt;p&gt;Of the Sandisk flash drives I&amp;#39;ve owned, I&amp;#39;ve experienced ~50% failure rate on the newer drives (16GB and higher). I just had a (seldom used) Sandisk Ultra Flair 128GB fail last night.&lt;/p&gt;\n\n&lt;p&gt;/rant&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11uq21w", "is_robot_indexable": true, "report_reasons": null, "author": "Ho_Lee_Fuk", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uq21w/psa_sandisk_sd_cards_and_flash_drives_suck_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uq21w/psa_sandisk_sd_cards_and_flash_drives_suck_for/", "subreddit_subscribers": 673606, "created_utc": 1679150005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All,\n\nI used to have a Mac system and used Carbon Copy.\n\nI have a system with a 500GB SSD, and I bought a 1TB SSD. My MB has only one slot, so I'd like to carbon copy the content of the 500GB (SO and everything in it) into the 1TB so that I don't have to reinstall everything.\n\nDo you have any suggestions on windows 10?\n\nThanks", "author_fullname": "t2_2rqya8a6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Carbon copy a 500 GB SSD into a bigger one", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11upm44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679148862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I used to have a Mac system and used Carbon Copy.&lt;/p&gt;\n\n&lt;p&gt;I have a system with a 500GB SSD, and I bought a 1TB SSD. My MB has only one slot, so I&amp;#39;d like to carbon copy the content of the 500GB (SO and everything in it) into the 1TB so that I don&amp;#39;t have to reinstall everything.&lt;/p&gt;\n\n&lt;p&gt;Do you have any suggestions on windows 10?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11upm44", "is_robot_indexable": true, "report_reasons": null, "author": "ff8mania", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11upm44/carbon_copy_a_500_gb_ssd_into_a_bigger_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11upm44/carbon_copy_a_500_gb_ssd_into_a_bigger_one/", "subreddit_subscribers": 673606, "created_utc": 1679148862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title", "author_fullname": "t2_4jurunac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to upload firmware for ISP only fiber router for Open Source developers.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11uv5mu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679162307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11uv5mu", "is_robot_indexable": true, "report_reasons": null, "author": "RedditNoobie777", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uv5mu/where_to_upload_firmware_for_isp_only_fiber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uv5mu/where_to_upload_firmware_for_isp_only_fiber/", "subreddit_subscribers": 673606, "created_utc": 1679162307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My WD NVME SN850x 2TB is not recognized on my motherboard (Asus Deluxe x99) while my AORUS RGB M.2 NVMe SSD 256GB is and works. (I wanted to upgrade it to the WD 2TB)      \n\n\nWhat I did :  \n\n\nEnable CSM   \nUpgrade bios to the last build  \n\n\nhttps://preview.redd.it/irpiyx1ycjoa1.jpg?width=2000&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a5af9f0f020cbf9a3c09a419cf600beb8158dbf5\n\nhttps://preview.redd.it/s9bn2y1ycjoa1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ce8291c209c058ccb9a626b3941b5e5a07f150f3", "author_fullname": "t2_19xn5yon", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD SN850X NVMe Drive NOT recognized on Deluxe x99", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "media_metadata": {"irpiyx1ycjoa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/irpiyx1ycjoa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63299ad1a85ee5fe2f7a334256de4f755a4084c4"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/irpiyx1ycjoa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb9984ef1ca1e45f26f56fd021184e18c7b0a88f"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/irpiyx1ycjoa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f79f10b7dbe62c4fafde4e92820504e627b204bf"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/irpiyx1ycjoa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=80e889d7566c8289f8b0ec26b334065e02a6b241"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/irpiyx1ycjoa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cbc018e1a5548fc3eb619eaf34b9eef59beda452"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/irpiyx1ycjoa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ad810e7705ed415b0fba4ce4571c718a5e5e3b4"}], "s": {"y": 1125, "x": 2000, "u": "https://preview.redd.it/irpiyx1ycjoa1.jpg?width=2000&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a5af9f0f020cbf9a3c09a419cf600beb8158dbf5"}, "id": "irpiyx1ycjoa1"}, "s9bn2y1ycjoa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 61, "x": 108, "u": "https://preview.redd.it/s9bn2y1ycjoa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d664295501d5b3afed09142913ea76caca58460a"}, {"y": 123, "x": 216, "u": "https://preview.redd.it/s9bn2y1ycjoa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0da2747b6db00c476cfb59fc1d76e80cb8fc40a3"}, {"y": 183, "x": 320, "u": "https://preview.redd.it/s9bn2y1ycjoa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c023afd24c3fceeb1855123b0c187cb2606e2948"}, {"y": 366, "x": 640, "u": "https://preview.redd.it/s9bn2y1ycjoa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a885e2d945b1b26a555b9b8b9f6cb7237d2cb9db"}, {"y": 550, "x": 960, "u": "https://preview.redd.it/s9bn2y1ycjoa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0882a80dfe10553fb3ecc065416ef499b1d6882d"}, {"y": 618, "x": 1080, "u": "https://preview.redd.it/s9bn2y1ycjoa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5f3d1c5955b02035f8953a5337b02a668679b33"}], "s": {"y": 1100, "x": 1920, "u": "https://preview.redd.it/s9bn2y1ycjoa1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ce8291c209c058ccb9a626b3941b5e5a07f150f3"}, "id": "s9bn2y1ycjoa1"}}, "name": "t3_11uuzvd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/2q70kpO-9NwX5CD8D45kohiJaJo1xAbcmtkGrH_IdX8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679161916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My WD NVME SN850x 2TB is not recognized on my motherboard (Asus Deluxe x99) while my AORUS RGB M.2 NVMe SSD 256GB is and works. (I wanted to upgrade it to the WD 2TB)      &lt;/p&gt;\n\n&lt;p&gt;What I did :  &lt;/p&gt;\n\n&lt;p&gt;Enable CSM&lt;br/&gt;\nUpgrade bios to the last build  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/irpiyx1ycjoa1.jpg?width=2000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a5af9f0f020cbf9a3c09a419cf600beb8158dbf5\"&gt;https://preview.redd.it/irpiyx1ycjoa1.jpg?width=2000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a5af9f0f020cbf9a3c09a419cf600beb8158dbf5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s9bn2y1ycjoa1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ce8291c209c058ccb9a626b3941b5e5a07f150f3\"&gt;https://preview.redd.it/s9bn2y1ycjoa1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ce8291c209c058ccb9a626b3941b5e5a07f150f3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uuzvd", "is_robot_indexable": true, "report_reasons": null, "author": "zikha", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uuzvd/wd_sn850x_nvme_drive_not_recognized_on_deluxe_x99/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uuzvd/wd_sn850x_nvme_drive_not_recognized_on_deluxe_x99/", "subreddit_subscribers": 673606, "created_utc": 1679161916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "When can all these things be integrated into a single package and accessed like a super fast reliable block device? I am not even asking for full text search (yet). We have all the tech but the offerings for hybrid drives have afaik all been profit maximizing cost cutting plays by manufacturers instead of playing to the strengths of all these technologies.\n\nThis requires no new technology. Hell, HD could even have the ability to attach an external NVMe flash drive.", "author_fullname": "t2_406sj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When will spinning drives move to NVMe and include battery backed RAM, flash and rust in a single package?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11uuzuh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679161914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When can all these things be integrated into a single package and accessed like a super fast reliable block device? I am not even asking for full text search (yet). We have all the tech but the offerings for hybrid drives have afaik all been profit maximizing cost cutting plays by manufacturers instead of playing to the strengths of all these technologies.&lt;/p&gt;\n\n&lt;p&gt;This requires no new technology. Hell, HD could even have the ability to attach an external NVMe flash drive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11uuzuh", "is_robot_indexable": true, "report_reasons": null, "author": "fullouterjoin", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uuzuh/when_will_spinning_drives_move_to_nvme_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uuzuh/when_will_spinning_drives_move_to_nvme_and/", "subreddit_subscribers": 673606, "created_utc": 1679161914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there\n\nYou all probably know that Samsung publishes source code (only what they have to by GPL) for their phones here: [https://opensource.samsung.com/uploadList?menuItem=mobile](https://opensource.samsung.com/uploadList?menuItem=mobile)\n\n&amp;#x200B;\n\nI was lucky once and got a complete FMRadio app (com.sec.android.fm) source code in one of the archives, its for Android 6 Marshmallow and its from 2016\n\n&amp;#x200B;\n\nand I was looking maybe, they also published some newer version, but sadly if I look the phones that have FM Radio (SM-G975N for instance) [com.sec.android.fm](https://com.sec.android.fm) is nowhere to be found (I guess I got lucky back then) in their software archive they only publish  jlayer1.0.1.zip \t\n\nwhich is just an open source library that the applicatoin is using\n\n&amp;#x200B;\n\nSo I got the idea to download every phone package (each zip is from 100MB to 500MB but sadly downloads are rate limited to about 100kb which takes a while (but its per file basis, so if you download 5 100MB files (they will all download at the same time)\n\n&amp;#x200B;\n\nSo I was wondering if anyone maybe has a script to download it all, or someone maybe already did that and has a torrent link or something\n\n&amp;#x200B;\n\nI just need to download every archive open it up and check if folder Platform.tar.gz has vendor/samsung/packages/apps/HybridRadio\\*\\*\\*/AndroidManifest.xml or [Android.mk](https://Android.mk) or more then 2 folders inside or HybridRadio folder even exists\n\nHybridRadio\\*\\*\\* is because I found HybridRadio2016\\_M to be the folder name that my source code for Android 6 came from\n\n&amp;#x200B;\n\nM meaning Android version (6 in this case), it can also be P (for Android9), I also saw app names like HybridRadio\\_P.apk\n\nI think the hardest part is downloading, I could probably cobble together a tool that goes through  2601 (if we assume each file is about 500MB in size (which it usualy is not, that translates to about 1.3TB of space if we keep all the files) files and deletes those that don't have HybridRadio folder\n\n&amp;#x200B;\n\nI believe out of 2601 I could get like 2 or maybe 4 matches (maybe 3 has the source code), idk\n\n&amp;#x200B;\n\nSo yea, if anyone has a torrent file or maybe knows whats the best way to automate the downloading and extraxting relevant files, tell me\n\n&amp;#x200B;\n\nThanks for Anwsering and Best Regards", "author_fullname": "t2_13m5l5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to scrape Samsung Open source phone archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11uuz4p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679161867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;You all probably know that Samsung publishes source code (only what they have to by GPL) for their phones here: &lt;a href=\"https://opensource.samsung.com/uploadList?menuItem=mobile\"&gt;https://opensource.samsung.com/uploadList?menuItem=mobile&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was lucky once and got a complete FMRadio app (com.sec.android.fm) source code in one of the archives, its for Android 6 Marshmallow and its from 2016&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;and I was looking maybe, they also published some newer version, but sadly if I look the phones that have FM Radio (SM-G975N for instance) &lt;a href=\"https://com.sec.android.fm\"&gt;com.sec.android.fm&lt;/a&gt; is nowhere to be found (I guess I got lucky back then) in their software archive they only publish  jlayer1.0.1.zip     &lt;/p&gt;\n\n&lt;p&gt;which is just an open source library that the applicatoin is using&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I got the idea to download every phone package (each zip is from 100MB to 500MB but sadly downloads are rate limited to about 100kb which takes a while (but its per file basis, so if you download 5 100MB files (they will all download at the same time)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I was wondering if anyone maybe has a script to download it all, or someone maybe already did that and has a torrent link or something&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I just need to download every archive open it up and check if folder Platform.tar.gz has vendor/samsung/packages/apps/HybridRadio***/AndroidManifest.xml or &lt;a href=\"https://Android.mk\"&gt;Android.mk&lt;/a&gt; or more then 2 folders inside or HybridRadio folder even exists&lt;/p&gt;\n\n&lt;p&gt;HybridRadio*** is because I found HybridRadio2016_M to be the folder name that my source code for Android 6 came from&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;M meaning Android version (6 in this case), it can also be P (for Android9), I also saw app names like HybridRadio_P.apk&lt;/p&gt;\n\n&lt;p&gt;I think the hardest part is downloading, I could probably cobble together a tool that goes through  2601 (if we assume each file is about 500MB in size (which it usualy is not, that translates to about 1.3TB of space if we keep all the files) files and deletes those that don&amp;#39;t have HybridRadio folder&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I believe out of 2601 I could get like 2 or maybe 4 matches (maybe 3 has the source code), idk&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So yea, if anyone has a torrent file or maybe knows whats the best way to automate the downloading and extraxting relevant files, tell me&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for Anwsering and Best Regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11uuz4p", "is_robot_indexable": true, "report_reasons": null, "author": "veso266", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uuz4p/how_to_scrape_samsung_open_source_phone_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uuz4p/how_to_scrape_samsung_open_source_phone_archive/", "subreddit_subscribers": 673606, "created_utc": 1679161867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I made the mistake of using up all the 256GB of storage on my phone until I decided to back it up (have been traveling). Since there wasn\u2019t much storage left on the device connecting it via a cable to my PC and going through iTunes failed many many times. I then tried iCloud, ended up paying for the subscription, it took about a month to sync my photos+videos from the phone to the cloud, this freed up some space on my phone since \u201coptimized\u201d versions are stored on the phone and \u201cfull size\u201d versions are on the cloud. When I opened iCloud on my PC via a browser, the info tab confirms an image is a 12MP file but when I try to download I can only get 4MP even though I choose to downlod \u201cfull size\u201d. \n\nI have read several forum posts stating that apple punishes PC users and iCloud doesnt allow full size downloads on Windows. Sounded like a terrible conspiracy theory but I am definitely not seeing a full size image on my PC. I am really pulling my hair on this one. \n\nHow can I get fullsize iPhone photos+videos downloaded onto a PC?\n\nI appreciate any help. Thanks!", "author_fullname": "t2_npzc32a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to back up photos+videos from iPhone to PC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11utv0w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679159247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made the mistake of using up all the 256GB of storage on my phone until I decided to back it up (have been traveling). Since there wasn\u2019t much storage left on the device connecting it via a cable to my PC and going through iTunes failed many many times. I then tried iCloud, ended up paying for the subscription, it took about a month to sync my photos+videos from the phone to the cloud, this freed up some space on my phone since \u201coptimized\u201d versions are stored on the phone and \u201cfull size\u201d versions are on the cloud. When I opened iCloud on my PC via a browser, the info tab confirms an image is a 12MP file but when I try to download I can only get 4MP even though I choose to downlod \u201cfull size\u201d. &lt;/p&gt;\n\n&lt;p&gt;I have read several forum posts stating that apple punishes PC users and iCloud doesnt allow full size downloads on Windows. Sounded like a terrible conspiracy theory but I am definitely not seeing a full size image on my PC. I am really pulling my hair on this one. &lt;/p&gt;\n\n&lt;p&gt;How can I get fullsize iPhone photos+videos downloaded onto a PC?&lt;/p&gt;\n\n&lt;p&gt;I appreciate any help. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11utv0w", "is_robot_indexable": true, "report_reasons": null, "author": "yowhywouldyoudothat", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11utv0w/how_to_back_up_photosvideos_from_iphone_to_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11utv0w/how_to_back_up_photosvideos_from_iphone_to_pc/", "subreddit_subscribers": 673606, "created_utc": 1679159247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, sorry to bother. I recently got my old PC running for to becoming a NAS system, I am planning to install windows tomorrow. I was wondering, is TrueNAS a good option for running NAS? Or I can just stick to windows server instead like how [LTT does it (Your Old PC is Your New Server)](https://youtu.be/zPmqbtKwtgw). \n\nI was thinking about 8x8 TB. Or 4 X 4 X 4 X 4 TB with the toshiba N300s\n\nI am planning to use this NAS for my video editing, and transfering files between my two computers, Windows (rendering) and Mac (Editing) and storing important documents. Is there any advice you guys can give me? Sorry to bother again", "author_fullname": "t2_8siy7ss6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is TrueNAS recomended for a DIY NAS doing two drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11uthhb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679158387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, sorry to bother. I recently got my old PC running for to becoming a NAS system, I am planning to install windows tomorrow. I was wondering, is TrueNAS a good option for running NAS? Or I can just stick to windows server instead like how &lt;a href=\"https://youtu.be/zPmqbtKwtgw\"&gt;LTT does it (Your Old PC is Your New Server)&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;I was thinking about 8x8 TB. Or 4 X 4 X 4 X 4 TB with the toshiba N300s&lt;/p&gt;\n\n&lt;p&gt;I am planning to use this NAS for my video editing, and transfering files between my two computers, Windows (rendering) and Mac (Editing) and storing important documents. Is there any advice you guys can give me? Sorry to bother again&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/h4DQneQwkuZjvznaRQIp-MID1wcIHPnWo-JEwRqbALA.jpg?auto=webp&amp;v=enabled&amp;s=a88b61e85e73458632f38fb1fdde277ee9a39e2c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/h4DQneQwkuZjvznaRQIp-MID1wcIHPnWo-JEwRqbALA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ddc7593b581aff186db021f28cd55be8249cdf6c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/h4DQneQwkuZjvznaRQIp-MID1wcIHPnWo-JEwRqbALA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b6f77210c3e6de34d6ada657e103069a2416fa6", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/h4DQneQwkuZjvznaRQIp-MID1wcIHPnWo-JEwRqbALA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84ebb46638c838099c767e26afaa8a51dcca2f83", "width": 320, "height": 240}], "variants": {}, "id": "_jJ5Sazmo3ahlr8BUh_433Nz-su2F_4QaHVh3ThLmoY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uthhb", "is_robot_indexable": true, "report_reasons": null, "author": "SciencioGT", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uthhb/is_truenas_recomended_for_a_diy_nas_doing_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uthhb/is_truenas_recomended_for_a_diy_nas_doing_two/", "subreddit_subscribers": 673606, "created_utc": 1679158387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI am looking to update my case to get more hdd slots but am not sure how to deal with the backplanes...  \nCurrently I am using a IT mode Dell Perc H310 with two 4x sata breakout cables, so with that and the mobo I am up to 11 drives.\n\nNext I would like to get a 24 drive enclosure (4U) but I see 6g,12g,and sata backplanes and am not sure where to go next in terms of controllers vs mini-sas expanders.  \nI don't really need much in terms of speed (being that it's mostly shucked or WD Red 5400), so I am not sure if the current controller would do the trick (it supports 32 drives). If I stuck with it what expander card would support a backplane with mini-sas x 6 (so I guess the expander would need to be 2 in , 6 out mini-sas)?  \nAny thoughts?\n\nThanks!", "author_fullname": "t2_1rwxyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Case Upgrade Questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ur4d0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679152643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I am looking to update my case to get more hdd slots but am not sure how to deal with the backplanes...&lt;br/&gt;\nCurrently I am using a IT mode Dell Perc H310 with two 4x sata breakout cables, so with that and the mobo I am up to 11 drives.&lt;/p&gt;\n\n&lt;p&gt;Next I would like to get a 24 drive enclosure (4U) but I see 6g,12g,and sata backplanes and am not sure where to go next in terms of controllers vs mini-sas expanders.&lt;br/&gt;\nI don&amp;#39;t really need much in terms of speed (being that it&amp;#39;s mostly shucked or WD Red 5400), so I am not sure if the current controller would do the trick (it supports 32 drives). If I stuck with it what expander card would support a backplane with mini-sas x 6 (so I guess the expander would need to be 2 in , 6 out mini-sas)?&lt;br/&gt;\nAny thoughts?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ur4d0", "is_robot_indexable": true, "report_reasons": null, "author": "hand_in_every_pot", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ur4d0/case_upgrade_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ur4d0/case_upgrade_questions/", "subreddit_subscribers": 673606, "created_utc": 1679152643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nI used to have a PC case that had 4 HDD bays, now the one I have only has 1. Ideally i'd like to keep my 2x 14TB &amp; 2x 10TB drives, but if i have to shrink down to 2x 14TB I will.\n\n&amp;#x200B;\n\nHere is my issue. I used windows to expand/create a raid 0 for each pair. If i plug it loosely into Sata and power Windows detects it. if i use my External HDD enclosure, it doesn't. \n\nI Bought this thinking i could put all 4 in, and windows would still recognize it.\n\n[https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;ref=ppx\\_yo2ov\\_dt\\_b\\_product\\_details](https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details)\n\n&amp;#x200B;\n\nNONE of this data is important...its just alot lol\n\n&amp;#x200B;\n\nis it a bad unit for this? is it not possible? any guidance is appreciated.", "author_fullname": "t2_4e2n8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage help! Windows Extended volume", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uqqok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679151720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I used to have a PC case that had 4 HDD bays, now the one I have only has 1. Ideally i&amp;#39;d like to keep my 2x 14TB &amp;amp; 2x 10TB drives, but if i have to shrink down to 2x 14TB I will.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here is my issue. I used windows to expand/create a raid 0 for each pair. If i plug it loosely into Sata and power Windows detects it. if i use my External HDD enclosure, it doesn&amp;#39;t. &lt;/p&gt;\n\n&lt;p&gt;I Bought this thinking i could put all 4 in, and windows would still recognize it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details\"&gt;https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;NONE of this data is important...its just alot lol&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;is it a bad unit for this? is it not possible? any guidance is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uqqok", "is_robot_indexable": true, "report_reasons": null, "author": "ChiefSpoonS", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uqqok/storage_help_windows_extended_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uqqok/storage_help_windows_extended_volume/", "subreddit_subscribers": 673606, "created_utc": 1679151720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm totally new with storage systems. I bought a netapp 4486 jbod connected to my server with a 9200-8e in IT mode. I have starting to populate with services (docker containers) with great use of HDDs and I noticed that server doesn't respond very well. CPU have big wait %  (4% user 8%sys 30%wait). Mem is free not totally used.\n\nWhen I type with keyboard sometimes system stuck for 1/2s and double my letters.. sometime write ten times the same letter. I'm thinking that HBA card is the big problem here. what do you think? So:\n\n1) If i change 9200 with a new 9207 and restart my server... I have to reconfigure entire jbod hdds in my system or nothing have to be done?\n\n2) If I connect both 9207 exit ports to JBOD will help?", "author_fullname": "t2_7xrhh3tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "JBOD: Server slowing down. Changing HBA card", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uk08r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679133768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m totally new with storage systems. I bought a netapp 4486 jbod connected to my server with a 9200-8e in IT mode. I have starting to populate with services (docker containers) with great use of HDDs and I noticed that server doesn&amp;#39;t respond very well. CPU have big wait %  (4% user 8%sys 30%wait). Mem is free not totally used.&lt;/p&gt;\n\n&lt;p&gt;When I type with keyboard sometimes system stuck for 1/2s and double my letters.. sometime write ten times the same letter. I&amp;#39;m thinking that HBA card is the big problem here. what do you think? So:&lt;/p&gt;\n\n&lt;p&gt;1) If i change 9200 with a new 9207 and restart my server... I have to reconfigure entire jbod hdds in my system or nothing have to be done?&lt;/p&gt;\n\n&lt;p&gt;2) If I connect both 9207 exit ports to JBOD will help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uk08r", "is_robot_indexable": true, "report_reasons": null, "author": "Reddolo80", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uk08r/jbod_server_slowing_down_changing_hba_card/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uk08r/jbod_server_slowing_down_changing_hba_card/", "subreddit_subscribers": 673606, "created_utc": 1679133768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've noticed I often clean install windows and I like that it leaves the system clean. I do just back up my user folder as that's all the files I like to keep like videos or photos I've made from that time life of that install. Was wondering if there is a way to restore the files to the same locations but keep the junk out like random desktop.ini files or .exe files that lead to nowhere. Thx much appreciated", "author_fullname": "t2_vzj8emxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a good way to backup personal files on windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ucyot", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679109533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve noticed I often clean install windows and I like that it leaves the system clean. I do just back up my user folder as that&amp;#39;s all the files I like to keep like videos or photos I&amp;#39;ve made from that time life of that install. Was wondering if there is a way to restore the files to the same locations but keep the junk out like random desktop.ini files or .exe files that lead to nowhere. Thx much appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ucyot", "is_robot_indexable": true, "report_reasons": null, "author": "AfterwardsFriendly", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ucyot/what_is_a_good_way_to_backup_personal_files_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ucyot/what_is_a_good_way_to_backup_personal_files_on/", "subreddit_subscribers": 673606, "created_utc": 1679109533.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}