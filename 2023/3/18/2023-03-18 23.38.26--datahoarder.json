{"kind": "Listing", "data": {"after": "t3_11uk08r", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ucb1wzy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A major lawsuit against the nonprofit Internet Archive threatens the future of all libraries. Big publishers are suing to cut off libraries\u2019 ownership and control of digital books, opening new paths for censorship. Oral arguments are on March 20.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_11uumi6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 735, "total_awards_received": 2, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 735, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3lyR3uZXSb9fF3pTRLEM_Kp7yQSDpSO5Vvd2Dm9FK-E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679161046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "battleforlibraries.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.battleforlibraries.com/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?auto=webp&amp;v=enabled&amp;s=d72a8ba1d5166bdc96f6b2f9b93c3bcd9936ef2e", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7de28828f340945b38242f8be460aa6ace453dc5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50d5ab55255f912cfd4ac7d600c914b3783a5603", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60d72111fbc6a41da63bada636e16309199bad1b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40b4c4c77aac15edba831bcdcef96328480dd781", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3272e6d1bdee77555e10f5cde91759575138f218", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/i1ejpNIwt9VqXKSpidVMDX2O-o9xZPxf_fY8f0J_vRA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1686e6d24136af4ca004ee03775b7c899d80f2ec", "width": 1080, "height": 567}], "variants": {}, "id": "C2YqM5Wf7oqn1V33WBtgm3CxoSFhDUc-wG6Hr9jQC7g"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 30, "id": "award_a2506925-fc82-4d6c-ae3b-b7217e09d7f0", "penny_donate": null, "award_sub_type": "PREMIUM", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=8a2a2acbd09705a194b77e9d555b630ec0ed9966", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=a1b4dc570c54d4835d396ce6f327cc5df2cd067c", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e4979f617effb9d355397aa2c3f8032870f96fd3", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=12d699f41173c6ed50555c5689b463020696a5d3", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=5716709f7a329eb81103a5acfee4a545ec0be9b0", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "A golden splash of respect", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 2, "static_icon_height": 2048, "name": "Narwhal Salute", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=8a2a2acbd09705a194b77e9d555b630ec0ed9966", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=a1b4dc570c54d4835d396ce6f327cc5df2cd067c", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e4979f617effb9d355397aa2c3f8032870f96fd3", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=12d699f41173c6ed50555c5689b463020696a5d3", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=5716709f7a329eb81103a5acfee4a545ec0be9b0", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uumi6", "is_robot_indexable": true, "report_reasons": null, "author": "BananaBus43", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uumi6/a_major_lawsuit_against_the_nonprofit_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.battleforlibraries.com/", "subreddit_subscribers": 673632, "created_utc": 1679161046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_uegnp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this ticking sound normal? I fell like the drive is failing slowly.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11ukbsr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 175, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/mwufgnac4hoa1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/mwufgnac4hoa1/DASH_96.mp4", "dash_url": "https://v.redd.it/mwufgnac4hoa1/DASHPlaylist.mpd?a=1681774706%2CMWQwNjM1ZTIzYmJhOTc5Y2U2NmE0NTlmMDkwZTE1MjQ1NDUyMmMzNzY4NzFlNTA1ZjNjODczNWJmOGZjYzBhNw%3D%3D&amp;v=1&amp;f=sd", "duration": 16, "hls_url": "https://v.redd.it/mwufgnac4hoa1/HLSPlaylist.m3u8?a=1681774706%2CZTI1YjU2NWM2MzFhNjY3MDE2YzVhOWNjZTAwMzhjNjI2ZTMxMzJiMjZkZjYyZmFmMWE2OTA0ODk0NDVkNjQ1OA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 175, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EvrgGIMo7XEA2e_xYCC2GBum8KLuZH8G1OEx-HZ14qw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679134808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/mwufgnac4hoa1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5fce8a121d34a69c4ea8c99736a9d3a846f4d8e4", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=bfaebbd27a240e39ff44e2e5cb8d3c17178386dc", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d46a8071b805744234bd8b2057cebb60110890a8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d57fe217791b51b6293ee6d9ec2fe7957ed30e08", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=67d1dcebc5e1ce942bd46d0e32724575f1edeb94", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0fc5332ac68de049b31e0f893e3d8cfba097c16e", "width": 960, "height": 540}], "variants": {}, "id": "l6IRE2oY_8wu1Ho9CejFerWMIKwLTBqzDymm5csVmhA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ukbsr", "is_robot_indexable": true, "report_reasons": null, "author": "CrazyYAY", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ukbsr/is_this_ticking_sound_normal_i_fell_like_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/mwufgnac4hoa1", "subreddit_subscribers": 673632, "created_utc": 1679134808.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/mwufgnac4hoa1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/mwufgnac4hoa1/DASH_96.mp4", "dash_url": "https://v.redd.it/mwufgnac4hoa1/DASHPlaylist.mpd?a=1681774706%2CMWQwNjM1ZTIzYmJhOTc5Y2U2NmE0NTlmMDkwZTE1MjQ1NDUyMmMzNzY4NzFlNTA1ZjNjODczNWJmOGZjYzBhNw%3D%3D&amp;v=1&amp;f=sd", "duration": 16, "hls_url": "https://v.redd.it/mwufgnac4hoa1/HLSPlaylist.m3u8?a=1681774706%2CZTI1YjU2NWM2MzFhNjY3MDE2YzVhOWNjZTAwMzhjNjI2ZTMxMzJiMjZkZjYyZmFmMWE2OTA0ODk0NDVkNjQ1OA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I've been working on this script all week. I literally thought it would take a few hours and it's consumed every hour of this past week. \n\nSo I've made a script in powershell that uses yt-dlp to download the latest youtube videos from your subscriptions, creates a playlist from all the files in the resulting folder, and creates a notification showing the names of the channels from the latest downloads. \n\nNote, all of this can be modified fairly straightforward.\n\n1. Create folder to hold everything. &lt;mainFolder&gt;\n\n2. create &lt;powershellScriptName&gt;.ps1, &lt;vbsScriptName&gt;.vbs in `mainFolder`\n\n3. make sure `mainFolder` also includes yt-dlp.exe, ffmpeg.exe, ffprobe.exe (not 100% sure the last one is necessary)\n\n4. fill `powershellSciptName` with [this pasteBin](https://pastebin.com/zFPU7f1M)\n\n**PowerShell script:**\n\nReplace the following:\n\n`&lt;browser&gt;` - use the browser you have logged into youtube, or you can follow [this comment](https://www.reddit.com/r/youtubedl/comments/rq0zms/ytdlp_how_do_i_use_the_cookie_function/hqac3ic/)\n\n`&lt;destinationDirectory&gt;` - where you want the files to finally end up\n\n`&lt;downloadDirectory&gt;` - where to initially download the files to\n\nThe following are my own options, feel free to adjust as you like\n\n `--match-filter \"!is_live &amp; !post_live &amp; !was_live\" ` - doesn't download any live videos \n\n`notificationTitle` - Change to whatever you want the notification to say\n\n`-o \"$downloadDir\\[%(channel)s] - %(title)s.%(ext)s\" :ytsubs://user/` - this is how the files will be organized and names formatted. Feel free to adjust to your liking. yt-dlp's github will help if you need guidance\n\nmoving the items is not mandatory - I like to download first to my C drive, then move them all to my NAS. Since I run this every five minutes, it doesn't matter.\n\n**vbsScript**\n\nCopy this:\n\n    Set objShell = CreateObject(\"WScript.Shell\")\n    \n    objShell.Run \"powershell.exe -ExecutionPolicy Bypass -WindowStyle Hidden -File \"\"&lt;pathToMainScript&gt;\"\"\", 0, True\n\nreplace `&lt;pathToMainScript&gt;`with the absolute path to your powershell script.\n\n\n#####**Automating the script** \nThis was fairly frustrating because the powershell window would popup every 5 minutes, even if you set window to hidden in the arguments. That's why you make the vbs script, as it will actually run silently\n\n1. open Task Scheduler\n2. click the arow to expand the `Task Scheduler Library` in the lefthand directory\n3. It's advisable to create your own folder for your own tasks if you haven't already. Select the Task Scheduler Library. select `Action &gt; New Folder...` from the menu bar. Name how you like.\n4. With your new folder selected, select `Create Task` from the Action pane on the right hand side.\n5. Name however you like\n6. Go to triggers tab. This will be where you select your preferred interval. To run every 5 minutes, I've created 3 triggers. one that runs daily at 12:00:00am, one that runs on startup, and one that runs when the task is altered. On each of these I have it set to run every 5 minutes.\n7. Go to the Actions tab. This will be where you call the vbs script, which in turn calls the powershell script.\n8. under program/script, enter the following: `C:\\Windows\\System32\\wscript.exe`\n9. under add arguments enter `\"&lt;pathToVBScript&gt;\"`\n10. under Start In enter: `&lt;pathToMainFolder&gt;`\n11. Go to the settings tab. check `Run task as soon as possible after a scheduled start is missed` select `Queue a new instance` for the bottom option: `If the task is already running, then the following rule applies`\n12. hit OK, then select Run from the Action pane.\n\n\nThat's it! There's some jank but like I said, I've already spent way too long on this. Hopefully this helps you out! \n\nA couple improvements I'd like to make eventually (very open to help here):\n\n*  click on the notification to open the playlist - should open automatically in the m3u associated player.\n*  better file organization\n* make a gui to make it easier to run, and potentially convert from windows task scheduler task to a daemon or service with option to adjust frequency of checks\n* any of your suggestions!\n\nI'm still really new to this, so I'm happy to hear any suggestions for improvements!", "author_fullname": "t2_37kkn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Auto download latest youtube videos from your subscriptions, with options and notification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ualom", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679169191.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679102795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;ve been working on this script all week. I literally thought it would take a few hours and it&amp;#39;s consumed every hour of this past week. &lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve made a script in powershell that uses yt-dlp to download the latest youtube videos from your subscriptions, creates a playlist from all the files in the resulting folder, and creates a notification showing the names of the channels from the latest downloads. &lt;/p&gt;\n\n&lt;p&gt;Note, all of this can be modified fairly straightforward.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Create folder to hold everything. &amp;lt;mainFolder&amp;gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;create &amp;lt;powershellScriptName&amp;gt;.ps1, &amp;lt;vbsScriptName&amp;gt;.vbs in &lt;code&gt;mainFolder&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;make sure &lt;code&gt;mainFolder&lt;/code&gt; also includes yt-dlp.exe, ffmpeg.exe, ffprobe.exe (not 100% sure the last one is necessary)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;fill &lt;code&gt;powershellSciptName&lt;/code&gt; with &lt;a href=\"https://pastebin.com/zFPU7f1M\"&gt;this pasteBin&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;PowerShell script:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Replace the following:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;browser&amp;gt;&lt;/code&gt; - use the browser you have logged into youtube, or you can follow &lt;a href=\"https://www.reddit.com/r/youtubedl/comments/rq0zms/ytdlp_how_do_i_use_the_cookie_function/hqac3ic/\"&gt;this comment&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;destinationDirectory&amp;gt;&lt;/code&gt; - where you want the files to finally end up&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;downloadDirectory&amp;gt;&lt;/code&gt; - where to initially download the files to&lt;/p&gt;\n\n&lt;p&gt;The following are my own options, feel free to adjust as you like&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--match-filter &amp;quot;!is_live &amp;amp; !post_live &amp;amp; !was_live&amp;quot;&lt;/code&gt; - doesn&amp;#39;t download any live videos &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;notificationTitle&lt;/code&gt; - Change to whatever you want the notification to say&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;-o &amp;quot;$downloadDir\\[%(channel)s] - %(title)s.%(ext)s&amp;quot; :ytsubs://user/&lt;/code&gt; - this is how the files will be organized and names formatted. Feel free to adjust to your liking. yt-dlp&amp;#39;s github will help if you need guidance&lt;/p&gt;\n\n&lt;p&gt;moving the items is not mandatory - I like to download first to my C drive, then move them all to my NAS. Since I run this every five minutes, it doesn&amp;#39;t matter.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;vbsScript&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Copy this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Set objShell = CreateObject(&amp;quot;WScript.Shell&amp;quot;)\n\nobjShell.Run &amp;quot;powershell.exe -ExecutionPolicy Bypass -WindowStyle Hidden -File &amp;quot;&amp;quot;&amp;lt;pathToMainScript&amp;gt;&amp;quot;&amp;quot;&amp;quot;, 0, True\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;replace &lt;code&gt;&amp;lt;pathToMainScript&amp;gt;&lt;/code&gt;with the absolute path to your powershell script.&lt;/p&gt;\n\n&lt;h5&gt;&lt;strong&gt;Automating the script&lt;/strong&gt;&lt;/h5&gt;\n\n&lt;p&gt;This was fairly frustrating because the powershell window would popup every 5 minutes, even if you set window to hidden in the arguments. That&amp;#39;s why you make the vbs script, as it will actually run silently&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;open Task Scheduler&lt;/li&gt;\n&lt;li&gt;click the arow to expand the &lt;code&gt;Task Scheduler Library&lt;/code&gt; in the lefthand directory&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s advisable to create your own folder for your own tasks if you haven&amp;#39;t already. Select the Task Scheduler Library. select &lt;code&gt;Action &amp;gt; New Folder...&lt;/code&gt; from the menu bar. Name how you like.&lt;/li&gt;\n&lt;li&gt;With your new folder selected, select &lt;code&gt;Create Task&lt;/code&gt; from the Action pane on the right hand side.&lt;/li&gt;\n&lt;li&gt;Name however you like&lt;/li&gt;\n&lt;li&gt;Go to triggers tab. This will be where you select your preferred interval. To run every 5 minutes, I&amp;#39;ve created 3 triggers. one that runs daily at 12:00:00am, one that runs on startup, and one that runs when the task is altered. On each of these I have it set to run every 5 minutes.&lt;/li&gt;\n&lt;li&gt;Go to the Actions tab. This will be where you call the vbs script, which in turn calls the powershell script.&lt;/li&gt;\n&lt;li&gt;under program/script, enter the following: &lt;code&gt;C:\\Windows\\System32\\wscript.exe&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;under add arguments enter &lt;code&gt;&amp;quot;&amp;lt;pathToVBScript&amp;gt;&amp;quot;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;under Start In enter: &lt;code&gt;&amp;lt;pathToMainFolder&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Go to the settings tab. check &lt;code&gt;Run task as soon as possible after a scheduled start is missed&lt;/code&gt; select &lt;code&gt;Queue a new instance&lt;/code&gt; for the bottom option: &lt;code&gt;If the task is already running, then the following rule applies&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;hit OK, then select Run from the Action pane.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That&amp;#39;s it! There&amp;#39;s some jank but like I said, I&amp;#39;ve already spent way too long on this. Hopefully this helps you out! &lt;/p&gt;\n\n&lt;p&gt;A couple improvements I&amp;#39;d like to make eventually (very open to help here):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; click on the notification to open the playlist - should open automatically in the m3u associated player.&lt;/li&gt;\n&lt;li&gt; better file organization&lt;/li&gt;\n&lt;li&gt;make a gui to make it easier to run, and potentially convert from windows task scheduler task to a daemon or service with option to adjust frequency of checks&lt;/li&gt;\n&lt;li&gt;any of your suggestions!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m still really new to this, so I&amp;#39;m happy to hear any suggestions for improvements!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;v=enabled&amp;s=decc328886393c0699bb01cf9d08b602f60525c8", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cda4e996a0e1c77c65ec3810a634071f4573481", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ualom", "is_robot_indexable": true, "report_reasons": null, "author": "MonkAndCanatella", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ualom/auto_download_latest_youtube_videos_from_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ualom/auto_download_latest_youtube_videos_from_your/", "subreddit_subscribers": 673632, "created_utc": 1679102795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_16cmrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: Hachette v. Internet Archive. Oral Arguments at Southern District of New York on Monday(2023-03-20 at 1PM ET OR 6 PM UTC). This hearing is happening via telephone. You can join via 1-888-363-4749, with access code 8140049.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 100, "top_awarded_type": null, "hide_score": false, "name": "t3_11utujv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0Qpq9etcpLnkqm670muYjirhW-oPHpkaup77c1Ui4xI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679159216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.archive.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.archive.org/2023/03/17/heres-how-to-participate-in-mondays-oral-arguments/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?auto=webp&amp;v=enabled&amp;s=e7232a685c447dbfd10e31e6fa562aebba34ee32", "width": 1000, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58e6e7603724de9b6bad1f33bc2f7c258e82b315", "width": 108, "height": 77}, {"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3c72a8b6e89f24e8f5ff96258e95c342849975c", "width": 216, "height": 155}, {"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2e7c884ca760a347c935db701b0aaef8478ae8c", "width": 320, "height": 230}, {"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a30468c80ebd064e80f36b9d91a6070466083a5", "width": 640, "height": 460}, {"url": "https://external-preview.redd.it/QD1Q2vl2A2j5LYfwbUVx13c0c5dHGq46vQkgwgwTMxo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=264582fefb9f3dd77f1f3c93f4f4da1c9815aedf", "width": 960, "height": 691}], "variants": {}, "id": "iGhrtRYFD0Ku0C467L1P1D6b9k_N6WUieUKRTXoKgZw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11utujv", "is_robot_indexable": true, "report_reasons": null, "author": "rpollost", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11utujv/psa_hachette_v_internet_archive_oral_arguments_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.archive.org/2023/03/17/heres-how-to-participate-in-mondays-oral-arguments/", "subreddit_subscribers": 673632, "created_utc": 1679159216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_baej04p7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Locally cheapest 4TB pro NVMe TLC drive. What's the catch? People say it runs hot.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 127, "top_awarded_type": null, "hide_score": false, "name": "t3_11ud6wv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GhBwijbcDpwKQhZb5BddanrIdN42gijnIL2w6jUerLM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679110218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/x2hkh6cv2foa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?auto=webp&amp;v=enabled&amp;s=1460986d1696cf1ae8c3e86a3eef2fab3822bae4", "width": 940, "height": 855}, "resolutions": [{"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be8440724585a44a7efea6b65cb8d68f0c9c8b84", "width": 108, "height": 98}, {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=911e433563111cc4bfb63cd8500189bf038c343a", "width": 216, "height": 196}, {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb110f2ba1c09e2d287a5d85158053ed6f141127", "width": 320, "height": 291}, {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e30c04109c1d5317db258a670a3413f1ef312f2", "width": 640, "height": 582}], "variants": {}, "id": "NHDqbeaHaJPCSNVmOLmnr2oyYe1OwYnbFQ7G9Ugd9H0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11ud6wv", "is_robot_indexable": true, "report_reasons": null, "author": "voinian", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ud6wv/locally_cheapest_4tb_pro_nvme_tlc_drive_whats_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/x2hkh6cv2foa1.png", "subreddit_subscribers": 673632, "created_utc": 1679110218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A triple layer 100GB BD-R disc fits 33.3GB onto a single layer.  Anyone know why there aren't any single layer 33GB or dual layer 66GB BD-R discs?  Always been curious about this.", "author_fullname": "t2_mgmtm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Are There No 33GB or 66GB BD-R Discs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uecgk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679113799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A triple layer 100GB BD-R disc fits 33.3GB onto a single layer.  Anyone know why there aren&amp;#39;t any single layer 33GB or dual layer 66GB BD-R discs?  Always been curious about this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11uecgk", "is_robot_indexable": true, "report_reasons": null, "author": "HarryMuscle", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uecgk/why_are_there_no_33gb_or_66gb_bdr_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uecgk/why_are_there_no_33gb_or_66gb_bdr_discs/", "subreddit_subscribers": 673632, "created_utc": 1679113799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is a bit of an unconventional ask that I haven't seen in other posts about Spotify downloads.  \n\nI pay for spotify premium (student), and I'm quite happy with it for the sheer number of hours I use it, so I really have no need for local copies currently. HOWEVER- I'd like to download *lists* of the songs in my playlists into some kind of parseable text file per-playlist, containing song name, artist, maybe release date/runtime, etc.  \n\nI've amassed about 1600 songs so far in various playlists with a lot of \"mood specialization\" for each list. Making list backups of just the song titles is good enough for me to be able to recreate the same playlists from alternate sources if Spotify is to ever kick the bucket. Does a tool like this exist? I tried the old \"CTRL-A/CTRL-C\" trick listed in posts a few years ago but Spotify now only provides direct Spotify links instead of the name and artist when you do that.", "author_fullname": "t2_grolz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading spotify playlists - but Just the Names?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ufysj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679119150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a bit of an unconventional ask that I haven&amp;#39;t seen in other posts about Spotify downloads.  &lt;/p&gt;\n\n&lt;p&gt;I pay for spotify premium (student), and I&amp;#39;m quite happy with it for the sheer number of hours I use it, so I really have no need for local copies currently. HOWEVER- I&amp;#39;d like to download &lt;em&gt;lists&lt;/em&gt; of the songs in my playlists into some kind of parseable text file per-playlist, containing song name, artist, maybe release date/runtime, etc.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve amassed about 1600 songs so far in various playlists with a lot of &amp;quot;mood specialization&amp;quot; for each list. Making list backups of just the song titles is good enough for me to be able to recreate the same playlists from alternate sources if Spotify is to ever kick the bucket. Does a tool like this exist? I tried the old &amp;quot;CTRL-A/CTRL-C&amp;quot; trick listed in posts a few years ago but Spotify now only provides direct Spotify links instead of the name and artist when you do that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "still think Betamax shoulda won ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ufysj", "is_robot_indexable": true, "report_reasons": null, "author": "empirebuilder1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11ufysj/downloading_spotify_playlists_but_just_the_names/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ufysj/downloading_spotify_playlists_but_just_the_names/", "subreddit_subscribers": 673632, "created_utc": 1679119150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all! \n\nI have [this](https://www.fractal-design.com/products/cases/define/define-7-xl/black-tg-dark-tint/) case, the fractal Define 7 XL. I got it really low cost used with no HDD racks. I'm wondering if anyone knows of something that would be compatible for the front of the case near the front intake fans for about 4-5 HDD's?\n\nThanks!", "author_fullname": "t2_igthflmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an internal case mount for HDD's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u9h6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679100000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! &lt;/p&gt;\n\n&lt;p&gt;I have &lt;a href=\"https://www.fractal-design.com/products/cases/define/define-7-xl/black-tg-dark-tint/\"&gt;this&lt;/a&gt; case, the fractal Define 7 XL. I got it really low cost used with no HDD racks. I&amp;#39;m wondering if anyone knows of something that would be compatible for the front of the case near the front intake fans for about 4-5 HDD&amp;#39;s?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?auto=webp&amp;v=enabled&amp;s=48ad768a2ef3270064417e446895481cc66bc164", "width": 2560, "height": 1810}, "resolutions": [{"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bfedd6360c8bbea98a53921264465aa41d5e444", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b07f44f5a2eba78790934483cb735a877578e403", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca7e4a8c36ec0861796e0b7acf41095d26e7d543", "width": 320, "height": 226}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2fc4ebe975020d0ebaff111075ac1aeab7252e9", "width": 640, "height": 452}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a8c7cc9b3420c41669a397a52b7ebd57ae155d5", "width": 960, "height": 678}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7177bb9403f795eec118516462ad239ef3cf4f8", "width": 1080, "height": 763}], "variants": {}, "id": "Bn8gSTySoqn4Q4oRa1BLtFVkcDdV-5abpUxWxfiR8_A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11u9h6h", "is_robot_indexable": true, "report_reasons": null, "author": "Novel_Excitement5693", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u9h6h/looking_for_an_internal_case_mount_for_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u9h6h/looking_for_an_internal_case_mount_for_hdds/", "subreddit_subscribers": 673632, "created_utc": 1679100000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What are some good tools to quickly and easily archive a forum like Tapatalk for example? : [https://www.tapatalk.com/](https://www.tapatalk.com/)", "author_fullname": "t2_7gl7ysbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best tools to archive a forum quickly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ujp39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679132792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some good tools to quickly and easily archive a forum like Tapatalk for example? : &lt;a href=\"https://www.tapatalk.com/\"&gt;https://www.tapatalk.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ujp39", "is_robot_indexable": true, "report_reasons": null, "author": "DarknessMoonlight", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ujp39/what_are_the_best_tools_to_archive_a_forum_quickly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ujp39/what_are_the_best_tools_to_archive_a_forum_quickly/", "subreddit_subscribers": 673632, "created_utc": 1679132792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I saw it on 4chan a few weeks ago but forgot to save it. I believe it was a seperate web app that improved searching for youtube videos by not suppressing old results. Does anyone know the name or something similar?", "author_fullname": "t2_5j6uzuz4m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Youtube search tool that doesn't suppress old videos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11v052x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679172500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw it on 4chan a few weeks ago but forgot to save it. I believe it was a seperate web app that improved searching for youtube videos by not suppressing old results. Does anyone know the name or something similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11v052x", "is_robot_indexable": true, "report_reasons": null, "author": "PetroleumHousing", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11v052x/youtube_search_tool_that_doesnt_suppress_old/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11v052x/youtube_search_tool_that_doesnt_suppress_old/", "subreddit_subscribers": 673632, "created_utc": 1679172500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm doing a little personal thing and I'm wondering if there is a way to scrape and download ALL images from website... not just a page, or a profile, THE WHOLE WEBSITE.I know it'd probably take a while to download all of it, and also probably I'd need a lot of TB for it. But I'm wondering anyway.\n\nMost tutorials I see show how to scrap it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).\n\n&amp;#x200B;\n\nI've been doing it manually so far, but it's taking a LONG time and also I don't wanna get carpal tunnel, so I got myself thinking 'there must be an easier way to do it!'.\n\nKnowing the internet as I do I also know that if I do scrape a website, no matter which website it is, I'm gonna end up getting loads of unwanted images, like graphic imagery and porn and... less then legal stuff I suppose. If there's a way to filter out those I'd also like to know, otherwise I'd just have to delete them out mannually.\n\nI'd also like to convert all images to .pngm but if not possible I'm willing to do it mannually.\n\nMost tutorials I see show how to scrape it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).", "author_fullname": "t2_65yetpx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to scrape ALL images from a website (not a single page, the whole website)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uq1z6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679150000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing a little personal thing and I&amp;#39;m wondering if there is a way to scrape and download ALL images from website... not just a page, or a profile, THE WHOLE WEBSITE.I know it&amp;#39;d probably take a while to download all of it, and also probably I&amp;#39;d need a lot of TB for it. But I&amp;#39;m wondering anyway.&lt;/p&gt;\n\n&lt;p&gt;Most tutorials I see show how to scrap it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been doing it manually so far, but it&amp;#39;s taking a LONG time and also I don&amp;#39;t wanna get carpal tunnel, so I got myself thinking &amp;#39;there must be an easier way to do it!&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Knowing the internet as I do I also know that if I do scrape a website, no matter which website it is, I&amp;#39;m gonna end up getting loads of unwanted images, like graphic imagery and porn and... less then legal stuff I suppose. If there&amp;#39;s a way to filter out those I&amp;#39;d also like to know, otherwise I&amp;#39;d just have to delete them out mannually.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d also like to convert all images to .pngm but if not possible I&amp;#39;m willing to do it mannually.&lt;/p&gt;\n\n&lt;p&gt;Most tutorials I see show how to scrape it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uq1z6", "is_robot_indexable": true, "report_reasons": null, "author": "abcbibi", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uq1z6/is_there_a_way_to_scrape_all_images_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uq1z6/is_there_a_way_to_scrape_all_images_from_a/", "subreddit_subscribers": 673632, "created_utc": 1679150000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All,\n\nI used to have a Mac system and used Carbon Copy.\n\nI have a system with a 500GB SSD, and I bought a 1TB SSD. My MB has only one slot, so I'd like to carbon copy the content of the 500GB (SO and everything in it) into the 1TB so that I don't have to reinstall everything.\n\nDo you have any suggestions on windows 10?\n\nThanks", "author_fullname": "t2_2rqya8a6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Carbon copy a 500 GB SSD into a bigger one", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11upm44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679148862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I used to have a Mac system and used Carbon Copy.&lt;/p&gt;\n\n&lt;p&gt;I have a system with a 500GB SSD, and I bought a 1TB SSD. My MB has only one slot, so I&amp;#39;d like to carbon copy the content of the 500GB (SO and everything in it) into the 1TB so that I don&amp;#39;t have to reinstall everything.&lt;/p&gt;\n\n&lt;p&gt;Do you have any suggestions on windows 10?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11upm44", "is_robot_indexable": true, "report_reasons": null, "author": "ff8mania", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11upm44/carbon_copy_a_500_gb_ssd_into_a_bigger_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11upm44/carbon_copy_a_500_gb_ssd_into_a_bigger_one/", "subreddit_subscribers": 673632, "created_utc": 1679148862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I remember couple years ago an older build leaked. But MVG made [video](https://www.youtube.com/watch?v=1fwSR8gjrCk) with newer version that sound impressive. Have these been preserved yet?", "author_fullname": "t2_w6lyzny8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has Nintendo's official NSO GBA/GB emulators been preserved yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ual61", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679102755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember couple years ago an older build leaked. But MVG made &lt;a href=\"https://www.youtube.com/watch?v=1fwSR8gjrCk\"&gt;video&lt;/a&gt; with newer version that sound impressive. Have these been preserved yet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?auto=webp&amp;v=enabled&amp;s=a6b9844c7c72095912b81476a84e34b8abe06c63", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c04916d2bc6f2aab163bd7f2e61be73fff275a6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9abb6417309562656febad8fc2a317fe469bcbe6", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f073e41099e13b18258feafcf673dd1693d90b17", "width": 320, "height": 240}], "variants": {}, "id": "VTvIbMpG2MB-WJO9Ipc_obgXKuScYk-EBzz_frA02Oo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ual61", "is_robot_indexable": true, "report_reasons": null, "author": "JebryyathHS", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ual61/has_nintendos_official_nso_gbagb_emulators_been/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ual61/has_nintendos_official_nso_gbagb_emulators_been/", "subreddit_subscribers": 673632, "created_utc": 1679102755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is this the original quality when a picture is posted that now instagram is deciding to give us or is it some kind of upscale? Even pictures from as far back as 2019 are showing 1440p now, but they don't really look any clearer at all? idk im no expert, thats why im asking lol", "author_fullname": "t2_13xvgu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone notice Instagram pictures \"upgraded\" from 1080p to 1440p?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11v3r9e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679180479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is this the original quality when a picture is posted that now instagram is deciding to give us or is it some kind of upscale? Even pictures from as far back as 2019 are showing 1440p now, but they don&amp;#39;t really look any clearer at all? idk im no expert, thats why im asking lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11v3r9e", "is_robot_indexable": true, "report_reasons": null, "author": "lolflesh", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11v3r9e/anyone_notice_instagram_pictures_upgraded_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11v3r9e/anyone_notice_instagram_pictures_upgraded_from/", "subreddit_subscribers": 673632, "created_utc": 1679180479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After a pause from organizing my data, I'm finally catching up on backing up the last 1-2 years of iPhone photos, including big life events like wedding, honeymoon, travel, etc.\n\nPrevious process: export max-resolution JPEGs directly from Photos app on my Macbook, batch edit file names with Adobe Bridge (sequential date/time/content naming scheme), &amp; transfer to multiple backup systems.\n\nHowever, in terms of long-term longevity, has HEIC/HEIF format caught on enough that folks are just leaving iPhone photos in this format for long-term storage? My primary concern is preserving the quality for optimal future viewing/printing of these images, as well as keeping the attached location/date/time data intact. I don't have a ton of live photos, so less concerned about that, but I suppose live photos are a perk of HEIC.\n\nNot inherently opposed to continuing to convert to JPEGs, just wondering if this is an unnecessary step at this point in the HEIC/HEIF game &amp; what others are doing with these files.", "author_fullname": "t2_3bkjlsy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archiving iPhone pics: JPEG vs HEIF?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11v1a01", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679174554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After a pause from organizing my data, I&amp;#39;m finally catching up on backing up the last 1-2 years of iPhone photos, including big life events like wedding, honeymoon, travel, etc.&lt;/p&gt;\n\n&lt;p&gt;Previous process: export max-resolution JPEGs directly from Photos app on my Macbook, batch edit file names with Adobe Bridge (sequential date/time/content naming scheme), &amp;amp; transfer to multiple backup systems.&lt;/p&gt;\n\n&lt;p&gt;However, in terms of long-term longevity, has HEIC/HEIF format caught on enough that folks are just leaving iPhone photos in this format for long-term storage? My primary concern is preserving the quality for optimal future viewing/printing of these images, as well as keeping the attached location/date/time data intact. I don&amp;#39;t have a ton of live photos, so less concerned about that, but I suppose live photos are a perk of HEIC.&lt;/p&gt;\n\n&lt;p&gt;Not inherently opposed to continuing to convert to JPEGs, just wondering if this is an unnecessary step at this point in the HEIC/HEIF game &amp;amp; what others are doing with these files.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11v1a01", "is_robot_indexable": true, "report_reasons": null, "author": "prairiepoppins", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11v1a01/archiving_iphone_pics_jpeg_vs_heif/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11v1a01/archiving_iphone_pics_jpeg_vs_heif/", "subreddit_subscribers": 673632, "created_utc": 1679174554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to run a shadow library: operations at Anna\u2019s Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_11v0h4e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_ucst1pa3", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EQn-7ZWQ5bSD73SrMGYMYTtcDzanG4VxNFj1lVBTD1E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Annas_Archive", "selftext": "", "author_fullname": "t2_ucst1pa3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wrote a little blog post: \u201cHow to run a shadow library: operations at Anna\u2019s Archive\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Annas_Archive", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_11uzu6c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EQn-7ZWQ5bSD73SrMGYMYTtcDzanG4VxNFj1lVBTD1E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1679171999.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "annas-blog.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://annas-blog.org/how-to-run-a-shadow-library.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?auto=webp&amp;v=enabled&amp;s=5f748ed4db1a74a4a62a32dd6334b7d1e5e39d11", "width": 1900, "height": 1405}, "resolutions": [{"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=230fa2eaa0ce1286b752352371c4c35317ca49a0", "width": 108, "height": 79}, {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b325b76aa71c245c57b6ccb53bf79450ba5ba52", "width": 216, "height": 159}, {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7284618380e926ba6e9903d93790c87b1cb7899f", "width": 320, "height": 236}, {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d97e14823e52bea8034996eb6f8bcadf325a3cb", "width": 640, "height": 473}, {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be6fff6720be56cbb29461dac83ac6f1cd1794bf", "width": 960, "height": 709}, {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=887ff70cb1639e64b9f15252ee135c64ca476c3f", "width": 1080, "height": 798}], "variants": {}, "id": "p1MDhi0btJRpNpfOCsR-Venq_CIpsLwKBDxTrp8p9Nk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_7otj4s", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11uzu6c", "is_robot_indexable": true, "report_reasons": null, "author": "AnnaArchivist", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Annas_Archive/comments/11uzu6c/wrote_a_little_blog_post_how_to_run_a_shadow/", "parent_whitelist_status": null, "stickied": false, "url": "https://annas-blog.org/how-to-run-a-shadow-library.html", "subreddit_subscribers": 4775, "created_utc": 1679171999.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1679173073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "annas-blog.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://annas-blog.org/how-to-run-a-shadow-library.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?auto=webp&amp;v=enabled&amp;s=5f748ed4db1a74a4a62a32dd6334b7d1e5e39d11", "width": 1900, "height": 1405}, "resolutions": [{"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=230fa2eaa0ce1286b752352371c4c35317ca49a0", "width": 108, "height": 79}, {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b325b76aa71c245c57b6ccb53bf79450ba5ba52", "width": 216, "height": 159}, {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7284618380e926ba6e9903d93790c87b1cb7899f", "width": 320, "height": 236}, {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d97e14823e52bea8034996eb6f8bcadf325a3cb", "width": 640, "height": 473}, {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be6fff6720be56cbb29461dac83ac6f1cd1794bf", "width": 960, "height": 709}, {"url": "https://external-preview.redd.it/TPKBiy33gE2ThZ-EF3BY7J4fbxwf8whTIGKN0klHrHE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=887ff70cb1639e64b9f15252ee135c64ca476c3f", "width": 1080, "height": 798}], "variants": {}, "id": "p1MDhi0btJRpNpfOCsR-Venq_CIpsLwKBDxTrp8p9Nk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11v0h4e", "is_robot_indexable": true, "report_reasons": null, "author": "AnnaArchivist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_11uzu6c", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11v0h4e/how_to_run_a_shadow_library_operations_at_annas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://annas-blog.org/how-to-run-a-shadow-library.html", "subreddit_subscribers": 673632, "created_utc": 1679173073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm running into a problem where my motherboard has more SATA ports to add more drives, but my tiny case can't fit more drives. I'm thinking of getting some eSATA (+power) adapters to convert the internal SATA ports to eSATA ports in an expansion slot, and then connect the eSATA ports to an external enclosure.\n\nMy understanding of eSATA is, it should be possible to convert SATA to eSATA, and from eSATA back to SATA, using purely passive adapters. This is essentially changing the shape of the connector, without changing the signaling, right? So I looked around for passive eSATA enclosures. Something that would just be holding the drives, with a passive converter that takes eSATA cable on the outside and plug into SATA + power on the inside. This enclosure wouldn't even need to be plugged in, since all power is coming from eSATAp. However, it doesn't look like something like this exists. The only eSATA enclosures I've been able to find are combination USB+eSATA enclosures. They are all powered and active enclosures, with a chipset doing the signal conversion, exactly what I didn't want.\n\nIs it because my use case is so niche that nobody makes something like that? Or am I just not using the right search terms? Or is it possible that my understanding of eSATA is incorrect, and you do indeed need an active converter to plug SATA drives into an eSATA port?", "author_fullname": "t2_gvq3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Passive eSATA enclosures?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uzlmt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679171515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running into a problem where my motherboard has more SATA ports to add more drives, but my tiny case can&amp;#39;t fit more drives. I&amp;#39;m thinking of getting some eSATA (+power) adapters to convert the internal SATA ports to eSATA ports in an expansion slot, and then connect the eSATA ports to an external enclosure.&lt;/p&gt;\n\n&lt;p&gt;My understanding of eSATA is, it should be possible to convert SATA to eSATA, and from eSATA back to SATA, using purely passive adapters. This is essentially changing the shape of the connector, without changing the signaling, right? So I looked around for passive eSATA enclosures. Something that would just be holding the drives, with a passive converter that takes eSATA cable on the outside and plug into SATA + power on the inside. This enclosure wouldn&amp;#39;t even need to be plugged in, since all power is coming from eSATAp. However, it doesn&amp;#39;t look like something like this exists. The only eSATA enclosures I&amp;#39;ve been able to find are combination USB+eSATA enclosures. They are all powered and active enclosures, with a chipset doing the signal conversion, exactly what I didn&amp;#39;t want.&lt;/p&gt;\n\n&lt;p&gt;Is it because my use case is so niche that nobody makes something like that? Or am I just not using the right search terms? Or is it possible that my understanding of eSATA is incorrect, and you do indeed need an active converter to plug SATA drives into an eSATA port?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uzlmt", "is_robot_indexable": true, "report_reasons": null, "author": "induality", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uzlmt/passive_esata_enclosures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uzlmt/passive_esata_enclosures/", "subreddit_subscribers": 673632, "created_utc": 1679171515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a raid 0 built that will be used for a cache drive for some extremely large video edits I am helping my father with for archiving old film reels and negatives. That being said, I am new to the world of data hording and really finally needing speed for quicker writes for these initially uncompressed videos.\n\nThis cache will push via fiberoptics to a dell storage server purchased from a work auction and replaced with brand new 10 TB drives for local storage and backed up to a google cloud instance. (yay educators discounts!)\n\nThe machine doing the work is a custom build with quadro graphics card for video editing and all that jazz. This is a really neat project as we're going back to family photos from the 1800's and using stable diffusion to do restoration work in gimp. When we decided to add on the video processing, we realized we were going to need a bigger boat.\n\nWindows System Assessment Tool\n\n\\&gt; Running: Feature Enumeration ''\n\n\\&gt; Run Time 00:00:00.00\n\n\\&gt; Running: Storage Assessment '-ran -read -n 0'\n\n\\&gt; Run Time 00:00:00.13\n\n\\&gt; Running: Storage Assessment '-seq -read -n 0'\n\n\\&gt; Run Time 00:00:01.39\n\n\\&gt; Running: Storage Assessment '-seq -write -drive C:'\n\n\\&gt; Run Time 00:00:00.91\n\n\\&gt; Running: Storage Assessment '-flush -drive C: -seq'\n\n\\&gt; Run Time 00:00:00.38\n\n\\&gt; Running: Storage Assessment '-flush -drive C: -ran'\n\n\\&gt; Run Time 00:00:00.38\n\n\\&gt; Dshow Video Encode Time                      0.00000 s\n\n\\&gt; Dshow Video Decode Time                      0.00000 s\n\n\\&gt; Media Foundation Decode Time                 0.00000 s\n\n\\&gt; Disk  Random 16.0 Read                       1485.41 MB/s          9.1\n\n\\&gt; Disk  Sequential 64.0 Read                   5819.14 MB/s          9.8\n\n\\&gt; Disk  Sequential 64.0 Write                  3220.95 MB/s          9.3\n\n\\&gt; Average Read Time with Sequential Writes     0.268 ms          8.5\n\n\\&gt; Latency: 95th Percentile                     0.393 ms          8.7\n\n\\&gt; Latency: Maximum                             5.680 ms          8.4\n\n\\&gt; Average Read Time with Random Writes         0.283 ms          8.8\n\n\\&gt; Total Run Time 00:00:03.30", "author_fullname": "t2_415ia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finally needing the actual speed of an SSD; is this good?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uyak2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679168795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a raid 0 built that will be used for a cache drive for some extremely large video edits I am helping my father with for archiving old film reels and negatives. That being said, I am new to the world of data hording and really finally needing speed for quicker writes for these initially uncompressed videos.&lt;/p&gt;\n\n&lt;p&gt;This cache will push via fiberoptics to a dell storage server purchased from a work auction and replaced with brand new 10 TB drives for local storage and backed up to a google cloud instance. (yay educators discounts!)&lt;/p&gt;\n\n&lt;p&gt;The machine doing the work is a custom build with quadro graphics card for video editing and all that jazz. This is a really neat project as we&amp;#39;re going back to family photos from the 1800&amp;#39;s and using stable diffusion to do restoration work in gimp. When we decided to add on the video processing, we realized we were going to need a bigger boat.&lt;/p&gt;\n\n&lt;p&gt;Windows System Assessment Tool&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Running: Feature Enumeration &amp;#39;&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Run Time 00:00:00.00&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Running: Storage Assessment &amp;#39;-ran -read -n 0&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Run Time 00:00:00.13&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Running: Storage Assessment &amp;#39;-seq -read -n 0&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Run Time 00:00:01.39&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Running: Storage Assessment &amp;#39;-seq -write -drive C:&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Run Time 00:00:00.91&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Running: Storage Assessment &amp;#39;-flush -drive C: -seq&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Run Time 00:00:00.38&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Running: Storage Assessment &amp;#39;-flush -drive C: -ran&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Run Time 00:00:00.38&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Dshow Video Encode Time                      0.00000 s&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Dshow Video Decode Time                      0.00000 s&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Media Foundation Decode Time                 0.00000 s&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Disk  Random 16.0 Read                       1485.41 MB/s          9.1&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Disk  Sequential 64.0 Read                   5819.14 MB/s          9.8&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Disk  Sequential 64.0 Write                  3220.95 MB/s          9.3&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Average Read Time with Sequential Writes     0.268 ms          8.5&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Latency: 95th Percentile                     0.393 ms          8.7&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Latency: Maximum                             5.680 ms          8.4&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Average Read Time with Random Writes         0.283 ms          8.8&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Total Run Time 00:00:03.30&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uyak2", "is_robot_indexable": true, "report_reasons": null, "author": "pDizzle", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uyak2/finally_needing_the_actual_speed_of_an_ssd_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uyak2/finally_needing_the_actual_speed_of_an_ssd_is/", "subreddit_subscribers": 673632, "created_utc": 1679168795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nI have a My Book Duo - 20TB (two 10TB drives) and I'd like to know if it would be possible to purchase two 20TB Red Nas drives to swap out for each of the 10TB drives currently in the My Book Duo enclosure. I effectively want to double the storage, but I didn't know if the My Book Duo was swappable like this or not.", "author_fullname": "t2_bxzwg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Book Duo - Is it possible to swap out drives to increase storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uvcrj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679162742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I have a My Book Duo - 20TB (two 10TB drives) and I&amp;#39;d like to know if it would be possible to purchase two 20TB Red Nas drives to swap out for each of the 10TB drives currently in the My Book Duo enclosure. I effectively want to double the storage, but I didn&amp;#39;t know if the My Book Duo was swappable like this or not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uvcrj", "is_robot_indexable": true, "report_reasons": null, "author": "BlackHarbor", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uvcrj/my_book_duo_is_it_possible_to_swap_out_drives_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uvcrj/my_book_duo_is_it_possible_to_swap_out_drives_to/", "subreddit_subscribers": 673632, "created_utc": 1679162742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there\n\nYou all probably know that Samsung publishes source code (only what they have to by GPL) for their phones here: [https://opensource.samsung.com/uploadList?menuItem=mobile](https://opensource.samsung.com/uploadList?menuItem=mobile)\n\n&amp;#x200B;\n\nI was lucky once and got a complete FMRadio app (com.sec.android.fm) source code in one of the archives, its for Android 6 Marshmallow and its from 2016\n\n&amp;#x200B;\n\nand I was looking maybe, they also published some newer version, but sadly if I look the phones that have FM Radio (SM-G975N for instance) [com.sec.android.fm](https://com.sec.android.fm) is nowhere to be found (I guess I got lucky back then) in their software archive they only publish  jlayer1.0.1.zip \t\n\nwhich is just an open source library that the applicatoin is using\n\n&amp;#x200B;\n\nSo I got the idea to download every phone package (each zip is from 100MB to 500MB but sadly downloads are rate limited to about 100kb which takes a while (but its per file basis, so if you download 5 100MB files (they will all download at the same time)\n\n&amp;#x200B;\n\nSo I was wondering if anyone maybe has a script to download it all, or someone maybe already did that and has a torrent link or something\n\n&amp;#x200B;\n\nI just need to download every archive open it up and check if folder Platform.tar.gz has vendor/samsung/packages/apps/HybridRadio\\*\\*\\*/AndroidManifest.xml or [Android.mk](https://Android.mk) or more then 2 folders inside or HybridRadio folder even exists\n\nHybridRadio\\*\\*\\* is because I found HybridRadio2016\\_M to be the folder name that my source code for Android 6 came from\n\n&amp;#x200B;\n\nM meaning Android version (6 in this case), it can also be P (for Android9), I also saw app names like HybridRadio\\_P.apk\n\nI think the hardest part is downloading, I could probably cobble together a tool that goes through  2601 (if we assume each file is about 500MB in size (which it usualy is not, that translates to about 1.3TB of space if we keep all the files) files and deletes those that don't have HybridRadio folder\n\n&amp;#x200B;\n\nI believe out of 2601 I could get like 2 or maybe 4 matches (maybe 3 has the source code), idk\n\n&amp;#x200B;\n\nSo yea, if anyone has a torrent file or maybe knows whats the best way to automate the downloading and extraxting relevant files, tell me\n\n&amp;#x200B;\n\nThanks for Anwsering and Best Regards", "author_fullname": "t2_13m5l5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to scrape Samsung Open source phone archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uuz4p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679161867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;You all probably know that Samsung publishes source code (only what they have to by GPL) for their phones here: &lt;a href=\"https://opensource.samsung.com/uploadList?menuItem=mobile\"&gt;https://opensource.samsung.com/uploadList?menuItem=mobile&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was lucky once and got a complete FMRadio app (com.sec.android.fm) source code in one of the archives, its for Android 6 Marshmallow and its from 2016&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;and I was looking maybe, they also published some newer version, but sadly if I look the phones that have FM Radio (SM-G975N for instance) &lt;a href=\"https://com.sec.android.fm\"&gt;com.sec.android.fm&lt;/a&gt; is nowhere to be found (I guess I got lucky back then) in their software archive they only publish  jlayer1.0.1.zip     &lt;/p&gt;\n\n&lt;p&gt;which is just an open source library that the applicatoin is using&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I got the idea to download every phone package (each zip is from 100MB to 500MB but sadly downloads are rate limited to about 100kb which takes a while (but its per file basis, so if you download 5 100MB files (they will all download at the same time)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I was wondering if anyone maybe has a script to download it all, or someone maybe already did that and has a torrent link or something&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I just need to download every archive open it up and check if folder Platform.tar.gz has vendor/samsung/packages/apps/HybridRadio***/AndroidManifest.xml or &lt;a href=\"https://Android.mk\"&gt;Android.mk&lt;/a&gt; or more then 2 folders inside or HybridRadio folder even exists&lt;/p&gt;\n\n&lt;p&gt;HybridRadio*** is because I found HybridRadio2016_M to be the folder name that my source code for Android 6 came from&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;M meaning Android version (6 in this case), it can also be P (for Android9), I also saw app names like HybridRadio_P.apk&lt;/p&gt;\n\n&lt;p&gt;I think the hardest part is downloading, I could probably cobble together a tool that goes through  2601 (if we assume each file is about 500MB in size (which it usualy is not, that translates to about 1.3TB of space if we keep all the files) files and deletes those that don&amp;#39;t have HybridRadio folder&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I believe out of 2601 I could get like 2 or maybe 4 matches (maybe 3 has the source code), idk&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So yea, if anyone has a torrent file or maybe knows whats the best way to automate the downloading and extraxting relevant files, tell me&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for Anwsering and Best Regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11uuz4p", "is_robot_indexable": true, "report_reasons": null, "author": "veso266", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uuz4p/how_to_scrape_samsung_open_source_phone_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uuz4p/how_to_scrape_samsung_open_source_phone_archive/", "subreddit_subscribers": 673632, "created_utc": 1679161867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I made the mistake of using up all the 256GB of storage on my phone until I decided to back it up (have been traveling). Since there wasn\u2019t much storage left on the device connecting it via a cable to my PC and going through iTunes failed many many times. I then tried iCloud, ended up paying for the subscription, it took about a month to sync my photos+videos from the phone to the cloud, this freed up some space on my phone since \u201coptimized\u201d versions are stored on the phone and \u201cfull size\u201d versions are on the cloud. When I opened iCloud on my PC via a browser, the info tab confirms an image is a 12MP file but when I try to download I can only get 4MP even though I choose to downlod \u201cfull size\u201d. \n\nI have read several forum posts stating that apple punishes PC users and iCloud doesnt allow full size downloads on Windows. Sounded like a terrible conspiracy theory but I am definitely not seeing a full size image on my PC. I am really pulling my hair on this one. \n\nHow can I get fullsize iPhone photos+videos downloaded onto a PC?\n\nI appreciate any help. Thanks!", "author_fullname": "t2_npzc32a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to back up photos+videos from iPhone to PC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11utv0w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679159247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made the mistake of using up all the 256GB of storage on my phone until I decided to back it up (have been traveling). Since there wasn\u2019t much storage left on the device connecting it via a cable to my PC and going through iTunes failed many many times. I then tried iCloud, ended up paying for the subscription, it took about a month to sync my photos+videos from the phone to the cloud, this freed up some space on my phone since \u201coptimized\u201d versions are stored on the phone and \u201cfull size\u201d versions are on the cloud. When I opened iCloud on my PC via a browser, the info tab confirms an image is a 12MP file but when I try to download I can only get 4MP even though I choose to downlod \u201cfull size\u201d. &lt;/p&gt;\n\n&lt;p&gt;I have read several forum posts stating that apple punishes PC users and iCloud doesnt allow full size downloads on Windows. Sounded like a terrible conspiracy theory but I am definitely not seeing a full size image on my PC. I am really pulling my hair on this one. &lt;/p&gt;\n\n&lt;p&gt;How can I get fullsize iPhone photos+videos downloaded onto a PC?&lt;/p&gt;\n\n&lt;p&gt;I appreciate any help. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11utv0w", "is_robot_indexable": true, "report_reasons": null, "author": "yowhywouldyoudothat", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11utv0w/how_to_back_up_photosvideos_from_iphone_to_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11utv0w/how_to_back_up_photosvideos_from_iphone_to_pc/", "subreddit_subscribers": 673632, "created_utc": 1679159247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI am looking to update my case to get more hdd slots but am not sure how to deal with the backplanes...  \nCurrently I am using a IT mode Dell Perc H310 with two 4x sata breakout cables, so with that and the mobo I am up to 11 drives.\n\nNext I would like to get a 24 drive enclosure (4U) but I see 6g,12g,and sata backplanes and am not sure where to go next in terms of controllers vs mini-sas expanders.  \nI don't really need much in terms of speed (being that it's mostly shucked or WD Red 5400), so I am not sure if the current controller would do the trick (it supports 32 drives). If I stuck with it what expander card would support a backplane with mini-sas x 6 (so I guess the expander would need to be 2 in , 6 out mini-sas)?  \nAny thoughts?\n\nThanks!", "author_fullname": "t2_1rwxyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Case Upgrade Questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ur4d0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679152643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I am looking to update my case to get more hdd slots but am not sure how to deal with the backplanes...&lt;br/&gt;\nCurrently I am using a IT mode Dell Perc H310 with two 4x sata breakout cables, so with that and the mobo I am up to 11 drives.&lt;/p&gt;\n\n&lt;p&gt;Next I would like to get a 24 drive enclosure (4U) but I see 6g,12g,and sata backplanes and am not sure where to go next in terms of controllers vs mini-sas expanders.&lt;br/&gt;\nI don&amp;#39;t really need much in terms of speed (being that it&amp;#39;s mostly shucked or WD Red 5400), so I am not sure if the current controller would do the trick (it supports 32 drives). If I stuck with it what expander card would support a backplane with mini-sas x 6 (so I guess the expander would need to be 2 in , 6 out mini-sas)?&lt;br/&gt;\nAny thoughts?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ur4d0", "is_robot_indexable": true, "report_reasons": null, "author": "hand_in_every_pot", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ur4d0/case_upgrade_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ur4d0/case_upgrade_questions/", "subreddit_subscribers": 673632, "created_utc": 1679152643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nI used to have a PC case that had 4 HDD bays, now the one I have only has 1. Ideally i'd like to keep my 2x 14TB &amp; 2x 10TB drives, but if i have to shrink down to 2x 14TB I will.\n\n&amp;#x200B;\n\nHere is my issue. I used windows to expand/create a raid 0 for each pair. If i plug it loosely into Sata and power Windows detects it. if i use my External HDD enclosure, it doesn't. \n\nI Bought this thinking i could put all 4 in, and windows would still recognize it.\n\n[https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;ref=ppx\\_yo2ov\\_dt\\_b\\_product\\_details](https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details)\n\n&amp;#x200B;\n\nNONE of this data is important...its just alot lol\n\n&amp;#x200B;\n\nis it a bad unit for this? is it not possible? any guidance is appreciated.", "author_fullname": "t2_4e2n8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage help! Windows Extended volume", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uqqok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679151720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I used to have a PC case that had 4 HDD bays, now the one I have only has 1. Ideally i&amp;#39;d like to keep my 2x 14TB &amp;amp; 2x 10TB drives, but if i have to shrink down to 2x 14TB I will.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here is my issue. I used windows to expand/create a raid 0 for each pair. If i plug it loosely into Sata and power Windows detects it. if i use my External HDD enclosure, it doesn&amp;#39;t. &lt;/p&gt;\n\n&lt;p&gt;I Bought this thinking i could put all 4 in, and windows would still recognize it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details\"&gt;https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;NONE of this data is important...its just alot lol&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;is it a bad unit for this? is it not possible? any guidance is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uqqok", "is_robot_indexable": true, "report_reasons": null, "author": "ChiefSpoonS", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uqqok/storage_help_windows_extended_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uqqok/storage_help_windows_extended_volume/", "subreddit_subscribers": 673632, "created_utc": 1679151720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Of the Micro SD card brands I've owned over the years, ALL but one of the Sandisk cards has failed. ALL of the Transcend, Lexar, Kingston, and Samsung cards are still going strong.\n\nOf the Sandisk flash drives I've owned, I've experienced ~50% failure rate on the newer drives (16GB and higher). I just had a (seldom used) Sandisk Ultra Flair 128GB fail last night.\n\n/rant", "author_fullname": "t2_cd00t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: Sandisk SD cards and flash drives suck for reliability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uq21w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679150005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Of the Micro SD card brands I&amp;#39;ve owned over the years, ALL but one of the Sandisk cards has failed. ALL of the Transcend, Lexar, Kingston, and Samsung cards are still going strong.&lt;/p&gt;\n\n&lt;p&gt;Of the Sandisk flash drives I&amp;#39;ve owned, I&amp;#39;ve experienced ~50% failure rate on the newer drives (16GB and higher). I just had a (seldom used) Sandisk Ultra Flair 128GB fail last night.&lt;/p&gt;\n\n&lt;p&gt;/rant&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11uq21w", "is_robot_indexable": true, "report_reasons": null, "author": "Ho_Lee_Fuk", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uq21w/psa_sandisk_sd_cards_and_flash_drives_suck_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uq21w/psa_sandisk_sd_cards_and_flash_drives_suck_for/", "subreddit_subscribers": 673632, "created_utc": 1679150005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm totally new with storage systems. I bought a netapp 4486 jbod connected to my server with a 9200-8e in IT mode. I have starting to populate with services (docker containers) with great use of HDDs and I noticed that server doesn't respond very well. CPU have big wait %  (4% user 8%sys 30%wait). Mem is free not totally used.\n\nWhen I type with keyboard sometimes system stuck for 1/2s and double my letters.. sometime write ten times the same letter. I'm thinking that HBA card is the big problem here. what do you think? So:\n\n1) If i change 9200 with a new 9207 and restart my server... I have to reconfigure entire jbod hdds in my system or nothing have to be done?\n\n2) If I connect both 9207 exit ports to JBOD will help?", "author_fullname": "t2_7xrhh3tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "JBOD: Server slowing down. Changing HBA card", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uk08r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679133768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m totally new with storage systems. I bought a netapp 4486 jbod connected to my server with a 9200-8e in IT mode. I have starting to populate with services (docker containers) with great use of HDDs and I noticed that server doesn&amp;#39;t respond very well. CPU have big wait %  (4% user 8%sys 30%wait). Mem is free not totally used.&lt;/p&gt;\n\n&lt;p&gt;When I type with keyboard sometimes system stuck for 1/2s and double my letters.. sometime write ten times the same letter. I&amp;#39;m thinking that HBA card is the big problem here. what do you think? So:&lt;/p&gt;\n\n&lt;p&gt;1) If i change 9200 with a new 9207 and restart my server... I have to reconfigure entire jbod hdds in my system or nothing have to be done?&lt;/p&gt;\n\n&lt;p&gt;2) If I connect both 9207 exit ports to JBOD will help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uk08r", "is_robot_indexable": true, "report_reasons": null, "author": "Reddolo80", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uk08r/jbod_server_slowing_down_changing_hba_card/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uk08r/jbod_server_slowing_down_changing_hba_card/", "subreddit_subscribers": 673632, "created_utc": 1679133768.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}