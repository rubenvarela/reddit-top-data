{"kind": "Listing", "data": {"after": "t3_11ue07i", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_uegnp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this ticking sound normal? I fell like the drive is failing slowly.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11ukbsr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 121, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/mwufgnac4hoa1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/mwufgnac4hoa1/DASH_96.mp4", "dash_url": "https://v.redd.it/mwufgnac4hoa1/DASHPlaylist.mpd?a=1681749821%2CN2VmZTQ2YzljNTVkOTYxMDk2Y2U4MWIyOGNhNGY1YTBjYzdmODc2MTkzMmNmODQ3OWRlMWM1ZGY0M2Y5NmUzNw%3D%3D&amp;v=1&amp;f=sd", "duration": 16, "hls_url": "https://v.redd.it/mwufgnac4hoa1/HLSPlaylist.m3u8?a=1681749821%2CZGMwN2Y4ZWEyYTMxYzNkZTk4MjQwNDE4MWY3MzljMzBjNzY4NTMyOTY1M2ZkZGE1MTcyZjEwYmY3Y2IxYjk1OQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 121, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EvrgGIMo7XEA2e_xYCC2GBum8KLuZH8G1OEx-HZ14qw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679134808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/mwufgnac4hoa1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5fce8a121d34a69c4ea8c99736a9d3a846f4d8e4", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=bfaebbd27a240e39ff44e2e5cb8d3c17178386dc", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d46a8071b805744234bd8b2057cebb60110890a8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d57fe217791b51b6293ee6d9ec2fe7957ed30e08", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=67d1dcebc5e1ce942bd46d0e32724575f1edeb94", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/3_8Q0uV8Ny0WhZiSfC5c5CxwUc-VlTLU7E0Agk4DRWI.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0fc5332ac68de049b31e0f893e3d8cfba097c16e", "width": 960, "height": 540}], "variants": {}, "id": "l6IRE2oY_8wu1Ho9CejFerWMIKwLTBqzDymm5csVmhA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ukbsr", "is_robot_indexable": true, "report_reasons": null, "author": "CrazyYAY", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ukbsr/is_this_ticking_sound_normal_i_fell_like_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/mwufgnac4hoa1", "subreddit_subscribers": 673598, "created_utc": 1679134808.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/mwufgnac4hoa1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/mwufgnac4hoa1/DASH_96.mp4", "dash_url": "https://v.redd.it/mwufgnac4hoa1/DASHPlaylist.mpd?a=1681749821%2CN2VmZTQ2YzljNTVkOTYxMDk2Y2U4MWIyOGNhNGY1YTBjYzdmODc2MTkzMmNmODQ3OWRlMWM1ZGY0M2Y5NmUzNw%3D%3D&amp;v=1&amp;f=sd", "duration": 16, "hls_url": "https://v.redd.it/mwufgnac4hoa1/HLSPlaylist.m3u8?a=1681749821%2CZGMwN2Y4ZWEyYTMxYzNkZTk4MjQwNDE4MWY3MzljMzBjNzY4NTMyOTY1M2ZkZGE1MTcyZjEwYmY3Y2IxYjk1OQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Due to a lack of data management skills and being afraid of deleting the wrong files I have too many duplicate files, especially images now. (I rather have duplicates than data loss!) Google Files is working very well for finding duplicate images and comparing them on Android. Does anyone know a good and cheap or free software, maybe even open-source, for Windows that can be used for finding and comparing duplicate images in chosen folders? I found some but they look like from Windows 2000 and I want to use something modern.", "author_fullname": "t2_2m2bms7e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for finding duplicate images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u1xyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679083105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Due to a lack of data management skills and being afraid of deleting the wrong files I have too many duplicate files, especially images now. (I rather have duplicates than data loss!) Google Files is working very well for finding duplicate images and comparing them on Android. Does anyone know a good and cheap or free software, maybe even open-source, for Windows that can be used for finding and comparing duplicate images in chosen folders? I found some but they look like from Windows 2000 and I want to use something modern.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11u1xyd", "is_robot_indexable": true, "report_reasons": null, "author": "Rationale-Glum-Power", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u1xyd/software_for_finding_duplicate_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u1xyd/software_for_finding_duplicate_images/", "subreddit_subscribers": 673598, "created_utc": 1679083105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I've been working on this script all week. I literally thought it would take a few hours and it's consumed every hour of this past week. \n\nSo I've made a script in powershell that uses yt-dlp to download the latest youtube videos from your subscriptions, creates a playlist from all the files in the resulting folder, and creates a notification showing the names of the channels from the latest downloads. \n\nNote, all of this can be modified fairly straightforward.\n\n1. Create folder to hold everything. &lt;mainFolder&gt;\n\n2. create &lt;powershellScriptName&gt;.ps1, &lt;vbsScriptName&gt;.vbs in `mainFolder`\n\n3. make sure `mainFolder` also includes yt-dlp.exe, ffmpeg.exe, ffprobe.exe (not 100% sure the last one is necessary)\n\n4. fill `powershellSciptName` with [this pasteBin](https://pastebin.com/zFPU7f1M)\n\n4. fill `vbsScriptName` with [this pasteBin](https://pastebin.com/QsgQrXd9)\n\n5. fill in the previous 2 files using the direction below\n\n**PowerShell script:**\n\nReplace the following:\n\n`&lt;browser&gt;` - use the browser you have logged into youtube, or you can follow [this comment](https://www.reddit.com/r/youtubedl/comments/rq0zms/ytdlp_how_do_i_use_the_cookie_function/hqac3ic/)\n\n`&lt;destinationDirectory&gt;` - where you want the files to finally end up\n\n`&lt;downloadDirectory&gt;` - where to initially download the files to\n\nThe following are my own options, feel free to adjust as you like\n\n `--match-filter \"!is_live &amp; !post_live &amp; !was_live\" ` - doesn't download any live videos \n\n`notificationTitle` - Change to whatever you want the notification to say\n\n`-o \"$downloadDir\\[%(channel)s] - %(title)s.%(ext)s\" :ytsubs://user/` - this is how the files will be organized and names formatted. Feel free to adjust to your liking. yt-dlp's github will help if you need guidance\n\nmoving the items is not mandatory - I like to download first to my C drive, then move them all to my NAS. Since I run this every five minutes, it doesn't matter.\n\n**vbsScript**\n\nreplace the following:\n\n`&lt;pathToMainScript&gt;` - this will be the absolute path to your powershell script.\n\n\n#####**Automating the script** \nThis was fairly frustrating because the powershell window would popup every 5 minutes, even if you set window to hidden in the arguments. That's why you make the vbs script, as it will actually run silently\n\n1. open Task Scheduler\n2. click the arow to expand the `Task Scheduler Library` in the lefthand directory\n3. It's advisable to create your own folder for your own tasks if you haven't already. Select the Task Scheduler Library. select `Action &gt; New Folder...` from the menu bar. Name how you like.\n4. With your new folder selected, select `Create Task` from the Action pane on the right hand side.\n5. Name however you like\n6. Go to triggers tab. This will be where you select your preferred interval. To run every 5 minutes, I've created 3 triggers. one that runs daily at 12:00:00am, one that runs on startup, and one that runs when the task is altered. On each of these I have it set to run every 5 minutes.\n7. Go to the Actions tab. This will be where you call the vbs script, which in turn calls the powershell script.\n8. under program/script, enter the following: `C:\\Windows\\System32\\wscript.exe`\n9. under add arguments enter `\"&lt;pathToVBScript&gt;\"`\n10. under Start In enter: `&lt;pathToMainFolder&gt;`\n11. Go to the settings tab. check `Run task as soon as possible after a scheduled start is missed` select `Queue a new instance` for the bottom option: `If the task is already running, then the following rule applies`\n12. hit OK, then select Run from the Action pane.\n\n\nThat's it! There's some jank but like I said, I've already spent way too long on this. Hopefully this helps you out! \n\nA couple improvements I'd like to make eventually (very open to help here):\n\n*  click on the notification to open the playlist - should open automatically in the m3u associated player.\n*  better file organization\n* make a gui to make it easier to run, and potentially convert from windows task scheduler task to a daemon or service with option to adjust frequency of checks\n* any of your suggestions!\n\nI'm still really new to this, so I'm happy to hear any suggestions for improvements!", "author_fullname": "t2_37kkn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Auto download latest youtube videos from your subscriptions, with options and notification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ualom", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679120460.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679102795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;ve been working on this script all week. I literally thought it would take a few hours and it&amp;#39;s consumed every hour of this past week. &lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve made a script in powershell that uses yt-dlp to download the latest youtube videos from your subscriptions, creates a playlist from all the files in the resulting folder, and creates a notification showing the names of the channels from the latest downloads. &lt;/p&gt;\n\n&lt;p&gt;Note, all of this can be modified fairly straightforward.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Create folder to hold everything. &amp;lt;mainFolder&amp;gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;create &amp;lt;powershellScriptName&amp;gt;.ps1, &amp;lt;vbsScriptName&amp;gt;.vbs in &lt;code&gt;mainFolder&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;make sure &lt;code&gt;mainFolder&lt;/code&gt; also includes yt-dlp.exe, ffmpeg.exe, ffprobe.exe (not 100% sure the last one is necessary)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;fill &lt;code&gt;powershellSciptName&lt;/code&gt; with &lt;a href=\"https://pastebin.com/zFPU7f1M\"&gt;this pasteBin&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;fill &lt;code&gt;vbsScriptName&lt;/code&gt; with &lt;a href=\"https://pastebin.com/QsgQrXd9\"&gt;this pasteBin&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;fill in the previous 2 files using the direction below&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;PowerShell script:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Replace the following:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;browser&amp;gt;&lt;/code&gt; - use the browser you have logged into youtube, or you can follow &lt;a href=\"https://www.reddit.com/r/youtubedl/comments/rq0zms/ytdlp_how_do_i_use_the_cookie_function/hqac3ic/\"&gt;this comment&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;destinationDirectory&amp;gt;&lt;/code&gt; - where you want the files to finally end up&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;downloadDirectory&amp;gt;&lt;/code&gt; - where to initially download the files to&lt;/p&gt;\n\n&lt;p&gt;The following are my own options, feel free to adjust as you like&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--match-filter &amp;quot;!is_live &amp;amp; !post_live &amp;amp; !was_live&amp;quot;&lt;/code&gt; - doesn&amp;#39;t download any live videos &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;notificationTitle&lt;/code&gt; - Change to whatever you want the notification to say&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;-o &amp;quot;$downloadDir\\[%(channel)s] - %(title)s.%(ext)s&amp;quot; :ytsubs://user/&lt;/code&gt; - this is how the files will be organized and names formatted. Feel free to adjust to your liking. yt-dlp&amp;#39;s github will help if you need guidance&lt;/p&gt;\n\n&lt;p&gt;moving the items is not mandatory - I like to download first to my C drive, then move them all to my NAS. Since I run this every five minutes, it doesn&amp;#39;t matter.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;vbsScript&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;replace the following:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;lt;pathToMainScript&amp;gt;&lt;/code&gt; - this will be the absolute path to your powershell script.&lt;/p&gt;\n\n&lt;h5&gt;&lt;strong&gt;Automating the script&lt;/strong&gt;&lt;/h5&gt;\n\n&lt;p&gt;This was fairly frustrating because the powershell window would popup every 5 minutes, even if you set window to hidden in the arguments. That&amp;#39;s why you make the vbs script, as it will actually run silently&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;open Task Scheduler&lt;/li&gt;\n&lt;li&gt;click the arow to expand the &lt;code&gt;Task Scheduler Library&lt;/code&gt; in the lefthand directory&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s advisable to create your own folder for your own tasks if you haven&amp;#39;t already. Select the Task Scheduler Library. select &lt;code&gt;Action &amp;gt; New Folder...&lt;/code&gt; from the menu bar. Name how you like.&lt;/li&gt;\n&lt;li&gt;With your new folder selected, select &lt;code&gt;Create Task&lt;/code&gt; from the Action pane on the right hand side.&lt;/li&gt;\n&lt;li&gt;Name however you like&lt;/li&gt;\n&lt;li&gt;Go to triggers tab. This will be where you select your preferred interval. To run every 5 minutes, I&amp;#39;ve created 3 triggers. one that runs daily at 12:00:00am, one that runs on startup, and one that runs when the task is altered. On each of these I have it set to run every 5 minutes.&lt;/li&gt;\n&lt;li&gt;Go to the Actions tab. This will be where you call the vbs script, which in turn calls the powershell script.&lt;/li&gt;\n&lt;li&gt;under program/script, enter the following: &lt;code&gt;C:\\Windows\\System32\\wscript.exe&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;under add arguments enter &lt;code&gt;&amp;quot;&amp;lt;pathToVBScript&amp;gt;&amp;quot;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;under Start In enter: &lt;code&gt;&amp;lt;pathToMainFolder&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Go to the settings tab. check &lt;code&gt;Run task as soon as possible after a scheduled start is missed&lt;/code&gt; select &lt;code&gt;Queue a new instance&lt;/code&gt; for the bottom option: &lt;code&gt;If the task is already running, then the following rule applies&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;hit OK, then select Run from the Action pane.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That&amp;#39;s it! There&amp;#39;s some jank but like I said, I&amp;#39;ve already spent way too long on this. Hopefully this helps you out! &lt;/p&gt;\n\n&lt;p&gt;A couple improvements I&amp;#39;d like to make eventually (very open to help here):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; click on the notification to open the playlist - should open automatically in the m3u associated player.&lt;/li&gt;\n&lt;li&gt; better file organization&lt;/li&gt;\n&lt;li&gt;make a gui to make it easier to run, and potentially convert from windows task scheduler task to a daemon or service with option to adjust frequency of checks&lt;/li&gt;\n&lt;li&gt;any of your suggestions!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m still really new to this, so I&amp;#39;m happy to hear any suggestions for improvements!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;v=enabled&amp;s=decc328886393c0699bb01cf9d08b602f60525c8", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cda4e996a0e1c77c65ec3810a634071f4573481", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ualom", "is_robot_indexable": true, "report_reasons": null, "author": "MonkAndCanatella", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ualom/auto_download_latest_youtube_videos_from_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ualom/auto_download_latest_youtube_videos_from_your/", "subreddit_subscribers": 673598, "created_utc": 1679102795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_baej04p7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Locally cheapest 4TB pro NVMe TLC drive. What's the catch? People say it runs hot.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 127, "top_awarded_type": null, "hide_score": false, "name": "t3_11ud6wv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GhBwijbcDpwKQhZb5BddanrIdN42gijnIL2w6jUerLM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679110218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/x2hkh6cv2foa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?auto=webp&amp;v=enabled&amp;s=1460986d1696cf1ae8c3e86a3eef2fab3822bae4", "width": 940, "height": 855}, "resolutions": [{"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be8440724585a44a7efea6b65cb8d68f0c9c8b84", "width": 108, "height": 98}, {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=911e433563111cc4bfb63cd8500189bf038c343a", "width": 216, "height": 196}, {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb110f2ba1c09e2d287a5d85158053ed6f141127", "width": 320, "height": 291}, {"url": "https://preview.redd.it/x2hkh6cv2foa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e30c04109c1d5317db258a670a3413f1ef312f2", "width": 640, "height": 582}], "variants": {}, "id": "NHDqbeaHaJPCSNVmOLmnr2oyYe1OwYnbFQ7G9Ugd9H0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11ud6wv", "is_robot_indexable": true, "report_reasons": null, "author": "voinian", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ud6wv/locally_cheapest_4tb_pro_nvme_tlc_drive_whats_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/x2hkh6cv2foa1.png", "subreddit_subscribers": 673598, "created_utc": 1679110218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A triple layer 100GB BD-R disc fits 33.3GB onto a single layer.  Anyone know why there aren't any single layer 33GB or dual layer 66GB BD-R discs?  Always been curious about this.", "author_fullname": "t2_mgmtm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Are There No 33GB or 66GB BD-R Discs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uecgk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679113799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A triple layer 100GB BD-R disc fits 33.3GB onto a single layer.  Anyone know why there aren&amp;#39;t any single layer 33GB or dual layer 66GB BD-R discs?  Always been curious about this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11uecgk", "is_robot_indexable": true, "report_reasons": null, "author": "HarryMuscle", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uecgk/why_are_there_no_33gb_or_66gb_bdr_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uecgk/why_are_there_no_33gb_or_66gb_bdr_discs/", "subreddit_subscribers": 673598, "created_utc": 1679113799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is a bit of an unconventional ask that I haven't seen in other posts about Spotify downloads.  \n\nI pay for spotify premium (student), and I'm quite happy with it for the sheer number of hours I use it, so I really have no need for local copies currently. HOWEVER- I'd like to download *lists* of the songs in my playlists into some kind of parseable text file per-playlist, containing song name, artist, maybe release date/runtime, etc.  \n\nI've amassed about 1600 songs so far in various playlists with a lot of \"mood specialization\" for each list. Making list backups of just the song titles is good enough for me to be able to recreate the same playlists from alternate sources if Spotify is to ever kick the bucket. Does a tool like this exist? I tried the old \"CTRL-A/CTRL-C\" trick listed in posts a few years ago but Spotify now only provides direct Spotify links instead of the name and artist when you do that.", "author_fullname": "t2_grolz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading spotify playlists - but Just the Names?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ufysj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679119150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a bit of an unconventional ask that I haven&amp;#39;t seen in other posts about Spotify downloads.  &lt;/p&gt;\n\n&lt;p&gt;I pay for spotify premium (student), and I&amp;#39;m quite happy with it for the sheer number of hours I use it, so I really have no need for local copies currently. HOWEVER- I&amp;#39;d like to download &lt;em&gt;lists&lt;/em&gt; of the songs in my playlists into some kind of parseable text file per-playlist, containing song name, artist, maybe release date/runtime, etc.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve amassed about 1600 songs so far in various playlists with a lot of &amp;quot;mood specialization&amp;quot; for each list. Making list backups of just the song titles is good enough for me to be able to recreate the same playlists from alternate sources if Spotify is to ever kick the bucket. Does a tool like this exist? I tried the old &amp;quot;CTRL-A/CTRL-C&amp;quot; trick listed in posts a few years ago but Spotify now only provides direct Spotify links instead of the name and artist when you do that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "still think Betamax shoulda won ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ufysj", "is_robot_indexable": true, "report_reasons": null, "author": "empirebuilder1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11ufysj/downloading_spotify_playlists_but_just_the_names/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ufysj/downloading_spotify_playlists_but_just_the_names/", "subreddit_subscribers": 673598, "created_utc": 1679119150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all! \n\nI have [this](https://www.fractal-design.com/products/cases/define/define-7-xl/black-tg-dark-tint/) case, the fractal Define 7 XL. I got it really low cost used with no HDD racks. I'm wondering if anyone knows of something that would be compatible for the front of the case near the front intake fans for about 4-5 HDD's?\n\nThanks!", "author_fullname": "t2_igthflmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an internal case mount for HDD's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u9h6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679100000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! &lt;/p&gt;\n\n&lt;p&gt;I have &lt;a href=\"https://www.fractal-design.com/products/cases/define/define-7-xl/black-tg-dark-tint/\"&gt;this&lt;/a&gt; case, the fractal Define 7 XL. I got it really low cost used with no HDD racks. I&amp;#39;m wondering if anyone knows of something that would be compatible for the front of the case near the front intake fans for about 4-5 HDD&amp;#39;s?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?auto=webp&amp;v=enabled&amp;s=48ad768a2ef3270064417e446895481cc66bc164", "width": 2560, "height": 1810}, "resolutions": [{"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bfedd6360c8bbea98a53921264465aa41d5e444", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b07f44f5a2eba78790934483cb735a877578e403", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca7e4a8c36ec0861796e0b7acf41095d26e7d543", "width": 320, "height": 226}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2fc4ebe975020d0ebaff111075ac1aeab7252e9", "width": 640, "height": 452}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a8c7cc9b3420c41669a397a52b7ebd57ae155d5", "width": 960, "height": 678}, {"url": "https://external-preview.redd.it/NyeYG7u6-otc3iSJ80edQlWdt05xiQQBaM1NyAIWDss.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7177bb9403f795eec118516462ad239ef3cf4f8", "width": 1080, "height": 763}], "variants": {}, "id": "Bn8gSTySoqn4Q4oRa1BLtFVkcDdV-5abpUxWxfiR8_A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11u9h6h", "is_robot_indexable": true, "report_reasons": null, "author": "Novel_Excitement5693", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u9h6h/looking_for_an_internal_case_mount_for_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u9h6h/looking_for_an_internal_case_mount_for_hdds/", "subreddit_subscribers": 673598, "created_utc": 1679100000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_3xunvi1q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Update on 3rd party seller tosh MG series 16tb for 230 free ship -", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"w49zsbm4sboa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/w49zsbm4sboa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=827991dc95a64e40cc944e5069b9e10b4e258185"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/w49zsbm4sboa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dbb65961354c6ce119e3800f518d52b7389ed53e"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/w49zsbm4sboa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=950994625ddc33da0df80f3af6011cd9d3555a14"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/w49zsbm4sboa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dcf34c71b2c3f30346bd2b6b6d834d1c962628e7"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/w49zsbm4sboa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2979411fe7b8d2e94597348f959589819e28195b"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/w49zsbm4sboa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db680da022729ff58578fa954c890d93760bf923"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/w49zsbm4sboa1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=bea428de0888c4e0a44f2376801b7e269841c96a"}, "id": "w49zsbm4sboa1"}, "rzgg9dm4sboa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/rzgg9dm4sboa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa47e89c4fae7dce98868fb9c1a6935e4c75909c"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/rzgg9dm4sboa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1802715b47ebd165b21e74131dee67c47eb6561b"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/rzgg9dm4sboa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32c5a028386d3ec710aaf1aef394c476da7c95a8"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/rzgg9dm4sboa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e77dfc7769bdb0ba65a8aaaecb5a1e5ca2ee91d"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/rzgg9dm4sboa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a98cb7d88a2f605032c2245c521adaa66f4f682"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/rzgg9dm4sboa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c18e27739a83c7121d4b2154f06ed1202334c1cf"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/rzgg9dm4sboa1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=f3ebb2ff3406e37d10b56a41a3e91336e9cea34a"}, "id": "rzgg9dm4sboa1"}}, "name": "t3_11tvu0v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "ups": 7, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "w49zsbm4sboa1", "id": 252153086}, {"media_id": "rzgg9dm4sboa1", "id": 252153087}]}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/tIBynVEW-NNSMISUV6OxsWmh-gJzGBFJcL4Gg2qMaI8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679070175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/11tvu0v", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11tvu0v", "is_robot_indexable": true, "report_reasons": null, "author": "PythonsByX", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11tvu0v/update_on_3rd_party_seller_tosh_mg_series_16tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/11tvu0v", "subreddit_subscribers": 673598, "created_utc": 1679070175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently I have a 4TB Seagate Barracuda drive on my PC for general storage.\n\nI was thinking about replacing it with an IronWolf 8TB. Keep in mind this is not for NAS, but for inside my PC.\n\nIs this the way to go, or do you have other suggestions?", "author_fullname": "t2_37lnd1hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "8TB Drive Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u09bx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679079526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I have a 4TB Seagate Barracuda drive on my PC for general storage.&lt;/p&gt;\n\n&lt;p&gt;I was thinking about replacing it with an IronWolf 8TB. Keep in mind this is not for NAS, but for inside my PC.&lt;/p&gt;\n\n&lt;p&gt;Is this the way to go, or do you have other suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11u09bx", "is_robot_indexable": true, "report_reasons": null, "author": "JohnsonZ887", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u09bx/8tb_drive_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u09bx/8tb_drive_suggestions/", "subreddit_subscribers": 673598, "created_utc": 1679079526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What are some good tools to quickly and easily archive a forum like Tapatalk for example? : [https://www.tapatalk.com/](https://www.tapatalk.com/)", "author_fullname": "t2_7gl7ysbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best tools to archive a forum quickly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ujp39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679132792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some good tools to quickly and easily archive a forum like Tapatalk for example? : &lt;a href=\"https://www.tapatalk.com/\"&gt;https://www.tapatalk.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ujp39", "is_robot_indexable": true, "report_reasons": null, "author": "DarknessMoonlight", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ujp39/what_are_the_best_tools_to_archive_a_forum_quickly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ujp39/what_are_the_best_tools_to_archive_a_forum_quickly/", "subreddit_subscribers": 673598, "created_utc": 1679132792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm doing a little personal thing and I'm wondering if there is a way to scrape and download ALL images from website... not just a page, or a profile, THE WHOLE WEBSITE.I know it'd probably take a while to download all of it, and also probably I'd need a lot of TB for it. But I'm wondering anyway.\n\nMost tutorials I see show how to scrap it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).\n\n&amp;#x200B;\n\nI've been doing it manually so far, but it's taking a LONG time and also I don't wanna get carpal tunnel, so I got myself thinking 'there must be an easier way to do it!'.\n\nKnowing the internet as I do I also know that if I do scrape a website, no matter which website it is, I'm gonna end up getting loads of unwanted images, like graphic imagery and porn and... less then legal stuff I suppose. If there's a way to filter out those I'd also like to know, otherwise I'd just have to delete them out mannually.\n\nI'd also like to convert all images to .pngm but if not possible I'm willing to do it mannually.\n\nMost tutorials I see show how to scrape it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).", "author_fullname": "t2_65yetpx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to scrape ALL images from a website (not a single page, the whole website)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uq1z6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679150000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing a little personal thing and I&amp;#39;m wondering if there is a way to scrape and download ALL images from website... not just a page, or a profile, THE WHOLE WEBSITE.I know it&amp;#39;d probably take a while to download all of it, and also probably I&amp;#39;d need a lot of TB for it. But I&amp;#39;m wondering anyway.&lt;/p&gt;\n\n&lt;p&gt;Most tutorials I see show how to scrap it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been doing it manually so far, but it&amp;#39;s taking a LONG time and also I don&amp;#39;t wanna get carpal tunnel, so I got myself thinking &amp;#39;there must be an easier way to do it!&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Knowing the internet as I do I also know that if I do scrape a website, no matter which website it is, I&amp;#39;m gonna end up getting loads of unwanted images, like graphic imagery and porn and... less then legal stuff I suppose. If there&amp;#39;s a way to filter out those I&amp;#39;d also like to know, otherwise I&amp;#39;d just have to delete them out mannually.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d also like to convert all images to .pngm but if not possible I&amp;#39;m willing to do it mannually.&lt;/p&gt;\n\n&lt;p&gt;Most tutorials I see show how to scrape it on only one page or profile, or even for just one tag. No. I wanna see EVERYTHING. (perhaps even other types of files later, but one step at a time I guess).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uq1z6", "is_robot_indexable": true, "report_reasons": null, "author": "abcbibi", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uq1z6/is_there_a_way_to_scrape_all_images_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uq1z6/is_there_a_way_to_scrape_all_images_from_a/", "subreddit_subscribers": 673598, "created_utc": 1679150000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I remember couple years ago an older build leaked. But MVG made [video](https://www.youtube.com/watch?v=1fwSR8gjrCk) with newer version that sound impressive. Have these been preserved yet?", "author_fullname": "t2_w6lyzny8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has Nintendo's official NSO GBA/GB emulators been preserved yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ual61", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679102755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember couple years ago an older build leaked. But MVG made &lt;a href=\"https://www.youtube.com/watch?v=1fwSR8gjrCk\"&gt;video&lt;/a&gt; with newer version that sound impressive. Have these been preserved yet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?auto=webp&amp;v=enabled&amp;s=a6b9844c7c72095912b81476a84e34b8abe06c63", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c04916d2bc6f2aab163bd7f2e61be73fff275a6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9abb6417309562656febad8fc2a317fe469bcbe6", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/bE_foOZ4ukwGRk2VQoLs5SWMZScDdWOX9vgKmWMRQ20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f073e41099e13b18258feafcf673dd1693d90b17", "width": 320, "height": 240}], "variants": {}, "id": "VTvIbMpG2MB-WJO9Ipc_obgXKuScYk-EBzz_frA02Oo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ual61", "is_robot_indexable": true, "report_reasons": null, "author": "JebryyathHS", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ual61/has_nintendos_official_nso_gbagb_emulators_been/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ual61/has_nintendos_official_nso_gbagb_emulators_been/", "subreddit_subscribers": 673598, "created_utc": 1679102755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI am looking to update my case to get more hdd slots but am not sure how to deal with the backplanes...  \nCurrently I am using a IT mode Dell Perc H310 with two 4x sata breakout cables, so with that and the mobo I am up to 11 drives.\n\nNext I would like to get a 24 drive enclosure (4U) but I see 6g,12g,and sata backplanes and am not sure where to go next in terms of controllers vs mini-sas expanders.  \nI don't really need much in terms of speed (being that it's mostly shucked or WD Red 5400), so I am not sure if the current controller would do the trick (it supports 32 drives). If I stuck with it what expander card would support a backplane with mini-sas x 6 (so I guess the expander would need to be 2 in , 6 out mini-sas)?  \nAny thoughts?\n\nThanks!", "author_fullname": "t2_1rwxyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Case Upgrade Questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11ur4d0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679152643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I am looking to update my case to get more hdd slots but am not sure how to deal with the backplanes...&lt;br/&gt;\nCurrently I am using a IT mode Dell Perc H310 with two 4x sata breakout cables, so with that and the mobo I am up to 11 drives.&lt;/p&gt;\n\n&lt;p&gt;Next I would like to get a 24 drive enclosure (4U) but I see 6g,12g,and sata backplanes and am not sure where to go next in terms of controllers vs mini-sas expanders.&lt;br/&gt;\nI don&amp;#39;t really need much in terms of speed (being that it&amp;#39;s mostly shucked or WD Red 5400), so I am not sure if the current controller would do the trick (it supports 32 drives). If I stuck with it what expander card would support a backplane with mini-sas x 6 (so I guess the expander would need to be 2 in , 6 out mini-sas)?&lt;br/&gt;\nAny thoughts?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ur4d0", "is_robot_indexable": true, "report_reasons": null, "author": "hand_in_every_pot", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ur4d0/case_upgrade_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ur4d0/case_upgrade_questions/", "subreddit_subscribers": 673598, "created_utc": 1679152643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nI used to have a PC case that had 4 HDD bays, now the one I have only has 1. Ideally i'd like to keep my 2x 14TB &amp; 2x 10TB drives, but if i have to shrink down to 2x 14TB I will.\n\n&amp;#x200B;\n\nHere is my issue. I used windows to expand/create a raid 0 for each pair. If i plug it loosely into Sata and power Windows detects it. if i use my External HDD enclosure, it doesn't. \n\nI Bought this thinking i could put all 4 in, and windows would still recognize it.\n\n[https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;ref=ppx\\_yo2ov\\_dt\\_b\\_product\\_details](https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details)\n\n&amp;#x200B;\n\nNONE of this data is important...its just alot lol\n\n&amp;#x200B;\n\nis it a bad unit for this? is it not possible? any guidance is appreciated.", "author_fullname": "t2_4e2n8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage help! Windows Extended volume", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11uqqok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679151720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I used to have a PC case that had 4 HDD bays, now the one I have only has 1. Ideally i&amp;#39;d like to keep my 2x 14TB &amp;amp; 2x 10TB drives, but if i have to shrink down to 2x 14TB I will.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here is my issue. I used windows to expand/create a raid 0 for each pair. If i plug it loosely into Sata and power Windows detects it. if i use my External HDD enclosure, it doesn&amp;#39;t. &lt;/p&gt;\n\n&lt;p&gt;I Bought this thinking i could put all 4 in, and windows would still recognize it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details\"&gt;https://www.amazon.com/dp/B078YQHWYW?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;NONE of this data is important...its just alot lol&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;is it a bad unit for this? is it not possible? any guidance is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uqqok", "is_robot_indexable": true, "report_reasons": null, "author": "ChiefSpoonS", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uqqok/storage_help_windows_extended_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uqqok/storage_help_windows_extended_volume/", "subreddit_subscribers": 673598, "created_utc": 1679151720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Of the Micro SD card brands I've owned over the years, ALL but one of the Sandisk cards has failed. ALL of the Transcend, Lexar, Kingston, and Samsung cards are still going strong.\n\nOf the Sandisk flash drives I've owned, I've experienced ~50% failure rate on the newer drives (16GB and higher). I just had a (seldom used) Sandisk Ultra Flair 128GB fail last night.\n\n/rant", "author_fullname": "t2_cd00t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: Sandisk SD cards and flash drives suck for reliability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uq21w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679150005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Of the Micro SD card brands I&amp;#39;ve owned over the years, ALL but one of the Sandisk cards has failed. ALL of the Transcend, Lexar, Kingston, and Samsung cards are still going strong.&lt;/p&gt;\n\n&lt;p&gt;Of the Sandisk flash drives I&amp;#39;ve owned, I&amp;#39;ve experienced ~50% failure rate on the newer drives (16GB and higher). I just had a (seldom used) Sandisk Ultra Flair 128GB fail last night.&lt;/p&gt;\n\n&lt;p&gt;/rant&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11uq21w", "is_robot_indexable": true, "report_reasons": null, "author": "Ho_Lee_Fuk", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uq21w/psa_sandisk_sd_cards_and_flash_drives_suck_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uq21w/psa_sandisk_sd_cards_and_flash_drives_suck_for/", "subreddit_subscribers": 673598, "created_utc": 1679150005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All,\n\nI used to have a Mac system and used Carbon Copy.\n\nI have a system with a 500GB SSD, and I bought a 1TB SSD. My MB has only one slot, so I'd like to carbon copy the content of the 500GB (SO and everything in it) into the 1TB so that I don't have to reinstall everything.\n\nDo you have any suggestions on windows 10?\n\nThanks", "author_fullname": "t2_2rqya8a6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Carbon copy a 500 GB SSD into a bigger one", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11upm44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679148862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I used to have a Mac system and used Carbon Copy.&lt;/p&gt;\n\n&lt;p&gt;I have a system with a 500GB SSD, and I bought a 1TB SSD. My MB has only one slot, so I&amp;#39;d like to carbon copy the content of the 500GB (SO and everything in it) into the 1TB so that I don&amp;#39;t have to reinstall everything.&lt;/p&gt;\n\n&lt;p&gt;Do you have any suggestions on windows 10?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11upm44", "is_robot_indexable": true, "report_reasons": null, "author": "ff8mania", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11upm44/carbon_copy_a_500_gb_ssd_into_a_bigger_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11upm44/carbon_copy_a_500_gb_ssd_into_a_bigger_one/", "subreddit_subscribers": 673598, "created_utc": 1679148862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm totally new with storage systems. I bought a netapp 4486 jbod connected to my server with a 9200-8e in IT mode. I have starting to populate with services (docker containers) with great use of HDDs and I noticed that server doesn't respond very well. CPU have big wait %  (4% user 8%sys 30%wait). Mem is free not totally used.\n\nWhen I type with keyboard sometimes system stuck for 1/2s and double my letters.. sometime write ten times the same letter. I'm thinking that HBA card is the big problem here. what do you think? So:\n\n1) If i change 9200 with a new 9207 and restart my server... I have to reconfigure entire jbod hdds in my system or nothing have to be done?\n\n2) If I connect both 9207 exit ports to JBOD will help?", "author_fullname": "t2_7xrhh3tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "JBOD: Server slowing down. Changing HBA card", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uk08r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679133768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m totally new with storage systems. I bought a netapp 4486 jbod connected to my server with a 9200-8e in IT mode. I have starting to populate with services (docker containers) with great use of HDDs and I noticed that server doesn&amp;#39;t respond very well. CPU have big wait %  (4% user 8%sys 30%wait). Mem is free not totally used.&lt;/p&gt;\n\n&lt;p&gt;When I type with keyboard sometimes system stuck for 1/2s and double my letters.. sometime write ten times the same letter. I&amp;#39;m thinking that HBA card is the big problem here. what do you think? So:&lt;/p&gt;\n\n&lt;p&gt;1) If i change 9200 with a new 9207 and restart my server... I have to reconfigure entire jbod hdds in my system or nothing have to be done?&lt;/p&gt;\n\n&lt;p&gt;2) If I connect both 9207 exit ports to JBOD will help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uk08r", "is_robot_indexable": true, "report_reasons": null, "author": "Reddolo80", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uk08r/jbod_server_slowing_down_changing_hba_card/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uk08r/jbod_server_slowing_down_changing_hba_card/", "subreddit_subscribers": 673598, "created_utc": 1679133768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've noticed I often clean install windows and I like that it leaves the system clean. I do just back up my user folder as that's all the files I like to keep like videos or photos I've made from that time life of that install. Was wondering if there is a way to restore the files to the same locations but keep the junk out like random desktop.ini files or .exe files that lead to nowhere. Thx much appreciated", "author_fullname": "t2_vzj8emxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a good way to backup personal files on windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ucyot", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679109533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve noticed I often clean install windows and I like that it leaves the system clean. I do just back up my user folder as that&amp;#39;s all the files I like to keep like videos or photos I&amp;#39;ve made from that time life of that install. Was wondering if there is a way to restore the files to the same locations but keep the junk out like random desktop.ini files or .exe files that lead to nowhere. Thx much appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ucyot", "is_robot_indexable": true, "report_reasons": null, "author": "AfterwardsFriendly", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ucyot/what_is_a_good_way_to_backup_personal_files_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ucyot/what_is_a_good_way_to_backup_personal_files_on/", "subreddit_subscribers": 673598, "created_utc": 1679109533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "so i know about dev=urandom on drives, shredding etc. but what if a fast way to make data unreadable on a drive would be:  \n\n\nfor example, have 32gb usb flash drive\n\n&amp;#x200B;\n\n1) encrypt it\n\n2) quick format it  \n\n\nwould this make the data unreadable?", "author_fullname": "t2_6n2n1f8zk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "encrypting then formatting a drive for deletion of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u5zfz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679092034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so i know about dev=urandom on drives, shredding etc. but what if a fast way to make data unreadable on a drive would be:  &lt;/p&gt;\n\n&lt;p&gt;for example, have 32gb usb flash drive&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;1) encrypt it&lt;/p&gt;\n\n&lt;p&gt;2) quick format it  &lt;/p&gt;\n\n&lt;p&gt;would this make the data unreadable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11u5zfz", "is_robot_indexable": true, "report_reasons": null, "author": "helloidkwhatthisis1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u5zfz/encrypting_then_formatting_a_drive_for_deletion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u5zfz/encrypting_then_formatting_a_drive_for_deletion/", "subreddit_subscribers": 673598, "created_utc": 1679092034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm wondering what solutions others have found for transferring large volumes of data (TB at a time) over their LANs with reasonable speed? SMB is just so unreliable and the transfer rates vary so wildly. It's so frustrating to download a 2GB something off the Internet in a matter of seconds but then for it to take a couple of minutes to transfer over SMB to another computer that's essentially right next to it (or is actually on the same hardware but going over a network mount).\n\nThere's no shortage of cores or memory provisioned to these boxes and there's quality network hose connected to Cisco gigabit switches in between but we're getting nowhere near ethernet saturation rates. Case in point, torrents come in several times faster than direct connections. I feel like it's just crappy protocols. Suggestions?\n\n&amp;#x200B;\n\nTIA", "author_fullname": "t2_b2n3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LAN Transfer Protocols at decent speed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u5dtx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679090702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering what solutions others have found for transferring large volumes of data (TB at a time) over their LANs with reasonable speed? SMB is just so unreliable and the transfer rates vary so wildly. It&amp;#39;s so frustrating to download a 2GB something off the Internet in a matter of seconds but then for it to take a couple of minutes to transfer over SMB to another computer that&amp;#39;s essentially right next to it (or is actually on the same hardware but going over a network mount).&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s no shortage of cores or memory provisioned to these boxes and there&amp;#39;s quality network hose connected to Cisco gigabit switches in between but we&amp;#39;re getting nowhere near ethernet saturation rates. Case in point, torrents come in several times faster than direct connections. I feel like it&amp;#39;s just crappy protocols. Suggestions?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11u5dtx", "is_robot_indexable": true, "report_reasons": null, "author": "deekaph", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u5dtx/lan_transfer_protocols_at_decent_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u5dtx/lan_transfer_protocols_at_decent_speed/", "subreddit_subscribers": 673598, "created_utc": 1679090702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everybody!\n\n(Hi, Dr. Nick!)\n\nI'm venturing into NAS territory. I feel comfortable tinkering around with tech, and it really scratches an itch. I'm an artist, which just adds to the media hoarding. Big PSD files with versions FTW.\n\nGoing to be using it for local \"backup\", but mostly as a jellyfin server. Unless I find something else that sounds fun.\n\nold case, mobo, cpu should be great for this.\n i5 3570 and a 1050ti if some help is needed with transcoding.\n\nMy issue is the widely varying costs of drives. I keep trying to find the sweet spot. I'm thinking raid5 4 6 or 8tb drives. I have a small ssd for an os drive already.\n\n$400 US budget. The cheap price of refurbished drives is a siren's call... but i dunno. Amazon also has low pricws on MaxDigitalData drives, but I haven't heard of them.\n\n\nAny suggestions?", "author_fullname": "t2_3myb8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building my first NAS from an old PC. Drives (cost) is the biggest issue.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u1a6d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679081659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody!&lt;/p&gt;\n\n&lt;p&gt;(Hi, Dr. Nick!)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m venturing into NAS territory. I feel comfortable tinkering around with tech, and it really scratches an itch. I&amp;#39;m an artist, which just adds to the media hoarding. Big PSD files with versions FTW.&lt;/p&gt;\n\n&lt;p&gt;Going to be using it for local &amp;quot;backup&amp;quot;, but mostly as a jellyfin server. Unless I find something else that sounds fun.&lt;/p&gt;\n\n&lt;p&gt;old case, mobo, cpu should be great for this.\n i5 3570 and a 1050ti if some help is needed with transcoding.&lt;/p&gt;\n\n&lt;p&gt;My issue is the widely varying costs of drives. I keep trying to find the sweet spot. I&amp;#39;m thinking raid5 4 6 or 8tb drives. I have a small ssd for an os drive already.&lt;/p&gt;\n\n&lt;p&gt;$400 US budget. The cheap price of refurbished drives is a siren&amp;#39;s call... but i dunno. Amazon also has low pricws on MaxDigitalData drives, but I haven&amp;#39;t heard of them.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11u1a6d", "is_robot_indexable": true, "report_reasons": null, "author": "Hurm", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u1a6d/building_my_first_nas_from_an_old_pc_drives_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u1a6d/building_my_first_nas_from_an_old_pc_drives_cost/", "subreddit_subscribers": 673598, "created_utc": 1679081659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI've had a go at searching for this without any clear results. I'm looking for a tool to bulk download Instagram profiles (Twitter and tiktok would also be handy, but Instagram is the focus for now) for academic research purposes. Is there anything out there that would let me do that? Ideally for free...\n\nThanks in advance \ud83d\ude0a", "author_fullname": "t2_rwhiewfa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bulk archiving tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11u0te2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679080683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had a go at searching for this without any clear results. I&amp;#39;m looking for a tool to bulk download Instagram profiles (Twitter and tiktok would also be handy, but Instagram is the focus for now) for academic research purposes. Is there anything out there that would let me do that? Ideally for free...&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11u0te2", "is_robot_indexable": true, "report_reasons": null, "author": "franklyeddie", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11u0te2/bulk_archiving_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11u0te2/bulk_archiving_tools/", "subreddit_subscribers": 673598, "created_utc": 1679080683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Pretty much like the subject says. We need to find an old file that was backed up on a Windows 2000 machine to LTO2 tape. Dusted off the Win2k machine and it fired right up. After locating all the right SCSI cables and terminator, we are able to access the tapes. But we have one for which there is no catalog in the Backup application, and I can't for the life of me figure out how to generate one so we can see what's on it. It was definitely made on this very machine, using Windows Backup, not with another application.\n\nAny ideas? It seems to only want to let me see what's on the tape if I have an entry for it in the Restore Tab. -- there has to be a way to get it to catalog a tape it's not familiar with or doesn't have a catalog file for, right?", "author_fullname": "t2_bbs73", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to restore an LTO2 tape made with Windows Backup on Win 2000 -- with no catalog?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ty7gh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679076339.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679075138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty much like the subject says. We need to find an old file that was backed up on a Windows 2000 machine to LTO2 tape. Dusted off the Win2k machine and it fired right up. After locating all the right SCSI cables and terminator, we are able to access the tapes. But we have one for which there is no catalog in the Backup application, and I can&amp;#39;t for the life of me figure out how to generate one so we can see what&amp;#39;s on it. It was definitely made on this very machine, using Windows Backup, not with another application.&lt;/p&gt;\n\n&lt;p&gt;Any ideas? It seems to only want to let me see what&amp;#39;s on the tape if I have an entry for it in the Restore Tab. -- there has to be a way to get it to catalog a tape it&amp;#39;s not familiar with or doesn&amp;#39;t have a catalog file for, right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ty7gh", "is_robot_indexable": true, "report_reasons": null, "author": "friolator", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ty7gh/how_to_restore_an_lto2_tape_made_with_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ty7gh/how_to_restore_an_lto2_tape_made_with_windows/", "subreddit_subscribers": 673598, "created_utc": 1679075138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi i need some help with the wayback time machine, i am trying to see some age restricted tweets on there but i can't get pass the age restriction and when i try to log in the page freezes (yes i am 18+)", "author_fullname": "t2_cdbhtsfn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11uk788", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679134394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi i need some help with the wayback time machine, i am trying to see some age restricted tweets on there but i can&amp;#39;t get pass the age restriction and when i try to log in the page freezes (yes i am 18+)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11uk788", "is_robot_indexable": true, "report_reasons": null, "author": "mircea_ms2005", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11uk788/help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11uk788/help/", "subreddit_subscribers": 673598, "created_utc": 1679134394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My PSU (Enermax D.F.) correctly powers some hard drives, while some others don't spin up. Those that are giving problems are HGST Ultrastar and the only way I can make them start is by using the Molex connectors with Molex-to-SATA adapters, instead of the regular SATA cables. I don't know if I should change PSU (nor which one would not have this issue) or just go on like this. My questions are\n\n1. Is it a problem if I keep using the Molex-to-SATA cables to power them? Will it damage the drives?\n2. Is this kind of issues normal between different brands of PSUs?\n3. Is it advisable to change the PSU?", "author_fullname": "t2_5xmhvkmp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSU doesn't power SOME hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ue07i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679112723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My PSU (Enermax D.F.) correctly powers some hard drives, while some others don&amp;#39;t spin up. Those that are giving problems are HGST Ultrastar and the only way I can make them start is by using the Molex connectors with Molex-to-SATA adapters, instead of the regular SATA cables. I don&amp;#39;t know if I should change PSU (nor which one would not have this issue) or just go on like this. My questions are&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is it a problem if I keep using the Molex-to-SATA cables to power them? Will it damage the drives?&lt;/li&gt;\n&lt;li&gt;Is this kind of issues normal between different brands of PSUs?&lt;/li&gt;\n&lt;li&gt;Is it advisable to change the PSU?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ue07i", "is_robot_indexable": true, "report_reasons": null, "author": "Sufficient-Word", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ue07i/psu_doesnt_power_some_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ue07i/psu_doesnt_power_some_hard_drives/", "subreddit_subscribers": 673598, "created_utc": 1679112723.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}