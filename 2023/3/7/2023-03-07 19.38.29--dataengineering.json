{"kind": "Listing", "data": {"after": "t3_11ky7mj", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to see more substantial content in this Reddit.  Lately the depth and quality of recent posts have not really added anything to the community.\n\nAre there any thoughts on how to lift and improve it?", "author_fullname": "t2_fer0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting tired of \u201cHow do I break into DE posts\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kth8p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 162, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 162, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678177294.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to see more substantial content in this Reddit.  Lately the depth and quality of recent posts have not really added anything to the community.&lt;/p&gt;\n\n&lt;p&gt;Are there any thoughts on how to lift and improve it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11kth8p", "is_robot_indexable": true, "report_reasons": null, "author": "TheCauthon", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kth8p/getting_tired_of_how_do_i_break_into_de_posts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kth8p/getting_tired_of_how_do_i_break_into_de_posts/", "subreddit_subscribers": 92187, "created_utc": 1678177294.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say I have a python app that every day performs some data transformations and then loads the data into the warehouse (postgres). It loads parquet files using pyarrow and then using polars/pandas do some stuff and then it needs to save it to the db. \n\nWhat would be the best way to insert the data into the DB?\n\na. use pandas to_sql method\n\nb. iterate over each dataframe row and send an insert statement\n\nc. convert dataframe to e.g. list of dicts and send an insert statements (i suppose worse than b.)\n\nd. asyncronously iterate over dataframe rows and send an insert statements\n\ne. save dataframe to CSV and ?somehow import it into the DB\n\nf. some better alternative?\n\nAlso - is it common to send the insert statements directly to the DB or should I use Kafka/RabbitMQ, send the data to the messaging system, create a consumer app that would take the data from queue and insert into the db?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Insert data into DB best practice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kdvkr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678137570.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678136504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say I have a python app that every day performs some data transformations and then loads the data into the warehouse (postgres). It loads parquet files using pyarrow and then using polars/pandas do some stuff and then it needs to save it to the db. &lt;/p&gt;\n\n&lt;p&gt;What would be the best way to insert the data into the DB?&lt;/p&gt;\n\n&lt;p&gt;a. use pandas to_sql method&lt;/p&gt;\n\n&lt;p&gt;b. iterate over each dataframe row and send an insert statement&lt;/p&gt;\n\n&lt;p&gt;c. convert dataframe to e.g. list of dicts and send an insert statements (i suppose worse than b.)&lt;/p&gt;\n\n&lt;p&gt;d. asyncronously iterate over dataframe rows and send an insert statements&lt;/p&gt;\n\n&lt;p&gt;e. save dataframe to CSV and ?somehow import it into the DB&lt;/p&gt;\n\n&lt;p&gt;f. some better alternative?&lt;/p&gt;\n\n&lt;p&gt;Also - is it common to send the insert statements directly to the DB or should I use Kafka/RabbitMQ, send the data to the messaging system, create a consumer app that would take the data from queue and insert into the db?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11kdvkr", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kdvkr/insert_data_into_db_best_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kdvkr/insert_data_into_db_best_practice/", "subreddit_subscribers": 92187, "created_utc": 1678136504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I moved from a Python + SQL heavy role into one where the bulk of the coding is left to more junior members. Meanwhile, I build infrastructure and set up the bigger components. It\u2019s still technical work but I hardly get to write code. It\u2019s mostly configuration through YAML or terraform, or system diagramming. The code that I do write is Ops heavy. I\u2019ll be an expert in k8s, terraform, CICD soon enough\u2026. (Not really, this shit\u2019s complicated).\n\nI\u2019m not sure I enjoy it as much as writing Python, Go, or even SQL. \n\nDo y\u2019all enjoy setting up off-the-shelf tools, writing config files in YAML or toml or terraform/infra when you have to? What do you enjoy about it? Has your system design sense improved? \n\nI\u2019m just tossing around the idea of making a transition towards more traditional SWE. Thanks", "author_fullname": "t2_q03sptgw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ever feel like a professional YAMLer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kckqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678133795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I moved from a Python + SQL heavy role into one where the bulk of the coding is left to more junior members. Meanwhile, I build infrastructure and set up the bigger components. It\u2019s still technical work but I hardly get to write code. It\u2019s mostly configuration through YAML or terraform, or system diagramming. The code that I do write is Ops heavy. I\u2019ll be an expert in k8s, terraform, CICD soon enough\u2026. (Not really, this shit\u2019s complicated).&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not sure I enjoy it as much as writing Python, Go, or even SQL. &lt;/p&gt;\n\n&lt;p&gt;Do y\u2019all enjoy setting up off-the-shelf tools, writing config files in YAML or toml or terraform/infra when you have to? What do you enjoy about it? Has your system design sense improved? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m just tossing around the idea of making a transition towards more traditional SWE. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11kckqy", "is_robot_indexable": true, "report_reasons": null, "author": "bingbangbio", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kckqy/ever_feel_like_a_professional_yamler/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kckqy/ever_feel_like_a_professional_yamler/", "subreddit_subscribers": 92187, "created_utc": 1678133795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data engineers! After having struggled at work with frequently having to transfer files, or even entire directories, between different cloud storage services, I decided to try and make this task easy for me, so I created Fluke, a Python package that offers a simple API which hides away all this complexity regarding file transfer. If your day-to-day work is similar to mine, I think that this project can help you in a big way!\n\nAs of yet, you can use Fluke to transfer files to/from the following locations, but I'll try adding even more in the future:\n\n* Local file system\n* Remote file system (through SSH/SFTP)\n* Amazon S3 (through HTTP)\n* ADLSv2 (through HTTP)\n\nGive it a go and tell me what you think!\n\n* Github: [https://github.com/manoss96/fluke](https://github.com/manoss96/fluke)\n* Docs: [fluke.rtfd.io](https://fluke.rtfd.io/)\n* Example: [https://github.com/manoss96/fluke#usage-example](https://github.com/manoss96/fluke#usage-example)", "author_fullname": "t2_q7l1xoqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created an open source project to help you transfer files between various remote locations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kyceb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678193362.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678192931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data engineers! After having struggled at work with frequently having to transfer files, or even entire directories, between different cloud storage services, I decided to try and make this task easy for me, so I created Fluke, a Python package that offers a simple API which hides away all this complexity regarding file transfer. If your day-to-day work is similar to mine, I think that this project can help you in a big way!&lt;/p&gt;\n\n&lt;p&gt;As of yet, you can use Fluke to transfer files to/from the following locations, but I&amp;#39;ll try adding even more in the future:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Local file system&lt;/li&gt;\n&lt;li&gt;Remote file system (through SSH/SFTP)&lt;/li&gt;\n&lt;li&gt;Amazon S3 (through HTTP)&lt;/li&gt;\n&lt;li&gt;ADLSv2 (through HTTP)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Give it a go and tell me what you think!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Github: &lt;a href=\"https://github.com/manoss96/fluke\"&gt;https://github.com/manoss96/fluke&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Docs: &lt;a href=\"https://fluke.rtfd.io/\"&gt;fluke.rtfd.io&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Example: &lt;a href=\"https://github.com/manoss96/fluke#usage-example\"&gt;https://github.com/manoss96/fluke#usage-example&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?auto=webp&amp;v=enabled&amp;s=03923c7e18e6975fa0a3e01654a87a2b6f0cc2b3", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f521a8cb43c395aeca794cf0f3ec60f80f55269b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f34131bc19b7a2460d2a180839b9720bf985f3f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86ce753cf5795c7b4d99c1fe48a8ced1918f7952", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=678ed5c0637d5d5fc0a9955230dd5f28cab16df6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7620a8bdf7a387d600a8110309c627f2cc9946c4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/e27iJwdTDiMUMjMwyihMu8n-S08RMcoarP6ObneMzW8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=784adf700ac1116ddeddb06409d4e8f7022d314e", "width": 1080, "height": 540}], "variants": {}, "id": "jLtdjHFEEy8dK1PXEN0uUAKPCiwcnjgJmAi80-3-9Xw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11kyceb", "is_robot_indexable": true, "report_reasons": null, "author": "WerdenWissen", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kyceb/i_created_an_open_source_project_to_help_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kyceb/i_created_an_open_source_project_to_help_you/", "subreddit_subscribers": 92187, "created_utc": 1678192931.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! Anyone recently cleared a Tesla data engineer 30 min Python coding interview? Would love some pointers as to what to prepare. Would love some advice please. Thanks a lot!", "author_fullname": "t2_8pd9nebp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tesla data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kpw2w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678166197.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678165595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! Anyone recently cleared a Tesla data engineer 30 min Python coding interview? Would love some pointers as to what to prepare. Would love some advice please. Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "11kpw2w", "is_robot_indexable": true, "report_reasons": null, "author": "Existing_Comment_919", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kpw2w/tesla_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kpw2w/tesla_data_engineer/", "subreddit_subscribers": 92187, "created_utc": 1678165595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I am new here and looking to get some input from people that have more experience in the industry.\n\nI worked in a digital advertising industry before, but very early on realised moving to a data role would suit me. So over 3 years I spent a lot of time learning R and Python, mostly to interact with APIs and automate reporting.\n\nA year ago I finally landed a role of a data analyst for a DTC ecomm company.\n\nThe only problem is I don't really spend a lot of time analysing data. \n\nSo far I have implemented Airbyte for data extraction, suplemented with Airflow orchestrated python scripts for stuff that Airbyte does not support, I wrote all of our DBT models and on top I am creating tableau dashboards for end users.\n\nOur stack currently is Airbyte, Airflow, DBT, BigQuery.\n\nAlso I am the only person in the company that does anything to do with data. So I had to figure out everything on my own.\n\nI have a feeling I have learned a ton past year, but I am also starting to realise that some of the stuff I did is very subpar (no dimensional modeling, non incrimental models in dbt, busines logic scattered all over the place etc.).\n\nMy contract is expiring end of this year and I am looking to you guys for some advice.\n\nFirst of all I want to know if this kind of work I have described is typicall for a data engineer? \n\nI work remotely and have not had a lot of opportunities to talk to people who actually are employed as data engineers.\n\nSecond, is it enough to ge me into a junior data engineer position, or should I aim to learn some new skills?\n\nI am pretty good at python, sql, R, Airflow, i know how to work with VMs and have experience with GCP. I am learning about dimensional modeling now.\n\n\nThirdly, should I stay at current position?\n\nI am currently making $100k a year. But I am not really seing my company hiring anyone more experienced then me, if anything they might hire people under me.\n\nI would really like an opportunity to work with a team and learn from more experienced people. Kinda sick of having to figure out everything on my own, but otherwise I like the type of work I have been doing this past year.\n\nThank you for a long read and thanks in advance for sharing your thoughts.", "author_fullname": "t2_9rpn5ks6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I a data analyst or a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kim2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678146940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I am new here and looking to get some input from people that have more experience in the industry.&lt;/p&gt;\n\n&lt;p&gt;I worked in a digital advertising industry before, but very early on realised moving to a data role would suit me. So over 3 years I spent a lot of time learning R and Python, mostly to interact with APIs and automate reporting.&lt;/p&gt;\n\n&lt;p&gt;A year ago I finally landed a role of a data analyst for a DTC ecomm company.&lt;/p&gt;\n\n&lt;p&gt;The only problem is I don&amp;#39;t really spend a lot of time analysing data. &lt;/p&gt;\n\n&lt;p&gt;So far I have implemented Airbyte for data extraction, suplemented with Airflow orchestrated python scripts for stuff that Airbyte does not support, I wrote all of our DBT models and on top I am creating tableau dashboards for end users.&lt;/p&gt;\n\n&lt;p&gt;Our stack currently is Airbyte, Airflow, DBT, BigQuery.&lt;/p&gt;\n\n&lt;p&gt;Also I am the only person in the company that does anything to do with data. So I had to figure out everything on my own.&lt;/p&gt;\n\n&lt;p&gt;I have a feeling I have learned a ton past year, but I am also starting to realise that some of the stuff I did is very subpar (no dimensional modeling, non incrimental models in dbt, busines logic scattered all over the place etc.).&lt;/p&gt;\n\n&lt;p&gt;My contract is expiring end of this year and I am looking to you guys for some advice.&lt;/p&gt;\n\n&lt;p&gt;First of all I want to know if this kind of work I have described is typicall for a data engineer? &lt;/p&gt;\n\n&lt;p&gt;I work remotely and have not had a lot of opportunities to talk to people who actually are employed as data engineers.&lt;/p&gt;\n\n&lt;p&gt;Second, is it enough to ge me into a junior data engineer position, or should I aim to learn some new skills?&lt;/p&gt;\n\n&lt;p&gt;I am pretty good at python, sql, R, Airflow, i know how to work with VMs and have experience with GCP. I am learning about dimensional modeling now.&lt;/p&gt;\n\n&lt;p&gt;Thirdly, should I stay at current position?&lt;/p&gt;\n\n&lt;p&gt;I am currently making $100k a year. But I am not really seing my company hiring anyone more experienced then me, if anything they might hire people under me.&lt;/p&gt;\n\n&lt;p&gt;I would really like an opportunity to work with a team and learn from more experienced people. Kinda sick of having to figure out everything on my own, but otherwise I like the type of work I have been doing this past year.&lt;/p&gt;\n\n&lt;p&gt;Thank you for a long read and thanks in advance for sharing your thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11kim2m", "is_robot_indexable": true, "report_reasons": null, "author": "CryptographerMain698", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kim2m/am_i_a_data_analyst_or_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kim2m/am_i_a_data_analyst_or_a_data_engineer/", "subreddit_subscribers": 92187, "created_utc": 1678146940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_172g1e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Discord Stores Trillions of Messages", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "name": "t3_11kyewm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wlCEH9KOY9PKb0zBLXsGRK_cTvvVYsWiwgfU5Q6i9ZA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678193118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "discord.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://discord.com/blog/how-discord-stores-trillions-of-messages", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?auto=webp&amp;v=enabled&amp;s=b1fc9516d12abf7d5fa616af3ff87ade48a9bcd5", "width": 1800, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f34b671a3572c6eff7efbf99c11b0c2930d5595d", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bfd681691b142c52ae5548ab621e9eb4da4dfcb", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5c9cb59417ea4eed46919dc15bf294251c6dd4c", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60d656f86b1a63ba5119e5c7daf3b96dd8a725f3", "width": 640, "height": 256}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75f56faf6057b97e7ca7fcdf037130a66d9aedfc", "width": 960, "height": 384}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa538c60fc7d4c5bd81151e90fb222772908404f", "width": 1080, "height": 432}], "variants": {}, "id": "pyoGenD5-m13RONB-xGYn7K1gfpx0aUjaCSfI-OcLdE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11kyewm", "is_robot_indexable": true, "report_reasons": null, "author": "stevecrox0914", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kyewm/how_discord_stores_trillions_of_messages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://discord.com/blog/how-discord-stores-trillions-of-messages", "subreddit_subscribers": 92187, "created_utc": 1678193118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a data engineer who has been working with data pipelines in Airflow with Scala and Apache Spark, I am now tasked with building a data quality check framework that requires planning, high level design, low level design, unit testing, integration testing, and overall software development lifecycle.\nI am looking for resources to help me understand how to deliver an end-to-end project like this, with a focus on proper design for building data quality frameworks.\n\nCan anyone recommend any books, blogs, courses, or other resources that could help me with this? Any advice or guidance would be greatly appreciated. Thank you!", "author_fullname": "t2_2bfywe95", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Request] Tips on building a data quality check framework with proper design in data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kuh6c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678181001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a data engineer who has been working with data pipelines in Airflow with Scala and Apache Spark, I am now tasked with building a data quality check framework that requires planning, high level design, low level design, unit testing, integration testing, and overall software development lifecycle.\nI am looking for resources to help me understand how to deliver an end-to-end project like this, with a focus on proper design for building data quality frameworks.&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend any books, blogs, courses, or other resources that could help me with this? Any advice or guidance would be greatly appreciated. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11kuh6c", "is_robot_indexable": true, "report_reasons": null, "author": "A-n-d-y-R-e-d", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kuh6c/request_tips_on_building_a_data_quality_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kuh6c/request_tips_on_building_a_data_quality_check/", "subreddit_subscribers": 92187, "created_utc": 1678181001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am not sure whether this question has been asked earlier or not. Just needed to know how most of data engineers started their journey\n\n1. Directly joined a data related field, say data analytics or data engineering\n2. Or transitioned from some other related backend role, such as server side or something similar\n3. Or some other domain\n\nI ask this because I wanted to know whether directly getting into data side without much grounding in other backend related technologies would be good choice or not (maybe it could be learned alongside)\n\nEdit: would like to add a bit about myself, came out of college and employed straight into de role. Do not have much footing in other software domains", "author_fullname": "t2_jx4zrwe0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did current data engineers start their software journey as data engineer or something else", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l0m2w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678201604.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678198857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am not sure whether this question has been asked earlier or not. Just needed to know how most of data engineers started their journey&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Directly joined a data related field, say data analytics or data engineering&lt;/li&gt;\n&lt;li&gt;Or transitioned from some other related backend role, such as server side or something similar&lt;/li&gt;\n&lt;li&gt;Or some other domain&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I ask this because I wanted to know whether directly getting into data side without much grounding in other backend related technologies would be good choice or not (maybe it could be learned alongside)&lt;/p&gt;\n\n&lt;p&gt;Edit: would like to add a bit about myself, came out of college and employed straight into de role. Do not have much footing in other software domains&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11l0m2w", "is_robot_indexable": true, "report_reasons": null, "author": "sjdevelop", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l0m2w/did_current_data_engineers_start_their_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l0m2w/did_current_data_engineers_start_their_software/", "subreddit_subscribers": 92187, "created_utc": 1678198857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_d45r1", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Using an AI plugin I made to guess gender from email, do sentiment analysis, and perform a simple segmentation analysis (link in comments)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_11l6h9m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 9, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/4xzjt1lbwcma1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/4xzjt1lbwcma1/DASH_96.mp4", "dash_url": "https://v.redd.it/4xzjt1lbwcma1/DASHPlaylist.mpd?a=1680809909%2COGU4ZDRmY2Q5ZTg2MDhhYjA0NTkyY2FkODY1NTc5ZTE3MTdjZDljNDdkNDU0NTkyYzdhZTk3YjI4MTYzYmFhMw%3D%3D&amp;v=1&amp;f=sd", "duration": 72, "hls_url": "https://v.redd.it/4xzjt1lbwcma1/HLSPlaylist.m3u8?a=1680809909%2CNzcyZWYzZDhjNGFjY2Y2OTk3ZmRhMjkxZWI2M2Y5YmVjOGRkMjNkYTdhOGFmYzEwMmY4OWExZmIwMjEwZDhhOA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://a.thumbs.redditmedia.com/Ph9-GL169jKUNi0bih25e2YVrTWmrSjRrsrcig0Joc4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678212372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/4xzjt1lbwcma1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0Sb7_9OON3MtUmH2ecdgGBUboXZUdbQ7HZ-itT1omrI.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3bca941dbc00b79faecb792a7d4355f206db2af1", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/0Sb7_9OON3MtUmH2ecdgGBUboXZUdbQ7HZ-itT1omrI.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a56566fa40dc834f296d8fa256f16660f41e2efd", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/0Sb7_9OON3MtUmH2ecdgGBUboXZUdbQ7HZ-itT1omrI.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8ea42c2e41dc757b351c9cdfd449535f36da19d0", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/0Sb7_9OON3MtUmH2ecdgGBUboXZUdbQ7HZ-itT1omrI.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=aeb3cee16e481c7dff4e577f69dad34911ad3e28", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/0Sb7_9OON3MtUmH2ecdgGBUboXZUdbQ7HZ-itT1omrI.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a4d5e71815e927a10d79fe390f5113ec2118c994", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/0Sb7_9OON3MtUmH2ecdgGBUboXZUdbQ7HZ-itT1omrI.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e6ecac863e48752e8d7c226296bde135607c40a1", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/0Sb7_9OON3MtUmH2ecdgGBUboXZUdbQ7HZ-itT1omrI.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=22e4197a7fd358a705f6bdac84c2be97752a5a69", "width": 1080, "height": 607}], "variants": {}, "id": "jXv8Ie-nvWFfOIxHl9PsXhMUXyN7Kmywlxirl9jxvVw"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "11l6h9m", "is_robot_indexable": true, "report_reasons": null, "author": "rtwalz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l6h9m/using_an_ai_plugin_i_made_to_guess_gender_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/4xzjt1lbwcma1", "subreddit_subscribers": 92187, "created_utc": 1678212372.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/4xzjt1lbwcma1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/4xzjt1lbwcma1/DASH_96.mp4", "dash_url": "https://v.redd.it/4xzjt1lbwcma1/DASHPlaylist.mpd?a=1680809909%2COGU4ZDRmY2Q5ZTg2MDhhYjA0NTkyY2FkODY1NTc5ZTE3MTdjZDljNDdkNDU0NTkyYzdhZTk3YjI4MTYzYmFhMw%3D%3D&amp;v=1&amp;f=sd", "duration": 72, "hls_url": "https://v.redd.it/4xzjt1lbwcma1/HLSPlaylist.m3u8?a=1680809909%2CNzcyZWYzZDhjNGFjY2Y2OTk3ZmRhMjkxZWI2M2Y5YmVjOGRkMjNkYTdhOGFmYzEwMmY4OWExZmIwMjEwZDhhOA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am not sure how should I partition my data in the S3 data lake. \n\nI decided to go with \u201chive\u201d partitions format so I am gonna use \u201ckey=value\u201d style. But now, how the actual structure should look like?\n\na. year=2023/month=3/day=1/file.parquet (one file only)\n\nb. year=2023/month=3/day1.parquet (multiple files)\n\nc. date=2023-03-01/file.parquet\n\nWhen using b) - how would query like \u201cwhere year=2023 and month = 3 and day = 1\u201d work? It would read only the file \u201cday1\u201d or all files in this \u201cfolder\u201d and then filter the records from these files?\n\nAlso, if using the c) approach - how it would work if I would like to query one month of data? Would the app (say Spark or anything else) know it should take a look only at \u201c2023-03-*\u201d? Or it would scan all the folders?\n\nWhat is the strategy when designing data lake partitions?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lake partitioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l5aft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678209684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not sure how should I partition my data in the S3 data lake. &lt;/p&gt;\n\n&lt;p&gt;I decided to go with \u201chive\u201d partitions format so I am gonna use \u201ckey=value\u201d style. But now, how the actual structure should look like?&lt;/p&gt;\n\n&lt;p&gt;a. year=2023/month=3/day=1/file.parquet (one file only)&lt;/p&gt;\n\n&lt;p&gt;b. year=2023/month=3/day1.parquet (multiple files)&lt;/p&gt;\n\n&lt;p&gt;c. date=2023-03-01/file.parquet&lt;/p&gt;\n\n&lt;p&gt;When using b) - how would query like \u201cwhere year=2023 and month = 3 and day = 1\u201d work? It would read only the file \u201cday1\u201d or all files in this \u201cfolder\u201d and then filter the records from these files?&lt;/p&gt;\n\n&lt;p&gt;Also, if using the c) approach - how it would work if I would like to query one month of data? Would the app (say Spark or anything else) know it should take a look only at \u201c2023-03-*\u201d? Or it would scan all the folders?&lt;/p&gt;\n\n&lt;p&gt;What is the strategy when designing data lake partitions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11l5aft", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l5aft/data_lake_partitioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l5aft/data_lake_partitioning/", "subreddit_subscribers": 92187, "created_utc": 1678209684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team at RudderStack (https://github.com/rudderlabs/rudder-server) \nis organizing [an online challenge for data engineers](https://www.rudderstack.com/blog/join-the-transformations-challenge-for-a-chance-to-win/). There will be prizes for the winners. To make the evaluation fair, I am searching for judges who are not associated with RudderStack and are experts/leaders in data emgineering. As a judge, you will play a critical role in evaluating the submissions. Appreciate your feedback.", "author_fullname": "t2_cbh6ollo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Searching a judge to make a DE challenge evaluation fair", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l4f0m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678207737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team at RudderStack (&lt;a href=\"https://github.com/rudderlabs/rudder-server\"&gt;https://github.com/rudderlabs/rudder-server&lt;/a&gt;) \nis organizing &lt;a href=\"https://www.rudderstack.com/blog/join-the-transformations-challenge-for-a-chance-to-win/\"&gt;an online challenge for data engineers&lt;/a&gt;. There will be prizes for the winners. To make the evaluation fair, I am searching for judges who are not associated with RudderStack and are experts/leaders in data emgineering. As a judge, you will play a critical role in evaluating the submissions. Appreciate your feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?auto=webp&amp;v=enabled&amp;s=594d25c58b78bf4996f7f6ff00cc9a88712b91ca", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ea915fbdb742c7cba2c80924e732a6d3c5a5267", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c289244ad4033c689691d1512f857857c57ed5c2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=611d7dfef702d66f239cd39d44b7811efe88d49d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=085360398328ec0f370bf6a430cf29e0f6371009", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c449cbfad694616b0371e5c1e6e5f1f53f30c859", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/VmeaniG_0xOfB_0K-J2-KLsBbpJ92MZ4WKOA6dqnaGc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45f421afdb5f19e221bed96ad4f0aaa9cd17bf0f", "width": 1080, "height": 540}], "variants": {}, "id": "r1MPAi3-_zC2mqW1Gq01mVQS0WYhEmD0wiuszDr-2yg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11l4f0m", "is_robot_indexable": true, "report_reasons": null, "author": "ephemeral404", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l4f0m/searching_a_judge_to_make_a_de_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l4f0m/searching_a_judge_to_make_a_de_challenge/", "subreddit_subscribers": 92187, "created_utc": 1678207737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently using BigQuery and we are given new instructions to use Snowflake. Don't ask me the why's and whatnot because I also do not know. \nWith that, will it be easy to understand the concepts/functionalities in Snowflake? Or what critical concept should I focus on learning?\nThanks!", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it easy to transition from BigQuery to Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kw2bm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678186359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently using BigQuery and we are given new instructions to use Snowflake. Don&amp;#39;t ask me the why&amp;#39;s and whatnot because I also do not know. \nWith that, will it be easy to understand the concepts/functionalities in Snowflake? Or what critical concept should I focus on learning?\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11kw2bm", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kw2bm/is_it_easy_to_transition_from_bigquery_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kw2bm/is_it_easy_to_transition_from_bigquery_to/", "subreddit_subscribers": 92187, "created_utc": 1678186359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! Looking to implement - call it version 0.001 of a data catalog on top of our AWS-backed data lake.\n\nI can attempt to roll my own on using tools native to AWS.\n\nI can start with an open source tool such as Amundsen.\n\nI may be able to start with an inexpensive SaaS Data Catalog, but I will not have budget for Data Catalog until I'm able to show exactly what I mean by Data Catalog, and I'd love to provide the team something tangible to seed the conversation with our community of data consumers.\n\nDoes anyone use Amundsen?  Is it still a good option in 2023, or is it a dormant project?\n\nIf you are using a data catalog to enable discoverability and self-service analytics, what are you using, and is it working for you?  \n\nDon't know whether to call this a request for help or for discussion; went with \"discussion\" but maybe that's a data quality issue :)\n\nTruly appreciate any information, observations, thoughts, ideas.\n\nBest!", "author_fullname": "t2_w9nubpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Catalog, Data Discovery, Active Metadata Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kfaeu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678139495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Looking to implement - call it version 0.001 of a data catalog on top of our AWS-backed data lake.&lt;/p&gt;\n\n&lt;p&gt;I can attempt to roll my own on using tools native to AWS.&lt;/p&gt;\n\n&lt;p&gt;I can start with an open source tool such as Amundsen.&lt;/p&gt;\n\n&lt;p&gt;I may be able to start with an inexpensive SaaS Data Catalog, but I will not have budget for Data Catalog until I&amp;#39;m able to show exactly what I mean by Data Catalog, and I&amp;#39;d love to provide the team something tangible to seed the conversation with our community of data consumers.&lt;/p&gt;\n\n&lt;p&gt;Does anyone use Amundsen?  Is it still a good option in 2023, or is it a dormant project?&lt;/p&gt;\n\n&lt;p&gt;If you are using a data catalog to enable discoverability and self-service analytics, what are you using, and is it working for you?  &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t know whether to call this a request for help or for discussion; went with &amp;quot;discussion&amp;quot; but maybe that&amp;#39;s a data quality issue :)&lt;/p&gt;\n\n&lt;p&gt;Truly appreciate any information, observations, thoughts, ideas.&lt;/p&gt;\n\n&lt;p&gt;Best!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11kfaeu", "is_robot_indexable": true, "report_reasons": null, "author": "realrussgreen", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kfaeu/data_catalog_data_discovery_active_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kfaeu/data_catalog_data_discovery_active_metadata/", "subreddit_subscribers": 92187, "created_utc": 1678139495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nMy team wants to enable an automated way to run SQL pipelines in our AWS EKS environment.\nOur first idea is to use dbt as the SQL framework. However we can't find an easy way to have a transient spark cluster running theses dbt pipelines.\n\nHave anybody already worked with dbt and AWS EKS transient cluster?\nAny thoughts about it?\n\nWhat we have in mind to test:\n\nOption A\n- Apply a spark cluter in AWS EKS with a thrift server\n- Find its host and use it to run dbt pipeline\n- Kill the cluster\n\nOption B\n- Add dbt to the EMR on EKS image with a spark thrift server (we need to understand how it can be done)\n- Run dbt pipeline as entrypoint for this image using the local thrift server\n\nThanks for any thoughts!", "author_fullname": "t2_n0qka4td", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using dbt with transient spark cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11l7dz2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678214387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;My team wants to enable an automated way to run SQL pipelines in our AWS EKS environment.\nOur first idea is to use dbt as the SQL framework. However we can&amp;#39;t find an easy way to have a transient spark cluster running theses dbt pipelines.&lt;/p&gt;\n\n&lt;p&gt;Have anybody already worked with dbt and AWS EKS transient cluster?\nAny thoughts about it?&lt;/p&gt;\n\n&lt;p&gt;What we have in mind to test:&lt;/p&gt;\n\n&lt;p&gt;Option A\n- Apply a spark cluter in AWS EKS with a thrift server\n- Find its host and use it to run dbt pipeline\n- Kill the cluster&lt;/p&gt;\n\n&lt;p&gt;Option B\n- Add dbt to the EMR on EKS image with a spark thrift server (we need to understand how it can be done)\n- Run dbt pipeline as entrypoint for this image using the local thrift server&lt;/p&gt;\n\n&lt;p&gt;Thanks for any thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11l7dz2", "is_robot_indexable": true, "report_reasons": null, "author": "Data_Broccoli_PII", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l7dz2/using_dbt_with_transient_spark_cluster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l7dz2/using_dbt_with_transient_spark_cluster/", "subreddit_subscribers": 92187, "created_utc": 1678214387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been doing Data Engineering and Architecture work for past 15 odd years. I have built various data warehouses / data Marts /data lakes across multiple big companies. I have managed people before but in my current role I am mainly responsible for architecture as per my job title. But I have been doing hiring , product management and workload management etc. To me my natural progression can be into more of leadership (Director ) role. But company (my boss ) wants me to stick to architecture path. I have seen in past where people managers have taken undue advantage of technical talent in my company and have say over hiring /firing etc. I am 39 and feel like if I focus on strategic side of data world, I might have longer run and more satisfying career in the end. Please give me your thoughts.", "author_fullname": "t2_jbyap7dk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architect Vs Leadership", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l2bjs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678202972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been doing Data Engineering and Architecture work for past 15 odd years. I have built various data warehouses / data Marts /data lakes across multiple big companies. I have managed people before but in my current role I am mainly responsible for architecture as per my job title. But I have been doing hiring , product management and workload management etc. To me my natural progression can be into more of leadership (Director ) role. But company (my boss ) wants me to stick to architecture path. I have seen in past where people managers have taken undue advantage of technical talent in my company and have say over hiring /firing etc. I am 39 and feel like if I focus on strategic side of data world, I might have longer run and more satisfying career in the end. Please give me your thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11l2bjs", "is_robot_indexable": true, "report_reasons": null, "author": "Dry_Damage_6629", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l2bjs/architect_vs_leadership/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l2bjs/architect_vs_leadership/", "subreddit_subscribers": 92187, "created_utc": 1678202972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a customer success/technical support team member tasked with supporting our company's need to both distribute reports to our customers on a regular basis (i.e. daily, weekly, monthly) and support our internal teams (finance, accounting, analytics etc) in obtaining data/reports from our partners which we need ourselves.\n\nThere are a few issues...\n\n1. Our customers would like their reports delivered to different locations (FTP, dropbox, s3 etc) depending on who needs what where... and we need to use their credentials\n2. Some deliveries / downloads fail...so we need to resend / retry.\n3. Files aren't always available when they should be. We often have to check back if the file is available or raise an alarm if things get really late... (this applies to both reports we are responsible to deliver and the reports we need to download)\n\nWe have built some internal tooling but it's not the best, doesn't cover all the edge cases and has become one more thing for me to babysit. \n\nWhat are people doing for these kinds of things?", "author_fullname": "t2_w3xdre53", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically transferring files to where they need to go?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kv60j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678183451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a customer success/technical support team member tasked with supporting our company&amp;#39;s need to both distribute reports to our customers on a regular basis (i.e. daily, weekly, monthly) and support our internal teams (finance, accounting, analytics etc) in obtaining data/reports from our partners which we need ourselves.&lt;/p&gt;\n\n&lt;p&gt;There are a few issues...&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Our customers would like their reports delivered to different locations (FTP, dropbox, s3 etc) depending on who needs what where... and we need to use their credentials&lt;/li&gt;\n&lt;li&gt;Some deliveries / downloads fail...so we need to resend / retry.&lt;/li&gt;\n&lt;li&gt;Files aren&amp;#39;t always available when they should be. We often have to check back if the file is available or raise an alarm if things get really late... (this applies to both reports we are responsible to deliver and the reports we need to download)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We have built some internal tooling but it&amp;#39;s not the best, doesn&amp;#39;t cover all the edge cases and has become one more thing for me to babysit. &lt;/p&gt;\n\n&lt;p&gt;What are people doing for these kinds of things?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11kv60j", "is_robot_indexable": true, "report_reasons": null, "author": "qsconetwothree", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kv60j/automatically_transferring_files_to_where_they/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kv60j/automatically_transferring_files_to_where_they/", "subreddit_subscribers": 92187, "created_utc": 1678183451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking to run a simple pipeline in Azure with whatever service is easiest to stand up. Have a few python files pulling from a web api, they are helper modules that execute in main python file. Need to run them once a week. And store output in some kind of storage\u2026 sql pool or dedicated, blob. Doesn\u2019t matter. Any suggestions helpful. Mainly looking for an easy orchestration or trigger for the weekly batch load. Thanks again", "author_fullname": "t2_vmi9ivlo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline in azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kmmqx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678156718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to run a simple pipeline in Azure with whatever service is easiest to stand up. Have a few python files pulling from a web api, they are helper modules that execute in main python file. Need to run them once a week. And store output in some kind of storage\u2026 sql pool or dedicated, blob. Doesn\u2019t matter. Any suggestions helpful. Mainly looking for an easy orchestration or trigger for the weekly batch load. Thanks again&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11kmmqx", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable-Watch-79", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kmmqx/pipeline_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kmmqx/pipeline_in_azure/", "subreddit_subscribers": 92187, "created_utc": 1678156718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI have been working with Data Fusion for the first time, i was able to get Data from Salesforce Marketing Cloud and create a Table (BIG QUERY - GCP)\n\nNow, i\u00b4m trying to get a CSV file from a STFP and produce a BigQuery Table, but I wasn\u2019t able to find a proper guideline on how to do it.\n\nHere\u00b4s an example of how i\u00b4m trying to achieve it. with SFTP Properties\n\nAny help or link that you can provide will be welcome.\n\nThank you all.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4kzpdhsk6dma1.jpg?width=691&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=92f1567912823c440ade88d4410a94b814962373\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1xr6bcwl6dma1.jpg?width=903&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b04901b09a8bad75c5510c8ac1faeb313793f956", "author_fullname": "t2_m2kvp34t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataFusion: From SFTP to Bigquery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 45, "top_awarded_type": null, "hide_score": true, "media_metadata": {"4kzpdhsk6dma1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/4kzpdhsk6dma1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97c376d7d710326773ce35714f31832f3f070c95"}, {"y": 70, "x": 216, "u": "https://preview.redd.it/4kzpdhsk6dma1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c98ecb365057671a06e96ad71d5278b782ac42ea"}, {"y": 104, "x": 320, "u": "https://preview.redd.it/4kzpdhsk6dma1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ffbc9187882e7044cc1062ec01296d614c1a137"}, {"y": 208, "x": 640, "u": "https://preview.redd.it/4kzpdhsk6dma1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a94b8c03db05916d5036480bdf91156cd061fb77"}], "s": {"y": 225, "x": 691, "u": "https://preview.redd.it/4kzpdhsk6dma1.jpg?width=691&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=92f1567912823c440ade88d4410a94b814962373"}, "id": "4kzpdhsk6dma1"}, "1xr6bcwl6dma1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/1xr6bcwl6dma1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=225f25cde6010b957bce7221e8545201fef4c602"}, {"y": 128, "x": 216, "u": "https://preview.redd.it/1xr6bcwl6dma1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2b73b39624e57dd2f15def3e49e570754529c04"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/1xr6bcwl6dma1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a19c472581de6eb724caeffe56d4fbcfb9a5d19"}, {"y": 379, "x": 640, "u": "https://preview.redd.it/1xr6bcwl6dma1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f92d0ecada731d5d2c8e9fb6f115a03fa5be6fe"}], "s": {"y": 536, "x": 903, "u": "https://preview.redd.it/1xr6bcwl6dma1.jpg?width=903&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b04901b09a8bad75c5510c8ac1faeb313793f956"}, "id": "1xr6bcwl6dma1"}}, "name": "t3_11l7v28", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1KmZQVgiyi2Y9HG3ogU4HBRbEmcZGsSltc_xuQmH5-M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678215465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working with Data Fusion for the first time, i was able to get Data from Salesforce Marketing Cloud and create a Table (BIG QUERY - GCP)&lt;/p&gt;\n\n&lt;p&gt;Now, i\u00b4m trying to get a CSV file from a STFP and produce a BigQuery Table, but I wasn\u2019t able to find a proper guideline on how to do it.&lt;/p&gt;\n\n&lt;p&gt;Here\u00b4s an example of how i\u00b4m trying to achieve it. with SFTP Properties&lt;/p&gt;\n\n&lt;p&gt;Any help or link that you can provide will be welcome.&lt;/p&gt;\n\n&lt;p&gt;Thank you all.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4kzpdhsk6dma1.jpg?width=691&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=92f1567912823c440ade88d4410a94b814962373\"&gt;https://preview.redd.it/4kzpdhsk6dma1.jpg?width=691&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=92f1567912823c440ade88d4410a94b814962373&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1xr6bcwl6dma1.jpg?width=903&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b04901b09a8bad75c5510c8ac1faeb313793f956\"&gt;https://preview.redd.it/1xr6bcwl6dma1.jpg?width=903&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b04901b09a8bad75c5510c8ac1faeb313793f956&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11l7v28", "is_robot_indexable": true, "report_reasons": null, "author": "neromerob", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l7v28/datafusion_from_sftp_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l7v28/datafusion_from_sftp_to_bigquery/", "subreddit_subscribers": 92187, "created_utc": 1678215465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m searching for a tool I can use to request data from an API (Spotify\u2019s Web API), transform it, and load it to a MongoDB Atlas Database. My jobs will need to run both on scheduled basis and on a triggered basis, on the cloud.\n\nI tried to implement Atlas Trigger Functions as a solution, but that was a dead end due to their execution-time and socket limitations. \n\nAdditionally, I\u2019ve looked into a handful of IPaaS platforms (SnapLogic, Zapier, Workato) but their pricing models seem pretty steep for my use case.\n\nSome of my jobs will need to make 1,000\u2019s calls to the API, and will likely have an execution time over an hour.\n\nAny ideas for a potential solution? Thoughts on ETL tools like AWS Glue?", "author_fullname": "t2_5l0hu6ke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need a Data Integration Tool (Spotify Web API =&gt; MongoDB Atlas DB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11l7pqc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678215125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m searching for a tool I can use to request data from an API (Spotify\u2019s Web API), transform it, and load it to a MongoDB Atlas Database. My jobs will need to run both on scheduled basis and on a triggered basis, on the cloud.&lt;/p&gt;\n\n&lt;p&gt;I tried to implement Atlas Trigger Functions as a solution, but that was a dead end due to their execution-time and socket limitations. &lt;/p&gt;\n\n&lt;p&gt;Additionally, I\u2019ve looked into a handful of IPaaS platforms (SnapLogic, Zapier, Workato) but their pricing models seem pretty steep for my use case.&lt;/p&gt;\n\n&lt;p&gt;Some of my jobs will need to make 1,000\u2019s calls to the API, and will likely have an execution time over an hour.&lt;/p&gt;\n\n&lt;p&gt;Any ideas for a potential solution? Thoughts on ETL tools like AWS Glue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11l7pqc", "is_robot_indexable": true, "report_reasons": null, "author": "Level-Gur8656", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l7pqc/need_a_data_integration_tool_spotify_web_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l7pqc/need_a_data_integration_tool_spotify_web_api/", "subreddit_subscribers": 92187, "created_utc": 1678215125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey - I know this topic has been discussed a million times, but I am still not convinced.   \n\n\nWhat would we want to load  structured data sets in a datalake first (s3, blob, gcp, ...) before our target dwh. I feel it's doubling the workload on data engineers. Instead, it would be just easier to take your data from the source and load it directly to the endpoint. I deal with structured data only and I don't get the point of data lakes. If someone can illuminate me on the question, I would appreciate it :) \n\nThank you !!!", "author_fullname": "t2_g0kwnodf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question of the Day", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l3bs7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678205253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey - I know this topic has been discussed a million times, but I am still not convinced.   &lt;/p&gt;\n\n&lt;p&gt;What would we want to load  structured data sets in a datalake first (s3, blob, gcp, ...) before our target dwh. I feel it&amp;#39;s doubling the workload on data engineers. Instead, it would be just easier to take your data from the source and load it directly to the endpoint. I deal with structured data only and I don&amp;#39;t get the point of data lakes. If someone can illuminate me on the question, I would appreciate it :) &lt;/p&gt;\n\n&lt;p&gt;Thank you !!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11l3bs7", "is_robot_indexable": true, "report_reasons": null, "author": "That-Refrigerator901", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l3bs7/question_of_the_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l3bs7/question_of_the_day/", "subreddit_subscribers": 92187, "created_utc": 1678205253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It seems like there is definitely an angle where people prefer the Querybook experience (esp the SQL editor w/ charts and results in one screen + the sharing experience) but curious why it has been so quiet. \n\nIs it the team not getting enough resources to back the community? Or is the Querybook value proposition itself not fitting or salvageable in the modern data stack landscape?", "author_fullname": "t2_13l4vh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Querybook dead?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11l0f07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678198373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems like there is definitely an angle where people prefer the Querybook experience (esp the SQL editor w/ charts and results in one screen + the sharing experience) but curious why it has been so quiet. &lt;/p&gt;\n\n&lt;p&gt;Is it the team not getting enough resources to back the community? Or is the Querybook value proposition itself not fitting or salvageable in the modern data stack landscape?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11l0f07", "is_robot_indexable": true, "report_reasons": null, "author": "yagummoth", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11l0f07/is_querybook_dead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11l0f07/is_querybook_dead/", "subreddit_subscribers": 92187, "created_utc": 1678198373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our company is moving to the Medallion architecture using Databricks and I am a bit worried/confused about how we are going to handle so many joins in Spark. So many of our tables use anywhere between 5-20 joins to create them. This is mostly due to one of our main data sources being itself a DataWarehouse that we extract and rebuild every day and use that data to help build our datawarehouse. Are we just going to need to constantly performance tuning the joins to death or is there a better way to do it? Im in my first year of DE but its my understanding that Spark is not great at handling joins.", "author_fullname": "t2_4bsgo8fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Conceptual Handling of Joins to Gold Layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kzmx9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678196446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our company is moving to the Medallion architecture using Databricks and I am a bit worried/confused about how we are going to handle so many joins in Spark. So many of our tables use anywhere between 5-20 joins to create them. This is mostly due to one of our main data sources being itself a DataWarehouse that we extract and rebuild every day and use that data to help build our datawarehouse. Are we just going to need to constantly performance tuning the joins to death or is there a better way to do it? Im in my first year of DE but its my understanding that Spark is not great at handling joins.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11kzmx9", "is_robot_indexable": true, "report_reasons": null, "author": "Hexboy3", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kzmx9/conceptual_handling_of_joins_to_gold_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kzmx9/conceptual_handling_of_joins_to_gold_layer/", "subreddit_subscribers": 92187, "created_utc": 1678196446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Databrew](https://docs.aws.amazon.com/databrew/latest/dg/what-is.html) as far as I can tell, is a no-code visual data prep tool based on spark that allows you to output csv files based on an input source and a series of actions to take on it.  \n\nLike most visual tools, I'm not a fan of using it myself-- however, it _might_ be possible for a researcher or analyst to use this tool without having to know python/SQL, in order to do their own last-mile data cleaning, joining, filtering, and modifying.\n\nOne of the nice things about it is that it seems to be able to connect to a range of data sources: local files, files in object storage, databases, datalakes, etc. \n\nLike most visual tools though, it lacks version control, ability to write custom code, and doesn't feel like the best tool for the job. \n\nI estimate that this tool might be decent in a narrow range of applications when you: want to offload responsibility of last-mile data delivery to someone who has little experience coding or you specifically want to give someone the power of spark compute that wouldn't otherwise be able to get it, AND they are familiar enough with the data to be confident to make changes. \n\nHas anybody used or set up databrew for others? Was there any notable upsides/downsides?", "author_fullname": "t2_kdj5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone utilize AWS Glue Databrew in their organization?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11kzmb2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678196401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://docs.aws.amazon.com/databrew/latest/dg/what-is.html\"&gt;Databrew&lt;/a&gt; as far as I can tell, is a no-code visual data prep tool based on spark that allows you to output csv files based on an input source and a series of actions to take on it.  &lt;/p&gt;\n\n&lt;p&gt;Like most visual tools, I&amp;#39;m not a fan of using it myself-- however, it &lt;em&gt;might&lt;/em&gt; be possible for a researcher or analyst to use this tool without having to know python/SQL, in order to do their own last-mile data cleaning, joining, filtering, and modifying.&lt;/p&gt;\n\n&lt;p&gt;One of the nice things about it is that it seems to be able to connect to a range of data sources: local files, files in object storage, databases, datalakes, etc. &lt;/p&gt;\n\n&lt;p&gt;Like most visual tools though, it lacks version control, ability to write custom code, and doesn&amp;#39;t feel like the best tool for the job. &lt;/p&gt;\n\n&lt;p&gt;I estimate that this tool might be decent in a narrow range of applications when you: want to offload responsibility of last-mile data delivery to someone who has little experience coding or you specifically want to give someone the power of spark compute that wouldn&amp;#39;t otherwise be able to get it, AND they are familiar enough with the data to be confident to make changes. &lt;/p&gt;\n\n&lt;p&gt;Has anybody used or set up databrew for others? Was there any notable upsides/downsides?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11kzmb2", "is_robot_indexable": true, "report_reasons": null, "author": "Touvejs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11kzmb2/does_anyone_utilize_aws_glue_databrew_in_their/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11kzmb2/does_anyone_utilize_aws_glue_databrew_in_their/", "subreddit_subscribers": 92187, "created_utc": 1678196401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI know that a Azure Data Factory is used for batch ingestion but I'm wondering what would be the smallest intervall to use a DF Pipeline rather than a real time ingestion solution? Are 1 Minute, 2 Minute, 3 Minutes ingestion Intervall already too small to use Data Factory Pipelines? What would be the alternative I'm Azure to stream for example data from a blob storage to a data lake in a 2 Minute Intervall?", "author_fullname": "t2_w28f7k0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When to use Data factory and when real time ingestion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ky7mj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678192569.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I know that a Azure Data Factory is used for batch ingestion but I&amp;#39;m wondering what would be the smallest intervall to use a DF Pipeline rather than a real time ingestion solution? Are 1 Minute, 2 Minute, 3 Minutes ingestion Intervall already too small to use Data Factory Pipelines? What would be the alternative I&amp;#39;m Azure to stream for example data from a blob storage to a data lake in a 2 Minute Intervall?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11ky7mj", "is_robot_indexable": true, "report_reasons": null, "author": "Throwa01221", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ky7mj/when_to_use_data_factory_and_when_real_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ky7mj/when_to_use_data_factory_and_when_real_time/", "subreddit_subscribers": 92187, "created_utc": 1678192569.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}