{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys. I can't find a single decent kafka tutorial on youtube. I have a Code with Mosh subscription but there's nothing on there either. Does anyone have a good recommendation? There's lots of videos that explain the theory but not a lot that put it into practice.\n\nThank you in advance!", "author_fullname": "t2_keo5p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good kafka tutorial", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g2rby", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677763643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I can&amp;#39;t find a single decent kafka tutorial on youtube. I have a Code with Mosh subscription but there&amp;#39;s nothing on there either. Does anyone have a good recommendation? There&amp;#39;s lots of videos that explain the theory but not a lot that put it into practice.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11g2rby", "is_robot_indexable": true, "report_reasons": null, "author": "niniox", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11g2rby/good_kafka_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11g2rby/good_kafka_tutorial/", "subreddit_subscribers": 853296, "created_utc": 1677763643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been struggling with staying organized as an MLE for quite a long time. \n\nI have multiple model classes to work on and multiple iteration per model class. Then each have experiments assigned or not. Some have resource constraints  that need to be resolved. Some models need to finish at the same time and be experimented simultanously, etcetra.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nWondering, how do you keep your self organized and the modelling pipeline progress smooth? Any app, toolbox, methodology helpful for this?", "author_fullname": "t2_e6ppgt2u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Meta] How do you stay organized as a machine learning engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fk7to", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677709591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been struggling with staying organized as an MLE for quite a long time. &lt;/p&gt;\n\n&lt;p&gt;I have multiple model classes to work on and multiple iteration per model class. Then each have experiments assigned or not. Some have resource constraints  that need to be resolved. Some models need to finish at the same time and be experimented simultanously, etcetra.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Wondering, how do you keep your self organized and the modelling pipeline progress smooth? Any app, toolbox, methodology helpful for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11fk7to", "is_robot_indexable": true, "report_reasons": null, "author": "Which-Distance1384", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11fk7to/meta_how_do_you_stay_organized_as_a_machine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11fk7to/meta_how_do_you_stay_organized_as_a_machine/", "subreddit_subscribers": 853296, "created_utc": 1677709591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all. My team has just open sourced a Python library that hopefully makes Kafka a bit more user-friendly for data Science and ML folks (you can find it here: [**quix-streams**](https://github.com/quixio/quix-streams))   \nWhat I like about it is that you can send Pandas DataFrames straight to Kafka without any kind of conversion which makes things easier\u2014i.e. like this: \n\n    def on_parameter_data_handler(df: pd.DataFrame):\n    \n        # If the braking force applied is more than 50%, we mark HardBraking with True\n        df[\"HardBraking\"] = df.apply(lambda row: \"True\" if row.Brake &gt; 0.5 else \"False\", axis=1)\n    \n        stream_producer.timeseries.publish(df)  # Send data back to the stream\n\n Anyway, just posting it here with the hope that it makes someone\u2019s job easier.", "author_fullname": "t2_byx12w8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A more accessible python library for interacting with Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g68dw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677772668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all. My team has just open sourced a Python library that hopefully makes Kafka a bit more user-friendly for data Science and ML folks (you can find it here: &lt;a href=\"https://github.com/quixio/quix-streams\"&gt;&lt;strong&gt;quix-streams&lt;/strong&gt;&lt;/a&gt;)&lt;br/&gt;\nWhat I like about it is that you can send Pandas DataFrames straight to Kafka without any kind of conversion which makes things easier\u2014i.e. like this: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def on_parameter_data_handler(df: pd.DataFrame):\n\n    # If the braking force applied is more than 50%, we mark HardBraking with True\n    df[&amp;quot;HardBraking&amp;quot;] = df.apply(lambda row: &amp;quot;True&amp;quot; if row.Brake &amp;gt; 0.5 else &amp;quot;False&amp;quot;, axis=1)\n\n    stream_producer.timeseries.publish(df)  # Send data back to the stream\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Anyway, just posting it here with the hope that it makes someone\u2019s job easier.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7J1m6iMTVT8_bsCsiQK-bnfSYSw1FFcL5AAAxKD45tw.jpg?auto=webp&amp;v=enabled&amp;s=78d5675a84f6726e7618cd3c42250b7d51c363fa", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7J1m6iMTVT8_bsCsiQK-bnfSYSw1FFcL5AAAxKD45tw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13a6a9b0f62ec4033e162a3122ec7f912271ca6a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7J1m6iMTVT8_bsCsiQK-bnfSYSw1FFcL5AAAxKD45tw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc65e91fc8010c02faecf87eb3e910a374973c22", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/7J1m6iMTVT8_bsCsiQK-bnfSYSw1FFcL5AAAxKD45tw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=42f0e4e73e3e4a79267bd8b98e97ca66836c96e6", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/7J1m6iMTVT8_bsCsiQK-bnfSYSw1FFcL5AAAxKD45tw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19838c0ac27c309b1e866cc3aaae3b447bdd9671", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/7J1m6iMTVT8_bsCsiQK-bnfSYSw1FFcL5AAAxKD45tw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7215b5d40310d21d5d1e4a91848bac7d3147f437", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/7J1m6iMTVT8_bsCsiQK-bnfSYSw1FFcL5AAAxKD45tw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fdcf5eafb0a7070f4aea6e4bdfacc90c0c14c34d", "width": 1080, "height": 540}], "variants": {}, "id": "ZuoT62YF_0noD68oqz7qPsTsv9obcPgozqhj0lnVZyk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11g68dw", "is_robot_indexable": true, "report_reasons": null, "author": "Jota_Blanco", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11g68dw/a_more_accessible_python_library_for_interacting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11g68dw/a_more_accessible_python_library_for_interacting/", "subreddit_subscribers": 853296, "created_utc": 1677772668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm currently employing forward selection on a large dataset, and wondering, should one be striving for minimizing the RMSE, or maximizing the R2 score? I've found out that it doesn't give the same model parameters.", "author_fullname": "t2_6zlc2ji4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing for RMSE vs. R2, for feature selection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g416x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677767070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently employing forward selection on a large dataset, and wondering, should one be striving for minimizing the RMSE, or maximizing the R2 score? I&amp;#39;ve found out that it doesn&amp;#39;t give the same model parameters.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11g416x", "is_robot_indexable": true, "report_reasons": null, "author": "dilkur", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11g416x/optimizing_for_rmse_vs_r2_for_feature_selection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11g416x/optimizing_for_rmse_vs_r2_for_feature_selection/", "subreddit_subscribers": 853296, "created_utc": 1677767070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone! I'd love to hear your thoughts on my story. I applied for Slack DS internship early November. They reached out in December and I interviewed in mid-December. After the first round, they got me waiting for long. The recruiter reached out in Jan 20 finally and said I'm in for the second round which will be the last one ( except for the presentation round after that). I did my second interview on Jan 27 and again since then, they didn't get back to me. I emailed the recruiter - no response. I contacted the manager with whom I interviewed and she said she is not sure about the recruitment progress and that I should contact the recruiter. She also said that she will check with the recruiter herself. As of now, I'm still waiting. My workday status is interview. I know it isn't going right but can't stop thinking about what's happening. Is it just ghosting? Then why the manager got back with basically no updates? It just sucks to get no response whatsoever... \n\nThank you!", "author_fullname": "t2_31lqqzal", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slack DS Summer Intern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11foffz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677719751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I&amp;#39;d love to hear your thoughts on my story. I applied for Slack DS internship early November. They reached out in December and I interviewed in mid-December. After the first round, they got me waiting for long. The recruiter reached out in Jan 20 finally and said I&amp;#39;m in for the second round which will be the last one ( except for the presentation round after that). I did my second interview on Jan 27 and again since then, they didn&amp;#39;t get back to me. I emailed the recruiter - no response. I contacted the manager with whom I interviewed and she said she is not sure about the recruitment progress and that I should contact the recruiter. She also said that she will check with the recruiter herself. As of now, I&amp;#39;m still waiting. My workday status is interview. I know it isn&amp;#39;t going right but can&amp;#39;t stop thinking about what&amp;#39;s happening. Is it just ghosting? Then why the manager got back with basically no updates? It just sucks to get no response whatsoever... &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11foffz", "is_robot_indexable": true, "report_reasons": null, "author": "pdb29", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11foffz/slack_ds_summer_intern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11foffz/slack_ds_summer_intern/", "subreddit_subscribers": 853296, "created_utc": 1677719751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a new graduate working as a data analyst for a large health organization. I was formally trained in Stata and have some experience using R and Python. Our organization uses SQL to query data, and is moving more towards SAS as the standard for analyses. I had always planned on moving away from Stata and focusing more on SQL, R and Python, but I am now expected to learn SAS. I would still like to be proficient in R and Python, however, I'm worried that trying to learn too many programs will limit my ability to really excel in any one of them.\n\nMy question is how many statistical programs do you have in your \"toolkit\", and how many can one learn before reaching a point of diminishing returns?", "author_fullname": "t2_129ywp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many statistical programs have you learned?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fjzre", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677709091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a new graduate working as a data analyst for a large health organization. I was formally trained in Stata and have some experience using R and Python. Our organization uses SQL to query data, and is moving more towards SAS as the standard for analyses. I had always planned on moving away from Stata and focusing more on SQL, R and Python, but I am now expected to learn SAS. I would still like to be proficient in R and Python, however, I&amp;#39;m worried that trying to learn too many programs will limit my ability to really excel in any one of them.&lt;/p&gt;\n\n&lt;p&gt;My question is how many statistical programs do you have in your &amp;quot;toolkit&amp;quot;, and how many can one learn before reaching a point of diminishing returns?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11fjzre", "is_robot_indexable": true, "report_reasons": null, "author": "amipregananant", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11fjzre/how_many_statistical_programs_have_you_learned/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11fjzre/how_many_statistical_programs_have_you_learned/", "subreddit_subscribers": 853296, "created_utc": 1677709091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We have a model that's trained on the end of month data. But we need to use it to get predictions before end of month - at the end of each week.\n\nProblem is, for some features end of month missing percentage is ~10%, so that was the training data and we used median imputes which isn't too terrible in this case. But at the beginning this missing rate is about 50% - these are the scoring data. \n\nBut, what to do with these columns with 50% missing in scoring data? Imputing them with training data mean will inflate these values and we can't say missing is 0 since 0 is a valid data in these cols. Is using tree based algos RF/XG or CatBoost a better alternative since then we won't have to impute the missing values?\n\nAny tips would be highly appreciated \ud83d\ude4f", "author_fullname": "t2_9axqyq8u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Missing data related", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11g8u9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677778919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a model that&amp;#39;s trained on the end of month data. But we need to use it to get predictions before end of month - at the end of each week.&lt;/p&gt;\n\n&lt;p&gt;Problem is, for some features end of month missing percentage is ~10%, so that was the training data and we used median imputes which isn&amp;#39;t too terrible in this case. But at the beginning this missing rate is about 50% - these are the scoring data. &lt;/p&gt;\n\n&lt;p&gt;But, what to do with these columns with 50% missing in scoring data? Imputing them with training data mean will inflate these values and we can&amp;#39;t say missing is 0 since 0 is a valid data in these cols. Is using tree based algos RF/XG or CatBoost a better alternative since then we won&amp;#39;t have to impute the missing values?&lt;/p&gt;\n\n&lt;p&gt;Any tips would be highly appreciated \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11g8u9w", "is_robot_indexable": true, "report_reasons": null, "author": "Difficult-Big-3890", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11g8u9w/missing_data_related/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11g8u9w/missing_data_related/", "subreddit_subscribers": 853296, "created_utc": 1677778919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Introduction to NoSQL - Free online course\n\nThis free course will help you explore NoSQL from the basics. It begins by explaining why SQL is not enough, followed by an introduction to NoSQL and its necessity.\n\n[https://formationgratuite.net/Introduction-to-NoSQL/](https://formationgratuite.net/Introduction-to-NoSQL/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1g9s1coslcla1.jpg?width=1755&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=071ffc83482077bf4b4b944cef8bb72b4e6125b5", "author_fullname": "t2_ca2mv3e7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to NoSQL - Free online course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "media_metadata": {"1g9s1coslcla1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/1g9s1coslcla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6822a1f1391571e570c11d07666f40804cf96cdf"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/1g9s1coslcla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=434557402a3e59baf5f84e97d769883e28bf29c8"}, {"y": 226, "x": 320, "u": "https://preview.redd.it/1g9s1coslcla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c31fc48e0ead3286d3a592f0d8ed3b2853c77c7"}, {"y": 452, "x": 640, "u": "https://preview.redd.it/1g9s1coslcla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15cdddcdb34ff8d74688509214b0ba48926443a9"}, {"y": 678, "x": 960, "u": "https://preview.redd.it/1g9s1coslcla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68d2cf21b06dbffade53a2c34ace733d955ee181"}, {"y": 763, "x": 1080, "u": "https://preview.redd.it/1g9s1coslcla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9748f2253a02abf15393538419bdd77bc7b45db"}], "s": {"y": 1240, "x": 1755, "u": "https://preview.redd.it/1g9s1coslcla1.jpg?width=1755&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=071ffc83482077bf4b4b944cef8bb72b4e6125b5"}, "id": "1g9s1coslcla1"}}, "name": "t3_11g67he", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Tu9bwjeJ8rLagzrDj3EQs8NNicFCIpmoTVq92ScmW-U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677772604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Introduction to NoSQL - Free online course&lt;/p&gt;\n\n&lt;p&gt;This free course will help you explore NoSQL from the basics. It begins by explaining why SQL is not enough, followed by an introduction to NoSQL and its necessity.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://formationgratuite.net/Introduction-to-NoSQL/\"&gt;https://formationgratuite.net/Introduction-to-NoSQL/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1g9s1coslcla1.jpg?width=1755&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=071ffc83482077bf4b4b944cef8bb72b4e6125b5\"&gt;https://preview.redd.it/1g9s1coslcla1.jpg?width=1755&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=071ffc83482077bf4b4b944cef8bb72b4e6125b5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11g67he", "is_robot_indexable": true, "report_reasons": null, "author": "MDLearning", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11g67he/introduction_to_nosql_free_online_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11g67he/introduction_to_nosql_free_online_course/", "subreddit_subscribers": 853296, "created_utc": 1677772604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "You have a bunch of time series data from some sensors attached to different parts of a machine and one or more parts get damaged, which, in principle, should reflect in the signals from corresponding sensors.\n\nWhat approaches come to mind in order to detect/localize anomalies in the data that correspond to the damaging event and/or predicting/forecasting the same?\n\nTime series is discontinuous in time as the machine is operating only certain hours a day, with several breaks in between.", "author_fullname": "t2_8cvbnc48", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick and basic approaches for anomaly detection in time series data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fl6ht", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677711781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You have a bunch of time series data from some sensors attached to different parts of a machine and one or more parts get damaged, which, in principle, should reflect in the signals from corresponding sensors.&lt;/p&gt;\n\n&lt;p&gt;What approaches come to mind in order to detect/localize anomalies in the data that correspond to the damaging event and/or predicting/forecasting the same?&lt;/p&gt;\n\n&lt;p&gt;Time series is discontinuous in time as the machine is operating only certain hours a day, with several breaks in between.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11fl6ht", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious_Two_810", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11fl6ht/quick_and_basic_approaches_for_anomaly_detection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11fl6ht/quick_and_basic_approaches_for_anomaly_detection/", "subreddit_subscribers": 853296, "created_utc": 1677711781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This is happening on a daily basis at my company. Managers complete everything from queries to slides. The DAs will present the slides at meetings, but will not be able to answer questions and the manager that did the analysis will jump in and answer for them. Why are they doing this? I understand that some employees will work for less than others, but why retain them if they can't complete work on their own?", "author_fullname": "t2_as8zs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would managers complete work for H1 DAs and let them pass it off as their own?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ffb2r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": 1677700606.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677700221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is happening on a daily basis at my company. Managers complete everything from queries to slides. The DAs will present the slides at meetings, but will not be able to answer questions and the manager that did the analysis will jump in and answer for them. Why are they doing this? I understand that some employees will work for less than others, but why retain them if they can&amp;#39;t complete work on their own?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11ffb2r", "is_robot_indexable": true, "report_reasons": null, "author": "varicoseballs", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11ffb2r/why_would_managers_complete_work_for_h1_das_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11ffb2r/why_would_managers_complete_work_for_h1_das_and/", "subreddit_subscribers": 853296, "created_utc": 1677700221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This is going to sound like the most generic post on here because I - like so many - have hit a point in my career after 6 years where I am not sure what I should do next. \n\nFor context, I have a pretty low undergraduate GPA. If this wasn't the case, I would probably directly apply to Masters programs. I didn't try very hard and actually started working full-time as a Data Analyst before I had even graduated with my Economics degree. Now I have 6 years of work experience going from intern to Senior DA (5 at a reputable company + 1 ongoing at a FAANG) and I would really like to work on a Masters degree. \n\nI did poorly in Economics, but I think I would do much better academically now that I am older and more mature and have worked with Data Science tools and techniques for so long. In the past 6 years while working full time I spent a ton of time on Udemy, Coursera and did dozens of courses that allowed me to pick up bigger projects at work. Everything I've learned about writing good code and statistical techniques and building performant models, I've either learned on the job from a willing mentor or from these kinds of certification courses. I love learning for the sake of 'immediately applying' what I learned and doing these kinds of Python projects in my spare time on weekends gives me a lot of passion. \n\nI recently discovered the EdX micromasters programs and realized that this might be a unique opportunity for me to add a stepping stone before applying to a Masters program. Has anyone had any experience with these? I think that doing a Micromasters + GRE could help overcome my bad undergraduate GPA in an MS application.\n\nIs trying my best to go get an MS the right path forward for me? Thank you advance for reading through this and any advice is very much appreciated.", "author_fullname": "t2_12sn5u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Education Advice for mid-career", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fe2ay", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677698802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is going to sound like the most generic post on here because I - like so many - have hit a point in my career after 6 years where I am not sure what I should do next. &lt;/p&gt;\n\n&lt;p&gt;For context, I have a pretty low undergraduate GPA. If this wasn&amp;#39;t the case, I would probably directly apply to Masters programs. I didn&amp;#39;t try very hard and actually started working full-time as a Data Analyst before I had even graduated with my Economics degree. Now I have 6 years of work experience going from intern to Senior DA (5 at a reputable company + 1 ongoing at a FAANG) and I would really like to work on a Masters degree. &lt;/p&gt;\n\n&lt;p&gt;I did poorly in Economics, but I think I would do much better academically now that I am older and more mature and have worked with Data Science tools and techniques for so long. In the past 6 years while working full time I spent a ton of time on Udemy, Coursera and did dozens of courses that allowed me to pick up bigger projects at work. Everything I&amp;#39;ve learned about writing good code and statistical techniques and building performant models, I&amp;#39;ve either learned on the job from a willing mentor or from these kinds of certification courses. I love learning for the sake of &amp;#39;immediately applying&amp;#39; what I learned and doing these kinds of Python projects in my spare time on weekends gives me a lot of passion. &lt;/p&gt;\n\n&lt;p&gt;I recently discovered the EdX micromasters programs and realized that this might be a unique opportunity for me to add a stepping stone before applying to a Masters program. Has anyone had any experience with these? I think that doing a Micromasters + GRE could help overcome my bad undergraduate GPA in an MS application.&lt;/p&gt;\n\n&lt;p&gt;Is trying my best to go get an MS the right path forward for me? Thank you advance for reading through this and any advice is very much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11fe2ay", "is_robot_indexable": true, "report_reasons": null, "author": "AcridAcedia", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11fe2ay/education_advice_for_midcareer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11fe2ay/education_advice_for_midcareer/", "subreddit_subscribers": 853296, "created_utc": 1677698802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Trying to classify proteins based on certain properties, some of which are matrices. \n\nSome features of the dataset I have contain integers, floats, and text. However, there are vectors and matrices too.\n\nWhatever algorithm I use to classify these proteins returns an error because it can\u2019t handle the matrices/vectors as a single value.\n\nHow do I solve for this?", "author_fullname": "t2_15k55n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with vectors as features in a dataset for classification?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g7srv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677776438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to classify proteins based on certain properties, some of which are matrices. &lt;/p&gt;\n\n&lt;p&gt;Some features of the dataset I have contain integers, floats, and text. However, there are vectors and matrices too.&lt;/p&gt;\n\n&lt;p&gt;Whatever algorithm I use to classify these proteins returns an error because it can\u2019t handle the matrices/vectors as a single value.&lt;/p&gt;\n\n&lt;p&gt;How do I solve for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11g7srv", "is_robot_indexable": true, "report_reasons": null, "author": "colouredzindagi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11g7srv/how_to_deal_with_vectors_as_features_in_a_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11g7srv/how_to_deal_with_vectors_as_features_in_a_dataset/", "subreddit_subscribers": 853296, "created_utc": 1677776438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "tl;dr: **How \"static\" should holdout datasets be? should we have a single dataset that we compare against no matter how many times we retrain the model? should a new holdout dataset be created every time training happens?**\n\nFor example, should we create a new holdout dataset each time we train the model so that then we can assert that the recently trained model performs well in the new data compared to the old model?\n\n\\---\n\nI was having an argument with a colleague of mine where he argued the point *that a new holdout dataset should be created every time we train a new model*, it is against this holdout dataset that we should then perform a model comparison between what we currently have in production and the recently trained model. This holdout dataset would then be used to perform data drift checks in production, to guarantee that the model is receiving similar data to the one we trained the model on.\n\nHis reasoning is that if we are retraining a model every-so-often is because we assume data will change often, and having a static dataset doesn't make sense.\n\nMy argument was *in favour of having a single, static holdout dataset that we then use to validate both the data we will use to train the model, as well as data we are receiving at inference time*. \n\nMy reasoning is that yes, while we are retraining often, we probably want a fair measure of how far our data has drifted from the point we made decisions on how the model was built, and selecting the holdout data from the same distribution the model is being trained on is not ideal.", "author_fullname": "t2_u1cec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How \"static\" should holdout datasets be?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11g60h8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677772142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;tl;dr: &lt;strong&gt;How &amp;quot;static&amp;quot; should holdout datasets be? should we have a single dataset that we compare against no matter how many times we retrain the model? should a new holdout dataset be created every time training happens?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For example, should we create a new holdout dataset each time we train the model so that then we can assert that the recently trained model performs well in the new data compared to the old model?&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;I was having an argument with a colleague of mine where he argued the point &lt;em&gt;that a new holdout dataset should be created every time we train a new model&lt;/em&gt;, it is against this holdout dataset that we should then perform a model comparison between what we currently have in production and the recently trained model. This holdout dataset would then be used to perform data drift checks in production, to guarantee that the model is receiving similar data to the one we trained the model on.&lt;/p&gt;\n\n&lt;p&gt;His reasoning is that if we are retraining a model every-so-often is because we assume data will change often, and having a static dataset doesn&amp;#39;t make sense.&lt;/p&gt;\n\n&lt;p&gt;My argument was &lt;em&gt;in favour of having a single, static holdout dataset that we then use to validate both the data we will use to train the model, as well as data we are receiving at inference time&lt;/em&gt;. &lt;/p&gt;\n\n&lt;p&gt;My reasoning is that yes, while we are retraining often, we probably want a fair measure of how far our data has drifted from the point we made decisions on how the model was built, and selecting the holdout data from the same distribution the model is being trained on is not ideal.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11g60h8", "is_robot_indexable": true, "report_reasons": null, "author": "fferegrino", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11g60h8/how_static_should_holdout_datasets_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11g60h8/how_static_should_holdout_datasets_be/", "subreddit_subscribers": 853296, "created_utc": 1677772142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a report for school where I need to ask a data science professional some questions and then write a report on it. You don't have to answer all of them and the responses don't have to be that long they can be on the shorter side. Any help would be greatly appreciated. Thank you.\n\n1) As AI advances do you see any possible risk of mass job displacement in the future? Such as self driving cars replacing truck drivers, taxi drivers, etc. Do you think this will be a problem in the future or will it be the same as all the times we've automated something in the past and people will just adapt to the job market?\n\n\n2) Recently there has been some controversy around algorithmic bias, especially in data science. Such as automating parole decisions, job application screening, or applying for credit cards. Some people argue these machine learning algorithms have been somewhat racist/sexist in the past. Do you think data scientists should be doing their best to eliminate bias?\n\n\n3) With the recent release of chat bots like chatGPT students have been using it to help them write papers or help with their programming assignments. Do you see anything wrong with claiming chatGPT's responses as your own work? Do you consider it plagiarism?\n\n\n4) Art generation has gotten very popular recently and there have actually been some lawsuits against these AI's claiming they were trained on copyrighted images. Do you believe AI shouldn't be legally allowed to train on copyrighted material or will doing so significantly hinder AI's development as data scientists will have to be much more careful about how they obtain their datasets?\n\n\n5) As AI and image detection/recognition algorithms become more withspread do you think adversarial attacks will ever be a significant and practical problem?", "author_fullname": "t2_dj5j9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working on a report for school, can someone answer a few ethics related data science questions for me.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fpp9o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677722890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a report for school where I need to ask a data science professional some questions and then write a report on it. You don&amp;#39;t have to answer all of them and the responses don&amp;#39;t have to be that long they can be on the shorter side. Any help would be greatly appreciated. Thank you.&lt;/p&gt;\n\n&lt;p&gt;1) As AI advances do you see any possible risk of mass job displacement in the future? Such as self driving cars replacing truck drivers, taxi drivers, etc. Do you think this will be a problem in the future or will it be the same as all the times we&amp;#39;ve automated something in the past and people will just adapt to the job market?&lt;/p&gt;\n\n&lt;p&gt;2) Recently there has been some controversy around algorithmic bias, especially in data science. Such as automating parole decisions, job application screening, or applying for credit cards. Some people argue these machine learning algorithms have been somewhat racist/sexist in the past. Do you think data scientists should be doing their best to eliminate bias?&lt;/p&gt;\n\n&lt;p&gt;3) With the recent release of chat bots like chatGPT students have been using it to help them write papers or help with their programming assignments. Do you see anything wrong with claiming chatGPT&amp;#39;s responses as your own work? Do you consider it plagiarism?&lt;/p&gt;\n\n&lt;p&gt;4) Art generation has gotten very popular recently and there have actually been some lawsuits against these AI&amp;#39;s claiming they were trained on copyrighted images. Do you believe AI shouldn&amp;#39;t be legally allowed to train on copyrighted material or will doing so significantly hinder AI&amp;#39;s development as data scientists will have to be much more careful about how they obtain their datasets?&lt;/p&gt;\n\n&lt;p&gt;5) As AI and image detection/recognition algorithms become more withspread do you think adversarial attacks will ever be a significant and practical problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11fpp9o", "is_robot_indexable": true, "report_reasons": null, "author": "VelvetRevolver_", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11fpp9o/working_on_a_report_for_school_can_someone_answer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11fpp9o/working_on_a_report_for_school_can_someone_answer/", "subreddit_subscribers": 853296, "created_utc": 1677722890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I trained a classification model using XGboost.. Before the classification, I also performed EDA to understand which Variables could be important and to inside them. I noticed a variable which is showing significant correlation with the Target variable but I know from my domain knowledge that it's a proxy variable at the most and doesn't hold any predictive value on its own.\nTo confirm the importance of the features used in Boosting, I also plotted the inbuilt Feature importance and this proxy variable still came on top.\n\nNow I wanna understand how can I go about identifying the actual features this proxy represents and to include them in the dataset.\nContext : the proxy variable is just an ID of Ad campaigns but shows heavy correlation and predictive importance to Clicks on display advertising. We know campaign ID is just a placeholder for actual campaign features but which one is what I'd like to find.\n\nThanks.", "author_fullname": "t2_bvba4ue8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to find actual important variables for Proxies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fwel0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677742083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I trained a classification model using XGboost.. Before the classification, I also performed EDA to understand which Variables could be important and to inside them. I noticed a variable which is showing significant correlation with the Target variable but I know from my domain knowledge that it&amp;#39;s a proxy variable at the most and doesn&amp;#39;t hold any predictive value on its own.\nTo confirm the importance of the features used in Boosting, I also plotted the inbuilt Feature importance and this proxy variable still came on top.&lt;/p&gt;\n\n&lt;p&gt;Now I wanna understand how can I go about identifying the actual features this proxy represents and to include them in the dataset.\nContext : the proxy variable is just an ID of Ad campaigns but shows heavy correlation and predictive importance to Clicks on display advertising. We know campaign ID is just a placeholder for actual campaign features but which one is what I&amp;#39;d like to find.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11fwel0", "is_robot_indexable": true, "report_reasons": null, "author": "invincible_moron", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11fwel0/how_to_find_actual_important_variables_for_proxies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11fwel0/how_to_find_actual_important_variables_for_proxies/", "subreddit_subscribers": 853296, "created_utc": 1677742083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Newly promoted to a senior data/analytics position at my company. Administrative question here. How do you all structure your folders and files? In general. If you were able to start from scratch in your current role, or if you just started a new job.. how do you organize? Aside from files and folders, do you leverage apps like OneNote, Evernote, databases, etc., to keep track of clients, projects, sandbox scripts and whatnot?", "author_fullname": "t2_gn8s9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Folder Structuring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fu2ne", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677734804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Newly promoted to a senior data/analytics position at my company. Administrative question here. How do you all structure your folders and files? In general. If you were able to start from scratch in your current role, or if you just started a new job.. how do you organize? Aside from files and folders, do you leverage apps like OneNote, Evernote, databases, etc., to keep track of clients, projects, sandbox scripts and whatnot?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11fu2ne", "is_robot_indexable": true, "report_reasons": null, "author": "Thiseffingguy2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11fu2ne/folder_structuring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11fu2ne/folder_structuring/", "subreddit_subscribers": 853296, "created_utc": 1677734804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\nmetadataCA&lt;-read.csv(\"metadataCA.csv\")\nmaxtempcalifornia&lt;-read.csv(\"MaxTempCalifornia.csv\")\nhead(metadataCA)\n##LocationElevLatLongRef_LatRef_Long\n##1SanFrancisco45.737.7705-122.426937.76889-122.5156\n##2Napa4.338.2102-122.284738.39222-123.0892\n##3SanDiego4.632.7336-117.183132.72222-117.2683\n##4Fresno100.036.7525-119.701736.25833-121.8389\n##5SantaCruz39.636.9905-121.991136.95528-122.0933\n##6DeathValley-59.136.4622-116.866935.41750-120.8369\nhead(maxtempcalifornia)\n##XSan.FranciscoNapaSan.DiegoFresnoSanta.CruzDeath.ValleyOjai\n##12012010114.416.719.418.322.820.627.2\n##22012010212.816.720.618.315.021.127.2\n##32012010311.715.621.713.317.220.626.7\n##42012010413.919.426.116.718.921.127.2\n##52012010516.117.828.317.818.321.726.7\n##62012010613.314.420.017.815.021.123.9\n##BarstowLACedarParkRedding\n##120.627.219.417.2\n##217.223.921.715.0\n##318.324.410.618.3\n##418.929.43.319.4\n##519.428.38.919.4\n##620.022.816.117.2\nmaxtempcalifornia_long&lt;maxtempcalifornia%&gt;%\ngather(Location,Max_Temp,c(X))\nmaxtempcalifornia_long$Location&lt;-maxtempcalifornia_long$Location%&gt;%\nstrreplace(\"\\\\.\",\"\")\nmaxtempcalifornialong$Date&lt;-ymd(maxtempcalifornialong$X)\nhead(maxtempcalifornialong)\n##XLocationMax_TempDate\n##120120101SanFrancisco14.42012-01-01\n##220120102SanFrancisco12.82012-01-02\n##320120103SanFrancisco11.72012-01-03\n##420120104SanFrancisco13.92012-01-04\n##520120105SanFrancisco16.12012-01-05\n##620120106SanFrancisco13.32012-01-06\n1.NumericalandGraphicalsummariesofthedatafromeachsite\nsummary(metadataCA)\n##LocationElevLatLong\n##Length:11Min.:59.1Min.:32.73Min.:122.4\n##Class:character1stQu.:17.11stQu.:34.671stQu.:-122.1\n##Mode:characterMedian:45.7Median:36.75Median:-119.2\n##Mean:242.3Mean:36.32Mean:-119.6", "author_fullname": "t2_uwl5ul7j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "INITIAL DATA ANALYSIS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11ga3qs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677782002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;metadataCA&amp;lt;-read.csv(&amp;quot;metadataCA.csv&amp;quot;)\nmaxtempcalifornia&amp;lt;-read.csv(&amp;quot;MaxTempCalifornia.csv&amp;quot;)\nhead(metadataCA)&lt;/p&gt;\n\n&lt;h2&gt;LocationElevLatLongRef_LatRef_Long&lt;/h2&gt;\n\n&lt;h2&gt;1SanFrancisco45.737.7705-122.426937.76889-122.5156&lt;/h2&gt;\n\n&lt;h2&gt;2Napa4.338.2102-122.284738.39222-123.0892&lt;/h2&gt;\n\n&lt;h2&gt;3SanDiego4.632.7336-117.183132.72222-117.2683&lt;/h2&gt;\n\n&lt;h2&gt;4Fresno100.036.7525-119.701736.25833-121.8389&lt;/h2&gt;\n\n&lt;h2&gt;5SantaCruz39.636.9905-121.991136.95528-122.0933&lt;/h2&gt;\n\n&lt;h2&gt;6DeathValley-59.136.4622-116.866935.41750-120.8369&lt;/h2&gt;\n\n&lt;p&gt;head(maxtempcalifornia)&lt;/p&gt;\n\n&lt;h2&gt;XSan.FranciscoNapaSan.DiegoFresnoSanta.CruzDeath.ValleyOjai&lt;/h2&gt;\n\n&lt;h2&gt;12012010114.416.719.418.322.820.627.2&lt;/h2&gt;\n\n&lt;h2&gt;22012010212.816.720.618.315.021.127.2&lt;/h2&gt;\n\n&lt;h2&gt;32012010311.715.621.713.317.220.626.7&lt;/h2&gt;\n\n&lt;h2&gt;42012010413.919.426.116.718.921.127.2&lt;/h2&gt;\n\n&lt;h2&gt;52012010516.117.828.317.818.321.726.7&lt;/h2&gt;\n\n&lt;h2&gt;62012010613.314.420.017.815.021.123.9&lt;/h2&gt;\n\n&lt;h2&gt;BarstowLACedarParkRedding&lt;/h2&gt;\n\n&lt;h2&gt;120.627.219.417.2&lt;/h2&gt;\n\n&lt;h2&gt;217.223.921.715.0&lt;/h2&gt;\n\n&lt;h2&gt;318.324.410.618.3&lt;/h2&gt;\n\n&lt;h2&gt;418.929.43.319.4&lt;/h2&gt;\n\n&lt;h2&gt;519.428.38.919.4&lt;/h2&gt;\n\n&lt;h2&gt;620.022.816.117.2&lt;/h2&gt;\n\n&lt;p&gt;maxtempcalifornia_long&amp;lt;maxtempcalifornia%&amp;gt;%\ngather(Location,Max_Temp,c(X))\nmaxtempcalifornia_long$Location&amp;lt;-maxtempcalifornia_long$Location%&amp;gt;%\nstrreplace(&amp;quot;\\.&amp;quot;,&amp;quot;&amp;quot;)\nmaxtempcalifornialong$Date&amp;lt;-ymd(maxtempcalifornialong$X)\nhead(maxtempcalifornialong)&lt;/p&gt;\n\n&lt;h2&gt;XLocationMax_TempDate&lt;/h2&gt;\n\n&lt;h2&gt;120120101SanFrancisco14.42012-01-01&lt;/h2&gt;\n\n&lt;h2&gt;220120102SanFrancisco12.82012-01-02&lt;/h2&gt;\n\n&lt;h2&gt;320120103SanFrancisco11.72012-01-03&lt;/h2&gt;\n\n&lt;h2&gt;420120104SanFrancisco13.92012-01-04&lt;/h2&gt;\n\n&lt;h2&gt;520120105SanFrancisco16.12012-01-05&lt;/h2&gt;\n\n&lt;h2&gt;620120106SanFrancisco13.32012-01-06&lt;/h2&gt;\n\n&lt;p&gt;1.NumericalandGraphicalsummariesofthedatafromeachsite\nsummary(metadataCA)&lt;/p&gt;\n\n&lt;h2&gt;LocationElevLatLong&lt;/h2&gt;\n\n&lt;h2&gt;Length:11Min.:59.1Min.:32.73Min.:122.4&lt;/h2&gt;\n\n&lt;h2&gt;Class:character1stQu.:17.11stQu.:34.671stQu.:-122.1&lt;/h2&gt;\n\n&lt;h2&gt;Mode:characterMedian:45.7Median:36.75Median:-119.2&lt;/h2&gt;\n\n&lt;h2&gt;Mean:242.3Mean:36.32Mean:-119.6&lt;/h2&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11ga3qs", "is_robot_indexable": true, "report_reasons": null, "author": "clinton_CT", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11ga3qs/initial_data_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11ga3qs/initial_data_analysis/", "subreddit_subscribers": 853296, "created_utc": 1677782002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "ive been into ds for some 6 months and ive been selling my services based on supervised learning techniques only and it was all going good but now they've vetted the services meaning I have to place my gigs under one of their given 4 sub categories or I cant continue selling.\n\ni mean I am learning time series forecasting right now but they keep sending mails that I got a month to keep my gig alive n shit.\n\ncan yall suggest a good sol pls", "author_fullname": "t2_clohgnz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does the fiverr vetting system sucks or is it just me", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11g9y5o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677781615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ive been into ds for some 6 months and ive been selling my services based on supervised learning techniques only and it was all going good but now they&amp;#39;ve vetted the services meaning I have to place my gigs under one of their given 4 sub categories or I cant continue selling.&lt;/p&gt;\n\n&lt;p&gt;i mean I am learning time series forecasting right now but they keep sending mails that I got a month to keep my gig alive n shit.&lt;/p&gt;\n\n&lt;p&gt;can yall suggest a good sol pls&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11g9y5o", "is_robot_indexable": true, "report_reasons": null, "author": "futbolhead975", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11g9y5o/does_the_fiverr_vetting_system_sucks_or_is_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11g9y5o/does_the_fiverr_vetting_system_sucks_or_is_it/", "subreddit_subscribers": 853296, "created_utc": 1677781615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Newbie question! The project I'm currently on is user clustering of a food delivery company.\n\nI'm at the preprocessing stage and I'm feeling stuck, and I'm having 2 dataframe columns with list and dict like structure. Any ideas how to deal with such columns?\n\nAll I can think about is data normalization as to convert those 2 columns to 2 separate dataframes,  but I don't know if that is the common practice to such issue.\n\nAny help is appreciated, thanks so much in advance.\n\nhttps://preview.redd.it/bs1v640z6cla1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d7e3eb5438f6fae82fe6833d6bd1e169e7629da8\n\nhttps://preview.redd.it/fg12i10z6cla1.png?width=1661&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a86879e57b51ccf7b317054e40a51e7e7809a258", "author_fullname": "t2_vn7kuoox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with multiple columns containing multiple values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bs1v640z6cla1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/bs1v640z6cla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=454c4224db2cad798461c9103d82e4be0ae27ca9"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/bs1v640z6cla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1b16bbf2cb2ac0569897e7111bae07ec288507b"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/bs1v640z6cla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89874ec0bfaf139c8c8026d65e82273922631f99"}, {"y": 357, "x": 640, "u": "https://preview.redd.it/bs1v640z6cla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2631e786d2e2d2bc2f1460cc94344291098036f4"}, {"y": 536, "x": 960, "u": "https://preview.redd.it/bs1v640z6cla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fd074d9b9dbe14bcb28844e5da8593b4bbfaff6"}, {"y": 603, "x": 1080, "u": "https://preview.redd.it/bs1v640z6cla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e4649d215dac0d20404b20a3f83db88c0afb8c2a"}], "s": {"y": 715, "x": 1280, "u": "https://preview.redd.it/bs1v640z6cla1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d7e3eb5438f6fae82fe6833d6bd1e169e7629da8"}, "id": "bs1v640z6cla1"}, "fg12i10z6cla1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/fg12i10z6cla1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54204771d76ed62d30fc02a583d8b1b0991099c1"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/fg12i10z6cla1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98be7c74c9556d51c0ae1504cae664334b97c4f0"}, {"y": 155, "x": 320, "u": "https://preview.redd.it/fg12i10z6cla1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c136ca3af669f631fe1b17f24df01b8c19598354"}, {"y": 310, "x": 640, "u": "https://preview.redd.it/fg12i10z6cla1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e75a1bbc9c5aa64c2f435142b8a8036504f09364"}, {"y": 466, "x": 960, "u": "https://preview.redd.it/fg12i10z6cla1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a82a059df7e3260bf9e4464234173087fd7621f8"}, {"y": 524, "x": 1080, "u": "https://preview.redd.it/fg12i10z6cla1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46c9f56bac4513c28a1e280d1dcddaa000e1c80e"}], "s": {"y": 807, "x": 1661, "u": "https://preview.redd.it/fg12i10z6cla1.png?width=1661&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a86879e57b51ccf7b317054e40a51e7e7809a258"}, "id": "fg12i10z6cla1"}}, "name": "t3_11g49mz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4ArVN60qwbBnxhf8nGKC0ReNzvMDiYYDKU8f5pUvBYk.jpg", "edited": 1677770018.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677767699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Newbie question! The project I&amp;#39;m currently on is user clustering of a food delivery company.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m at the preprocessing stage and I&amp;#39;m feeling stuck, and I&amp;#39;m having 2 dataframe columns with list and dict like structure. Any ideas how to deal with such columns?&lt;/p&gt;\n\n&lt;p&gt;All I can think about is data normalization as to convert those 2 columns to 2 separate dataframes,  but I don&amp;#39;t know if that is the common practice to such issue.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated, thanks so much in advance.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bs1v640z6cla1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d7e3eb5438f6fae82fe6833d6bd1e169e7629da8\"&gt;https://preview.redd.it/bs1v640z6cla1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d7e3eb5438f6fae82fe6833d6bd1e169e7629da8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fg12i10z6cla1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a86879e57b51ccf7b317054e40a51e7e7809a258\"&gt;https://preview.redd.it/fg12i10z6cla1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a86879e57b51ccf7b317054e40a51e7e7809a258&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11g49mz", "is_robot_indexable": true, "report_reasons": null, "author": "Hatasu98", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11g49mz/how_to_deal_with_multiple_columns_containing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11g49mz/how_to_deal_with_multiple_columns_containing/", "subreddit_subscribers": 853296, "created_utc": 1677767699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "today my work mainly consists of doing some data wrangling (mostly satellite image sequences) with python scripts run on a cloud instance, then using pytorch to build and train a deep learning model on a gpu instance on aws (I connect via ssh with vscode). I use tensorboard to monitor training. when it's done I usually pass the weights to other people and they will take care of making it available via an API.\n\nhow do I give the next step as a machine learning engineer? which tools to use? which concepts to learn?", "author_fullname": "t2_rhw2olfy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to improve as a machine learning engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11flc7j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677712150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;today my work mainly consists of doing some data wrangling (mostly satellite image sequences) with python scripts run on a cloud instance, then using pytorch to build and train a deep learning model on a gpu instance on aws (I connect via ssh with vscode). I use tensorboard to monitor training. when it&amp;#39;s done I usually pass the weights to other people and they will take care of making it available via an API.&lt;/p&gt;\n\n&lt;p&gt;how do I give the next step as a machine learning engineer? which tools to use? which concepts to learn?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11flc7j", "is_robot_indexable": true, "report_reasons": null, "author": "pocolai", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11flc7j/how_to_improve_as_a_machine_learning_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11flc7j/how_to_improve_as_a_machine_learning_engineer/", "subreddit_subscribers": 853296, "created_utc": 1677712150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "posted by your anxious undergrad senior who can't even get an internship", "author_fullname": "t2_aghpz5g9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When do you think this tech recession will be over?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11fqpnj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677725482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;posted by your anxious undergrad senior who can&amp;#39;t even get an internship&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11fqpnj", "is_robot_indexable": true, "report_reasons": null, "author": "Careless-Tailor-2317", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11fqpnj/when_do_you_think_this_tech_recession_will_be_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11fqpnj/when_do_you_think_this_tech_recession_will_be_over/", "subreddit_subscribers": 853296, "created_utc": 1677725482.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}