{"kind": "Listing", "data": {"after": "t3_11gu903", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1zqb2jib", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download and store 900 3 hour videos in a month?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 111, "top_awarded_type": null, "hide_score": false, "name": "t3_11gwl82", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 574, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 574, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ITlbvE7gVbTwP1MPZEhXENu7FL-95hz01lI-R_hOGHk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677836341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/nf5076evcjla1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/nf5076evcjla1.jpg?auto=webp&amp;v=enabled&amp;s=4a59b90fd926ebf281834b8dcc2cf3e9ecc44355", "width": 1080, "height": 858}, "resolutions": [{"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cb79c537f5c0f47df6a981645cd56eed81eb09f", "width": 108, "height": 85}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3acaf7a28a7cb2f37b176dfe9ee4aeb44cd298d", "width": 216, "height": 171}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78c3ceec035fdd400d90f227f277264ef87eee8c", "width": 320, "height": 254}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=521f2912f298ead72ecf3b1d62b98d4e20283ab0", "width": 640, "height": 508}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=083fd0ea4ac390e888934284aa27460f4c7a02f4", "width": 960, "height": 762}, {"url": "https://preview.redd.it/nf5076evcjla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0347fa8686184d59b36d40475af30e82270f1f9", "width": 1080, "height": 858}], "variants": {}, "id": "6XkkTsTw7RgZSjNidbwi8XNuNWRlJf8vURQIoBd1DAY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gwl82", "is_robot_indexable": true, "report_reasons": null, "author": "Phixiately", "discussion_type": null, "num_comments": 151, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gwl82/how_to_download_and_store_900_3_hour_videos_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/nf5076evcjla1.jpg", "subreddit_subscribers": 671957, "created_utc": 1677836341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Countless backups, photos, videos, movies, torrents, Windows bootables, Linux bootables and what not hoarded on these over time.... And some are peripheral driver/sw disks.\n\nEven had a excel database with all of them marked by a number and the table listing exactly what's in them\n\nI think I still remember how cool it felt to be able to \"Write\" something to a DVD (was scared of what \"burning\" a DVD meant initially haha, the good old Nero !)\n\nI think it's an end for this good old pile of DVDs and CDs, with all important stuff moved to my recently built DIY NAS. Felt nostalgic getting rid of them, so I just ordered a slimline sata cable to use with a old laptop dvd drive, and gave each of them a run once , sort of like, a goodbye, before disposing them off :)  (I can't let go of things easily)\n\nAlso a friendly reminder that DVDs &amp; CDs do not have a very long shelf life, Igot data errors from like 50% of the old drives.\n\nSo long buddies....\n\n&amp;#x200B;\n\nhttps://preview.redd.it/lyde37ry6lla1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5fef573713d5aa7482ab3b2547782c649e37c633", "author_fullname": "t2_6fdo6pa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm letting go !", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"lyde37ry6lla1": {"status": "valid", "e": "Image", "m": "image/jpg", "o": [{"y": 4032, "x": 3024, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=1080&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=abbb316f70cc4f21cdd810faf2bcfebd8ec9693e"}], "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=266d5a008ba0a73665fb6c5149d35aa70238b854"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d73daf216f46bec21d7e8c113241e1151596195"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d9ebe6cc478b8461032a533e40993578c4bb457"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50be12f661a31fb2e7cbe928d3d95d4f30968075"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4517ea0ac78dc2b70fa7d9bdaf33292254ec56a7"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c458305109f569e1b76698eed54bb33e7d72b44e"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/lyde37ry6lla1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5fef573713d5aa7482ab3b2547782c649e37c633"}, "id": "lyde37ry6lla1"}}, "name": "t3_11heuto", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677877393.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Countless backups, photos, videos, movies, torrents, Windows bootables, Linux bootables and what not hoarded on these over time.... And some are peripheral driver/sw disks.&lt;/p&gt;\n\n&lt;p&gt;Even had a excel database with all of them marked by a number and the table listing exactly what&amp;#39;s in them&lt;/p&gt;\n\n&lt;p&gt;I think I still remember how cool it felt to be able to &amp;quot;Write&amp;quot; something to a DVD (was scared of what &amp;quot;burning&amp;quot; a DVD meant initially haha, the good old Nero !)&lt;/p&gt;\n\n&lt;p&gt;I think it&amp;#39;s an end for this good old pile of DVDs and CDs, with all important stuff moved to my recently built DIY NAS. Felt nostalgic getting rid of them, so I just ordered a slimline sata cable to use with a old laptop dvd drive, and gave each of them a run once , sort of like, a goodbye, before disposing them off :)  (I can&amp;#39;t let go of things easily)&lt;/p&gt;\n\n&lt;p&gt;Also a friendly reminder that DVDs &amp;amp; CDs do not have a very long shelf life, Igot data errors from like 50% of the old drives.&lt;/p&gt;\n\n&lt;p&gt;So long buddies....&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lyde37ry6lla1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5fef573713d5aa7482ab3b2547782c649e37c633\"&gt;https://preview.redd.it/lyde37ry6lla1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5fef573713d5aa7482ab3b2547782c649e37c633&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "8TB Raw", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11heuto", "is_robot_indexable": true, "report_reasons": null, "author": "abhishekr700", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11heuto/im_letting_go/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11heuto/im_letting_go/", "subreddit_subscribers": 671957, "created_utc": 1677877393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Been posting in this sub for a few weeks now and realised its one of the most helpful subs I've used on Reddit. The replies to my often dumb questions have been met with fantastic knowledge and some really smart folks (special ty to  u/HTWingNut  \\+ so many others).\n\nThe average IQ of a DataHoarder is evidently much higher than that of other subs and full of people take pride in sharing knowledge and not being a smartass or ridiculing dumb folks like me. When everyone's left for the day, and you spend an extra 30mins sorting out your data, know that you're doing it because you're SMART, and not because you're ill \ud83d\udc40\n\nAnyway, it's Friday, and I thought I'd say thanks to the regular guys and gals who make data interesting and exciting to understand for all. \ud83c\udf7a", "author_fullname": "t2_4d5ehhv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Random thanks to this sub for helpful replies rather than smartass ones", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11h1w2q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677853709.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677853076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been posting in this sub for a few weeks now and realised its one of the most helpful subs I&amp;#39;ve used on Reddit. The replies to my often dumb questions have been met with fantastic knowledge and some really smart folks (special ty to  &lt;a href=\"/u/HTWingNut\"&gt;u/HTWingNut&lt;/a&gt;  + so many others).&lt;/p&gt;\n\n&lt;p&gt;The average IQ of a DataHoarder is evidently much higher than that of other subs and full of people take pride in sharing knowledge and not being a smartass or ridiculing dumb folks like me. When everyone&amp;#39;s left for the day, and you spend an extra 30mins sorting out your data, know that you&amp;#39;re doing it because you&amp;#39;re SMART, and not because you&amp;#39;re ill \ud83d\udc40&lt;/p&gt;\n\n&lt;p&gt;Anyway, it&amp;#39;s Friday, and I thought I&amp;#39;d say thanks to the regular guys and gals who make data interesting and exciting to understand for all. \ud83c\udf7a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11h1w2q", "is_robot_indexable": true, "report_reasons": null, "author": "i-dm", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11h1w2q/random_thanks_to_this_sub_for_helpful_replies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11h1w2q/random_thanks_to_this_sub_for_helpful_replies/", "subreddit_subscribers": 671957, "created_utc": 1677853076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_10gstl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon shipped 20TB Ultrastars with proper packaging!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_11hk8ji", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w9KSfZxKY9AtvaY4Nj-IoQqReuXpHDn5rW3D0KVWKaA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677889969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/4hz0miw01mla1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/4hz0miw01mla1.jpg?auto=webp&amp;v=enabled&amp;s=cbd4d50802b914112c5683c3ea92d11b55c2254c", "width": 6000, "height": 4000}, "resolutions": [{"url": "https://preview.redd.it/4hz0miw01mla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dc89e355b77ee9ae10d92b00e7989a3985c52c0", "width": 108, "height": 72}, {"url": "https://preview.redd.it/4hz0miw01mla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=01a40d0aa267d0be9a5346da08a19a1c879ac45c", "width": 216, "height": 144}, {"url": "https://preview.redd.it/4hz0miw01mla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c83e497c35946e1666d6b13b9930e01ec6f7c86d", "width": 320, "height": 213}, {"url": "https://preview.redd.it/4hz0miw01mla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10bc229f45108f054b750fc7f009f9d9f7562a87", "width": 640, "height": 426}, {"url": "https://preview.redd.it/4hz0miw01mla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d18c8ad3b814a7f81866f78b4da19d6b53da121", "width": 960, "height": 640}, {"url": "https://preview.redd.it/4hz0miw01mla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d34c6251ea2fddd85fce495bb5bb7b52eb764e9", "width": 1080, "height": 720}], "variants": {}, "id": "_QwttnmDdn5Vqx3lRQM6ljJrhMXbO7rtpCIXPksQbKo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hk8ji", "is_robot_indexable": true, "report_reasons": null, "author": "denpa_", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hk8ji/amazon_shipped_20tb_ultrastars_with_proper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/4hz0miw01mla1.jpg", "subreddit_subscribers": 671957, "created_utc": 1677889969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I live in an area where symmetrical internet is either expensive or unavailable. My offsite backup is in my brother's house and just rsync'ing it didn't seem reasonable with my upload speed / bandwidth limitations. I decided to go with the tried and true sneakernet method. I decided to take advantage of ZFS' snapshotting functionality. The first script automatically creates incremental snapshots holding all the changes between the latest update on the backup machine and the current date as of running the script. These snapshots can be ferried between the original and backup machines via a portable hard drive. The second script applies runs the backup machine and will apply the snapshots from the portable hard drive to the backup system.  \n\nI thought I'd share the scripts in case their useful for anyone else sneakernetting their backups as well as some feedback from the community.  \n\nThe first script (ideally set to run periodically via cron/systemd.timers)\n\n    #!/bin/bash\n    \n     # This is part one of my 'patented' sneakernet backup system. The inte-\n     # nded usecase is to use a portable hard drive to shuttle incremental \n     # zfs snapshots from the original server (where this script will reside\n     # and run) to the backup server. There are a few requirements:\n     # 1) Both the original and backup servers have matching datasets\n     # 2) The portable hard drive is formatted with ZFS with a pool named\n     #    Sync\n     # 3) When the portable hard drive makes its way back to the original \n     #    server, you zpool import it and rsync the contents of the \n     #    ZFS_incremental folder onto it\n     \n    # Honestly this probably isn't needed in this script but I put it in all\n    # my bash scripts\n    set -e\n    \n    # Determine current date\n    currentdate=$(date +\"%Y-%m-%d\")\n    \n    # Determine the last time we synced with the backupserver. The file is\n    # generated by the backup server and either rsynced via SSH or pulled\n    # from the portable hard drive\n    lwsync=$(tail -n -1 /root/last_backup_sync.txt)\n    \n    # Using the current date to create snapshot names. Endpoints for the\n    # incremental snapshot\n    curISOssnapshot=\"Pool1/ISOs@${currentdate}\"\n    curlinuxsnapshot=\"Pool1/linux@${currentdate}\"\n    curpubdomainsnapshot=\"Pool1/pubdomain@${currentdate}\"\n    \n    # Using the last date we to create snapshot names. Start-points for the\n    # incremental snapshot\n    lwsyncISOssnapshot=\"Pool1/ISOs@${lwsync}\"\n    lwsynclinuxsnapshot=\"Pool1/linux@${lwsync}\"\n    lwsyncpubdomainsnapshot=\"Pool1/pubdomain@${lwsync}\"\n    \n    \n    # use zfs send to save incremental snapshots of each pool\n    echo $curISOssnapshot\n    echo $curlinuxsnapshot\n    echo $curpubdomainsnapshot\n    \n    \n    # Create snapshots of with current date\n    zfs snapshot $curISOssnapshot\n    zfs snapshot $curlinuxsnapshot\n    zfs snapshot $curpubdomainsnapshot\n    echo \"Snapshots made ${currentdate}\" &gt;&gt; /root/backupscript.log\n    \n    # Create Incremental Snapshots of ISOs, linux and pubdomain\n    zfs send -Ri $lwsyncISOssnapshot $curISOssnapshot &gt; /mnt/Pool1/ZFS_incremental/ISOs_latest\n    zfs send -Ri $lwsynclinuxsnapshot $curlinuxsnapshot &gt; /mnt/Pool1/ZFS_incremental/linux_latest\n    zfs send -Ri $lwsyncpubdomainsnapshot $curpubdomainsnapshot &gt; /mnt/Pool1/ZFS_incremental/pubdomain_latest\n    echo \"Incremental snapshots saved to /mnt/Pool1/ZFS_incremental/ \" &gt;&gt; /root/backupscript.log\n    \n\nThe second script (ideally run automatically via cron/systemd.timers. should be setup to run every day for when the backup machine is located at a relative with low technical abilities):\n\n    #! /bin/bash\n    \n    # This script is the second part of my 'patented' sneakernet update \n    # system. The script is intended to be run on the backup server once the \n    # portable hard drive is plugged into the machine. This script does have \n    # a few dependencies: \n    # 1) the backup server must have the same zfs datasets that are going to \n    #    be transferred from the portable hard drive. \n    # 2) the backup server must have automatic snapshotting turned OFF. \n    # 3) this script assumes that you can ssh into your primary server. \n    \n    # Basically stops the script when something goes wrong. Mostly for when\n    # the script is set to run automatically (via systemd.timers or crontab)\n    # and the \n    set -e\n    \n    # set variables for the latest snapshots of the backup system (useful\n    # for later\n    last_ISOs_snapshot=$(zfs list -H -o name -t snapshot BackupPool/ISOs | tail -n -1)\n    last_linux_snapshot=$(zfs list -H -o name -t snapshot BackupPool/linux | tail -n -1)\n    last_pubdomain_snapshot=$(zfs list -H -o name -t snapshot BackupPool/pubdomain | tail -n -1)\n    \n    # Honestly, just here to provide some output for logging to see where\n    # it might have failed\n    echo $last_ISOs_snapshot\n    echo $last_linux_snapshot\n    echo $last_pubdomain_snapshot\n    \n    echo $(date +\"%Y-%m-%d\") &gt;&gt; /root/sync.log\n    echo $(date +\"%Y-%m-%d\")\n    \n    # Imports the pool located on the portable hard drive\n    zpool import -o altroot=/mnt Sync\n    echo \"Import Done\" &gt;&gt; /root/sync.log\n    echo \"Import Done\"\n    \n    # Rollback to last snapshot. This is necessary in case the person using\n    # the backupserver happened to accidentaly ignore your instructions and\n    # alters any of the pools that are going to be synced.\n    zfs rollback $last_ISOs_snapshot\n    zfs rollback $last_linux_snapshot\n    zfs rollback $last_pubdomain_snapshot\n    echo \"Snapshot rollback done\" &gt;&gt; /root/sync.log\n    echo \"Snapshot rollback done\"\n    \n    # Sync Incremental Snapshots of ISOs, linux and pubdomain\n    zfs recv -F BackupPool/ISOs &lt; /mnt/Sync/ISOs_latest\n    echo \"ISOs updated\" &gt;&gt; /root/sync.log\n    echo \"ISOs updated\"\n    \n    zfs recv -F BackupPool/linux &lt; /mnt/Sync/linux_latest\n    echo \"linux updated\" &gt;&gt; /root/sync.log\n    echo \"linux updated\"\n    \n    zfs recv -F BackupPool/pubdomain &lt; /mnt/Sync/pubdomain_latest\n    echo \"pubdomain updated\" &gt;&gt; /root/sync.log\n    echo \"pubdomain updated\"\n    \n    echo \"Sync Complete\" &gt;&gt; /root/sync.log\n    echo \"Sync Complete\"\n    \n    # Create the file that will be referenced by the first script when det-\n    # ermining what the start point is for the incremental snapshot. Saves \n    # it to the portable hard drive in case ssh'ing into the progenitor \n    # server is not possible\n    zfs list -H -o name -t snapshot BackupPool/ISOs &gt; tempa.txt\n    sed 's/BackupPool\\/ISOs@//' tempa.txt &gt; /root/last_backup_sync.txt\n    rm tempa.txt\n    cp /root/last_backup_sync.txt /mnt/Sync/last_backup_sync.txt\n    echo \"Update last_backup_sync.txt done\" &gt;&gt; /root/sync.log\n    echo \"Update last_backup_sync.txt done\"\n    \n    # Sends the file to the original server. Means that the originator ser-\n    # ver can start making the correct incremental snapshots. This, in turn,\n    # means that when the original server receives the portable hard drive,\n    # it need only copy over the pregenerated incremental snapshots.\n    rsync -e \"ssh\" -avz /root/last_backup_sync.txt root@###.###.###.###:/root/\n    echo \"Send last_backup_sync.txt to Original server\" &gt;&gt; /root/sync.log\n    echo \"Send last_backup_sync.txt to Original server\"\n    \n    # Basically ejects the portable hard drive.\n    zpool export Sync\n    echo \"Disconnect Sync complete\" &gt;&gt; /root/sync.log\n    echo \"---------------------------------------------------\" &gt;&gt; /root/sync.log\n    echo \"Disconnect Sync complete\"\n\nCheers,  \nBasilisk\\_hunters", "author_fullname": "t2_2xnvildp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created some scripts to backup my data via sneakernet. thought I'd share", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gpedp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677812574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I live in an area where symmetrical internet is either expensive or unavailable. My offsite backup is in my brother&amp;#39;s house and just rsync&amp;#39;ing it didn&amp;#39;t seem reasonable with my upload speed / bandwidth limitations. I decided to go with the tried and true sneakernet method. I decided to take advantage of ZFS&amp;#39; snapshotting functionality. The first script automatically creates incremental snapshots holding all the changes between the latest update on the backup machine and the current date as of running the script. These snapshots can be ferried between the original and backup machines via a portable hard drive. The second script applies runs the backup machine and will apply the snapshots from the portable hard drive to the backup system.  &lt;/p&gt;\n\n&lt;p&gt;I thought I&amp;#39;d share the scripts in case their useful for anyone else sneakernetting their backups as well as some feedback from the community.  &lt;/p&gt;\n\n&lt;p&gt;The first script (ideally set to run periodically via cron/systemd.timers)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#!/bin/bash\n\n # This is part one of my &amp;#39;patented&amp;#39; sneakernet backup system. The inte-\n # nded usecase is to use a portable hard drive to shuttle incremental \n # zfs snapshots from the original server (where this script will reside\n # and run) to the backup server. There are a few requirements:\n # 1) Both the original and backup servers have matching datasets\n # 2) The portable hard drive is formatted with ZFS with a pool named\n #    Sync\n # 3) When the portable hard drive makes its way back to the original \n #    server, you zpool import it and rsync the contents of the \n #    ZFS_incremental folder onto it\n\n# Honestly this probably isn&amp;#39;t needed in this script but I put it in all\n# my bash scripts\nset -e\n\n# Determine current date\ncurrentdate=$(date +&amp;quot;%Y-%m-%d&amp;quot;)\n\n# Determine the last time we synced with the backupserver. The file is\n# generated by the backup server and either rsynced via SSH or pulled\n# from the portable hard drive\nlwsync=$(tail -n -1 /root/last_backup_sync.txt)\n\n# Using the current date to create snapshot names. Endpoints for the\n# incremental snapshot\ncurISOssnapshot=&amp;quot;Pool1/ISOs@${currentdate}&amp;quot;\ncurlinuxsnapshot=&amp;quot;Pool1/linux@${currentdate}&amp;quot;\ncurpubdomainsnapshot=&amp;quot;Pool1/pubdomain@${currentdate}&amp;quot;\n\n# Using the last date we to create snapshot names. Start-points for the\n# incremental snapshot\nlwsyncISOssnapshot=&amp;quot;Pool1/ISOs@${lwsync}&amp;quot;\nlwsynclinuxsnapshot=&amp;quot;Pool1/linux@${lwsync}&amp;quot;\nlwsyncpubdomainsnapshot=&amp;quot;Pool1/pubdomain@${lwsync}&amp;quot;\n\n\n# use zfs send to save incremental snapshots of each pool\necho $curISOssnapshot\necho $curlinuxsnapshot\necho $curpubdomainsnapshot\n\n\n# Create snapshots of with current date\nzfs snapshot $curISOssnapshot\nzfs snapshot $curlinuxsnapshot\nzfs snapshot $curpubdomainsnapshot\necho &amp;quot;Snapshots made ${currentdate}&amp;quot; &amp;gt;&amp;gt; /root/backupscript.log\n\n# Create Incremental Snapshots of ISOs, linux and pubdomain\nzfs send -Ri $lwsyncISOssnapshot $curISOssnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/ISOs_latest\nzfs send -Ri $lwsynclinuxsnapshot $curlinuxsnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/linux_latest\nzfs send -Ri $lwsyncpubdomainsnapshot $curpubdomainsnapshot &amp;gt; /mnt/Pool1/ZFS_incremental/pubdomain_latest\necho &amp;quot;Incremental snapshots saved to /mnt/Pool1/ZFS_incremental/ &amp;quot; &amp;gt;&amp;gt; /root/backupscript.log\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The second script (ideally run automatically via cron/systemd.timers. should be setup to run every day for when the backup machine is located at a relative with low technical abilities):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#! /bin/bash\n\n# This script is the second part of my &amp;#39;patented&amp;#39; sneakernet update \n# system. The script is intended to be run on the backup server once the \n# portable hard drive is plugged into the machine. This script does have \n# a few dependencies: \n# 1) the backup server must have the same zfs datasets that are going to \n#    be transferred from the portable hard drive. \n# 2) the backup server must have automatic snapshotting turned OFF. \n# 3) this script assumes that you can ssh into your primary server. \n\n# Basically stops the script when something goes wrong. Mostly for when\n# the script is set to run automatically (via systemd.timers or crontab)\n# and the \nset -e\n\n# set variables for the latest snapshots of the backup system (useful\n# for later\nlast_ISOs_snapshot=$(zfs list -H -o name -t snapshot BackupPool/ISOs | tail -n -1)\nlast_linux_snapshot=$(zfs list -H -o name -t snapshot BackupPool/linux | tail -n -1)\nlast_pubdomain_snapshot=$(zfs list -H -o name -t snapshot BackupPool/pubdomain | tail -n -1)\n\n# Honestly, just here to provide some output for logging to see where\n# it might have failed\necho $last_ISOs_snapshot\necho $last_linux_snapshot\necho $last_pubdomain_snapshot\n\necho $(date +&amp;quot;%Y-%m-%d&amp;quot;) &amp;gt;&amp;gt; /root/sync.log\necho $(date +&amp;quot;%Y-%m-%d&amp;quot;)\n\n# Imports the pool located on the portable hard drive\nzpool import -o altroot=/mnt Sync\necho &amp;quot;Import Done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Import Done&amp;quot;\n\n# Rollback to last snapshot. This is necessary in case the person using\n# the backupserver happened to accidentaly ignore your instructions and\n# alters any of the pools that are going to be synced.\nzfs rollback $last_ISOs_snapshot\nzfs rollback $last_linux_snapshot\nzfs rollback $last_pubdomain_snapshot\necho &amp;quot;Snapshot rollback done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Snapshot rollback done&amp;quot;\n\n# Sync Incremental Snapshots of ISOs, linux and pubdomain\nzfs recv -F BackupPool/ISOs &amp;lt; /mnt/Sync/ISOs_latest\necho &amp;quot;ISOs updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;ISOs updated&amp;quot;\n\nzfs recv -F BackupPool/linux &amp;lt; /mnt/Sync/linux_latest\necho &amp;quot;linux updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;linux updated&amp;quot;\n\nzfs recv -F BackupPool/pubdomain &amp;lt; /mnt/Sync/pubdomain_latest\necho &amp;quot;pubdomain updated&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;pubdomain updated&amp;quot;\n\necho &amp;quot;Sync Complete&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Sync Complete&amp;quot;\n\n# Create the file that will be referenced by the first script when det-\n# ermining what the start point is for the incremental snapshot. Saves \n# it to the portable hard drive in case ssh&amp;#39;ing into the progenitor \n# server is not possible\nzfs list -H -o name -t snapshot BackupPool/ISOs &amp;gt; tempa.txt\nsed &amp;#39;s/BackupPool\\/ISOs@//&amp;#39; tempa.txt &amp;gt; /root/last_backup_sync.txt\nrm tempa.txt\ncp /root/last_backup_sync.txt /mnt/Sync/last_backup_sync.txt\necho &amp;quot;Update last_backup_sync.txt done&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Update last_backup_sync.txt done&amp;quot;\n\n# Sends the file to the original server. Means that the originator ser-\n# ver can start making the correct incremental snapshots. This, in turn,\n# means that when the original server receives the portable hard drive,\n# it need only copy over the pregenerated incremental snapshots.\nrsync -e &amp;quot;ssh&amp;quot; -avz /root/last_backup_sync.txt root@###.###.###.###:/root/\necho &amp;quot;Send last_backup_sync.txt to Original server&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Send last_backup_sync.txt to Original server&amp;quot;\n\n# Basically ejects the portable hard drive.\nzpool export Sync\necho &amp;quot;Disconnect Sync complete&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;---------------------------------------------------&amp;quot; &amp;gt;&amp;gt; /root/sync.log\necho &amp;quot;Disconnect Sync complete&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Cheers,&lt;br/&gt;\nBasilisk_hunters&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gpedp", "is_robot_indexable": true, "report_reasons": null, "author": "Basilisk_hunters", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gpedp/i_created_some_scripts_to_backup_my_data_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gpedp/i_created_some_scripts_to_backup_my_data_via/", "subreddit_subscribers": 671957, "created_utc": 1677812574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Would prefer a Python method...to for example, sort it all by facial recognition or body-type/attributes, hair color, anything at all? Or are we still all stuck the stone-age when it comes to organizing this stuff? I'm using Lightroom and the nested key-wording is nice, but it's laborious with a large collection.", "author_fullname": "t2_ygrrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using AI (through Python scripts or otherwise) to organize their massively large porn images (or video?) through any auto-keyword/tagging method?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hdgfx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677874257.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would prefer a Python method...to for example, sort it all by facial recognition or body-type/attributes, hair color, anything at all? Or are we still all stuck the stone-age when it comes to organizing this stuff? I&amp;#39;m using Lightroom and the nested key-wording is nice, but it&amp;#39;s laborious with a large collection.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hdgfx", "is_robot_indexable": true, "report_reasons": null, "author": "clevnumb", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hdgfx/is_anyone_using_ai_through_python_scripts_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hdgfx/is_anyone_using_ai_through_python_scripts_or/", "subreddit_subscribers": 671957, "created_utc": 1677874257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had an Unraid server for 6 years now, and have decided on building another system - \\*which will definitely be ZFS\\* (speed considerations + I already have all the drives).  \nI'm really conflicted between buying another Unraid license and using the ZFS Plugin until 6.12 comes out with official ZFS support, but on the other hand there's TrueNAS Scale which is free and has a lot of users (and also TrueCharts so a lot of available apps).\n\nWhat are the cons of TrueNAS Scale? Is Kubernetes better than the Unraid Docker?And how about the community and support for the apps? (Like TrueCharts)\n\nThanks in advance for any help!", "author_fullname": "t2_143yl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS System - TrueNAS Scale or ZFS Plugin+Unraid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gsrmp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677822496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had an Unraid server for 6 years now, and have decided on building another system - *which will definitely be ZFS* (speed considerations + I already have all the drives).&lt;br/&gt;\nI&amp;#39;m really conflicted between buying another Unraid license and using the ZFS Plugin until 6.12 comes out with official ZFS support, but on the other hand there&amp;#39;s TrueNAS Scale which is free and has a lot of users (and also TrueCharts so a lot of available apps).&lt;/p&gt;\n\n&lt;p&gt;What are the cons of TrueNAS Scale? Is Kubernetes better than the Unraid Docker?And how about the community and support for the apps? (Like TrueCharts)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "59TB RAID6 | 43TB Usable", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gsrmp", "is_robot_indexable": true, "report_reasons": null, "author": "n0llbyte", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11gsrmp/zfs_system_truenas_scale_or_zfs_pluginunraid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gsrmp/zfs_system_truenas_scale_or_zfs_pluginunraid/", "subreddit_subscribers": 671957, "created_utc": 1677822496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://github.com/powerhousepro69/videoinfox](https://github.com/powerhousepro69/videoinfox)\n\nFind your videos fast.  Powerful playlist building and editing. A play queue to load up unlimited playlists for autoplay with last video played marker to pick up where you left off. Played List that keeps track of everything played by Play Clipboard, Last Download and Play.  Index unlimited video libraries and find videos by keyword from the path or file.  Yt-dlp integration with Download list building without leaving your web browser and a download queue to load up unlimited download lists for batch processing. Download entire URL playlists or pick and choose what videos to download with browse. A download queue in browse to add single videos from any of the playlists to batch download. \n\nMinimal dependencies.  Just one file around 1 megabyte to download.", "author_fullname": "t2_42enz2x8", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Videoinfox v5.9.38 Linux script with Yt-dlp integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hhzr0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677884580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/powerhousepro69/videoinfox\"&gt;https://github.com/powerhousepro69/videoinfox&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Find your videos fast.  Powerful playlist building and editing. A play queue to load up unlimited playlists for autoplay with last video played marker to pick up where you left off. Played List that keeps track of everything played by Play Clipboard, Last Download and Play.  Index unlimited video libraries and find videos by keyword from the path or file.  Yt-dlp integration with Download list building without leaving your web browser and a download queue to load up unlimited download lists for batch processing. Download entire URL playlists or pick and choose what videos to download with browse. A download queue in browse to add single videos from any of the playlists to batch download. &lt;/p&gt;\n\n&lt;p&gt;Minimal dependencies.  Just one file around 1 megabyte to download.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LtIuRCynH78qwbklY4Ii4dJp9UY_xEk8ZS0mhTtEQpo.jpg?auto=webp&amp;v=enabled&amp;s=d2e78e3211ee82c1c201da87d016a9f2c3a3cb41", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/LtIuRCynH78qwbklY4Ii4dJp9UY_xEk8ZS0mhTtEQpo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e0da94dbe5a31f76444e9589f9670510477c5e4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/LtIuRCynH78qwbklY4Ii4dJp9UY_xEk8ZS0mhTtEQpo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07e44ea53aca4002a24670612c0b416690c36521", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/LtIuRCynH78qwbklY4Ii4dJp9UY_xEk8ZS0mhTtEQpo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93f754d1d3cd46b4062140b469aaefc89abf8a32", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/LtIuRCynH78qwbklY4Ii4dJp9UY_xEk8ZS0mhTtEQpo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca5339e2b6bd36c60389bacbe5f59fb17fa5e6bb", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/LtIuRCynH78qwbklY4Ii4dJp9UY_xEk8ZS0mhTtEQpo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9111b8b0f783f092c36b3aa284c124d68345cdc1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/LtIuRCynH78qwbklY4Ii4dJp9UY_xEk8ZS0mhTtEQpo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b4750f80bf3e46c1f3124c9fc4c393fa66d571d", "width": 1080, "height": 540}], "variants": {}, "id": "PHs1AleT7KG-hlgOY3cPa8F27MSTk0CAJEERah40IiM"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hhzr0", "is_robot_indexable": true, "report_reasons": null, "author": "powerhousepro69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hhzr0/videoinfox_v5938_linux_script_with_ytdlp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hhzr0/videoinfox_v5938_linux_script_with_ytdlp/", "subreddit_subscribers": 671957, "created_utc": 1677884580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been thinking about using Amazon Photos as another redudant storage for my pictures, but it seems this sub doesn't have much love for Amazon Photos and it surprises me a bit, specially after Google limited Photos's storage, as Amazon Photos seems to maintain an unlimited policy for pictures, for now at least.\n\nIs there a reason for that? Maybe its due to privacy concerns, being an Amazon product? Or is it just a bad service (app, web, windows client...)?", "author_fullname": "t2_ekn6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon Photos vs Google Photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11h5dqh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677861845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been thinking about using Amazon Photos as another redudant storage for my pictures, but it seems this sub doesn&amp;#39;t have much love for Amazon Photos and it surprises me a bit, specially after Google limited Photos&amp;#39;s storage, as Amazon Photos seems to maintain an unlimited policy for pictures, for now at least.&lt;/p&gt;\n\n&lt;p&gt;Is there a reason for that? Maybe its due to privacy concerns, being an Amazon product? Or is it just a bad service (app, web, windows client...)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11h5dqh", "is_robot_indexable": true, "report_reasons": null, "author": "pelluchan", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11h5dqh/amazon_photos_vs_google_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11h5dqh/amazon_photos_vs_google_photos/", "subreddit_subscribers": 671957, "created_utc": 1677861845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have several disks of diffent sizes in my snapraid/mergerfs configuration. The latest addition is a 20TB drive which is wastly bigger than the smallest (4 TB). I did a mergerfs.balance operation and all drives are now filled equally (\\~45%). I'd like to keep it that way.\n\nHowever, my understanding of mfs is, that it will fill the disk based on \\*actual\\* most free space - not percentual. So my 20TB drive would be almost completely filled before any further writes will be made to the 4TB (about 45% filled right now).\n\nI check the documentation and couldn't find a policy that allows for that. Is there any setting/policy that enables me to fill the disks equally on a percentual basis - similar to how mergerfs.balance levels out the disks?\n\nThank you in advance for any ideas/guidance on that.", "author_fullname": "t2_khebnfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mergerfs - most free space (percentage basis)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gwrm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677837061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several disks of diffent sizes in my snapraid/mergerfs configuration. The latest addition is a 20TB drive which is wastly bigger than the smallest (4 TB). I did a mergerfs.balance operation and all drives are now filled equally (~45%). I&amp;#39;d like to keep it that way.&lt;/p&gt;\n\n&lt;p&gt;However, my understanding of mfs is, that it will fill the disk based on *actual* most free space - not percentual. So my 20TB drive would be almost completely filled before any further writes will be made to the 4TB (about 45% filled right now).&lt;/p&gt;\n\n&lt;p&gt;I check the documentation and couldn&amp;#39;t find a policy that allows for that. Is there any setting/policy that enables me to fill the disks equally on a percentual basis - similar to how mergerfs.balance levels out the disks?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for any ideas/guidance on that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gwrm0", "is_robot_indexable": true, "report_reasons": null, "author": "SillyMochi", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gwrm0/mergerfs_most_free_space_percentage_basis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gwrm0/mergerfs_most_free_space_percentage_basis/", "subreddit_subscribers": 671957, "created_utc": 1677837061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_k0z2c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "16 lane sas HBA less than $60? I want to fill this baby up with SSDs and HDDs.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "name": "t3_11hig2r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UPrSNGLiQlBVSMHvA-Mfj1_9nKkE51URyVx2Ey4npyc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677885632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/o250dlnffnla1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/o250dlnffnla1.jpg?auto=webp&amp;v=enabled&amp;s=de700ce3033f98928d4e74b92d7352b6f0b7c0a3", "width": 4000, "height": 1800}, "resolutions": [{"url": "https://preview.redd.it/o250dlnffnla1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03413f7ad55e490bf7daa673f9f160627bb419a1", "width": 108, "height": 48}, {"url": "https://preview.redd.it/o250dlnffnla1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd09fbf19208c0e944d4b927702da423dd6a6213", "width": 216, "height": 97}, {"url": "https://preview.redd.it/o250dlnffnla1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f5f2976e5183c865ea9da517e33452e909102b4", "width": 320, "height": 144}, {"url": "https://preview.redd.it/o250dlnffnla1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1322b7f72a5e7eac0b5f4a01de16a80a71606ee7", "width": 640, "height": 288}, {"url": "https://preview.redd.it/o250dlnffnla1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae785a8c8a47c0358211494791cfff848e5892ee", "width": 960, "height": 432}, {"url": "https://preview.redd.it/o250dlnffnla1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d5d7b2c2847279fa59d566f2008dfb505c48537", "width": 1080, "height": 486}], "variants": {}, "id": "jQPipsDxlx-Wf_cg9fQT-iNAw0DsiQOfzZtJUNPG5E8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hig2r", "is_robot_indexable": true, "report_reasons": null, "author": "corrpendragon", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hig2r/16_lane_sas_hba_less_than_60_i_want_to_fill_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/o250dlnffnla1.jpg", "subreddit_subscribers": 671957, "created_utc": 1677885632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've liked some auto-generated music videos by YouTube and they've been removed. Now I have no way to know what the music was.", "author_fullname": "t2_4pqljccy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YouTube Music. Is there anything that keeps metadata (title) on those auto-generated music videos that have been removed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hgsrl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677881807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve liked some auto-generated music videos by YouTube and they&amp;#39;ve been removed. Now I have no way to know what the music was.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hgsrl", "is_robot_indexable": true, "report_reasons": null, "author": "neuro__atypical", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hgsrl/youtube_music_is_there_anything_that_keeps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hgsrl/youtube_music_is_there_anything_that_keeps/", "subreddit_subscribers": 671957, "created_utc": 1677881807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys, been passively lurking here for years and I've finally decided to take the leap now that I can actually afford to buy things.\n\nI created a Master Folder which contains everything that I ever want to possess (software, media, games, courses, etc) or come across as I go through life. That folder will be the main folder with all the sub folders.\n\nJust bought a 2TB (small start) external hard disk and I will be using it as a backup for the Main Folder. I was wondering if there's any way to manage this data since there would be a gap of time (1-4 weeks roughly) before I update the HDD to sync with my laptop.\n\nI tried googling/searching but couldn't find anything (probably didn't use the right words)\n\nIt would be great to have a tool that can help me with this so that I can eventually get a back-up for the back-up and slowly archive everything under the sun!", "author_fullname": "t2_5csbxqyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manage backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hbq9e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677870285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, been passively lurking here for years and I&amp;#39;ve finally decided to take the leap now that I can actually afford to buy things.&lt;/p&gt;\n\n&lt;p&gt;I created a Master Folder which contains everything that I ever want to possess (software, media, games, courses, etc) or come across as I go through life. That folder will be the main folder with all the sub folders.&lt;/p&gt;\n\n&lt;p&gt;Just bought a 2TB (small start) external hard disk and I will be using it as a backup for the Main Folder. I was wondering if there&amp;#39;s any way to manage this data since there would be a gap of time (1-4 weeks roughly) before I update the HDD to sync with my laptop.&lt;/p&gt;\n\n&lt;p&gt;I tried googling/searching but couldn&amp;#39;t find anything (probably didn&amp;#39;t use the right words)&lt;/p&gt;\n\n&lt;p&gt;It would be great to have a tool that can help me with this so that I can eventually get a back-up for the back-up and slowly archive everything under the sun!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hbq9e", "is_robot_indexable": true, "report_reasons": null, "author": "Russell_fer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hbq9e/how_to_manage_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hbq9e/how_to_manage_backups/", "subreddit_subscribers": 671957, "created_utc": 1677870285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have had no luck with [archive.org](https://archive.org) at this point hope might be lost I've tried using a few url to mp4 converters still no luck. This is the video [https://www.youtube.com/watch?v=QCHfSfO2neU](https://www.youtube.com/watch?v=QCHfSfO2neU)", "author_fullname": "t2_vynj9t7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any sites besides archive.org that can play removed YouTube videos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11h4w46", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677860729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have had no luck with &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt; at this point hope might be lost I&amp;#39;ve tried using a few url to mp4 converters still no luck. This is the video &lt;a href=\"https://www.youtube.com/watch?v=QCHfSfO2neU\"&gt;https://www.youtube.com/watch?v=QCHfSfO2neU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11h4w46", "is_robot_indexable": true, "report_reasons": null, "author": "Independent-Hold-329", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11h4w46/are_there_any_sites_besides_archiveorg_that_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11h4w46/are_there_any_sites_besides_archiveorg_that_can/", "subreddit_subscribers": 671957, "created_utc": 1677860729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Text files in git? Flat file storage? Relational database? Bookmarks in a browser? How are you, if you are, managing a database of torrent files and/or magnet links? I am storing one torrent per file in a Dropbox folder.", "author_fullname": "t2_4vfk6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you store and catalog torrent metadata?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hj3co", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677887176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Text files in git? Flat file storage? Relational database? Bookmarks in a browser? How are you, if you are, managing a database of torrent files and/or magnet links? I am storing one torrent per file in a Dropbox folder.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hj3co", "is_robot_indexable": true, "report_reasons": null, "author": "toomuchtodotoday", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hj3co/how_do_you_store_and_catalog_torrent_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hj3co/how_do_you_store_and_catalog_torrent_metadata/", "subreddit_subscribers": 671957, "created_utc": 1677887176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im wondering how to make my sd card or folder read only, I'm using an android phone and noticing some of my files were not were Id left them I think maybe a music player or file manger mightve shoved my music in the wrong place, any help I'll be appreciative", "author_fullname": "t2_2vocurk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Make an sd card read only", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hhtqw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677884187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im wondering how to make my sd card or folder read only, I&amp;#39;m using an android phone and noticing some of my files were not were Id left them I think maybe a music player or file manger mightve shoved my music in the wrong place, any help I&amp;#39;ll be appreciative&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hhtqw", "is_robot_indexable": true, "report_reasons": null, "author": "finnkee1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hhtqw/make_an_sd_card_read_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hhtqw/make_an_sd_card_read_only/", "subreddit_subscribers": 671957, "created_utc": 1677884187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was wondering what tools /r/datahoarder uses to organize and manage lots of documents and massive text files.\n\nIdeally, the tool would parse the files and act as a local search engine. Due to the number of files and large sizes, I would like to find the most efficient and stable program.\n\n* [Datashare by ICIJ](https://datashare.icij.org/) is well polished and seems like the ideal fit for this application but after parsing a few of the larger documents, it ends up throwing out of memory errors (Uses Java, Windows 11) and gets stuck in a loop.\n\nI haven't had experience with the below tools but any feedback for them would be great!\n\n* [Spyglass](https://github.com/a5huynh/spyglass)\n* [Recoll](https://www.lesbonscomptes.com/recoll/pages/index-recoll.html)\n* [Paperless-ngx](https://github.com/paperless-ngx/paperless-ngx)\n* [Diskover](https://github.com/diskoverdata/diskover-community)\n* [Orange](https://github.com/naaive/orange)\n* [sist2](https://github.com/simon987/sist2)\n* [Datasette](https://datasette.io/)\n* [Perkeep](https://perkeep.org/)\n* [Filehunter](https://github.com/Ogefest/filehunter)\n* [Everything](https://www.voidtools.com/) \\- Not open source.\n\nUsing Linux for 'grep' or 'ripgrep' the files every time I want to run a search seems inefficient.", "author_fullname": "t2_icg85kno", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tool to parse, index, and search local documents? - Windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hhhqb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677894199.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677883395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering what tools &lt;a href=\"/r/datahoarder\"&gt;/r/datahoarder&lt;/a&gt; uses to organize and manage lots of documents and massive text files.&lt;/p&gt;\n\n&lt;p&gt;Ideally, the tool would parse the files and act as a local search engine. Due to the number of files and large sizes, I would like to find the most efficient and stable program.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://datashare.icij.org/\"&gt;Datashare by ICIJ&lt;/a&gt; is well polished and seems like the ideal fit for this application but after parsing a few of the larger documents, it ends up throwing out of memory errors (Uses Java, Windows 11) and gets stuck in a loop.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I haven&amp;#39;t had experience with the below tools but any feedback for them would be great!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/a5huynh/spyglass\"&gt;Spyglass&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.lesbonscomptes.com/recoll/pages/index-recoll.html\"&gt;Recoll&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/paperless-ngx/paperless-ngx\"&gt;Paperless-ngx&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/diskoverdata/diskover-community\"&gt;Diskover&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/naaive/orange\"&gt;Orange&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/simon987/sist2\"&gt;sist2&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://datasette.io/\"&gt;Datasette&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://perkeep.org/\"&gt;Perkeep&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/Ogefest/filehunter\"&gt;Filehunter&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.voidtools.com/\"&gt;Everything&lt;/a&gt; - Not open source.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Using Linux for &amp;#39;grep&amp;#39; or &amp;#39;ripgrep&amp;#39; the files every time I want to run a search seems inefficient.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hhhqb", "is_robot_indexable": true, "report_reasons": null, "author": "zBoyDojo", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hhhqb/tool_to_parse_index_and_search_local_documents/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hhhqb/tool_to_parse_index_and_search_local_documents/", "subreddit_subscribers": 671957, "created_utc": 1677883395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm tinkering with some cloud storage and thought it might be interesting to combine a few of them into a \"RAID\" for benefits similar to local arrays. If one of the providers goes down, the account gets banned or policy changes or whatever, then I won't lose everything immediately like on a RAID 0 equivalent. And I'll be able to use the aggregated storage more effectively since I have don't have to worry about how full each individual drive is.\n\nI figure it's going to be annoying as hell to combine all the free 2 or 5GB of storage here and there so I mainly want to do this with large 3 disks; Oracle Cloud (150/200GB, using SFTP), Storj (150GB) and SFTP / local.\n\nOur other lord and savior (the primary one being zfs of course lol) rclone doesn't seem to have a trick for this one. The \"combine\" policies I found seem to be oriented towards where to place the data (lowest free space used, fewest objects stored etc) and doesn't have a function for RAID. Does rclone have a feature for this?\n\nIf not, I think I can try splitting the files up and semi manully adding 33% parity data. I don't know if that guarantees single drive redundancy though, since a data chunk and parity of that chunk could end up on the same disk. I don't suppose there's an rclone solution for this... Lol.", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is 1 drive redundancy possible with cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gziv0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677846528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m tinkering with some cloud storage and thought it might be interesting to combine a few of them into a &amp;quot;RAID&amp;quot; for benefits similar to local arrays. If one of the providers goes down, the account gets banned or policy changes or whatever, then I won&amp;#39;t lose everything immediately like on a RAID 0 equivalent. And I&amp;#39;ll be able to use the aggregated storage more effectively since I have don&amp;#39;t have to worry about how full each individual drive is.&lt;/p&gt;\n\n&lt;p&gt;I figure it&amp;#39;s going to be annoying as hell to combine all the free 2 or 5GB of storage here and there so I mainly want to do this with large 3 disks; Oracle Cloud (150/200GB, using SFTP), Storj (150GB) and SFTP / local.&lt;/p&gt;\n\n&lt;p&gt;Our other lord and savior (the primary one being zfs of course lol) rclone doesn&amp;#39;t seem to have a trick for this one. The &amp;quot;combine&amp;quot; policies I found seem to be oriented towards where to place the data (lowest free space used, fewest objects stored etc) and doesn&amp;#39;t have a function for RAID. Does rclone have a feature for this?&lt;/p&gt;\n\n&lt;p&gt;If not, I think I can try splitting the files up and semi manully adding 33% parity data. I don&amp;#39;t know if that guarantees single drive redundancy though, since a data chunk and parity of that chunk could end up on the same disk. I don&amp;#39;t suppose there&amp;#39;s an rclone solution for this... Lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "vTrueNAS 72TB / Hyper-V", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gziv0", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11gziv0/is_1_drive_redundancy_possible_with_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gziv0/is_1_drive_redundancy_possible_with_cloud/", "subreddit_subscribers": 671957, "created_utc": 1677846528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for software that will scan every photo in my library and add tags to them to allow searching. Ideally, this will function similarly to the photo search from iOS where you can search \"shoe\" and shoes will appear.", "author_fullname": "t2_50n4upyn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo Library Scan &amp; Organization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11go6ou", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677809271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for software that will scan every photo in my library and add tags to them to allow searching. Ideally, this will function similarly to the photo search from iOS where you can search &amp;quot;shoe&amp;quot; and shoes will appear.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11go6ou", "is_robot_indexable": true, "report_reasons": null, "author": "c00pdwg", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11go6ou/photo_library_scan_organization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11go6ou/photo_library_scan_organization/", "subreddit_subscribers": 671957, "created_utc": 1677809271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have data about 15TB, but only 2 TB is actually important enough to backup. I currently have duplicati backing it up weekly (and keeping different versions depending on the severity of the data). I'd like to update this 2TB monthly in the cloud.\n\nI'm seeing a lot of BackBlaze, Wasabi, and OneDrive recommendations. Looks like OneDrive can be 6TB but you gotta set up multiple accounts and share which is too messy for me. I'm leaning toward Backblaze and just rclone the duplicati files up. \n\nI won't need the data unless both primary and backup drives go out and I know I'll have to pay to get them. Anything I'm missing that I should consider with the cloud backups?", "author_fullname": "t2_8f0ul39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Another Cloud Offsite Backup Opinion Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11hloiz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677893705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have data about 15TB, but only 2 TB is actually important enough to backup. I currently have duplicati backing it up weekly (and keeping different versions depending on the severity of the data). I&amp;#39;d like to update this 2TB monthly in the cloud.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeing a lot of BackBlaze, Wasabi, and OneDrive recommendations. Looks like OneDrive can be 6TB but you gotta set up multiple accounts and share which is too messy for me. I&amp;#39;m leaning toward Backblaze and just rclone the duplicati files up. &lt;/p&gt;\n\n&lt;p&gt;I won&amp;#39;t need the data unless both primary and backup drives go out and I know I&amp;#39;ll have to pay to get them. Anything I&amp;#39;m missing that I should consider with the cloud backups?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hloiz", "is_robot_indexable": true, "report_reasons": null, "author": "DudePrude5", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hloiz/another_cloud_offsite_backup_opinion_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hloiz/another_cloud_offsite_backup_opinion_question/", "subreddit_subscribers": 671957, "created_utc": 1677893705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I'm thinking about buying a couple of toshiba drives, but I'm a bit confused on their warranty.\n\n I can see on their website that their NAS models have up to 1.0M hours guaranteed, but I can't see any clear mention about their desktop drives. When I ctrl-f to find the \"hours\" on the desktop drive page, it mentions on the bottom that it also guarantees 1M hrs, if I understand correcty, but it isn't mentioned clearly on the product description. If that is the case, why would I buy the nas drive? Both are CMR, and one is much cheaper than the other.", "author_fullname": "t2_6jk51wm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help on toshiba drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11hlnvd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677893656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m thinking about buying a couple of toshiba drives, but I&amp;#39;m a bit confused on their warranty.&lt;/p&gt;\n\n&lt;p&gt;I can see on their website that their NAS models have up to 1.0M hours guaranteed, but I can&amp;#39;t see any clear mention about their desktop drives. When I ctrl-f to find the &amp;quot;hours&amp;quot; on the desktop drive page, it mentions on the bottom that it also guarantees 1M hrs, if I understand correcty, but it isn&amp;#39;t mentioned clearly on the product description. If that is the case, why would I buy the nas drive? Both are CMR, and one is much cheaper than the other.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hlnvd", "is_robot_indexable": true, "report_reasons": null, "author": "whatthesamuel", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hlnvd/help_on_toshiba_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hlnvd/help_on_toshiba_drives/", "subreddit_subscribers": 671957, "created_utc": 1677893656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to set this up as a mirror with a stripped parity. It's only going to be used for Plex and maybe some small backups here and there. That would give me 64TB with some significant redundancy. I tried looking it up and have no idea what raid level that is. Can I get some input on the best way to do this? Is my approach wrong?", "author_fullname": "t2_5ihr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm setting up a server with (10) 14TB hard drives. What is the best raid configuration for me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hj8bg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677887513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to set this up as a mirror with a stripped parity. It&amp;#39;s only going to be used for Plex and maybe some small backups here and there. That would give me 64TB with some significant redundancy. I tried looking it up and have no idea what raid level that is. Can I get some input on the best way to do this? Is my approach wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11hj8bg", "is_robot_indexable": true, "report_reasons": null, "author": "CassandraVindicated", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11hj8bg/im_setting_up_a_server_with_10_14tb_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11hj8bg/im_setting_up_a_server_with_10_14tb_hard_drives/", "subreddit_subscribers": 671957, "created_utc": 1677887513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[http://www.liveleak.com/view?i=3ad\\_1356876614](http://www.liveleak.com/view?i=3ad_1356876614) work for a syrian american NGO that tracks warcrimes in the Syrian Conflict  \n\n\nWTF is up with all the censorship on the internet? Takdown requests from the 2000s with US  govt etc...these are prosecutable offenses. [archive.com](https://archive.com) loads the webpage but cnanot load the videos. is all that footage gone for good?", "author_fullname": "t2_lww73a7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "i can't load any liveleak videos from archive.com ....i am trying to find war criminals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11h4gcl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677859713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"http://www.liveleak.com/view?i=3ad_1356876614\"&gt;http://www.liveleak.com/view?i=3ad_1356876614&lt;/a&gt; work for a syrian american NGO that tracks warcrimes in the Syrian Conflict  &lt;/p&gt;\n\n&lt;p&gt;WTF is up with all the censorship on the internet? Takdown requests from the 2000s with US  govt etc...these are prosecutable offenses. &lt;a href=\"https://archive.com\"&gt;archive.com&lt;/a&gt; loads the webpage but cnanot load the videos. is all that footage gone for good?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IRMwLVcCmMSPC-e0XYKJjD9O0GFrV_xJ2W30oy0c-o8.jpg?auto=webp&amp;v=enabled&amp;s=dedee15f39fb46836812cb9fb256b0396539e058", "width": 138, "height": 33}, "resolutions": [{"url": "https://external-preview.redd.it/IRMwLVcCmMSPC-e0XYKJjD9O0GFrV_xJ2W30oy0c-o8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0872c80d8c6abc2bc4ce74535900ea95b869df74", "width": 108, "height": 25}], "variants": {}, "id": "W3WFk2B6T_I4K-vzuU_x5eiocMGdOOQKy1t4-7FEHMQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11h4gcl", "is_robot_indexable": true, "report_reasons": null, "author": "fartuni4", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11h4gcl/i_cant_load_any_liveleak_videos_from_archivecom_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11h4gcl/i_cant_load_any_liveleak_videos_from_archivecom_i/", "subreddit_subscribers": 671957, "created_utc": 1677859713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a cheap backup for 1 Mac with a NAS, between 1TB to 2TB\n\nAny opinions good or bad on PolarBackup? I've been using them for the last year at a price of 64 USD per year for 650 GB (probably up to 1TB). I must admit when the renewal came up, I wondered if there was anything better/cheaper.\n\nAlso their advertising is not clear about costs.\n\niCloud presently 8USD for a year then about 80 USD ...\n\nAnything else?\n\nBackblaze still the favourite for a small home setup?\n\nThanks", "author_fullname": "t2_xzqlc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a cheap backup for 1 Mac with a NAS, between 1TB to 2TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gwxpw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677837721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a cheap backup for 1 Mac with a NAS, between 1TB to 2TB&lt;/p&gt;\n\n&lt;p&gt;Any opinions good or bad on PolarBackup? I&amp;#39;ve been using them for the last year at a price of 64 USD per year for 650 GB (probably up to 1TB). I must admit when the renewal came up, I wondered if there was anything better/cheaper.&lt;/p&gt;\n\n&lt;p&gt;Also their advertising is not clear about costs.&lt;/p&gt;\n\n&lt;p&gt;iCloud presently 8USD for a year then about 80 USD ...&lt;/p&gt;\n\n&lt;p&gt;Anything else?&lt;/p&gt;\n\n&lt;p&gt;Backblaze still the favourite for a small home setup?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gwxpw", "is_robot_indexable": true, "report_reasons": null, "author": "crabbiesgreenginger", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gwxpw/looking_for_a_cheap_backup_for_1_mac_with_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gwxpw/looking_for_a_cheap_backup_for_1_mac_with_a_nas/", "subreddit_subscribers": 671957, "created_utc": 1677837721.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just purchased a 12TB Seagate Ironwolf drive from Amazon and it wouldn't spin up. It beeped when powered on and wasn't recognized by Windows. I figured that the drive was DOA, so I got a replacement from Amazon. \n\nThe replacement drive behaved the same way. I'm powering it using an external enclosure as a temporary solution until I get a better setup. The enclosure works perfectly for my 4TB Barracuda drives, and it's rated for drives up to 18TB.\nI also tried both drives in a desktop environment and had the same result.\n\nI'm curious if I'm just unlucky or if I'm missing something. I know that sometimes 3.3v can cause issues if the drive supports PWDIS, but I haven't found any documentation that states that this drive has PWDIS, so I haven't tried the tape method yet.", "author_fullname": "t2_tn5jk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My new Ironwolf drives beep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11gu903", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677827433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just purchased a 12TB Seagate Ironwolf drive from Amazon and it wouldn&amp;#39;t spin up. It beeped when powered on and wasn&amp;#39;t recognized by Windows. I figured that the drive was DOA, so I got a replacement from Amazon. &lt;/p&gt;\n\n&lt;p&gt;The replacement drive behaved the same way. I&amp;#39;m powering it using an external enclosure as a temporary solution until I get a better setup. The enclosure works perfectly for my 4TB Barracuda drives, and it&amp;#39;s rated for drives up to 18TB.\nI also tried both drives in a desktop environment and had the same result.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious if I&amp;#39;m just unlucky or if I&amp;#39;m missing something. I know that sometimes 3.3v can cause issues if the drive supports PWDIS, but I haven&amp;#39;t found any documentation that states that this drive has PWDIS, so I haven&amp;#39;t tried the tape method yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11gu903", "is_robot_indexable": true, "report_reasons": null, "author": "Scrambles225", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11gu903/my_new_ironwolf_drives_beep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11gu903/my_new_ironwolf_drives_beep/", "subreddit_subscribers": 671957, "created_utc": 1677827433.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}