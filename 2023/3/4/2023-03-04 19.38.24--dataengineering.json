{"kind": "Listing", "data": {"after": "t3_11hwvz2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I work or a large pharma company and we have been asked to start exploring azure as a cloud service. We currently do all our work on prem with sql servers and SSIS as our ETL tool. We just finished up a one week training on adf and I don't think this is the right tool for my team. We write lots of code and ADF is very low code and in the low code set up, simple tasks feel painful to accomplish and very clunky.  My question can azure databricks be set up where all our pipelines run in an a databricks notebook? My goal would be to have notebooks set up and then use adf to kick off those notebooks on a schedule. I want to stay as far away from all the tools and widgets within adf. I simply want to use it as a means to kick off a notebook with a pipeline in it. Is this something that a lot of you do or have seen people do?\n\n&amp;#x200B;\n\nthanks,", "author_fullname": "t2_stdv2q27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "azure datafactory.....ouch...help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hngbs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677898524.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I work or a large pharma company and we have been asked to start exploring azure as a cloud service. We currently do all our work on prem with sql servers and SSIS as our ETL tool. We just finished up a one week training on adf and I don&amp;#39;t think this is the right tool for my team. We write lots of code and ADF is very low code and in the low code set up, simple tasks feel painful to accomplish and very clunky.  My question can azure databricks be set up where all our pipelines run in an a databricks notebook? My goal would be to have notebooks set up and then use adf to kick off those notebooks on a schedule. I want to stay as far away from all the tools and widgets within adf. I simply want to use it as a means to kick off a notebook with a pipeline in it. Is this something that a lot of you do or have seen people do?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11hngbs", "is_robot_indexable": true, "report_reasons": null, "author": "xrt679", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hngbs/azure_datafactoryouchhelp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hngbs/azure_datafactoryouchhelp/", "subreddit_subscribers": 91886, "created_utc": 1677898524.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologies if this is already covered somewhere as it seems like a simple question, but I'm not clear why I'd ever choose Snowflake.\n\nThe costs seem to be higher, and the integrated tooling seems to lock me into a vendor in the way that using 3rd party or open source tools would not.\n\nI have nowhere near enough data to come close to hitting size or performance limits on either.\n\nWhat am I missing?", "author_fullname": "t2_lf4it7cs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would I choose Snowflake over BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11i4n2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677947522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this is already covered somewhere as it seems like a simple question, but I&amp;#39;m not clear why I&amp;#39;d ever choose Snowflake.&lt;/p&gt;\n\n&lt;p&gt;The costs seem to be higher, and the integrated tooling seems to lock me into a vendor in the way that using 3rd party or open source tools would not.&lt;/p&gt;\n\n&lt;p&gt;I have nowhere near enough data to come close to hitting size or performance limits on either.&lt;/p&gt;\n\n&lt;p&gt;What am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11i4n2h", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Deer_8686", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11i4n2h/why_would_i_choose_snowflake_over_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11i4n2h/why_would_i_choose_snowflake_over_bigquery/", "subreddit_subscribers": 91886, "created_utc": 1677947522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We recently started using Cloud Composer for our data engineering pipelines. For even a small environment that autoscales from 1 to 3 (in fact using just 1 worker most of the time), it's quite expensive at \\~$350/month. We don't currently have many DAGs running and each dag runs just daily for around 5 to 10 minutes. So our Cloud Composer environment is actually just sitting idlely the vast majority of the time. Since you get charged for the environment which is running 24/7, not just when the tasks are running, the value proposition is not great currently for us, as you can see.\n\nBecause of this, I am in quite a bit of contention with my teammate. Despite the unfortunate cost structure at the moment, I still think Airflow/Cloud Composer is the best solution for building and managing data pipelines and going forward we will certainly have more DAGs and more frequently running DAGs so the value proposition will surely improve significantly. I am very much in favor of using a technology that\u2019s future-proof.\n\nHowever my teammate just can't get over the fact that Cloud Composer is set up such that it's running 24/7 regardless of whether there's task running or not and we are getting charged for all those idling minutes. To him, the dollar cost per task execution time is just ridiculous. He thinks this is clearly a flaw in Airflow\u2019s implementation/design, that nodes are running and cost is building when nothing is being processed. He contends that we can build a much cheaper and equally capable system ourself, say using Cloud Scheduler, Cloud Run/Cloud Functions and taking advantage of background trigger functionalities in Google Cloud document store (i.e., Firestore), e.g., onCreate(), onUpdate() and etc to trigger dependencies between tasks. So in such a system, the fixed cost is much much lower and the vCPU/memory cost is only incurring when tasks are actually running.\n\nMy view is that reinventing the wheel, building our own data engineering tool, when there\u2019s a tried and tested solution for our need, is completely unnecessary, especially since the pubic available solution is proven to be scalable and reliable. But, I am really struggling to convince him and counter his arguments.\n\nSo here are some questions that I really appreciate your input:\n\n1. Why is Cloud Composer using GKE under the hood instead of Cloud Run, or some other mechanism that can completely turn on and off between task executions and hence not running up cost? Is this a flaw in Airflow/Cloud Composer\u2019s architecture but people are still willing to pay for it because of the convenience and there\u2019s no other alternative? Or was it an intentional design decision and engineering necessity?\n2. What advantages does a managed Airflow service, e.g., Cloud Composer, provide compared to an in-house solution like the one mentioned above, that are important but not immediately obvious?\n3. In general, if you are Cloud Composer user, how do you feel about it and how do you justify its expense; has it been worth it?", "author_fullname": "t2_o0hab", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you think Cloud Composer or other managed Airflow service is worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hjk4a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677888285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We recently started using Cloud Composer for our data engineering pipelines. For even a small environment that autoscales from 1 to 3 (in fact using just 1 worker most of the time), it&amp;#39;s quite expensive at ~$350/month. We don&amp;#39;t currently have many DAGs running and each dag runs just daily for around 5 to 10 minutes. So our Cloud Composer environment is actually just sitting idlely the vast majority of the time. Since you get charged for the environment which is running 24/7, not just when the tasks are running, the value proposition is not great currently for us, as you can see.&lt;/p&gt;\n\n&lt;p&gt;Because of this, I am in quite a bit of contention with my teammate. Despite the unfortunate cost structure at the moment, I still think Airflow/Cloud Composer is the best solution for building and managing data pipelines and going forward we will certainly have more DAGs and more frequently running DAGs so the value proposition will surely improve significantly. I am very much in favor of using a technology that\u2019s future-proof.&lt;/p&gt;\n\n&lt;p&gt;However my teammate just can&amp;#39;t get over the fact that Cloud Composer is set up such that it&amp;#39;s running 24/7 regardless of whether there&amp;#39;s task running or not and we are getting charged for all those idling minutes. To him, the dollar cost per task execution time is just ridiculous. He thinks this is clearly a flaw in Airflow\u2019s implementation/design, that nodes are running and cost is building when nothing is being processed. He contends that we can build a much cheaper and equally capable system ourself, say using Cloud Scheduler, Cloud Run/Cloud Functions and taking advantage of background trigger functionalities in Google Cloud document store (i.e., Firestore), e.g., onCreate(), onUpdate() and etc to trigger dependencies between tasks. So in such a system, the fixed cost is much much lower and the vCPU/memory cost is only incurring when tasks are actually running.&lt;/p&gt;\n\n&lt;p&gt;My view is that reinventing the wheel, building our own data engineering tool, when there\u2019s a tried and tested solution for our need, is completely unnecessary, especially since the pubic available solution is proven to be scalable and reliable. But, I am really struggling to convince him and counter his arguments.&lt;/p&gt;\n\n&lt;p&gt;So here are some questions that I really appreciate your input:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Why is Cloud Composer using GKE under the hood instead of Cloud Run, or some other mechanism that can completely turn on and off between task executions and hence not running up cost? Is this a flaw in Airflow/Cloud Composer\u2019s architecture but people are still willing to pay for it because of the convenience and there\u2019s no other alternative? Or was it an intentional design decision and engineering necessity?&lt;/li&gt;\n&lt;li&gt;What advantages does a managed Airflow service, e.g., Cloud Composer, provide compared to an in-house solution like the one mentioned above, that are important but not immediately obvious?&lt;/li&gt;\n&lt;li&gt;In general, if you are Cloud Composer user, how do you feel about it and how do you justify its expense; has it been worth it?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11hjk4a", "is_robot_indexable": true, "report_reasons": null, "author": "not_a_thrownaway", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hjk4a/do_you_think_cloud_composer_or_other_managed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hjk4a/do_you_think_cloud_composer_or_other_managed/", "subreddit_subscribers": 91886, "created_utc": 1677888285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've worked for 3-4 years in positions where I helped structure ETLs, DWs and alike. However, I'm now on the cusp of being hired to help structure the area in a big investment fund here, helping the research area have an easier time focusing on their models. My previous experience led me to grasp DBT, SQL, and most of my experience came from using a Microsoft stack with SSIS, Analysis Services and the like. I'm feeling wayyyy over my head to start building this, and the multitude of possible stacks make me very afraid that I might overengineer this, and I will initially be alone in the area. What do I do? Fake it till I make it? I never lied in my resume, so it's not like they expect a senior with plenty of experience but still... I read this: https://github.com/zsvoboda/ngods-stocks\nAnd it seems like a good starter, albeit overly complex for our use case. I could use suggestions, people to talk to, etc. Please help", "author_fullname": "t2_bdy92fyd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm way over my head", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hrslk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677911084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve worked for 3-4 years in positions where I helped structure ETLs, DWs and alike. However, I&amp;#39;m now on the cusp of being hired to help structure the area in a big investment fund here, helping the research area have an easier time focusing on their models. My previous experience led me to grasp DBT, SQL, and most of my experience came from using a Microsoft stack with SSIS, Analysis Services and the like. I&amp;#39;m feeling wayyyy over my head to start building this, and the multitude of possible stacks make me very afraid that I might overengineer this, and I will initially be alone in the area. What do I do? Fake it till I make it? I never lied in my resume, so it&amp;#39;s not like they expect a senior with plenty of experience but still... I read this: &lt;a href=\"https://github.com/zsvoboda/ngods-stocks\"&gt;https://github.com/zsvoboda/ngods-stocks&lt;/a&gt;\nAnd it seems like a good starter, albeit overly complex for our use case. I could use suggestions, people to talk to, etc. Please help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/x5qGvo9MIQSyBaMiEo0_R16WbBJr11puC9XKh4O9gLc.jpg?auto=webp&amp;v=enabled&amp;s=88e89113dc866e0e79fa88564a4f48a99ed855ad", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/x5qGvo9MIQSyBaMiEo0_R16WbBJr11puC9XKh4O9gLc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8897f11c5e7d5e70d199037ed7dfaccd3c3c0a1a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/x5qGvo9MIQSyBaMiEo0_R16WbBJr11puC9XKh4O9gLc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a160100e6d40cd3bdbb96cd34c20bbe25c13d78", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/x5qGvo9MIQSyBaMiEo0_R16WbBJr11puC9XKh4O9gLc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c9080ad13764e5f268aa4daded940814aeb7720", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/x5qGvo9MIQSyBaMiEo0_R16WbBJr11puC9XKh4O9gLc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5587ba5e9d5302f52bb216098a8a4fa7fd8aebb3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/x5qGvo9MIQSyBaMiEo0_R16WbBJr11puC9XKh4O9gLc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebdd3df5c4ea18925de03f6a8fed9046791ec9a4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/x5qGvo9MIQSyBaMiEo0_R16WbBJr11puC9XKh4O9gLc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=becdd468015d3ca5d3f9f328713ddcb43d451167", "width": 1080, "height": 540}], "variants": {}, "id": "MRb-72j-QWKEXts0uiG7NHtqLI7h3MqEkjGblqeZ07Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11hrslk", "is_robot_indexable": true, "report_reasons": null, "author": "BelugaGolfinho", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hrslk/im_way_over_my_head/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hrslk/im_way_over_my_head/", "subreddit_subscribers": 91886, "created_utc": 1677911084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am not even sure how to phrase the title of this question. I work as a data analyst, and I'm reading Kimball, but I haven't work at all on the database development side of things, but I'm interested in learning all aspects. so I have an incredibly dumb question for you guys...Go ahead and pass the dunce cap over to me.\n\nWhat takes up most of your time? Some things seem straightforward, but the developers/admins will give timelines of weeks or months. Example:\n\nWe are moving from Hadoop to another platform that has been in use for a few years, so there should be plenty of experienced people for this move. But what takes so long with moving these tables? Is it just that they are really big? Several tables have millions of rows. For certain the total is in the billions of rows across dozens of tables. But they are going to replicate just 7 tables to start with, to test.\n\nI am being very uninformed on this, I know, I think that there is a big difference between reading about database development and what actually happens. But even setting up the tables with no records has at least a week long process. It's just some DDL commands to create it? A database connections from 1 database to another and some more DDL to copy and insert the records, right?\n\nNot talking about data quality rules, or creating triggers based on events, or even setting up the realtime feed to the new DB. Just replicating. What takes weeks or months to do just that?", "author_fullname": "t2_2qobkynl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the time consuming activites of your job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hhajg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677882927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am not even sure how to phrase the title of this question. I work as a data analyst, and I&amp;#39;m reading Kimball, but I haven&amp;#39;t work at all on the database development side of things, but I&amp;#39;m interested in learning all aspects. so I have an incredibly dumb question for you guys...Go ahead and pass the dunce cap over to me.&lt;/p&gt;\n\n&lt;p&gt;What takes up most of your time? Some things seem straightforward, but the developers/admins will give timelines of weeks or months. Example:&lt;/p&gt;\n\n&lt;p&gt;We are moving from Hadoop to another platform that has been in use for a few years, so there should be plenty of experienced people for this move. But what takes so long with moving these tables? Is it just that they are really big? Several tables have millions of rows. For certain the total is in the billions of rows across dozens of tables. But they are going to replicate just 7 tables to start with, to test.&lt;/p&gt;\n\n&lt;p&gt;I am being very uninformed on this, I know, I think that there is a big difference between reading about database development and what actually happens. But even setting up the tables with no records has at least a week long process. It&amp;#39;s just some DDL commands to create it? A database connections from 1 database to another and some more DDL to copy and insert the records, right?&lt;/p&gt;\n\n&lt;p&gt;Not talking about data quality rules, or creating triggers based on events, or even setting up the realtime feed to the new DB. Just replicating. What takes weeks or months to do just that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11hhajg", "is_robot_indexable": true, "report_reasons": null, "author": "Shwoomie", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hhajg/what_is_the_time_consuming_activites_of_your_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hhajg/what_is_the_time_consuming_activites_of_your_job/", "subreddit_subscribers": 91886, "created_utc": 1677882927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://medium.com/@stefentaime\\_10958/building-a-scalable-rss-feed-pipeline-with-apache-airflow-kafka-and-mongodb-flask-api-da379cc2e3fb](https://medium.com/@stefentaime_10958/building-a-scalable-rss-feed-pipeline-with-apache-airflow-kafka-and-mongodb-flask-api-da379cc2e3fb)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/cw0ny0xe3lla1.png?width=500&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c579c53ceaf6a3d0c219768455602feb0f2c4d2a\n\n In today\u2019s data-driven world, processing large volumes of data in real-time has become essential for many organizations. The Extract, Transform, Load (ETL) process is a common way to manage the flow of data between systems. In this article, we\u2019ll walk through how to build a scalable ETL pipeline using Apache Airflow, Kafka, and Python, Mongo and Flask \n\nIn this pipeline, the RSS feeds are scraped using a Python library called feedparser. This library is used to parse the XML data in the RSS feeds and extract the relevant information. The parsed data is then transformed into a standardized JSON format using Python's built-in json library. This format includes fields such as title, summary, link, published\\_date, and language, which make the data easier to analyze and consume.", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Scalable RSS Feed Pipeline with Apache Airflow, Kafka, and MongoDB, Flask Api", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cw0ny0xe3lla1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/cw0ny0xe3lla1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=906a9e84c5c3b27a02ead564d9094ab316759988"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/cw0ny0xe3lla1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6731aebac6fb9bfc8d7d74b85cad5fc0a1d70f34"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/cw0ny0xe3lla1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0eee400b3475883b825d5980b3355998bbac6b8e"}], "s": {"y": 500, "x": 500, "u": "https://preview.redd.it/cw0ny0xe3lla1.png?width=500&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c579c53ceaf6a3d0c219768455602feb0f2c4d2a"}, "id": "cw0ny0xe3lla1"}}, "name": "t3_11hdy54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sZ25bG-V-FiLzLwMOO2O-bnR-pdaidqQq5_4ymlblrI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1677875379.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/building-a-scalable-rss-feed-pipeline-with-apache-airflow-kafka-and-mongodb-flask-api-da379cc2e3fb\"&gt;https://medium.com/@stefentaime_10958/building-a-scalable-rss-feed-pipeline-with-apache-airflow-kafka-and-mongodb-flask-api-da379cc2e3fb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cw0ny0xe3lla1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c579c53ceaf6a3d0c219768455602feb0f2c4d2a\"&gt;https://preview.redd.it/cw0ny0xe3lla1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c579c53ceaf6a3d0c219768455602feb0f2c4d2a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In today\u2019s data-driven world, processing large volumes of data in real-time has become essential for many organizations. The Extract, Transform, Load (ETL) process is a common way to manage the flow of data between systems. In this article, we\u2019ll walk through how to build a scalable ETL pipeline using Apache Airflow, Kafka, and Python, Mongo and Flask &lt;/p&gt;\n\n&lt;p&gt;In this pipeline, the RSS feeds are scraped using a Python library called feedparser. This library is used to parse the XML data in the RSS feeds and extract the relevant information. The parsed data is then transformed into a standardized JSON format using Python&amp;#39;s built-in json library. This format includes fields such as title, summary, link, published_date, and language, which make the data easier to analyze and consume.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4jHHWX47A37cnRvst-RD_iLAGfCS4hqxq4oTwK9vnRA.jpg?auto=webp&amp;v=enabled&amp;s=ee9818bce162ebfcb69268e842d337c035e5b567", "width": 1200, "height": 564}, "resolutions": [{"url": "https://external-preview.redd.it/4jHHWX47A37cnRvst-RD_iLAGfCS4hqxq4oTwK9vnRA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f784cea76c85073a0a9074b20d4bd68ab7d68ce", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/4jHHWX47A37cnRvst-RD_iLAGfCS4hqxq4oTwK9vnRA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce84ff7be54e3fcfb68e870f63fa6e2d4603aba5", "width": 216, "height": 101}, {"url": "https://external-preview.redd.it/4jHHWX47A37cnRvst-RD_iLAGfCS4hqxq4oTwK9vnRA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=52a34c1619925b79c20f923f917be2348dafe8dc", "width": 320, "height": 150}, {"url": "https://external-preview.redd.it/4jHHWX47A37cnRvst-RD_iLAGfCS4hqxq4oTwK9vnRA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43ff471735006a845946aa4e4cecba39fdab0d48", "width": 640, "height": 300}, {"url": "https://external-preview.redd.it/4jHHWX47A37cnRvst-RD_iLAGfCS4hqxq4oTwK9vnRA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83bf78180fbb5a751676350b1bb13764c353f73e", "width": 960, "height": 451}, {"url": "https://external-preview.redd.it/4jHHWX47A37cnRvst-RD_iLAGfCS4hqxq4oTwK9vnRA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7a04270a792befa0fafc11e1cdcb8e490969047", "width": 1080, "height": 507}], "variants": {}, "id": "tH9jmMo8wbU5z-SQC6-NTsGwT9qhuVhdZXUeDxBZR1c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "11hdy54", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hdy54/building_a_scalable_rss_feed_pipeline_with_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hdy54/building_a_scalable_rss_feed_pipeline_with_apache/", "subreddit_subscribers": 91886, "created_utc": 1677875379.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am 26F and finished my psychology bachelor last September. I have also studied two years of artificial intelligence before that but did not finish it. Now my question is, would I be able to do well in a data engineering job? I got approached by a recruiter who offered me an interview for a full time job at a good company as a data engineer but I\u2019m worried about my lack of programming skills (as it\u2019s a long time ago I last programmed anything).", "author_fullname": "t2_hiwfhmhv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering job with a psychology degree", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hiauo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677885296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am 26F and finished my psychology bachelor last September. I have also studied two years of artificial intelligence before that but did not finish it. Now my question is, would I be able to do well in a data engineering job? I got approached by a recruiter who offered me an interview for a full time job at a good company as a data engineer but I\u2019m worried about my lack of programming skills (as it\u2019s a long time ago I last programmed anything).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11hiauo", "is_robot_indexable": true, "report_reasons": null, "author": "Sapphicorns", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hiauo/data_engineering_job_with_a_psychology_degree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hiauo/data_engineering_job_with_a_psychology_degree/", "subreddit_subscribers": 91886, "created_utc": 1677885296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say there are two duplicated rows like it:\n\ncolA   colB\n\n10      10\n\n10      10\n\nShould the fact be modeled to store duplicated rows like it? or there is no rule?\n\ncolA   colB    Total\n\n10       10       2\n\nCurrently I've created a DW for my project, but there are some duplicated rows. Must I remove duplicated rows?", "author_fullname": "t2_h7b2azu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can fact table have duplicated rows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hpim1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677904364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say there are two duplicated rows like it:&lt;/p&gt;\n\n&lt;p&gt;colA   colB&lt;/p&gt;\n\n&lt;p&gt;10      10&lt;/p&gt;\n\n&lt;p&gt;10      10&lt;/p&gt;\n\n&lt;p&gt;Should the fact be modeled to store duplicated rows like it? or there is no rule?&lt;/p&gt;\n\n&lt;p&gt;colA   colB    Total&lt;/p&gt;\n\n&lt;p&gt;10       10       2&lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;ve created a DW for my project, but there are some duplicated rows. Must I remove duplicated rows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11hpim1", "is_robot_indexable": true, "report_reasons": null, "author": "SuddenlyCaralho", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hpim1/can_fact_table_have_duplicated_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hpim1/can_fact_table_have_duplicated_rows/", "subreddit_subscribers": 91886, "created_utc": 1677904364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I'm trying to set up a development environment with Kafka, Pyspark and zookeeper for a pet project but I've been facing some issues with stream reading the data from my Kafka broker, can someone please share a docker-compose file or tutorial on how to properly setup an environment with docker . Thx in advance", "author_fullname": "t2_386krewr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Spark + Kafka in docker", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hktx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677891500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m trying to set up a development environment with Kafka, Pyspark and zookeeper for a pet project but I&amp;#39;ve been facing some issues with stream reading the data from my Kafka broker, can someone please share a docker-compose file or tutorial on how to properly setup an environment with docker . Thx in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11hktx8", "is_robot_indexable": true, "report_reasons": null, "author": "Lord_Gonz0", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11hktx8/apache_spark_kafka_in_docker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hktx8/apache_spark_kafka_in_docker/", "subreddit_subscribers": 91886, "created_utc": 1677891500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't think I've attended a big data conference in the last 2 years that didn't have like 50% of all the talks be on datamesh. But I haven't come across anyone who's actually implemented it.\n\nSo, any of you guys in organisations following the data mesh model? How's it improving / ruining your lives?", "author_fullname": "t2_126zgy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone actually using data mesh?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11i5eq7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677948393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think I&amp;#39;ve attended a big data conference in the last 2 years that didn&amp;#39;t have like 50% of all the talks be on datamesh. But I haven&amp;#39;t come across anyone who&amp;#39;s actually implemented it.&lt;/p&gt;\n\n&lt;p&gt;So, any of you guys in organisations following the data mesh model? How&amp;#39;s it improving / ruining your lives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11i5eq7", "is_robot_indexable": true, "report_reasons": null, "author": "houseofleft", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11i5eq7/anyone_actually_using_data_mesh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11i5eq7/anyone_actually_using_data_mesh/", "subreddit_subscribers": 91886, "created_utc": 1677948393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I asked this question over in r/aws and got crickets. . . so asking here. Does anyone on r/dataengineering have experience using Textract and/or Rekognition and if so are they decent?\n\nI have a big pile of images in S3 and I need to extract text strings (labels, brands, sizes e.g. a lot of the images are consumer goods like clothes) in some cases and in other cases I need to generate descriptions and keywords  of images e.g. with an image of a black horse, I would expect output words like black, horse, mammal, etc.\n\nLike many AWS services, I can't tell immediately if these are worth pursuing.", "author_fullname": "t2_clyss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Textract and Rekognition: yay or nay", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hxdn2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677930389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I asked this question over in &lt;a href=\"/r/aws\"&gt;r/aws&lt;/a&gt; and got crickets. . . so asking here. Does anyone on &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; have experience using Textract and/or Rekognition and if so are they decent?&lt;/p&gt;\n\n&lt;p&gt;I have a big pile of images in S3 and I need to extract text strings (labels, brands, sizes e.g. a lot of the images are consumer goods like clothes) in some cases and in other cases I need to generate descriptions and keywords  of images e.g. with an image of a black horse, I would expect output words like black, horse, mammal, etc.&lt;/p&gt;\n\n&lt;p&gt;Like many AWS services, I can&amp;#39;t tell immediately if these are worth pursuing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11hxdn2", "is_robot_indexable": true, "report_reasons": null, "author": "tech_tuna", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hxdn2/textract_and_rekognition_yay_or_nay/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hxdn2/textract_and_rekognition_yay_or_nay/", "subreddit_subscribers": 91886, "created_utc": 1677930389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://github.com/danthelion/talksheet](https://github.com/danthelion/talksheet)  \n\n\nA small project showcasing how to create a \"self-serve\" analytical application, powered by the wonderful Langchain and DuckDB.\n\nThere are a bunch of features (like supporting other file formats such as parquet and json) planned for the future, just wanted to ship something quickly.", "author_fullname": "t2_cqao8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Talksheet, a CLI tool that answers your questions about your data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11i509i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677947935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/danthelion/talksheet\"&gt;https://github.com/danthelion/talksheet&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;A small project showcasing how to create a &amp;quot;self-serve&amp;quot; analytical application, powered by the wonderful Langchain and DuckDB.&lt;/p&gt;\n\n&lt;p&gt;There are a bunch of features (like supporting other file formats such as parquet and json) planned for the future, just wanted to ship something quickly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IYFcnhww-f25aShf7O1nm6ewmGOj8DJHKHapeRpluOo.jpg?auto=webp&amp;v=enabled&amp;s=f56aa33040674a3b8c52e7760f4dd21e9a2cf3d3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/IYFcnhww-f25aShf7O1nm6ewmGOj8DJHKHapeRpluOo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2d39fb244c37f2e235b59f752b35445fe73d157", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/IYFcnhww-f25aShf7O1nm6ewmGOj8DJHKHapeRpluOo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2205ad1f729859a0679b7851f8b3b60895ec0586", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/IYFcnhww-f25aShf7O1nm6ewmGOj8DJHKHapeRpluOo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=257c9854af2e1308d3d6be1b74102b0f5411f3d0", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/IYFcnhww-f25aShf7O1nm6ewmGOj8DJHKHapeRpluOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=347f52ac450029a75837bbff31dc4310d010b471", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/IYFcnhww-f25aShf7O1nm6ewmGOj8DJHKHapeRpluOo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5e6b64e225ea87408ea2090481b6524ba3a0d6b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/IYFcnhww-f25aShf7O1nm6ewmGOj8DJHKHapeRpluOo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3099a8ecfe795cdc6cf9bc008b30e81180bea6d", "width": 1080, "height": 540}], "variants": {}, "id": "m-I5DrIKVZ53TTneX2jorzf7Qrmy_Gkn2CNDhxWuToQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11i509i", "is_robot_indexable": true, "report_reasons": null, "author": "dan_the_lion", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11i509i/talksheet_a_cli_tool_that_answers_your_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11i509i/talksheet_a_cli_tool_that_answers_your_questions/", "subreddit_subscribers": 91886, "created_utc": 1677947935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\n\nI was wondering if there are any EL tools that make it easy to work with multiple environments.\n\nReason is, I have a postgresql database from which I want to extract data. However this database has some 165 tables, and some of these tables have over 100 columns.\n\nI definitely don't need all the tables and in most tables I don't need all the columns, often just a small subset of those.\n\nAt the same time, I want to be able to have at least 2 environments (staging and prod), and would like to manage my EL tool of choice with proper DevOps practices.\n\nDo you know of any solution that makes this easy to work with?\n\n\\_\\_\\_\n\n*PS: If you're thinking Fivetran with REST APIs, consider that the data volume is quite small and would easily qualify for Fivetran free tier, which however doesn't allow to use their REST APIs.*", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EL softwares that support CI/CD or multiple environments/deployments natively?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hy1c7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677932803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there are any EL tools that make it easy to work with multiple environments.&lt;/p&gt;\n\n&lt;p&gt;Reason is, I have a postgresql database from which I want to extract data. However this database has some 165 tables, and some of these tables have over 100 columns.&lt;/p&gt;\n\n&lt;p&gt;I definitely don&amp;#39;t need all the tables and in most tables I don&amp;#39;t need all the columns, often just a small subset of those.&lt;/p&gt;\n\n&lt;p&gt;At the same time, I want to be able to have at least 2 environments (staging and prod), and would like to manage my EL tool of choice with proper DevOps practices.&lt;/p&gt;\n\n&lt;p&gt;Do you know of any solution that makes this easy to work with?&lt;/p&gt;\n\n&lt;p&gt;___&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;PS: If you&amp;#39;re thinking Fivetran with REST APIs, consider that the data volume is quite small and would easily qualify for Fivetran free tier, which however doesn&amp;#39;t allow to use their REST APIs.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11hy1c7", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hy1c7/el_softwares_that_support_cicd_or_multiple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hy1c7/el_softwares_that_support_cicd_or_multiple/", "subreddit_subscribers": 91886, "created_utc": 1677932803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fbliz6iq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sending OpenTelemetry data to Google Cloud Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 119, "top_awarded_type": null, "hide_score": false, "name": "t3_11hvnme", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zhuvO2Be4-l8QVtLMm8joTHZxVlWDfmMaz6X_JpIq2w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677924098.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "keyval.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://keyval.dev/sending-otel-to-gcs/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GS01qHh_JdLNWzEB0M-NgzHYA3AWjgUkhDlQ9gvn4wM.jpg?auto=webp&amp;v=enabled&amp;s=d213a85174c20068a7faed9b818c0a54b23426b7", "width": 738, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/GS01qHh_JdLNWzEB0M-NgzHYA3AWjgUkhDlQ9gvn4wM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=304fd50828294e724d118605eeae316168bb424c", "width": 108, "height": 91}, {"url": "https://external-preview.redd.it/GS01qHh_JdLNWzEB0M-NgzHYA3AWjgUkhDlQ9gvn4wM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d5a5bde5877c53271251ee4782b62ceac09b38c", "width": 216, "height": 183}, {"url": "https://external-preview.redd.it/GS01qHh_JdLNWzEB0M-NgzHYA3AWjgUkhDlQ9gvn4wM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c763c707065652d4a31ba82ad47dba2fa57b1c46", "width": 320, "height": 272}, {"url": "https://external-preview.redd.it/GS01qHh_JdLNWzEB0M-NgzHYA3AWjgUkhDlQ9gvn4wM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ee2be40f0e8a6ce3b54050946875368d47e6d0f", "width": 640, "height": 544}], "variants": {}, "id": "bR1vLdd9Hcr6jlHtfCqi23c6-Qem3rzYz0QT0ObYEWI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11hvnme", "is_robot_indexable": true, "report_reasons": null, "author": "Barakikia", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hvnme/sending_opentelemetry_data_to_google_cloud_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://keyval.dev/sending-otel-to-gcs/", "subreddit_subscribers": 91886, "created_utc": 1677924098.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have almost no experience with R and I need to refactor a pipeline from R to Python. For now, I'm using pandas but eventually, I may need to use PySpark since the data is reasonably \"big\".\n\nAre there common \"gotchas\" to watch out for or \"best practices\" used in R that might not make sense to someone with no familiarity with R?\n\nAny suggestions or warnings are appreciated, thanks in advance!", "author_fullname": "t2_32t0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Refactoring a pipeline from R to Python -- advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hjvp9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677889078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have almost no experience with R and I need to refactor a pipeline from R to Python. For now, I&amp;#39;m using pandas but eventually, I may need to use PySpark since the data is reasonably &amp;quot;big&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Are there common &amp;quot;gotchas&amp;quot; to watch out for or &amp;quot;best practices&amp;quot; used in R that might not make sense to someone with no familiarity with R?&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or warnings are appreciated, thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11hjvp9", "is_robot_indexable": true, "report_reasons": null, "author": "monocongo", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hjvp9/refactoring_a_pipeline_from_r_to_python_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hjvp9/refactoring_a_pipeline_from_r_to_python_advice/", "subreddit_subscribers": 91886, "created_utc": 1677889078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'd like to know how tools like Spark achieve the same benefits as dbt\u2014specifically, documentation, testing, and lineage. In dbt, we create models with references to other models, which helps generate a DAG view. Documentation can be natively created at the table and column level. We can also specify how models are materialized in the warehouse. All of this information can easily be viewed in one place (dbt docs). \n\nHow do Spark developers organize their models? Is there different tooling that handles documentation, testing, lineage? If so, what are they?", "author_fullname": "t2_7iitruic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lineage, Testing, and Documentation in Transformation Tools Like Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11i6nab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677949806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to know how tools like Spark achieve the same benefits as dbt\u2014specifically, documentation, testing, and lineage. In dbt, we create models with references to other models, which helps generate a DAG view. Documentation can be natively created at the table and column level. We can also specify how models are materialized in the warehouse. All of this information can easily be viewed in one place (dbt docs). &lt;/p&gt;\n\n&lt;p&gt;How do Spark developers organize their models? Is there different tooling that handles documentation, testing, lineage? If so, what are they?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11i6nab", "is_robot_indexable": true, "report_reasons": null, "author": "jduran9987", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11i6nab/lineage_testing_and_documentation_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11i6nab/lineage_testing_and_documentation_in/", "subreddit_subscribers": 91886, "created_utc": 1677949806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Kubeflow 1.7 is around the corner. If you would like to be the first one who tries a beta, follow us closely. We got big news.\n\nJoin us on 8th of March live, learn more about the latest release and ask your questions right away.\n\nLink:\nhttps://www.linkedin.com/video/event/urn:li:ugcPost:7035904245740539904/", "author_fullname": "t2_3z4miuvs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kubeflow 1.7 Beta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hrwp9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677911429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kubeflow 1.7 is around the corner. If you would like to be the first one who tries a beta, follow us closely. We got big news.&lt;/p&gt;\n\n&lt;p&gt;Join us on 8th of March live, learn more about the latest release and ask your questions right away.&lt;/p&gt;\n\n&lt;p&gt;Link:\n&lt;a href=\"https://www.linkedin.com/video/event/urn:li:ugcPost:7035904245740539904/\"&gt;https://www.linkedin.com/video/event/urn:li:ugcPost:7035904245740539904/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5ZI7oL3JTPPt59G0vTfOaQMHvka17QCAdFnF87leUeA.jpg?auto=webp&amp;v=enabled&amp;s=751b05e77b1c50dfc8477f4c599cb33affc7e2fc", "width": 64, "height": 64}, "resolutions": [], "variants": {}, "id": "QqSY3F9i2BgB-OdT_JpQr1vBqr2oq4spYNzkghHXwCM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11hrwp9", "is_robot_indexable": true, "report_reasons": null, "author": "andreea-mun", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hrwp9/kubeflow_17_beta/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hrwp9/kubeflow_17_beta/", "subreddit_subscribers": 91886, "created_utc": 1677911429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hope someone can guide me in the right direction.\n\nWe have a hub for team members. It draws primarily from our HR system.\n\nWe have a second for appointments.\n\nWe might have multiple team members linked to an appointment, but fulfilling different roles.\n\nDoes our link become three-way (as in not just a appointment-customer link, but an appointment-customer-role link)? It seems so, but I wanted to double check.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DV2.0 links question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hosx6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677902305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hope someone can guide me in the right direction.&lt;/p&gt;\n\n&lt;p&gt;We have a hub for team members. It draws primarily from our HR system.&lt;/p&gt;\n\n&lt;p&gt;We have a second for appointments.&lt;/p&gt;\n\n&lt;p&gt;We might have multiple team members linked to an appointment, but fulfilling different roles.&lt;/p&gt;\n\n&lt;p&gt;Does our link become three-way (as in not just a appointment-customer link, but an appointment-customer-role link)? It seems so, but I wanted to double check.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11hosx6", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hosx6/dv20_links_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hosx6/dv20_links_question/", "subreddit_subscribers": 91886, "created_utc": 1677902305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are good product companies in NCR for Data Engineers?  \nPlease list out all , which has total remote culture too. Planning to start by April or May.", "author_fullname": "t2_sr3rc27q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product Companies to aim for Data Engineers in NCR?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11i8qwn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677953498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are good product companies in NCR for Data Engineers?&lt;br/&gt;\nPlease list out all , which has total remote culture too. Planning to start by April or May.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11i8qwn", "is_robot_indexable": true, "report_reasons": null, "author": "honey12123", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11i8qwn/product_companies_to_aim_for_data_engineers_in_ncr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11i8qwn/product_companies_to_aim_for_data_engineers_in_ncr/", "subreddit_subscribers": 91886, "created_utc": 1677953498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am not really sure I 100% understand the role of aurogenerated integer surrogate keys.\n\nLet's say I have table User and table Address. Each user has only 1 Address, but 1 Address could have more Users.\n\nSo I would create a table User like this `user_key | first_name | last_name | address_key` and Address like `address_key | street | city | zip | apartment_no`.\n\nScenario 1:\n\nNow, when inserting the records, from the source system I would obviously get a record for user like: `John | Doe | Street1 | City1 | Zip1 | No1`.  So I need to extract the address fields and put them into the Address table (if it does not exist there already) and the DB will generate a surrogate key for the record. Now. What `address_key` should I put into the User table? How do I know what value the `address_key` will have? It makes more sense to me to have a surrogate key maybe as a hash of all fields so I can identify the value of the surrogate key easily. Or how should this be solved?\n\nAnd 2nd scenario:\n\nI get `John | Doe | address_source_id` from the source. The `address_source_id` is the id of the address stored in the source system (= natural key). Should I store `address_source_id` in the User table and therefore use the natural key as a foreign key? Or I should always create foreign keys as a reference to the surrogate key in the other table? (but then I have the problem as described in scenario 1).", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Joins on surrogate keys", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11i3t98", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677946586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not really sure I 100% understand the role of aurogenerated integer surrogate keys.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I have table User and table Address. Each user has only 1 Address, but 1 Address could have more Users.&lt;/p&gt;\n\n&lt;p&gt;So I would create a table User like this &lt;code&gt;user_key | first_name | last_name | address_key&lt;/code&gt; and Address like &lt;code&gt;address_key | street | city | zip | apartment_no&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;Scenario 1:&lt;/p&gt;\n\n&lt;p&gt;Now, when inserting the records, from the source system I would obviously get a record for user like: &lt;code&gt;John | Doe | Street1 | City1 | Zip1 | No1&lt;/code&gt;.  So I need to extract the address fields and put them into the Address table (if it does not exist there already) and the DB will generate a surrogate key for the record. Now. What &lt;code&gt;address_key&lt;/code&gt; should I put into the User table? How do I know what value the &lt;code&gt;address_key&lt;/code&gt; will have? It makes more sense to me to have a surrogate key maybe as a hash of all fields so I can identify the value of the surrogate key easily. Or how should this be solved?&lt;/p&gt;\n\n&lt;p&gt;And 2nd scenario:&lt;/p&gt;\n\n&lt;p&gt;I get &lt;code&gt;John | Doe | address_source_id&lt;/code&gt; from the source. The &lt;code&gt;address_source_id&lt;/code&gt; is the id of the address stored in the source system (= natural key). Should I store &lt;code&gt;address_source_id&lt;/code&gt; in the User table and therefore use the natural key as a foreign key? Or I should always create foreign keys as a reference to the surrogate key in the other table? (but then I have the problem as described in scenario 1).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11i3t98", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11i3t98/joins_on_surrogate_keys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11i3t98/joins_on_surrogate_keys/", "subreddit_subscribers": 91886, "created_utc": 1677946586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a vendor who have a REST API with a bunch of endpoints for different data on the same entity, maybe ten or so. \nSince we do the same chain of calls in several places in our code base, I would like to build an API Proxy to do all the requests and return a large object with data from all endpoints. \n\nIs there some SAAS tool or similar to do this? Something with little code or no code would be great, since I don't really want to maintain more stuff :)", "author_fullname": "t2_82s0a64", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API Proxy builder", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11i289n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677944554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a vendor who have a REST API with a bunch of endpoints for different data on the same entity, maybe ten or so. \nSince we do the same chain of calls in several places in our code base, I would like to build an API Proxy to do all the requests and return a large object with data from all endpoints. &lt;/p&gt;\n\n&lt;p&gt;Is there some SAAS tool or similar to do this? Something with little code or no code would be great, since I don&amp;#39;t really want to maintain more stuff :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11i289n", "is_robot_indexable": true, "report_reasons": null, "author": "Ootoootooo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11i289n/api_proxy_builder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11i289n/api_proxy_builder/", "subreddit_subscribers": 91886, "created_utc": 1677944554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there guys I appreciate you taking the time to read this. I hope you have had a good day.\n\nMy current challenge: I am 6 months in to my job as a graduate data consultant. During that time, I have gained a love for data engineering and I have been applying to data engineering jobs. I have done many fun projects including building ETL, ELT pipelines etc. The problem is that all the junior roles are so hard to get. How did you guys get your first job as a data engineer. I'd love to try replicate your steps.\n\nFeel free to ask any more questions you feel are relevant. Thank you :)", "author_fullname": "t2_k952ln94", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The great step up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11i20n8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677944036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there guys I appreciate you taking the time to read this. I hope you have had a good day.&lt;/p&gt;\n\n&lt;p&gt;My current challenge: I am 6 months in to my job as a graduate data consultant. During that time, I have gained a love for data engineering and I have been applying to data engineering jobs. I have done many fun projects including building ETL, ELT pipelines etc. The problem is that all the junior roles are so hard to get. How did you guys get your first job as a data engineer. I&amp;#39;d love to try replicate your steps.&lt;/p&gt;\n\n&lt;p&gt;Feel free to ask any more questions you feel are relevant. Thank you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11i20n8", "is_robot_indexable": true, "report_reasons": null, "author": "FelipePendejo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11i20n8/the_great_step_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11i20n8/the_great_step_up/", "subreddit_subscribers": 91886, "created_utc": 1677944036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a 75M record table of contract lines from Oracle EBS (okc_k_lines_b for you EBS nerds) in a snowflake table. Each top line contains a reference to the id of the contract it belongs to, and each sub line references its parent line but no the top level contract. There are also sub lines of sub lines of sub lines, etc. \n\nI want to materialize the contract id to each sub line at all levels, but this operation with its recursive cte is taking a lot of time. One of my solutions is to just try a gargantuan warehouse on the problem, but is there a strategy someone knows that could be more efficient?\n\nTo clarify, the incremental operation is likely going to be fast regardless because there\u2019s a lot less changing day over day, but this initial one is going to be a bear\u2026", "author_fullname": "t2_1eht5os", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to materialize parent metadata to all children records at all levels", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11i1qpm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677943357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 75M record table of contract lines from Oracle EBS (okc_k_lines_b for you EBS nerds) in a snowflake table. Each top line contains a reference to the id of the contract it belongs to, and each sub line references its parent line but no the top level contract. There are also sub lines of sub lines of sub lines, etc. &lt;/p&gt;\n\n&lt;p&gt;I want to materialize the contract id to each sub line at all levels, but this operation with its recursive cte is taking a lot of time. One of my solutions is to just try a gargantuan warehouse on the problem, but is there a strategy someone knows that could be more efficient?&lt;/p&gt;\n\n&lt;p&gt;To clarify, the incremental operation is likely going to be fast regardless because there\u2019s a lot less changing day over day, but this initial one is going to be a bear\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11i1qpm", "is_robot_indexable": true, "report_reasons": null, "author": "mbsquad24", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11i1qpm/best_way_to_materialize_parent_metadata_to_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11i1qpm/best_way_to_materialize_parent_metadata_to_all/", "subreddit_subscribers": 91886, "created_utc": 1677943357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Column populated using hash function in snowflake", "author_fullname": "t2_4ck30tok", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best data type for hash keys in DV model primary key column?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11hzesd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677937322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Column populated using hash function in snowflake&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11hzesd", "is_robot_indexable": true, "report_reasons": null, "author": "Nomad4455", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hzesd/what_is_the_best_data_type_for_hash_keys_in_dv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11hzesd/what_is_the_best_data_type_for_hash_keys_in_dv/", "subreddit_subscribers": 91886, "created_utc": 1677937322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m2u4lsxd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Applications of Databricks Certified Data Engineer Associate Certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_11hwvz2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0cxNVrlAn15Yh8ImdY4SibGQkUIESutPIhiGSr8xZos.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677928464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "amaaira.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://amaaira.medium.com/applications-of-databricks-certified-data-engineer-associate-certification-7012b280777", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_9xtibO49R0ZYfabP0HEe4oQdgXjQK6cHfz6SdGJJJU.jpg?auto=webp&amp;v=enabled&amp;s=c5bcc0e04a593333574c1717c4493723d760a1b9", "width": 700, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/_9xtibO49R0ZYfabP0HEe4oQdgXjQK6cHfz6SdGJJJU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9bea3f44f11434b8d582bb711f355663afd7a33f", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/_9xtibO49R0ZYfabP0HEe4oQdgXjQK6cHfz6SdGJJJU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81cf9d7f4f32227445a586c79a21f596da31774b", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/_9xtibO49R0ZYfabP0HEe4oQdgXjQK6cHfz6SdGJJJU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ee064baf6a417b75961818d228d360486f941eb", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/_9xtibO49R0ZYfabP0HEe4oQdgXjQK6cHfz6SdGJJJU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cedc119d2b7b1ba59183496e6cbe1de927e8d5ba", "width": 640, "height": 365}], "variants": {}, "id": "RJnwMUotgoAorp6h3pkOVgfZWhCUlskVXYq0HVII79M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11hwvz2", "is_robot_indexable": true, "report_reasons": null, "author": "certfun", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11hwvz2/applications_of_databricks_certified_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://amaaira.medium.com/applications-of-databricks-certified-data-engineer-associate-certification-7012b280777", "subreddit_subscribers": 91886, "created_utc": 1677928464.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}