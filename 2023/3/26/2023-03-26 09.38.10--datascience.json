{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve got a job with an awesome company, working on projects I feel good about, but holy smokes I hate dealing with the shitshow that is my teams data entry practices from pre-2021. I spend a solid half of my time figuring out where the hell the inconsistencies are, then the other half figuring out what values to sub in for the inconsistencies to make the data workable. At that point how useful is the data, how trustworthy? Not useful or trustworthy! The teams leads wonder why I\u2019m giving them models that explain nothing, and then act surprised when I tell them it\u2019s because the old data was essentially made up. The circle is endless. This isn\u2019t science, this is like working at a daycare cleaning up after toddlers, and when the parents pick them up they complain about the child\u2019s poor behavior.", "author_fullname": "t2_59l9bdfsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gotta vent about wasted time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121r76h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 178, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 178, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679762068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve got a job with an awesome company, working on projects I feel good about, but holy smokes I hate dealing with the shitshow that is my teams data entry practices from pre-2021. I spend a solid half of my time figuring out where the hell the inconsistencies are, then the other half figuring out what values to sub in for the inconsistencies to make the data workable. At that point how useful is the data, how trustworthy? Not useful or trustworthy! The teams leads wonder why I\u2019m giving them models that explain nothing, and then act surprised when I tell them it\u2019s because the old data was essentially made up. The circle is endless. This isn\u2019t science, this is like working at a daycare cleaning up after toddlers, and when the parents pick them up they complain about the child\u2019s poor behavior.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121r76h", "is_robot_indexable": true, "report_reasons": null, "author": "ExtraSpecialCake", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121r76h/gotta_vent_about_wasted_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121r76h/gotta_vent_about_wasted_time/", "subreddit_subscribers": 862719, "created_utc": 1679762068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "When building a multiple linear regression through Python, to select my independent variables, i was thinking of building a heat map to determine correlation using pearson. Does anyone have a better alternative?", "author_fullname": "t2_a24n8dl6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Asking for a friend \ud83d\ude02", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121pizx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679758423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When building a multiple linear regression through Python, to select my independent variables, i was thinking of building a heat map to determine correlation using pearson. Does anyone have a better alternative?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121pizx", "is_robot_indexable": true, "report_reasons": null, "author": "MiamiTaco627", "discussion_type": null, "num_comments": 86, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121pizx/asking_for_a_friend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121pizx/asking_for_a_friend/", "subreddit_subscribers": 862719, "created_utc": 1679758423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_j6enx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some of the best twitter accounts you follow for data science news and articles.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121gvv0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679737541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121gvv0", "is_robot_indexable": true, "report_reasons": null, "author": "amcelvanna783", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121gvv0/what_are_some_of_the_best_twitter_accounts_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121gvv0/what_are_some_of_the_best_twitter_accounts_you/", "subreddit_subscribers": 862719, "created_utc": 1679737541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello\n\nI have refrained from outlining my specific example, and am asking this in a more general way. However, for some background, I am  implementing a basic Logistic Regression algorithm in R using glm. I am an Intensive Care physician, so am a keen amateur rather than operating on a professional level.\n\nI ran in to the warning:\n\n`Warning: glm.fit: algorithm did not converge`\n`Warning: fitted probabilities numerically 0 or 1 occurred`\n\nI know this most commonly means that one of my predictors is acting perfectly to allow the algorithm to classify the outcome. However, after examining the variables I couldn't figure which. I created multiple separate logistic regression model-fits (one with each individual predictor variable) to see if I could locate it that way. I could not.\n\nSo, my question is: Can you get this warning with many variables reaching some sort of 'critical mass' that means the algorithm doesn't work? I am unsure why I am getting the warning as no individual predictor seems to be to blame.", "author_fullname": "t2_8v5hm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A general question about an problem with Logistic Regression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121wirk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679772756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I have refrained from outlining my specific example, and am asking this in a more general way. However, for some background, I am  implementing a basic Logistic Regression algorithm in R using glm. I am an Intensive Care physician, so am a keen amateur rather than operating on a professional level.&lt;/p&gt;\n\n&lt;p&gt;I ran in to the warning:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Warning: glm.fit: algorithm did not converge&lt;/code&gt;\n&lt;code&gt;Warning: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I know this most commonly means that one of my predictors is acting perfectly to allow the algorithm to classify the outcome. However, after examining the variables I couldn&amp;#39;t figure which. I created multiple separate logistic regression model-fits (one with each individual predictor variable) to see if I could locate it that way. I could not.&lt;/p&gt;\n\n&lt;p&gt;So, my question is: Can you get this warning with many variables reaching some sort of &amp;#39;critical mass&amp;#39; that means the algorithm doesn&amp;#39;t work? I am unsure why I am getting the warning as no individual predictor seems to be to blame.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121wirk", "is_robot_indexable": true, "report_reasons": null, "author": "e05bf027", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121wirk/a_general_question_about_an_problem_with_logistic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121wirk/a_general_question_about_an_problem_with_logistic/", "subreddit_subscribers": 862719, "created_utc": 1679772756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I used to not mind doing take home assignments because they were a way to build out my GitHub profile. \n\nHowever, I have two kids now and would dread if every interview had an 8 hour take home assignment. \n\nHow long of an assignment is acceptable? Are longer assignments a form of discrimination against people with kids and therefore less spare time? Have you ever refused to take an assignment?", "author_fullname": "t2_f7akb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long of a take home assignment is acceptable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1223z2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679788475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to not mind doing take home assignments because they were a way to build out my GitHub profile. &lt;/p&gt;\n\n&lt;p&gt;However, I have two kids now and would dread if every interview had an 8 hour take home assignment. &lt;/p&gt;\n\n&lt;p&gt;How long of an assignment is acceptable? Are longer assignments a form of discrimination against people with kids and therefore less spare time? Have you ever refused to take an assignment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1223z2m", "is_robot_indexable": true, "report_reasons": null, "author": "Dezireless", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1223z2m/how_long_of_a_take_home_assignment_is_acceptable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1223z2m/how_long_of_a_take_home_assignment_is_acceptable/", "subreddit_subscribers": 862719, "created_utc": 1679788475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nI have an interesting problem I thought you guys might be able to help with. I found a couple of similar questions on SO, but the methods mentioned there don't seem to work very well with my data. \n\nI have survey data of compensation percentiles for a certain job, but it only gives me the 10th, 25th, median, 75th, and 90th percentiles. No min/max, SD, mean, etc... \n\nI want to be able to compare any salary with the data and determine its percentile rank, despite the fact I do not know the upper/lower bounds. \n\nExample:\n10th - 184,000\n25th - 222,000\nMedian - 264,000\n75th - 323,000\n90th - 398,000\n\nInputs: 450,000, 100,000, 300,000, etc... \nPercentile ranks: ??? \n\nI heard from a colleague that he has seen this done before, but doesn't know how to do it himself. Ideally, I will be able to solve this in Excel or with VBA. \n\nAny ideas would be appreciated!", "author_fullname": "t2_aj5mt11i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to Calculate the Percentile Rank of a Salary Based on Incomplete Survey Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121w2lv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679771823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I have an interesting problem I thought you guys might be able to help with. I found a couple of similar questions on SO, but the methods mentioned there don&amp;#39;t seem to work very well with my data. &lt;/p&gt;\n\n&lt;p&gt;I have survey data of compensation percentiles for a certain job, but it only gives me the 10th, 25th, median, 75th, and 90th percentiles. No min/max, SD, mean, etc... &lt;/p&gt;\n\n&lt;p&gt;I want to be able to compare any salary with the data and determine its percentile rank, despite the fact I do not know the upper/lower bounds. &lt;/p&gt;\n\n&lt;p&gt;Example:\n10th - 184,000\n25th - 222,000\nMedian - 264,000\n75th - 323,000\n90th - 398,000&lt;/p&gt;\n\n&lt;p&gt;Inputs: 450,000, 100,000, 300,000, etc... \nPercentile ranks: ??? &lt;/p&gt;\n\n&lt;p&gt;I heard from a colleague that he has seen this done before, but doesn&amp;#39;t know how to do it himself. Ideally, I will be able to solve this in Excel or with VBA. &lt;/p&gt;\n\n&lt;p&gt;Any ideas would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121w2lv", "is_robot_indexable": true, "report_reasons": null, "author": "EMoneymaker99", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121w2lv/trying_to_calculate_the_percentile_rank_of_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121w2lv/trying_to_calculate_the_percentile_rank_of_a/", "subreddit_subscribers": 862719, "created_utc": 1679771823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI am still a student so I'd like some tips and some ideas or directions I could take. **I am not asking you to do this for me, I just want some ideas. How would you approach this problem?**\n\n# More about the dataset:\n\nThe Y labels are fairly straight forward. Int values between 1 and 4, three samples for each. The X values vary between 0 and very large numbers, sometimes 10\\^18. So we are talking about a dataset with 12 samples, each containing widely variating values for 15000 dimensions. Much of these dimensions do not change too much between one sample and the other: we need to do feature selection.\n\nI know for sure that the dataset has logic, because of how this dataset was obtained. It's from a published paper from a bio lab experiment, the details are not important right now.\n\n# What I have tried so far:\n\n* Pipeline 1: first a PCA, with number of components between 1 and 11. Then, a sklearn [Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html)(norm = 'max'). This is a unit norm normalizer, using the max value as the norm. And then, a SVR with Linear Kernel, and C variating between 0.0001 and 100000.\n\n&amp;#x200B;\n\n    pipe\u00a0=\u00a0make_pipeline(PCA(n_components\u00a0=\u00a0n_dimensions),\u00a0Normalizer(norm='max'),\u00a0SVR(kernel='linear',\u00a0C=c))\n\n&amp;#x200B;\n\n* Pipeline 2: first, I do feature selection with a DecisionTreeRegressor. This outputs 3 features (which I find weird, shouldn't it be 4 I guess?), since I only have 11 samples. Then I normalize the features selected with the Normalizer(norm = 'max') again, just like pipeline1. Then I use a SVR again with Linear Kernel, with C between 0.0001 and 100000.\n\n&amp;#x200B;\n\n    pipe = make_pipeline(SelectFromModel(DecisionTreeRegressor(min_samples_split=1, min_samples_leaf=0.000000001)), Normalizer(norm='max'), SVR(kernel='linear', C=c))\n\nSo all that changes between pipeline 1 and 2 is what I use to reduce the number of dimensions in the problem: one is a PCA, the other is a DecisionTreeRegressor.\n\n# My results:\n\nI am using a Leave One Out test. So I fit for 11 and then test for 1, for each sample.\n\nFor both pipelines, my regressor simply predicts a more or less average value for every sample. It doesn't even try to predict anything, it just guesses in the middle, somewhere between 2 and 3.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nMaybe a SVR is simply not suited for this problem? But I don't think I can train a neural network for this, since I only have 12 samples.\n\nWhat else could I try? Should I invest time in trying new regressors, or is the SVR enough and my problem is actually the feature selector? Or maybe I am messing up the normalization.\n\nAny 2 cents welcome.", "author_fullname": "t2_orxwarav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need some tips and directions on how to approach a regression problem with a very challenging dataset (12 samples, ~15000 dimensions). Give me your 2 cents", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1226752", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679793441.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679793148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am still a student so I&amp;#39;d like some tips and some ideas or directions I could take. &lt;strong&gt;I am not asking you to do this for me, I just want some ideas. How would you approach this problem?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;More about the dataset:&lt;/h1&gt;\n\n&lt;p&gt;The Y labels are fairly straight forward. Int values between 1 and 4, three samples for each. The X values vary between 0 and very large numbers, sometimes 10^18. So we are talking about a dataset with 12 samples, each containing widely variating values for 15000 dimensions. Much of these dimensions do not change too much between one sample and the other: we need to do feature selection.&lt;/p&gt;\n\n&lt;p&gt;I know for sure that the dataset has logic, because of how this dataset was obtained. It&amp;#39;s from a published paper from a bio lab experiment, the details are not important right now.&lt;/p&gt;\n\n&lt;h1&gt;What I have tried so far:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pipeline 1: first a PCA, with number of components between 1 and 11. Then, a sklearn &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\"&gt;Normalizer&lt;/a&gt;(norm = &amp;#39;max&amp;#39;). This is a unit norm normalizer, using the max value as the norm. And then, a SVR with Linear Kernel, and C variating between 0.0001 and 100000.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipe\u00a0=\u00a0make_pipeline(PCA(n_components\u00a0=\u00a0n_dimensions),\u00a0Normalizer(norm=&amp;#39;max&amp;#39;),\u00a0SVR(kernel=&amp;#39;linear&amp;#39;,\u00a0C=c))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pipeline 2: first, I do feature selection with a DecisionTreeRegressor. This outputs 3 features (which I find weird, shouldn&amp;#39;t it be 4 I guess?), since I only have 11 samples. Then I normalize the features selected with the Normalizer(norm = &amp;#39;max&amp;#39;) again, just like pipeline1. Then I use a SVR again with Linear Kernel, with C between 0.0001 and 100000.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipe = make_pipeline(SelectFromModel(DecisionTreeRegressor(min_samples_split=1, min_samples_leaf=0.000000001)), Normalizer(norm=&amp;#39;max&amp;#39;), SVR(kernel=&amp;#39;linear&amp;#39;, C=c))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So all that changes between pipeline 1 and 2 is what I use to reduce the number of dimensions in the problem: one is a PCA, the other is a DecisionTreeRegressor.&lt;/p&gt;\n\n&lt;h1&gt;My results:&lt;/h1&gt;\n\n&lt;p&gt;I am using a Leave One Out test. So I fit for 11 and then test for 1, for each sample.&lt;/p&gt;\n\n&lt;p&gt;For both pipelines, my regressor simply predicts a more or less average value for every sample. It doesn&amp;#39;t even try to predict anything, it just guesses in the middle, somewhere between 2 and 3.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Maybe a SVR is simply not suited for this problem? But I don&amp;#39;t think I can train a neural network for this, since I only have 12 samples.&lt;/p&gt;\n\n&lt;p&gt;What else could I try? Should I invest time in trying new regressors, or is the SVR enough and my problem is actually the feature selector? Or maybe I am messing up the normalization.&lt;/p&gt;\n\n&lt;p&gt;Any 2 cents welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1226752", "is_robot_indexable": true, "report_reasons": null, "author": "perguntando", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1226752/i_need_some_tips_and_directions_on_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1226752/i_need_some_tips_and_directions_on_how_to/", "subreddit_subscribers": 862719, "created_utc": 1679793148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a building a dashboard using Streamlit, Plotly Ex, python, pandas, numpy, scikit-learn, etc. The goal is to allow the audience to interactively explore the data. Different people will be using the dashboard and their data would be different, but they all have some commonalities. I want to build a dashboard that is \"generic\" enough to be applicable for most situations.\n\nThe data can contain categorical and numeric data, but mostly categorical. They all deal with consumer studies. The column of interest is typically some kind of categorical preference question (do you prefer A or B), or (would you buy this or no). There would be some demographic info of the consumer, some numeric columns that might pertain to ratings of a certain feature, and some categorical responses (multiple choice questions). \n\nThere are usually many features compared to number of observations. (ie, you might have only 100 consumers, but you asked them a bunch of questions)\n\nI think pair plots can be useful. Maybe give the user to ability to select some variables of interest, and run pair plots on them. \n\nI also plan to give the user the ability to group by variables and do various aggregations. \n\nSome filtering capability would also be built. (say, only want to see those with age &gt; 30)\n\nI'm also thinking running a random forest on the column of interest (such as preference) and determining the most important features. (not sure if this is a good idea given that the data typically have many features and not enough observations. A typical dataset might have 100 rows, and 40 columns) \n\nI don't plan on doing anything too specific (like stat tests) as those requires an understanding of what they are trying to do. Just want to build something automated and is general enough, and has enough functionalities to be useful, without doing \"too much\" that can cause issues.\n\nPlease advise on what you would or would not include.\n\nThanks", "author_fullname": "t2_6fty441r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a generic interactive business dashboard, what would you include?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121s7p2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a building a dashboard using Streamlit, Plotly Ex, python, pandas, numpy, scikit-learn, etc. The goal is to allow the audience to interactively explore the data. Different people will be using the dashboard and their data would be different, but they all have some commonalities. I want to build a dashboard that is &amp;quot;generic&amp;quot; enough to be applicable for most situations.&lt;/p&gt;\n\n&lt;p&gt;The data can contain categorical and numeric data, but mostly categorical. They all deal with consumer studies. The column of interest is typically some kind of categorical preference question (do you prefer A or B), or (would you buy this or no). There would be some demographic info of the consumer, some numeric columns that might pertain to ratings of a certain feature, and some categorical responses (multiple choice questions). &lt;/p&gt;\n\n&lt;p&gt;There are usually many features compared to number of observations. (ie, you might have only 100 consumers, but you asked them a bunch of questions)&lt;/p&gt;\n\n&lt;p&gt;I think pair plots can be useful. Maybe give the user to ability to select some variables of interest, and run pair plots on them. &lt;/p&gt;\n\n&lt;p&gt;I also plan to give the user the ability to group by variables and do various aggregations. &lt;/p&gt;\n\n&lt;p&gt;Some filtering capability would also be built. (say, only want to see those with age &amp;gt; 30)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also thinking running a random forest on the column of interest (such as preference) and determining the most important features. (not sure if this is a good idea given that the data typically have many features and not enough observations. A typical dataset might have 100 rows, and 40 columns) &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t plan on doing anything too specific (like stat tests) as those requires an understanding of what they are trying to do. Just want to build something automated and is general enough, and has enough functionalities to be useful, without doing &amp;quot;too much&amp;quot; that can cause issues.&lt;/p&gt;\n\n&lt;p&gt;Please advise on what you would or would not include.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121s7p2", "is_robot_indexable": true, "report_reasons": null, "author": "engineheat2", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121s7p2/building_a_generic_interactive_business_dashboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121s7p2/building_a_generic_interactive_business_dashboard/", "subreddit_subscribers": 862719, "created_utc": 1679764140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi! I just learned matplotlib,\n\nand I was wondering it's actually being used in an actual work setting when there's already PowerBI and Tableau. Same with Excel, since it can do visualization as well, do people use it?\n\nWhat's the most used visualization tool in your experience?", "author_fullname": "t2_vc2t0lme", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Visualization Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122ds0f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679813465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I just learned matplotlib,&lt;/p&gt;\n\n&lt;p&gt;and I was wondering it&amp;#39;s actually being used in an actual work setting when there&amp;#39;s already PowerBI and Tableau. Same with Excel, since it can do visualization as well, do people use it?&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the most used visualization tool in your experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122ds0f", "is_robot_indexable": true, "report_reasons": null, "author": "_Miles_Morales", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122ds0f/data_visualization_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122ds0f/data_visualization_tools/", "subreddit_subscribers": 862719, "created_utc": 1679813465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm studying Machine Learning, and I came across several mathematical formulas and I would like to understand them, not just learn how to use and apply the formulas.\n\nfor example, when I see an \" e (Euler)\" in the formula, where did Euler come from? why did he put it in the formula?\n\nWhen I search about e (euler) on the internet, I usually only find content explaining how to apply and solve equations with (e).\n\nHow can I begin to understand more about math so that I understand more, in the same way that I see an English sentence and understand its meaning?", "author_fullname": "t2_igf9n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn the \"Math language\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122da21", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679811912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m studying Machine Learning, and I came across several mathematical formulas and I would like to understand them, not just learn how to use and apply the formulas.&lt;/p&gt;\n\n&lt;p&gt;for example, when I see an &amp;quot; e (Euler)&amp;quot; in the formula, where did Euler come from? why did he put it in the formula?&lt;/p&gt;\n\n&lt;p&gt;When I search about e (euler) on the internet, I usually only find content explaining how to apply and solve equations with (e).&lt;/p&gt;\n\n&lt;p&gt;How can I begin to understand more about math so that I understand more, in the same way that I see an English sentence and understand its meaning?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122da21", "is_robot_indexable": true, "report_reasons": null, "author": "rvitor", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122da21/how_to_learn_the_math_language/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122da21/how_to_learn_the_math_language/", "subreddit_subscribers": 862719, "created_utc": 1679811912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, I've got sort of an unusual research question. Basically, I'd like to perform a comprehensive review of all the literature of a particular topic. To do this, I'd like to use combinations of search terms. For example, I'd conduct a search using terms \"A\" and \"B\", then I'd conduct another search using terms \"A\" and \"C\", then again using \"A\" and \"D\", etc. The problem with this is that there's a decent amount of overlap of search results among these different combinations and there are thousands of search results for each combination so I want to minimize redundancy as much as possible in order to save time. Is there a way for me to conduct an initial search (e.g., A + B) and then conduct each subsequent search (A + C, A + D, etc.) that will only show search results that are NOT included in the initial A + B search?\n\nI'm using OVID Medline as the search database, but I'd be open to any general workaround solutions as well. From my limited knowledge on a possible solution, I was wondering if it's possible to export all the search results, copy them as a list into a column within Excel, and then use the Excel function that can highlight duplicate values. This method would allow me to avoid redundant search results from each search iteration. This isn't an elegant solution imo, but I imagined a possible solution like this. The most ideal solution would be for the database to filter out redundant search results for me automatically.\n\nI can explain or clarify the problem further if that's helpful. Thank you for any help or suggestions with this problem!!", "author_fullname": "t2_179w2x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Literature review - how to filter out redundant search results from similar search iterations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122cqmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679810258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve got sort of an unusual research question. Basically, I&amp;#39;d like to perform a comprehensive review of all the literature of a particular topic. To do this, I&amp;#39;d like to use combinations of search terms. For example, I&amp;#39;d conduct a search using terms &amp;quot;A&amp;quot; and &amp;quot;B&amp;quot;, then I&amp;#39;d conduct another search using terms &amp;quot;A&amp;quot; and &amp;quot;C&amp;quot;, then again using &amp;quot;A&amp;quot; and &amp;quot;D&amp;quot;, etc. The problem with this is that there&amp;#39;s a decent amount of overlap of search results among these different combinations and there are thousands of search results for each combination so I want to minimize redundancy as much as possible in order to save time. Is there a way for me to conduct an initial search (e.g., A + B) and then conduct each subsequent search (A + C, A + D, etc.) that will only show search results that are NOT included in the initial A + B search?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using OVID Medline as the search database, but I&amp;#39;d be open to any general workaround solutions as well. From my limited knowledge on a possible solution, I was wondering if it&amp;#39;s possible to export all the search results, copy them as a list into a column within Excel, and then use the Excel function that can highlight duplicate values. This method would allow me to avoid redundant search results from each search iteration. This isn&amp;#39;t an elegant solution imo, but I imagined a possible solution like this. The most ideal solution would be for the database to filter out redundant search results for me automatically.&lt;/p&gt;\n\n&lt;p&gt;I can explain or clarify the problem further if that&amp;#39;s helpful. Thank you for any help or suggestions with this problem!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122cqmn", "is_robot_indexable": true, "report_reasons": null, "author": "pantaloonsss", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122cqmn/literature_review_how_to_filter_out_redundant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122cqmn/literature_review_how_to_filter_out_redundant/", "subreddit_subscribers": 862719, "created_utc": 1679810258.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For teams responsible for developing, testing and maintaining product recommendation models for e-commerce companies;\n\n~How large is your team?\n~What percent of e-commerce revenue would you attribute to product recommendations?", "author_fullname": "t2_7v3tv87q8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product recommendation teams question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121vs0z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679771212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For teams responsible for developing, testing and maintaining product recommendation models for e-commerce companies;&lt;/p&gt;\n\n&lt;p&gt;~How large is your team?\n~What percent of e-commerce revenue would you attribute to product recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121vs0z", "is_robot_indexable": true, "report_reasons": null, "author": "ilovedataaa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121vs0z/product_recommendation_teams_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121vs0z/product_recommendation_teams_question/", "subreddit_subscribers": 862719, "created_utc": 1679771212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a data set that covers internet data usage of several hundred users over several month period. What is the best approach to analyze? My goal is to retain these customers or upsell. Any suggestions?", "author_fullname": "t2_bco8idf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best approach to analyze a simple data set?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121v2ol", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679769782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data set that covers internet data usage of several hundred users over several month period. What is the best approach to analyze? My goal is to retain these customers or upsell. Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121v2ol", "is_robot_indexable": true, "report_reasons": null, "author": "Environmental_Wind40", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121v2ol/what_is_the_best_approach_to_analyze_a_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121v2ol/what_is_the_best_approach_to_analyze_a_simple/", "subreddit_subscribers": 862719, "created_utc": 1679769782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "General question - A lot of folks I know can't use Github Copilot inside their company for data privacy issues - code cannot leave the companies firewall. Is this an issue for others?", "author_fullname": "t2_7t1fa3mp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Github Copilot allowed inside your company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122cr8d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679810311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;General question - A lot of folks I know can&amp;#39;t use Github Copilot inside their company for data privacy issues - code cannot leave the companies firewall. Is this an issue for others?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122cr8d", "is_robot_indexable": true, "report_reasons": null, "author": "Dry_Win9996", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122cr8d/is_github_copilot_allowed_inside_your_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122cr8d/is_github_copilot_allowed_inside_your_company/", "subreddit_subscribers": 862719, "created_utc": 1679810311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am confident I am getting  a job offer for company A later this week. I have been interning at a different place, company B, and was told they would consider sending me an offer in a few weeks from now.\n\nI am wondering if I should go to company B now and ask for them to send me an offer now or if I should wait for company A to officially give me an offer first. From what I read online I would only have a few days to a week to make a decision once I am given an offer, so I want to try and make the offers overlap as much as possible.", "author_fullname": "t2_48dmgjj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job offer timeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12279ow", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679795270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am confident I am getting  a job offer for company A later this week. I have been interning at a different place, company B, and was told they would consider sending me an offer in a few weeks from now.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if I should go to company B now and ask for them to send me an offer now or if I should wait for company A to officially give me an offer first. From what I read online I would only have a few days to a week to make a decision once I am given an offer, so I want to try and make the offers overlap as much as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12279ow", "is_robot_indexable": true, "report_reasons": null, "author": "loumani06", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12279ow/job_offer_timeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12279ow/job_offer_timeline/", "subreddit_subscribers": 862719, "created_utc": 1679795270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been given time series of N stock prices and time series of K sectoral indices. (Sectoral index is index made from all stocks belonging to that sector.)\nI have not been given names of any of those stocks or indices.\nI want to know which stocks belong to the same sector and sectoral indices.\nI am able to cluster the stocks. But how should I decide which sectoral index time series they correspond to? Should I just cluster stocks and indices together (N+K timeseries) instead of clustering N stocks independently? Will that be correct approach?", "author_fullname": "t2_aaamk24y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Determining clusters of time series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121k7ei", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679746800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been given time series of N stock prices and time series of K sectoral indices. (Sectoral index is index made from all stocks belonging to that sector.)\nI have not been given names of any of those stocks or indices.\nI want to know which stocks belong to the same sector and sectoral indices.\nI am able to cluster the stocks. But how should I decide which sectoral index time series they correspond to? Should I just cluster stocks and indices together (N+K timeseries) instead of clustering N stocks independently? Will that be correct approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121k7ei", "is_robot_indexable": true, "report_reasons": null, "author": "SnooHabits4550", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121k7ei/determining_clusters_of_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121k7ei/determining_clusters_of_time_series/", "subreddit_subscribers": 862719, "created_utc": 1679746800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_sppgx30r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Machine Learning with Python, Getting Started for Data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_121wrga", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7DC5P14iKM8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/121wrga", "height": 200}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TwPjHFympHbiLuuOgGyFADIEsXBoKkGF85TdyaZ-HcQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679773252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/7DC5P14iKM8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?auto=webp&amp;v=enabled&amp;s=2fa34b646cffa3bf70691a05860c888e9fd2449d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0aa8277ab41e4659d1f2641b7da280e0b10cd16b", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1effaa95d1777ee8642cc9a2dbd6bcfcf18d607", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5166cadbfa3780b1e376d3de0770ca8d0be27da5", "width": 320, "height": 240}], "variants": {}, "id": "LEBKQa13ztjayQE2R1vvT6gYphFwdkPbqZeK3aNkFbs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "121wrga", "is_robot_indexable": true, "report_reasons": null, "author": "thetech_learner", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121wrga/machine_learning_with_python_getting_started_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/7DC5P14iKM8", "subreddit_subscribers": 862719, "created_utc": 1679773252.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7DC5P14iKM8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone!\n\nTL;DR\n\n\\- Looking for an intensive full-time, in-person, 3-6 month long data program in Canada to become very proficient in coding after graduating from uni this summer. \n\nI'm going to be a graduating with a [B.Sc](https://B.Sc). in Psychology this summer (In Canada) and would like to spend part-time this summer and full-time this Fall (and maybe even Winter) becoming really proficient in coding (thinking Python and SQL). I'm not into the online-learning thing (did enough of that over COVID - hard to stay focused) so I'm trying to figure out a part time online program to learn basics of coding in the summer, and then jump into an in-person intensive data analytics/data science program of some sort in the fall (I'm willing to move for it - somewhere in Canada). \n\nCurrently thinking of doing the WeCloudData data analytics bootcamp then trying to figure out what to do after that in-person (was looking into BrainStation because its the best rated program in Canada for in-person bootcamps). I don't want to pursue career as a data scientist per say, I'm just interested in developing this skillset and see it as a skill that is growing in demand - especially in the humanities. \n\nAny thoughts would be helpful!", "author_fullname": "t2_scu1jkf0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help choosing a coding bootcamp/classes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121v4l2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679769893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;TL;DR&lt;/p&gt;\n\n&lt;p&gt;- Looking for an intensive full-time, in-person, 3-6 month long data program in Canada to become very proficient in coding after graduating from uni this summer. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to be a graduating with a &lt;a href=\"https://B.Sc\"&gt;B.Sc&lt;/a&gt;. in Psychology this summer (In Canada) and would like to spend part-time this summer and full-time this Fall (and maybe even Winter) becoming really proficient in coding (thinking Python and SQL). I&amp;#39;m not into the online-learning thing (did enough of that over COVID - hard to stay focused) so I&amp;#39;m trying to figure out a part time online program to learn basics of coding in the summer, and then jump into an in-person intensive data analytics/data science program of some sort in the fall (I&amp;#39;m willing to move for it - somewhere in Canada). &lt;/p&gt;\n\n&lt;p&gt;Currently thinking of doing the WeCloudData data analytics bootcamp then trying to figure out what to do after that in-person (was looking into BrainStation because its the best rated program in Canada for in-person bootcamps). I don&amp;#39;t want to pursue career as a data scientist per say, I&amp;#39;m just interested in developing this skillset and see it as a skill that is growing in demand - especially in the humanities. &lt;/p&gt;\n\n&lt;p&gt;Any thoughts would be helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121v4l2", "is_robot_indexable": true, "report_reasons": null, "author": "LearningToCode100", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121v4l2/help_choosing_a_coding_bootcampclasses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121v4l2/help_choosing_a_coding_bootcampclasses/", "subreddit_subscribers": 862719, "created_utc": 1679769893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "If you're a beginner in data analysis and looking for a practical way to apply your skills, I've got just the thing for you!\n\nDiscover the fascinating changes in Bangladesh's population and demographics over the past 70 years with my comprehensive analysis. From total population to life expectancy, my notebook covers the key indicators that you need to know. \n\nWhether you're a data enthusiast or just curious about Bangladesh, click here to explore the latest insights and trends today!\n\n[https://www.kaggle.com/code/abmsayem/demographic-shifts-in-bangladesh-1950-to-2020](https://www.kaggle.com/code/abmsayem/demographic-shifts-in-bangladesh-1950-to-2020)", "author_fullname": "t2_4oeq7ean1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Comprehensive Data Analysis on Bangladesh's Demographics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121m112", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679751278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re a beginner in data analysis and looking for a practical way to apply your skills, I&amp;#39;ve got just the thing for you!&lt;/p&gt;\n\n&lt;p&gt;Discover the fascinating changes in Bangladesh&amp;#39;s population and demographics over the past 70 years with my comprehensive analysis. From total population to life expectancy, my notebook covers the key indicators that you need to know. &lt;/p&gt;\n\n&lt;p&gt;Whether you&amp;#39;re a data enthusiast or just curious about Bangladesh, click here to explore the latest insights and trends today!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.kaggle.com/code/abmsayem/demographic-shifts-in-bangladesh-1950-to-2020\"&gt;https://www.kaggle.com/code/abmsayem/demographic-shifts-in-bangladesh-1950-to-2020&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ByZNzGIHn-EKXnyaT9e72zSDkK8l8fyiWtVmfxlp1yQ.jpg?auto=webp&amp;v=enabled&amp;s=1d3918344fcec915a145ffaf7839c2373c35282a", "width": 100, "height": 100}, "resolutions": [], "variants": {}, "id": "YtZsnKA_8wRxYy4xhxvjCtVCxiEdcyIPpYXLtQTk4FY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121m112", "is_robot_indexable": true, "report_reasons": null, "author": "abmsayem", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121m112/a_comprehensive_data_analysis_on_bangladeshs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121m112/a_comprehensive_data_analysis_on_bangladeshs/", "subreddit_subscribers": 862719, "created_utc": 1679751278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_8drjhm8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dataset required for list of professional IT, AI, Ml , Cloud certifications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121z1rc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679778128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121z1rc", "is_robot_indexable": true, "report_reasons": null, "author": "Helix-x", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121z1rc/dataset_required_for_list_of_professional_it_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121z1rc/dataset_required_for_list_of_professional_it_ai/", "subreddit_subscribers": 862719, "created_utc": 1679778128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT-4 can solve most SQL interview questions. In 5 years, do you think Acing a SQL Interview will still be important?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121tjz7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": "", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_6zm7tswg", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "SQL", "selftext": "GPT-4 can write SQL queries and solve most easy &amp; medium SQL interview questions on [DataLemur](https://datalemur.com/sql-interview-questions). So I'm curious, will Acing the SQL Interview still be important in 5 years? What about in 10?\n\n[View Poll](https://www.reddit.com/poll/121q7nt)", "author_fullname": "t2_6zm7tswg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT-4 can solve most SQL interview questions. In 5 years, do you think Acing a SQL Interview will still be important?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/SQL", "hidden": false, "pwls": 6, "link_flair_css_class": "h", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121q7nt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "01031502-de87-11e6-b05e-0e1ce54f6a26", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679799206.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679759953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.SQL", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;GPT-4 can write SQL queries and solve most easy &amp;amp; medium SQL interview questions on &lt;a href=\"https://datalemur.com/sql-interview-questions\"&gt;DataLemur&lt;/a&gt;. So I&amp;#39;m curious, will Acing the SQL Interview still be important in 5 years? What about in 10?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/121q7nt\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?auto=webp&amp;v=enabled&amp;s=ecad274028abaee33f7472ca7eb576890793e6f7", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf2bb12e9628e15e83ad865536a3b58187e2d8bd", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc4ed8504bdaba38a706ffb0291e7be0986001c5", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d05fdfa32d7834af5a37d7214541795e9f1e9be7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7cfdf240267973e8ea29c0cd5a3341c463c47e9", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3a957187d5c772bf1e2370ae9dd4809896be275", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2829f8314991838e362fd17457c99145d627d723", "width": 1080, "height": 567}], "variants": {}, "id": "wPNS14r1J4_L3TGYUAsqCV8FmgGrMho9XjGoyEnYUAQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5cb03a64-2769-11ea-aa04-0ed301f44875", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Author of Ace the Data Science Interview \ud83d\udcd5", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qp8q", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "121q7nt", "is_robot_indexable": true, "report_reasons": null, "author": "NickSinghTechCareers", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1680364753748, "options": [{"text": "SQL Interview Will Be Obsolete", "id": "22238479"}, {"text": "SQL Interviews Might Go Away", "id": "22238480"}, {"text": "SQL Interviews Won't Go Away", "id": "22238481"}, {"text": "Show Results", "id": "22238482"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 1589, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "subreddit_subscribers": 144334, "created_utc": 1679759953.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1679766802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.SQL", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?auto=webp&amp;v=enabled&amp;s=ecad274028abaee33f7472ca7eb576890793e6f7", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf2bb12e9628e15e83ad865536a3b58187e2d8bd", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc4ed8504bdaba38a706ffb0291e7be0986001c5", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d05fdfa32d7834af5a37d7214541795e9f1e9be7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7cfdf240267973e8ea29c0cd5a3341c463c47e9", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3a957187d5c772bf1e2370ae9dd4809896be275", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/kvAalAMUTwVx-RWPxt7OGwpP1OeLOb3EYj0o29ZGbKM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2829f8314991838e362fd17457c99145d627d723", "width": 1080, "height": 567}], "variants": {}, "id": "wPNS14r1J4_L3TGYUAsqCV8FmgGrMho9XjGoyEnYUAQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Author | Ace the Data Science Interview", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121tjz7", "is_robot_indexable": true, "report_reasons": null, "author": "NickSinghTechCareers", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_121q7nt", "author_flair_text_color": "dark", "permalink": "/r/datascience/comments/121tjz7/gpt4_can_solve_most_sql_interview_questions_in_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "subreddit_subscribers": 862719, "created_utc": 1679766802.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}