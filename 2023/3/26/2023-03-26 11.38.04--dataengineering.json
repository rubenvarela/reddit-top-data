{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Company recently made the decision to move to snowflake from a mishmash of on-prem databases (mostly sql server). \n\nData science team uses R and spark running on a handful of on prem workstations to build models. \n\nThe IT department is pushing hard to move everything into snowflake and the data science people are complaining that snowpark is half baked. Especially on R programming where they'll still have to take data out of snowflake into some other compute source to run R models, then insert results back into snowflake.\n\nTrying to think of solutions. Should we:\n\n1) convince IT so provision a small databricks account we can use just for running models?\n2) suck it up and convert everything to python, trying to adopt snowpark/anaconda as much as possible?\n3) keep our pile of on prem machines with do modeling by pulling data out of snowflake, computing locally, inserting model results back into snowflake for use in reports, decision dashboards, etc.", "author_fullname": "t2_5iwre", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "machine learning in snowflake, unhappy data scientists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121mm5c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 85, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 85, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679752633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Company recently made the decision to move to snowflake from a mishmash of on-prem databases (mostly sql server). &lt;/p&gt;\n\n&lt;p&gt;Data science team uses R and spark running on a handful of on prem workstations to build models. &lt;/p&gt;\n\n&lt;p&gt;The IT department is pushing hard to move everything into snowflake and the data science people are complaining that snowpark is half baked. Especially on R programming where they&amp;#39;ll still have to take data out of snowflake into some other compute source to run R models, then insert results back into snowflake.&lt;/p&gt;\n\n&lt;p&gt;Trying to think of solutions. Should we:&lt;/p&gt;\n\n&lt;p&gt;1) convince IT so provision a small databricks account we can use just for running models?\n2) suck it up and convert everything to python, trying to adopt snowpark/anaconda as much as possible?\n3) keep our pile of on prem machines with do modeling by pulling data out of snowflake, computing locally, inserting model results back into snowflake for use in reports, decision dashboards, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121mm5c", "is_robot_indexable": true, "report_reasons": null, "author": "CantGoogleMe", "discussion_type": null, "num_comments": 78, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121mm5c/machine_learning_in_snowflake_unhappy_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121mm5c/machine_learning_in_snowflake_unhappy_data/", "subreddit_subscribers": 94418, "created_utc": 1679752633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was asked to migrate 2500 tables from SQL Server to MariaDB and the ETL tool (Cdata sync) doesn\u2019t work for ~20 tables because they\u2019re too big and it keeps timing out. So I tried importing the data for the missing tables by expiring the CSVs through DBeaver then importing them and it still times out. Now I am trying to import the tables using python but my code errors out because of some issue with how it\u2019s reading the data. I don\u2019t even know if that\u2019s worth fixing because it\u2019s going to take forever to read in the tables with ~5 million records and other have even more or about the same. Whenever I use \u201cread_csv\u201d there is an error with tokenizing the data and when I use \u201cwith open\u201d\u2026 it says some error that hit all the data types converted. \n\nI told my manager(s) this and my co worker whom I was told to reach out to help for. I bounced some ideas off of my conworker and he said if I can come up with something in python that would be cool. I was like okay I don\u2019t know what I\u2019m doing is this ok?? What would be cool?? My manager said he\u2019s depending on me to do this and my co worker but idk what I\u2019m doing and when I ask for help they\u2019re like I don\u2019t know?? Like i dont know either what the fuck do you want from me?? I hesitate to download other tools like new ones and start over again. I need to find a way to automate increments of data too so I can\u2019t just manually import them and use the stupid failed ETL tool for others?  Like I don\u2019t know what to say anymore I am stuck and out of ideas?? I don\u2019t know what to say for my standup on Monday.", "author_fullname": "t2_8ewmlf41", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I\u2019m struggling with how to ask for help with my task.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1226vlf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679794333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was asked to migrate 2500 tables from SQL Server to MariaDB and the ETL tool (Cdata sync) doesn\u2019t work for ~20 tables because they\u2019re too big and it keeps timing out. So I tried importing the data for the missing tables by expiring the CSVs through DBeaver then importing them and it still times out. Now I am trying to import the tables using python but my code errors out because of some issue with how it\u2019s reading the data. I don\u2019t even know if that\u2019s worth fixing because it\u2019s going to take forever to read in the tables with ~5 million records and other have even more or about the same. Whenever I use \u201cread_csv\u201d there is an error with tokenizing the data and when I use \u201cwith open\u201d\u2026 it says some error that hit all the data types converted. &lt;/p&gt;\n\n&lt;p&gt;I told my manager(s) this and my co worker whom I was told to reach out to help for. I bounced some ideas off of my conworker and he said if I can come up with something in python that would be cool. I was like okay I don\u2019t know what I\u2019m doing is this ok?? What would be cool?? My manager said he\u2019s depending on me to do this and my co worker but idk what I\u2019m doing and when I ask for help they\u2019re like I don\u2019t know?? Like i dont know either what the fuck do you want from me?? I hesitate to download other tools like new ones and start over again. I need to find a way to automate increments of data too so I can\u2019t just manually import them and use the stupid failed ETL tool for others?  Like I don\u2019t know what to say anymore I am stuck and out of ideas?? I don\u2019t know what to say for my standup on Monday.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1226vlf", "is_robot_indexable": true, "report_reasons": null, "author": "jumpfordespair", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1226vlf/im_struggling_with_how_to_ask_for_help_with_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1226vlf/im_struggling_with_how_to_ask_for_help_with_my/", "subreddit_subscribers": 94418, "created_utc": 1679794333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As per title. How are you using DuckDB, either privately for passion/side projects, hobbies, or professionally in production environments?\n\nWould love to just hear the innovative ways in which people are using it.", "author_fullname": "t2_4bjsc4oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you using DuckDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121jccb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679744601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per title. How are you using DuckDB, either privately for passion/side projects, hobbies, or professionally in production environments?&lt;/p&gt;\n\n&lt;p&gt;Would love to just hear the innovative ways in which people are using it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121jccb", "is_robot_indexable": true, "report_reasons": null, "author": "jb7834", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121jccb/how_are_you_using_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121jccb/how_are_you_using_duckdb/", "subreddit_subscribers": 94418, "created_utc": 1679744601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, i am in the process of building a new data model. Our existing model is OBT, while it's good for analysts, it's a nightmare for dashboard use cases. I am thinking of building a traditional dimensional Model which I'll use in dashboards and build a OBT on top of it to give to analysts. But I had a few questions :\n\n1. It is said that dimensional models can be slow to process the data. Our current OBT model refreshes 8 times a day and the new model should be refreshed in the same frequency or most probably once every hour.\n\n2. We use databricks, while it could solve some of load time problems mentioned above, I am not really able to find any resources that tells me whether there's omr modelling approach that is more suitable to a spark platform like databricks compared to others.", "author_fullname": "t2_xap78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Platform considerations for modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121lqh0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679750588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, i am in the process of building a new data model. Our existing model is OBT, while it&amp;#39;s good for analysts, it&amp;#39;s a nightmare for dashboard use cases. I am thinking of building a traditional dimensional Model which I&amp;#39;ll use in dashboards and build a OBT on top of it to give to analysts. But I had a few questions :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;It is said that dimensional models can be slow to process the data. Our current OBT model refreshes 8 times a day and the new model should be refreshed in the same frequency or most probably once every hour.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We use databricks, while it could solve some of load time problems mentioned above, I am not really able to find any resources that tells me whether there&amp;#39;s omr modelling approach that is more suitable to a spark platform like databricks compared to others.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121lqh0", "is_robot_indexable": true, "report_reasons": null, "author": "totalsports1", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/121lqh0/platform_considerations_for_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121lqh0/platform_considerations_for_modelling/", "subreddit_subscribers": 94418, "created_utc": 1679750588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Interested to hear how (and what) teams are doing for data pipeline documentation. Where is the proper balance between maintainability and enough detail to provide value. What items are you covering the document (source, maybe associated jira details, high level business logic overview, etc...)?  Where are you keeping it github-pages, confluence, G drive (oh the horror)...?\n\nI don't feel code documentation should be included as the code itself should be well commented and clean and with the changes frequent in code bases it'd never be kept up anyway.", "author_fullname": "t2_puuzgu6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline documentation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121s86p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679764741.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interested to hear how (and what) teams are doing for data pipeline documentation. Where is the proper balance between maintainability and enough detail to provide value. What items are you covering the document (source, maybe associated jira details, high level business logic overview, etc...)?  Where are you keeping it github-pages, confluence, G drive (oh the horror)...?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t feel code documentation should be included as the code itself should be well commented and clean and with the changes frequent in code bases it&amp;#39;d never be kept up anyway.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121s86p", "is_robot_indexable": true, "report_reasons": null, "author": "getafterit123", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121s86p/pipeline_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121s86p/pipeline_documentation/", "subreddit_subscribers": 94418, "created_utc": 1679764169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\nI\u2019m exploring how to integrate Apache Airflow into our communicable disease unit workflow. We are mostly R users but are starting to use Python more. The basic workflow I have in mind is:\n\nTask 1 - trigger script 1 - pull immunization records from Snowflake &gt;&gt; Task 2 - trigger script 2 - clean and deduplicate, then add new records to historical dataset &gt;&gt; Update R Shiny\n\nI have Airflow up and running via docker, but I\u2019m stuck on how to trigger scripts via bash command. Can I have the script anywhere on our network drive, or does it have to be somewhere within the airflow folder? In the past I\u2019ve used windows task scheduler and .bat files for run scripts overnight, but trying to better automate some of our repetitive tasks.", "author_fullname": "t2_7e2bmavx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Airflow Triggering Python Scripts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121rg90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679762604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI\u2019m exploring how to integrate Apache Airflow into our communicable disease unit workflow. We are mostly R users but are starting to use Python more. The basic workflow I have in mind is:&lt;/p&gt;\n\n&lt;p&gt;Task 1 - trigger script 1 - pull immunization records from Snowflake &amp;gt;&amp;gt; Task 2 - trigger script 2 - clean and deduplicate, then add new records to historical dataset &amp;gt;&amp;gt; Update R Shiny&lt;/p&gt;\n\n&lt;p&gt;I have Airflow up and running via docker, but I\u2019m stuck on how to trigger scripts via bash command. Can I have the script anywhere on our network drive, or does it have to be somewhere within the airflow folder? In the past I\u2019ve used windows task scheduler and .bat files for run scripts overnight, but trying to better automate some of our repetitive tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121rg90", "is_robot_indexable": true, "report_reasons": null, "author": "Senior-Athlete-0", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121rg90/apache_airflow_triggering_python_scripts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121rg90/apache_airflow_triggering_python_scripts/", "subreddit_subscribers": 94418, "created_utc": 1679762604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Keen to know any managed service on data governance or Ranger gives the best bang for buck.\n\n[View Poll](https://www.reddit.com/poll/122cbp0)", "author_fullname": "t2_74eqv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What data governance tool are you folks using?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122cbp0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679808990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Keen to know any managed service on data governance or Ranger gives the best bang for buck.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/122cbp0\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "122cbp0", "is_robot_indexable": true, "report_reasons": null, "author": "shabaab", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1680068191004, "options": [{"text": "Apache Ranger", "id": "22248299"}, {"text": "Privacera", "id": "22248300"}, {"text": "Immuta", "id": "22248301"}, {"text": "Atlan", "id": "22248302"}, {"text": "Others (please comment)", "id": "22248303"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 101, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/122cbp0/what_data_governance_tool_are_you_folks_using/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/122cbp0/what_data_governance_tool_are_you_folks_using/", "subreddit_subscribers": 94418, "created_utc": 1679808990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, Redditors!\n\nI applied for a junior DE position at one company. I saw they use Airflow as an orchestration tool and I started learning it recently but can't seem to understand something, so if you guys are willing to help out I would be really grateful.\n\n1. So in ETL, let's say I have a script that extracts data from some source (doesn't matter which one) and that data is now loaded. Where is it loaded? I mean the location? Is it in some repository on the company's cloud or does Airflow has its own place where it temporarily stores it?\n2. Now let's say I have 3 different python scripts (pandas) that perform some transformation each. So transform\\_script1 takes that data, performs transformation and how exactly does it pass it to the transform\\_script2? Is the same for transform\\_script3?\n3. From what I understand, in my dag script, I should specify the downstream or upstream scripts. So it would look something like this:  \nextract\\_script &gt;&gt; transform\\_script1 &gt;&gt; transform\\_script2 &gt;&gt; transform\\_script3 &gt;&gt; load\\_script  \nBut what I fail to understand is how is the data passed between them.\n4. If someone has extra time I would really like to see what transform\\_script(s) look like. I might be asking a lot of your time, but if you have some time I would really appreciate it.\n\nI realize I am a total beginner, but really want this interview to go well. Thank you, guys!", "author_fullname": "t2_2jtk54zi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121ywna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679777841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, Redditors!&lt;/p&gt;\n\n&lt;p&gt;I applied for a junior DE position at one company. I saw they use Airflow as an orchestration tool and I started learning it recently but can&amp;#39;t seem to understand something, so if you guys are willing to help out I would be really grateful.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;So in ETL, let&amp;#39;s say I have a script that extracts data from some source (doesn&amp;#39;t matter which one) and that data is now loaded. Where is it loaded? I mean the location? Is it in some repository on the company&amp;#39;s cloud or does Airflow has its own place where it temporarily stores it?&lt;/li&gt;\n&lt;li&gt;Now let&amp;#39;s say I have 3 different python scripts (pandas) that perform some transformation each. So transform_script1 takes that data, performs transformation and how exactly does it pass it to the transform_script2? Is the same for transform_script3?&lt;/li&gt;\n&lt;li&gt;From what I understand, in my dag script, I should specify the downstream or upstream scripts. So it would look something like this:&lt;br/&gt;\nextract_script &amp;gt;&amp;gt; transform_script1 &amp;gt;&amp;gt; transform_script2 &amp;gt;&amp;gt; transform_script3 &amp;gt;&amp;gt; load_script&lt;br/&gt;\nBut what I fail to understand is how is the data passed between them.&lt;/li&gt;\n&lt;li&gt;If someone has extra time I would really like to see what transform_script(s) look like. I might be asking a lot of your time, but if you have some time I would really appreciate it.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I realize I am a total beginner, but really want this interview to go well. Thank you, guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121ywna", "is_robot_indexable": true, "report_reasons": null, "author": "d_underdog", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/121ywna/airflow_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121ywna/airflow_question/", "subreddit_subscribers": 94418, "created_utc": 1679777841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Usually in OLTP systems, we have a PK constraint that is enforced. In OLAP side, I see that Snowflake, Databricks, GCP BigQuery - None of them have a PK that is enforced. I understand that it causes data performance issues, but given that all of them store data in columnar fashion, how does checking a unique id in column affects performance?\n\nEven while working with MERGE statements, the system is going to compare the columns to check whether it needs to do INSERT OR UPDATE, then a PK column with corresponding index might increase the query performance right?", "author_fullname": "t2_2xxs9nne", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do Data warehouses don't have a Primary Key check that is enforced?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122eo0k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679816204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Usually in OLTP systems, we have a PK constraint that is enforced. In OLAP side, I see that Snowflake, Databricks, GCP BigQuery - None of them have a PK that is enforced. I understand that it causes data performance issues, but given that all of them store data in columnar fashion, how does checking a unique id in column affects performance?&lt;/p&gt;\n\n&lt;p&gt;Even while working with MERGE statements, the system is going to compare the columns to check whether it needs to do INSERT OR UPDATE, then a PK column with corresponding index might increase the query performance right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "122eo0k", "is_robot_indexable": true, "report_reasons": null, "author": "inglocines", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/122eo0k/why_do_data_warehouses_dont_have_a_primary_key/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/122eo0k/why_do_data_warehouses_dont_have_a_primary_key/", "subreddit_subscribers": 94418, "created_utc": 1679816204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "No, not the sql function lol\nHas anyone used this product? Pros, cons, thoughts? \nThanks!", "author_fullname": "t2_9rbxc91v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coalesce for Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122a093", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679802235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;No, not the sql function lol\nHas anyone used this product? Pros, cons, thoughts? \nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "122a093", "is_robot_indexable": true, "report_reasons": null, "author": "AnnualDepth8843", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/122a093/coalesce_for_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/122a093/coalesce_for_snowflake/", "subreddit_subscribers": 94418, "created_utc": 1679802235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using dbt, and I have a use case at the company I work at which I don't think is supported exactly. We use a green/blue deploy strategy for each table. When all the dbt tests for that specific table have completed successfully, I'd like to run a sql snippet to swap/promote it. Kind of like a model post-hook, but not after the model is run, instead for collection after all per-model tests pass. Does anyone have any ideas for a workaround, hack, or a package that might work?\n\nCurrently I'm writing test results to the database and having a second step to parse them, but a single pass option seems like it should be possible.", "author_fullname": "t2_dzfsy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt post-hook for successful tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121shh6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764674.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using dbt, and I have a use case at the company I work at which I don&amp;#39;t think is supported exactly. We use a green/blue deploy strategy for each table. When all the dbt tests for that specific table have completed successfully, I&amp;#39;d like to run a sql snippet to swap/promote it. Kind of like a model post-hook, but not after the model is run, instead for collection after all per-model tests pass. Does anyone have any ideas for a workaround, hack, or a package that might work?&lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m writing test results to the database and having a second step to parse them, but a single pass option seems like it should be possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121shh6", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_Again", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121shh6/dbt_posthook_for_successful_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121shh6/dbt_posthook_for_successful_tests/", "subreddit_subscribers": 94418, "created_utc": 1679764674.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I use RazorSQL to navigate in a DynamoDB database. What's the rationale behind the column order? http://i.stack.imgur.com/Lgcqr.png\n\nI am asking as the import tool uses the same order (which means if I import a CSV file it needs to be in the same ordering as the RazorSQL's column order): http://i.stack.imgur.com/fDyBL.png\n\nI use RazorSQL 6.3.14 with Windows 7 SP1 x64 Ultimate.", "author_fullname": "t2_kprlc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Column order in a DynamoDB table browsed through RazorSQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12237ot", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679786885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use RazorSQL to navigate in a DynamoDB database. What&amp;#39;s the rationale behind the column order? &lt;a href=\"http://i.stack.imgur.com/Lgcqr.png\"&gt;http://i.stack.imgur.com/Lgcqr.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am asking as the import tool uses the same order (which means if I import a CSV file it needs to be in the same ordering as the RazorSQL&amp;#39;s column order): &lt;a href=\"http://i.stack.imgur.com/fDyBL.png\"&gt;http://i.stack.imgur.com/fDyBL.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I use RazorSQL 6.3.14 with Windows 7 SP1 x64 Ultimate.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1HCf9QrSJkHQ0xOEkNIY3cR_7HgL5mGBprd5xwpQl18.png?auto=webp&amp;v=enabled&amp;s=e29d2560fcbf6f1c014341065dce924e6c06b93a", "width": 480, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/1HCf9QrSJkHQ0xOEkNIY3cR_7HgL5mGBprd5xwpQl18.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c399fdb73c0bf1a1159a61a0b0f9135bd69311be", "width": 108, "height": 162}, {"url": "https://external-preview.redd.it/1HCf9QrSJkHQ0xOEkNIY3cR_7HgL5mGBprd5xwpQl18.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a209863b315b2a004a8e9a57fc36c15bc327991e", "width": 216, "height": 324}, {"url": "https://external-preview.redd.it/1HCf9QrSJkHQ0xOEkNIY3cR_7HgL5mGBprd5xwpQl18.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9b497a7303e48a07900f1b9c1ff681cb15a66ca", "width": 320, "height": 480}], "variants": {}, "id": "LHrO0Nr6RjYhrhTbc1QR71XCyuantZcywjo_QIkTKlw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12237ot", "is_robot_indexable": true, "report_reasons": null, "author": "Franck_Dernoncourt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12237ot/column_order_in_a_dynamodb_table_browsed_through/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12237ot/column_order_in_a_dynamodb_table_browsed_through/", "subreddit_subscribers": 94418, "created_utc": 1679786885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone who has tried Polars in AWS Glue using the Shell script option, or whether it is even feasible at all? My work is dirt cheap and wants to minimize costs as much as possible lol. I'll be processing 500mb worth of data and not even sure if the 0.0625 DPU option would even cut it. For context, that is like 0.25 vCPU and 1GB RAM lol.", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars Library in AWS Glue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121ou6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679756928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone who has tried Polars in AWS Glue using the Shell script option, or whether it is even feasible at all? My work is dirt cheap and wants to minimize costs as much as possible lol. I&amp;#39;ll be processing 500mb worth of data and not even sure if the 0.0625 DPU option would even cut it. For context, that is like 0.25 vCPU and 1GB RAM lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121ou6t", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121ou6t/polars_library_in_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121ou6t/polars_library_in_aws_glue/", "subreddit_subscribers": 94418, "created_utc": 1679756928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I've got sort of an unusual research question. Basically, I'd like to perform a comprehensive review of all the literature of a particular topic. To do this, I'd like to use combinations of search terms. For example, I'd conduct a search using terms \"A\" and \"B\", then I'd conduct another search using terms \"A\" and \"C\", then again using \"A\" and \"D\", etc. The problem with this is that there's a decent amount of overlap of search results among these different combinations and there are thousands of search results for each combination so I want to minimize redundancy as much as possible in order to save time. Is there a way for me to conduct an initial search (e.g., A + B) and then conduct each subsequent search (A + C, A + D, etc.) that will only show search results that are NOT included in the initial A + B search?\n\nI'm using OVID Medline as the search database, but I'd be open to any general workaround solutions as well. From my limited knowledge on a possible solution, I was wondering if it's possible to export all the search results, copy them as a list into a column within Excel, and then use the Excel function that can highlight duplicate values. This method would allow me to avoid redundant search results from each search iteration. This isn't an elegant solution imo, but I imagined a possible solution like this. The most ideal solution would be for the database to filter out redundant search results for me automatically.\n\nI can explain or clarify the problem further if that's helpful. Thank you for any help or suggestions with this problem!!", "author_fullname": "t2_179w2x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Literature review - how to filter out redundant search results from similar search iterations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122cs7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679810401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve got sort of an unusual research question. Basically, I&amp;#39;d like to perform a comprehensive review of all the literature of a particular topic. To do this, I&amp;#39;d like to use combinations of search terms. For example, I&amp;#39;d conduct a search using terms &amp;quot;A&amp;quot; and &amp;quot;B&amp;quot;, then I&amp;#39;d conduct another search using terms &amp;quot;A&amp;quot; and &amp;quot;C&amp;quot;, then again using &amp;quot;A&amp;quot; and &amp;quot;D&amp;quot;, etc. The problem with this is that there&amp;#39;s a decent amount of overlap of search results among these different combinations and there are thousands of search results for each combination so I want to minimize redundancy as much as possible in order to save time. Is there a way for me to conduct an initial search (e.g., A + B) and then conduct each subsequent search (A + C, A + D, etc.) that will only show search results that are NOT included in the initial A + B search?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using OVID Medline as the search database, but I&amp;#39;d be open to any general workaround solutions as well. From my limited knowledge on a possible solution, I was wondering if it&amp;#39;s possible to export all the search results, copy them as a list into a column within Excel, and then use the Excel function that can highlight duplicate values. This method would allow me to avoid redundant search results from each search iteration. This isn&amp;#39;t an elegant solution imo, but I imagined a possible solution like this. The most ideal solution would be for the database to filter out redundant search results for me automatically.&lt;/p&gt;\n\n&lt;p&gt;I can explain or clarify the problem further if that&amp;#39;s helpful. Thank you for any help or suggestions with this problem!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "122cs7c", "is_robot_indexable": true, "report_reasons": null, "author": "pantaloonsss", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/122cs7c/literature_review_how_to_filter_out_redundant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/122cs7c/literature_review_how_to_filter_out_redundant/", "subreddit_subscribers": 94418, "created_utc": 1679810401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering if there was a way to go above and beyond in regards do dbt documentation , without buying a software. Ie taking their free html output and making it a lot better ?", "author_fullname": "t2_xo4dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone ever use dbt code and generate docs https://docs.getdbt.com/reference/commands/cmd-docs ; and instead of hosting the documentation website on your own per many blogs \u2014 you every create your own cool webpage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1223bq2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679787113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if there was a way to go above and beyond in regards do dbt documentation , without buying a software. Ie taking their free html output and making it a lot better ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1223bq2", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1223bq2/anyone_ever_use_dbt_code_and_generate_docs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1223bq2/anyone_ever_use_dbt_code_and_generate_docs/", "subreddit_subscribers": 94418, "created_utc": 1679787113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have experience in AWS, python, spark, SQL. \nUnable to get through ATS. Can anyone help me build a great resume and guide me to crack amazing jobs in india. I\u2019m open to 1 on 1 connect.", "author_fullname": "t2_gazacuar", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want 20LPA package in india.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122e98r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679814933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have experience in AWS, python, spark, SQL. \nUnable to get through ATS. Can anyone help me build a great resume and guide me to crack amazing jobs in india. I\u2019m open to 1 on 1 connect.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "122e98r", "is_robot_indexable": true, "report_reasons": null, "author": "Last-Storage-4861", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/122e98r/i_want_20lpa_package_in_india/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/122e98r/i_want_20lpa_package_in_india/", "subreddit_subscribers": 94418, "created_utc": 1679814933.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}