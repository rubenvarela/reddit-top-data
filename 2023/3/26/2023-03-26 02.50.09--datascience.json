{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve got a job with an awesome company, working on projects I feel good about, but holy smokes I hate dealing with the shitshow that is my teams data entry practices from pre-2021. I spend a solid half of my time figuring out where the hell the inconsistencies are, then the other half figuring out what values to sub in for the inconsistencies to make the data workable. At that point how useful is the data, how trustworthy? Not useful or trustworthy! The teams leads wonder why I\u2019m giving them models that explain nothing, and then act surprised when I tell them it\u2019s because the old data was essentially made up. The circle is endless. This isn\u2019t science, this is like working at a daycare cleaning up after toddlers, and when the parents pick them up they complain about the child\u2019s poor behavior.", "author_fullname": "t2_59l9bdfsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gotta vent about wasted time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121r76h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 124, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 124, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679762068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve got a job with an awesome company, working on projects I feel good about, but holy smokes I hate dealing with the shitshow that is my teams data entry practices from pre-2021. I spend a solid half of my time figuring out where the hell the inconsistencies are, then the other half figuring out what values to sub in for the inconsistencies to make the data workable. At that point how useful is the data, how trustworthy? Not useful or trustworthy! The teams leads wonder why I\u2019m giving them models that explain nothing, and then act surprised when I tell them it\u2019s because the old data was essentially made up. The circle is endless. This isn\u2019t science, this is like working at a daycare cleaning up after toddlers, and when the parents pick them up they complain about the child\u2019s poor behavior.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121r76h", "is_robot_indexable": true, "report_reasons": null, "author": "ExtraSpecialCake", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121r76h/gotta_vent_about_wasted_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121r76h/gotta_vent_about_wasted_time/", "subreddit_subscribers": 862567, "created_utc": 1679762068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "When building a multiple linear regression through Python, to select my independent variables, i was thinking of building a heat map to determine correlation using pearson. Does anyone have a better alternative?", "author_fullname": "t2_a24n8dl6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Asking for a friend \ud83d\ude02", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121pizx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679758423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When building a multiple linear regression through Python, to select my independent variables, i was thinking of building a heat map to determine correlation using pearson. Does anyone have a better alternative?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121pizx", "is_robot_indexable": true, "report_reasons": null, "author": "MiamiTaco627", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121pizx/asking_for_a_friend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121pizx/asking_for_a_friend/", "subreddit_subscribers": 862567, "created_utc": 1679758423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Been tasked with simulating the flow of ~150 parts inventory thru our process where there are two primary flows of planned demand (shipments scheduled to customers) and unplanned demand (units in the field that have failed and need replaced ASAP).  These are parts of capital equipment worth $100k+.  \n\nHave some limited historical data to support this and gain some insight on failures and future demand forecasting. \n\nMy idea was to generate hundreds of simulation runs using assumed distributions for demand and failure and the appropriate build and shipping times.  \n\nBut after generating these simulations be able to optimize across all the runs to see what level of inventory or build schedule satisfies most simulated conditions.  And what would be the most effective strategy computationally. \n\nMy initial thoughts sim thru time and store each day for each run in a matrix where the rows are diff days into the future and each column is a diff simulation.  Then have one matrix for planned demand, one for unplanned another matrix which is the cost of each component type.  Then use scalar changes in inventory levels across the matrices for the optimization component.  \n\nAnyone done something similar or have a better approach?", "author_fullname": "t2_2lc8bvy9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inventory Simulation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121b9ag", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679720358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been tasked with simulating the flow of ~150 parts inventory thru our process where there are two primary flows of planned demand (shipments scheduled to customers) and unplanned demand (units in the field that have failed and need replaced ASAP).  These are parts of capital equipment worth $100k+.  &lt;/p&gt;\n\n&lt;p&gt;Have some limited historical data to support this and gain some insight on failures and future demand forecasting. &lt;/p&gt;\n\n&lt;p&gt;My idea was to generate hundreds of simulation runs using assumed distributions for demand and failure and the appropriate build and shipping times.  &lt;/p&gt;\n\n&lt;p&gt;But after generating these simulations be able to optimize across all the runs to see what level of inventory or build schedule satisfies most simulated conditions.  And what would be the most effective strategy computationally. &lt;/p&gt;\n\n&lt;p&gt;My initial thoughts sim thru time and store each day for each run in a matrix where the rows are diff days into the future and each column is a diff simulation.  Then have one matrix for planned demand, one for unplanned another matrix which is the cost of each component type.  Then use scalar changes in inventory levels across the matrices for the optimization component.  &lt;/p&gt;\n\n&lt;p&gt;Anyone done something similar or have a better approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121b9ag", "is_robot_indexable": true, "report_reasons": null, "author": "jimtoberfest", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121b9ag/inventory_simulation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121b9ag/inventory_simulation/", "subreddit_subscribers": 862567, "created_utc": 1679720358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_j6enx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some of the best twitter accounts you follow for data science news and articles.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121gvv0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679737541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121gvv0", "is_robot_indexable": true, "report_reasons": null, "author": "amcelvanna783", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121gvv0/what_are_some_of_the_best_twitter_accounts_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121gvv0/what_are_some_of_the_best_twitter_accounts_you/", "subreddit_subscribers": 862567, "created_utc": 1679737541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello\n\nI have refrained from outlining my specific example, and am asking this in a more general way. However, for some background, I am  implementing a basic Logistic Regression algorithm in R using glm. I am an Intensive Care physician, so am a keen amateur rather than operating on a professional level.\n\nI ran in to the warning:\n\n`Warning: glm.fit: algorithm did not converge`\n`Warning: fitted probabilities numerically 0 or 1 occurred`\n\nI know this most commonly means that one of my predictors is acting perfectly to allow the algorithm to classify the outcome. However, after examining the variables I couldn't figure which. I created multiple separate logistic regression model-fits (one with each individual predictor variable) to see if I could locate it that way. I could not.\n\nSo, my question is: Can you get this warning with many variables reaching some sort of 'critical mass' that means the algorithm doesn't work? I am unsure why I am getting the warning as no individual predictor seems to be to blame.", "author_fullname": "t2_8v5hm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A general question about an problem with Logistic Regression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121wirk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679772756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I have refrained from outlining my specific example, and am asking this in a more general way. However, for some background, I am  implementing a basic Logistic Regression algorithm in R using glm. I am an Intensive Care physician, so am a keen amateur rather than operating on a professional level.&lt;/p&gt;\n\n&lt;p&gt;I ran in to the warning:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Warning: glm.fit: algorithm did not converge&lt;/code&gt;\n&lt;code&gt;Warning: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I know this most commonly means that one of my predictors is acting perfectly to allow the algorithm to classify the outcome. However, after examining the variables I couldn&amp;#39;t figure which. I created multiple separate logistic regression model-fits (one with each individual predictor variable) to see if I could locate it that way. I could not.&lt;/p&gt;\n\n&lt;p&gt;So, my question is: Can you get this warning with many variables reaching some sort of &amp;#39;critical mass&amp;#39; that means the algorithm doesn&amp;#39;t work? I am unsure why I am getting the warning as no individual predictor seems to be to blame.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121wirk", "is_robot_indexable": true, "report_reasons": null, "author": "e05bf027", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121wirk/a_general_question_about_an_problem_with_logistic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121wirk/a_general_question_about_an_problem_with_logistic/", "subreddit_subscribers": 862567, "created_utc": 1679772756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, see a lot of posts on here about technical courses. As much as I like the technical side of things, I appreciate I will probably never be able to implement technical components. I would love to know more about data science applications, when to use (or not use) them?\n\nAny ideas ? Thank you", "author_fullname": "t2_a7nec5bo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good courses on machine learning but at a managerial level?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121d5wv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679725715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, see a lot of posts on here about technical courses. As much as I like the technical side of things, I appreciate I will probably never be able to implement technical components. I would love to know more about data science applications, when to use (or not use) them?&lt;/p&gt;\n\n&lt;p&gt;Any ideas ? Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121d5wv", "is_robot_indexable": true, "report_reasons": null, "author": "TheCumCopter", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121d5wv/any_good_courses_on_machine_learning_but_at_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121d5wv/any_good_courses_on_machine_learning_but_at_a/", "subreddit_subscribers": 862567, "created_utc": 1679725715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nI have an interesting problem I thought you guys might be able to help with. I found a couple of similar questions on SO, but the methods mentioned there don't seem to work very well with my data. \n\nI have survey data of compensation percentiles for a certain job, but it only gives me the 10th, 25th, median, 75th, and 90th percentiles. No min/max, SD, mean, etc... \n\nI want to be able to compare any salary with the data and determine its percentile rank, despite the fact I do not know the upper/lower bounds. \n\nExample:\n10th - 184,000\n25th - 222,000\nMedian - 264,000\n75th - 323,000\n90th - 398,000\n\nInputs: 450,000, 100,000, 300,000, etc... \nPercentile ranks: ??? \n\nI heard from a colleague that he has seen this done before, but doesn't know how to do it himself. Ideally, I will be able to solve this in Excel or with VBA. \n\nAny ideas would be appreciated!", "author_fullname": "t2_aj5mt11i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to Calculate the Percentile Rank of a Salary Based on Incomplete Survey Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121w2lv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679771823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I have an interesting problem I thought you guys might be able to help with. I found a couple of similar questions on SO, but the methods mentioned there don&amp;#39;t seem to work very well with my data. &lt;/p&gt;\n\n&lt;p&gt;I have survey data of compensation percentiles for a certain job, but it only gives me the 10th, 25th, median, 75th, and 90th percentiles. No min/max, SD, mean, etc... &lt;/p&gt;\n\n&lt;p&gt;I want to be able to compare any salary with the data and determine its percentile rank, despite the fact I do not know the upper/lower bounds. &lt;/p&gt;\n\n&lt;p&gt;Example:\n10th - 184,000\n25th - 222,000\nMedian - 264,000\n75th - 323,000\n90th - 398,000&lt;/p&gt;\n\n&lt;p&gt;Inputs: 450,000, 100,000, 300,000, etc... \nPercentile ranks: ??? &lt;/p&gt;\n\n&lt;p&gt;I heard from a colleague that he has seen this done before, but doesn&amp;#39;t know how to do it himself. Ideally, I will be able to solve this in Excel or with VBA. &lt;/p&gt;\n\n&lt;p&gt;Any ideas would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121w2lv", "is_robot_indexable": true, "report_reasons": null, "author": "EMoneymaker99", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121w2lv/trying_to_calculate_the_percentile_rank_of_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121w2lv/trying_to_calculate_the_percentile_rank_of_a/", "subreddit_subscribers": 862567, "created_utc": 1679771823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a building a dashboard using Streamlit, Plotly Ex, python, pandas, numpy, scikit-learn, etc. The goal is to allow the audience to interactively explore the data. Different people will be using the dashboard and their data would be different, but they all have some commonalities. I want to build a dashboard that is \"generic\" enough to be applicable for most situations.\n\nThe data can contain categorical and numeric data, but mostly categorical. They all deal with consumer studies. The column of interest is typically some kind of categorical preference question (do you prefer A or B), or (would you buy this or no). There would be some demographic info of the consumer, some numeric columns that might pertain to ratings of a certain feature, and some categorical responses (multiple choice questions). \n\nThere are usually many features compared to number of observations. (ie, you might have only 100 consumers, but you asked them a bunch of questions)\n\nI think pair plots can be useful. Maybe give the user to ability to select some variables of interest, and run pair plots on them. \n\nI also plan to give the user the ability to group by variables and do various aggregations. \n\nSome filtering capability would also be built. (say, only want to see those with age &gt; 30)\n\nI'm also thinking running a random forest on the column of interest (such as preference) and determining the most important features. (not sure if this is a good idea given that the data typically have many features and not enough observations. A typical dataset might have 100 rows, and 40 columns) \n\nI don't plan on doing anything too specific (like stat tests) as those requires an understanding of what they are trying to do. Just want to build something automated and is general enough, and has enough functionalities to be useful, without doing \"too much\" that can cause issues.\n\nPlease advise on what you would or would not include.\n\nThanks", "author_fullname": "t2_6fty441r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a generic interactive business dashboard, what would you include?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121s7p2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a building a dashboard using Streamlit, Plotly Ex, python, pandas, numpy, scikit-learn, etc. The goal is to allow the audience to interactively explore the data. Different people will be using the dashboard and their data would be different, but they all have some commonalities. I want to build a dashboard that is &amp;quot;generic&amp;quot; enough to be applicable for most situations.&lt;/p&gt;\n\n&lt;p&gt;The data can contain categorical and numeric data, but mostly categorical. They all deal with consumer studies. The column of interest is typically some kind of categorical preference question (do you prefer A or B), or (would you buy this or no). There would be some demographic info of the consumer, some numeric columns that might pertain to ratings of a certain feature, and some categorical responses (multiple choice questions). &lt;/p&gt;\n\n&lt;p&gt;There are usually many features compared to number of observations. (ie, you might have only 100 consumers, but you asked them a bunch of questions)&lt;/p&gt;\n\n&lt;p&gt;I think pair plots can be useful. Maybe give the user to ability to select some variables of interest, and run pair plots on them. &lt;/p&gt;\n\n&lt;p&gt;I also plan to give the user the ability to group by variables and do various aggregations. &lt;/p&gt;\n\n&lt;p&gt;Some filtering capability would also be built. (say, only want to see those with age &amp;gt; 30)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also thinking running a random forest on the column of interest (such as preference) and determining the most important features. (not sure if this is a good idea given that the data typically have many features and not enough observations. A typical dataset might have 100 rows, and 40 columns) &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t plan on doing anything too specific (like stat tests) as those requires an understanding of what they are trying to do. Just want to build something automated and is general enough, and has enough functionalities to be useful, without doing &amp;quot;too much&amp;quot; that can cause issues.&lt;/p&gt;\n\n&lt;p&gt;Please advise on what you would or would not include.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121s7p2", "is_robot_indexable": true, "report_reasons": null, "author": "engineheat2", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121s7p2/building_a_generic_interactive_business_dashboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121s7p2/building_a_generic_interactive_business_dashboard/", "subreddit_subscribers": 862567, "created_utc": 1679764140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am confident I am getting  a job offer for company A later this week. I have been interning at a different place, company B, and was told they would consider sending me an offer in a few weeks from now.\n\nI am wondering if I should go to company B now and ask for them to send me an offer now or if I should wait for company A to officially give me an offer first. From what I read online I would only have a few days to a week to make a decision once I am given an offer, so I want to try and make the offers overlap as much as possible.", "author_fullname": "t2_48dmgjj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job offer timeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12279ow", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679795270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am confident I am getting  a job offer for company A later this week. I have been interning at a different place, company B, and was told they would consider sending me an offer in a few weeks from now.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if I should go to company B now and ask for them to send me an offer now or if I should wait for company A to officially give me an offer first. From what I read online I would only have a few days to a week to make a decision once I am given an offer, so I want to try and make the offers overlap as much as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12279ow", "is_robot_indexable": true, "report_reasons": null, "author": "loumani06", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12279ow/job_offer_timeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12279ow/job_offer_timeline/", "subreddit_subscribers": 862567, "created_utc": 1679795270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI am still a student so I'd like some tips and some ideas or directions I could take. **I am not asking you to do this for me, I just want some ideas. How would you approach this problem?**\n\n# More about the dataset:\n\nThe Y labels are fairly straight forward. Int values between 1 and 4, three samples for each. The X values vary between 0 and very large numbers, sometimes 10\\^18. So we are talking about a dataset with 12 samples, each containing widely variating values for 15000 dimensions. Much of these dimensions do not change too much between one sample and the other: we need to do feature selection.\n\nI know for sure that the dataset has logic, because of how this dataset was obtained. It's from a published paper from a bio lab experiment, the details are not important right now.\n\n# What I have tried so far:\n\n* Pipeline 1: first a PCA, with number of components between 1 and 11. Then, a sklearn [Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html)(norm = 'max'). This is a unit norm normalizer, using the max value as the norm. And then, a SVR with Linear Kernel, and C variating between 0.0001 and 100000.\n\n&amp;#x200B;\n\n    pipe\u00a0=\u00a0make_pipeline(PCA(n_components\u00a0=\u00a0n_dimensions),\u00a0Normalizer(norm='max'),\u00a0SVR(kernel='linear',\u00a0C=c))\n\n&amp;#x200B;\n\n* Pipeline 2: first, I do feature selection with a DecisionTreeRegressor. This outputs 3 features (which I find weird, shouldn't it be 4 I guess?), since I only have 11 samples. Then I normalize the features selected with the Normalizer(norm = 'max') again, just like pipeline1. Then I use a SVR again with Linear Kernel, with C between 0.0001 and 100000.\n\n&amp;#x200B;\n\n    pipe = make_pipeline(SelectFromModel(DecisionTreeRegressor(min_samples_split=1, min_samples_leaf=0.000000001)), Normalizer(norm='max'), SVR(kernel='linear', C=c))\n\nSo all that changes between pipeline 1 and 2 is what I use to reduce the number of dimensions in the problem: one is a PCA, the other is a DecisionTreeRegressor.\n\n# My results:\n\nI am using a Leave One Out test. So I fit for 11 and then test for 1, for each sample.\n\nFor both pipelines, my regressor simply predicts a more or less average value for every sample. It doesn't even try to predict anything, it just guesses in the middle, somewhere between 2 and 3.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nMaybe a SVR is simply not suited for this problem? But I don't think I can train a neural network for this, since I only have 12 samples.\n\nWhat else could I try? Should I invest time in trying new regressors, or is the SVR enough and my problem is actually the feature selector? Or maybe I am messing up the normalization.\n\nAny 2 cents welcome.", "author_fullname": "t2_orxwarav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need some tips and directions on how to approach a regression problem with a very challenging dataset (12 samples, ~15000 dimensions). Give me your 2 cents", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1226752", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679793441.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679793148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am still a student so I&amp;#39;d like some tips and some ideas or directions I could take. &lt;strong&gt;I am not asking you to do this for me, I just want some ideas. How would you approach this problem?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;More about the dataset:&lt;/h1&gt;\n\n&lt;p&gt;The Y labels are fairly straight forward. Int values between 1 and 4, three samples for each. The X values vary between 0 and very large numbers, sometimes 10^18. So we are talking about a dataset with 12 samples, each containing widely variating values for 15000 dimensions. Much of these dimensions do not change too much between one sample and the other: we need to do feature selection.&lt;/p&gt;\n\n&lt;p&gt;I know for sure that the dataset has logic, because of how this dataset was obtained. It&amp;#39;s from a published paper from a bio lab experiment, the details are not important right now.&lt;/p&gt;\n\n&lt;h1&gt;What I have tried so far:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pipeline 1: first a PCA, with number of components between 1 and 11. Then, a sklearn &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\"&gt;Normalizer&lt;/a&gt;(norm = &amp;#39;max&amp;#39;). This is a unit norm normalizer, using the max value as the norm. And then, a SVR with Linear Kernel, and C variating between 0.0001 and 100000.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipe\u00a0=\u00a0make_pipeline(PCA(n_components\u00a0=\u00a0n_dimensions),\u00a0Normalizer(norm=&amp;#39;max&amp;#39;),\u00a0SVR(kernel=&amp;#39;linear&amp;#39;,\u00a0C=c))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pipeline 2: first, I do feature selection with a DecisionTreeRegressor. This outputs 3 features (which I find weird, shouldn&amp;#39;t it be 4 I guess?), since I only have 11 samples. Then I normalize the features selected with the Normalizer(norm = &amp;#39;max&amp;#39;) again, just like pipeline1. Then I use a SVR again with Linear Kernel, with C between 0.0001 and 100000.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipe = make_pipeline(SelectFromModel(DecisionTreeRegressor(min_samples_split=1, min_samples_leaf=0.000000001)), Normalizer(norm=&amp;#39;max&amp;#39;), SVR(kernel=&amp;#39;linear&amp;#39;, C=c))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So all that changes between pipeline 1 and 2 is what I use to reduce the number of dimensions in the problem: one is a PCA, the other is a DecisionTreeRegressor.&lt;/p&gt;\n\n&lt;h1&gt;My results:&lt;/h1&gt;\n\n&lt;p&gt;I am using a Leave One Out test. So I fit for 11 and then test for 1, for each sample.&lt;/p&gt;\n\n&lt;p&gt;For both pipelines, my regressor simply predicts a more or less average value for every sample. It doesn&amp;#39;t even try to predict anything, it just guesses in the middle, somewhere between 2 and 3.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Maybe a SVR is simply not suited for this problem? But I don&amp;#39;t think I can train a neural network for this, since I only have 12 samples.&lt;/p&gt;\n\n&lt;p&gt;What else could I try? Should I invest time in trying new regressors, or is the SVR enough and my problem is actually the feature selector? Or maybe I am messing up the normalization.&lt;/p&gt;\n\n&lt;p&gt;Any 2 cents welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1226752", "is_robot_indexable": true, "report_reasons": null, "author": "perguntando", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1226752/i_need_some_tips_and_directions_on_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1226752/i_need_some_tips_and_directions_on_how_to/", "subreddit_subscribers": 862567, "created_utc": 1679793148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I used to not mind doing take home assignments because they were a way to build out my GitHub profile. \n\nHowever, I have two kids now and would dread if every interview had an 8 hour take home assignment. \n\nHow long of an assignment is acceptable? Are longer assignments a form of discrimination against people with kids and therefore less spare time? Have you ever refused to take an assignment?", "author_fullname": "t2_f7akb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long of a take home assignment is acceptable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1223z2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679788475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to not mind doing take home assignments because they were a way to build out my GitHub profile. &lt;/p&gt;\n\n&lt;p&gt;However, I have two kids now and would dread if every interview had an 8 hour take home assignment. &lt;/p&gt;\n\n&lt;p&gt;How long of an assignment is acceptable? Are longer assignments a form of discrimination against people with kids and therefore less spare time? Have you ever refused to take an assignment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1223z2m", "is_robot_indexable": true, "report_reasons": null, "author": "Dezireless", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1223z2m/how_long_of_a_take_home_assignment_is_acceptable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1223z2m/how_long_of_a_take_home_assignment_is_acceptable/", "subreddit_subscribers": 862567, "created_utc": 1679788475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For teams responsible for developing, testing and maintaining product recommendation models for e-commerce companies;\n\n~How large is your team?\n~What percent of e-commerce revenue would you attribute to product recommendations?", "author_fullname": "t2_7v3tv87q8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product recommendation teams question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121vs0z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679771212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For teams responsible for developing, testing and maintaining product recommendation models for e-commerce companies;&lt;/p&gt;\n\n&lt;p&gt;~How large is your team?\n~What percent of e-commerce revenue would you attribute to product recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121vs0z", "is_robot_indexable": true, "report_reasons": null, "author": "ilovedataaa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121vs0z/product_recommendation_teams_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121vs0z/product_recommendation_teams_question/", "subreddit_subscribers": 862567, "created_utc": 1679771212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a data set that covers internet data usage of several hundred users over several month period. What is the best approach to analyze? My goal is to retain these customers or upsell. Any suggestions?", "author_fullname": "t2_bco8idf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best approach to analyze a simple data set?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121v2ol", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679769782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data set that covers internet data usage of several hundred users over several month period. What is the best approach to analyze? My goal is to retain these customers or upsell. Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121v2ol", "is_robot_indexable": true, "report_reasons": null, "author": "Environmental_Wind40", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121v2ol/what_is_the_best_approach_to_analyze_a_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121v2ol/what_is_the_best_approach_to_analyze_a_simple/", "subreddit_subscribers": 862567, "created_utc": 1679769782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am pursuing my undergrad in statistics specialization in Data Science and Machine Learning at uni. One of the best uni in my country. It's just that there is a recession here and so much competition. I am struggling so much to get an internship. I have had emails from employers telling me that your profile is all that we want yet we can't hire you and write a better email that depicts your interest in our organization than when I did that they said we will let you know but right now not hiring interns. A few gave interviews but no luck. Can somebody please refer me for a good job? Also, any technical interview advice/resources are welcomed. Please help.\n\n&amp;#x200B;\n\nr/dataisbeautiful r/datascience r/DataScienceJobs", "author_fullname": "t2_1ttkuwgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling to get my first internship in Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1221l32", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679783435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am pursuing my undergrad in statistics specialization in Data Science and Machine Learning at uni. One of the best uni in my country. It&amp;#39;s just that there is a recession here and so much competition. I am struggling so much to get an internship. I have had emails from employers telling me that your profile is all that we want yet we can&amp;#39;t hire you and write a better email that depicts your interest in our organization than when I did that they said we will let you know but right now not hiring interns. A few gave interviews but no luck. Can somebody please refer me for a good job? Also, any technical interview advice/resources are welcomed. Please help.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/r/dataisbeautiful\"&gt;r/dataisbeautiful&lt;/a&gt; &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt; &lt;a href=\"/r/DataScienceJobs\"&gt;r/DataScienceJobs&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1221l32", "is_robot_indexable": true, "report_reasons": null, "author": "Frosty_Rate5985", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1221l32/struggling_to_get_my_first_internship_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1221l32/struggling_to_get_my_first_internship_in_data/", "subreddit_subscribers": 862567, "created_utc": 1679783435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been given time series of N stock prices and time series of K sectoral indices. (Sectoral index is index made from all stocks belonging to that sector.)\nI have not been given names of any of those stocks or indices.\nI want to know which stocks belong to the same sector and sectoral indices.\nI am able to cluster the stocks. But how should I decide which sectoral index time series they correspond to? Should I just cluster stocks and indices together (N+K timeseries) instead of clustering N stocks independently? Will that be correct approach?", "author_fullname": "t2_aaamk24y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Determining clusters of time series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121k7ei", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679746800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been given time series of N stock prices and time series of K sectoral indices. (Sectoral index is index made from all stocks belonging to that sector.)\nI have not been given names of any of those stocks or indices.\nI want to know which stocks belong to the same sector and sectoral indices.\nI am able to cluster the stocks. But how should I decide which sectoral index time series they correspond to? Should I just cluster stocks and indices together (N+K timeseries) instead of clustering N stocks independently? Will that be correct approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121k7ei", "is_robot_indexable": true, "report_reasons": null, "author": "SnooHabits4550", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121k7ei/determining_clusters_of_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121k7ei/determining_clusters_of_time_series/", "subreddit_subscribers": 862567, "created_utc": 1679746800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_sppgx30r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Machine Learning with Python, Getting Started for Data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_121wrga", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7DC5P14iKM8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/121wrga", "height": 200}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TwPjHFympHbiLuuOgGyFADIEsXBoKkGF85TdyaZ-HcQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679773252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/7DC5P14iKM8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?auto=webp&amp;v=enabled&amp;s=2fa34b646cffa3bf70691a05860c888e9fd2449d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0aa8277ab41e4659d1f2641b7da280e0b10cd16b", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1effaa95d1777ee8642cc9a2dbd6bcfcf18d607", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5166cadbfa3780b1e376d3de0770ca8d0be27da5", "width": 320, "height": 240}], "variants": {}, "id": "LEBKQa13ztjayQE2R1vvT6gYphFwdkPbqZeK3aNkFbs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "121wrga", "is_robot_indexable": true, "report_reasons": null, "author": "thetech_learner", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121wrga/machine_learning_with_python_getting_started_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/7DC5P14iKM8", "subreddit_subscribers": 862567, "created_utc": 1679773252.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7DC5P14iKM8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone!\n\nTL;DR\n\n\\- Looking for an intensive full-time, in-person, 3-6 month long data program in Canada to become very proficient in coding after graduating from uni this summer. \n\nI'm going to be a graduating with a [B.Sc](https://B.Sc). in Psychology this summer (In Canada) and would like to spend part-time this summer and full-time this Fall (and maybe even Winter) becoming really proficient in coding (thinking Python and SQL). I'm not into the online-learning thing (did enough of that over COVID - hard to stay focused) so I'm trying to figure out a part time online program to learn basics of coding in the summer, and then jump into an in-person intensive data analytics/data science program of some sort in the fall (I'm willing to move for it - somewhere in Canada). \n\nCurrently thinking of doing the WeCloudData data analytics bootcamp then trying to figure out what to do after that in-person (was looking into BrainStation because its the best rated program in Canada for in-person bootcamps). I don't want to pursue career as a data scientist per say, I'm just interested in developing this skillset and see it as a skill that is growing in demand - especially in the humanities. \n\nAny thoughts would be helpful!", "author_fullname": "t2_scu1jkf0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help choosing a coding bootcamp/classes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121v4l2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679769893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;TL;DR&lt;/p&gt;\n\n&lt;p&gt;- Looking for an intensive full-time, in-person, 3-6 month long data program in Canada to become very proficient in coding after graduating from uni this summer. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to be a graduating with a &lt;a href=\"https://B.Sc\"&gt;B.Sc&lt;/a&gt;. in Psychology this summer (In Canada) and would like to spend part-time this summer and full-time this Fall (and maybe even Winter) becoming really proficient in coding (thinking Python and SQL). I&amp;#39;m not into the online-learning thing (did enough of that over COVID - hard to stay focused) so I&amp;#39;m trying to figure out a part time online program to learn basics of coding in the summer, and then jump into an in-person intensive data analytics/data science program of some sort in the fall (I&amp;#39;m willing to move for it - somewhere in Canada). &lt;/p&gt;\n\n&lt;p&gt;Currently thinking of doing the WeCloudData data analytics bootcamp then trying to figure out what to do after that in-person (was looking into BrainStation because its the best rated program in Canada for in-person bootcamps). I don&amp;#39;t want to pursue career as a data scientist per say, I&amp;#39;m just interested in developing this skillset and see it as a skill that is growing in demand - especially in the humanities. &lt;/p&gt;\n\n&lt;p&gt;Any thoughts would be helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121v4l2", "is_robot_indexable": true, "report_reasons": null, "author": "LearningToCode100", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121v4l2/help_choosing_a_coding_bootcampclasses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121v4l2/help_choosing_a_coding_bootcampclasses/", "subreddit_subscribers": 862567, "created_utc": 1679769893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT-4 can solve most SQL interview questions. In 5 years, do you think Acing a SQL Interview will still be important?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121tjz7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": "", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_6zm7tswg", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "SQL", "selftext": "GPT-4 can write SQL queries and solve most easy &amp; medium SQL interview questions on DataLemur. So I'm curious, will Acing the SQL Interview still be important in 5 years? What about in 10?\n\n[View Poll](https://www.reddit.com/poll/121q7nt)", "author_fullname": "t2_6zm7tswg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT-4 can solve most SQL interview questions. In 5 years, do you think Acing a SQL Interview will still be important?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/SQL", "hidden": false, "pwls": 6, "link_flair_css_class": "h", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121q7nt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "01031502-de87-11e6-b05e-0e1ce54f6a26", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679766346.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679759953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.SQL", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;GPT-4 can write SQL queries and solve most easy &amp;amp; medium SQL interview questions on DataLemur. So I&amp;#39;m curious, will Acing the SQL Interview still be important in 5 years? What about in 10?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/121q7nt\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5cb03a64-2769-11ea-aa04-0ed301f44875", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Author of Ace the Data Science Interview \ud83d\udcd5", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qp8q", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "121q7nt", "is_robot_indexable": true, "report_reasons": null, "author": "NickSinghTechCareers", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1680364753748, "options": [{"text": "SQL Interview Will Be Obsolete", "id": "22238479"}, {"text": "SQL Interviews Might Go Away", "id": "22238480"}, {"text": "SQL Interviews Won't Go Away", "id": "22238481"}, {"text": "Show Results", "id": "22238482"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 1041, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "subreddit_subscribers": 144316, "created_utc": 1679759953.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1679766802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.SQL", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Author | Ace the Data Science Interview", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121tjz7", "is_robot_indexable": true, "report_reasons": null, "author": "NickSinghTechCareers", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_121q7nt", "author_flair_text_color": "dark", "permalink": "/r/datascience/comments/121tjz7/gpt4_can_solve_most_sql_interview_questions_in_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "subreddit_subscribers": 862567, "created_utc": 1679766802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "If you're a beginner in data analysis and looking for a practical way to apply your skills, I've got just the thing for you!\n\nDiscover the fascinating changes in Bangladesh's population and demographics over the past 70 years with my comprehensive analysis. From total population to life expectancy, my notebook covers the key indicators that you need to know. \n\nWhether you're a data enthusiast or just curious about Bangladesh, click here to explore the latest insights and trends today!\n\n[https://www.kaggle.com/code/abmsayem/demographic-shifts-in-bangladesh-1950-to-2020](https://www.kaggle.com/code/abmsayem/demographic-shifts-in-bangladesh-1950-to-2020)", "author_fullname": "t2_4oeq7ean1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Comprehensive Data Analysis on Bangladesh's Demographics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121m112", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679751278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re a beginner in data analysis and looking for a practical way to apply your skills, I&amp;#39;ve got just the thing for you!&lt;/p&gt;\n\n&lt;p&gt;Discover the fascinating changes in Bangladesh&amp;#39;s population and demographics over the past 70 years with my comprehensive analysis. From total population to life expectancy, my notebook covers the key indicators that you need to know. &lt;/p&gt;\n\n&lt;p&gt;Whether you&amp;#39;re a data enthusiast or just curious about Bangladesh, click here to explore the latest insights and trends today!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.kaggle.com/code/abmsayem/demographic-shifts-in-bangladesh-1950-to-2020\"&gt;https://www.kaggle.com/code/abmsayem/demographic-shifts-in-bangladesh-1950-to-2020&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ByZNzGIHn-EKXnyaT9e72zSDkK8l8fyiWtVmfxlp1yQ.jpg?auto=webp&amp;v=enabled&amp;s=1d3918344fcec915a145ffaf7839c2373c35282a", "width": 100, "height": 100}, "resolutions": [], "variants": {}, "id": "YtZsnKA_8wRxYy4xhxvjCtVCxiEdcyIPpYXLtQTk4FY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121m112", "is_robot_indexable": true, "report_reasons": null, "author": "abmsayem", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121m112/a_comprehensive_data_analysis_on_bangladeshs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121m112/a_comprehensive_data_analysis_on_bangladeshs/", "subreddit_subscribers": 862567, "created_utc": 1679751278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_8drjhm8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dataset required for list of professional IT, AI, Ml , Cloud certifications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121z1rc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679778128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121z1rc", "is_robot_indexable": true, "report_reasons": null, "author": "Helix-x", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121z1rc/dataset_required_for_list_of_professional_it_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121z1rc/dataset_required_for_list_of_professional_it_ai/", "subreddit_subscribers": 862567, "created_utc": 1679778128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_iv9jljrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone has a database of italian xml invoices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121d0df", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679725251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121d0df", "is_robot_indexable": true, "report_reasons": null, "author": "bbmgrcmlgmo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121d0df/does_anyone_has_a_database_of_italian_xml_invoices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121d0df/does_anyone_has_a_database_of_italian_xml_invoices/", "subreddit_subscribers": 862567, "created_utc": 1679725251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The software industry is kinda shot rn and I want a job, lately I've realised I actually kinda like math so I'd like something that would help me improve my skills in that regard. I've got 5 years of experience doing software, and I've done a sliver of data science in my previous role.\n\nWhat kind of demand is there for data science? What's it like day to day? What kind of problems do you work on?", "author_fullname": "t2_w8o1jpct", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching to data science from software engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1217nam", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679711322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The software industry is kinda shot rn and I want a job, lately I&amp;#39;ve realised I actually kinda like math so I&amp;#39;d like something that would help me improve my skills in that regard. I&amp;#39;ve got 5 years of experience doing software, and I&amp;#39;ve done a sliver of data science in my previous role.&lt;/p&gt;\n\n&lt;p&gt;What kind of demand is there for data science? What&amp;#39;s it like day to day? What kind of problems do you work on?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1217nam", "is_robot_indexable": true, "report_reasons": null, "author": "Optimal-Blueberry137", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1217nam/switching_to_data_science_from_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1217nam/switching_to_data_science_from_software/", "subreddit_subscribers": 862567, "created_utc": 1679711322.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}