{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm studying Machine Learning, and I came across several mathematical formulas and I would like to understand them, not just learn how to use and apply the formulas.\n\nfor example, when I see an \" e (Euler)\" in the formula, where did Euler come from? why did he put it in the formula?\n\nWhen I search about e (euler) on the internet, I usually only find content explaining how to apply and solve equations with (e).\n\nHow can I begin to understand more about math so that I understand more, in the same way that I see an English sentence and understand its meaning?", "author_fullname": "t2_igf9n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn the \"Math language\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122da21", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 100, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 100, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679811912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m studying Machine Learning, and I came across several mathematical formulas and I would like to understand them, not just learn how to use and apply the formulas.&lt;/p&gt;\n\n&lt;p&gt;for example, when I see an &amp;quot; e (Euler)&amp;quot; in the formula, where did Euler come from? why did he put it in the formula?&lt;/p&gt;\n\n&lt;p&gt;When I search about e (euler) on the internet, I usually only find content explaining how to apply and solve equations with (e).&lt;/p&gt;\n\n&lt;p&gt;How can I begin to understand more about math so that I understand more, in the same way that I see an English sentence and understand its meaning?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122da21", "is_robot_indexable": true, "report_reasons": null, "author": "rvitor", "discussion_type": null, "num_comments": 74, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122da21/how_to_learn_the_math_language/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122da21/how_to_learn_the_math_language/", "subreddit_subscribers": 862896, "created_utc": 1679811912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I used to not mind doing take home assignments because they were a way to build out my GitHub profile. \n\nHowever, I have two kids now and would dread if every interview had an 8 hour take home assignment. \n\nHow long of an assignment is acceptable? Are longer assignments a form of discrimination against people with kids and therefore less spare time? Have you ever refused to take an assignment?", "author_fullname": "t2_f7akb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long of a take home assignment is acceptable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1223z2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679788475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to not mind doing take home assignments because they were a way to build out my GitHub profile. &lt;/p&gt;\n\n&lt;p&gt;However, I have two kids now and would dread if every interview had an 8 hour take home assignment. &lt;/p&gt;\n\n&lt;p&gt;How long of an assignment is acceptable? Are longer assignments a form of discrimination against people with kids and therefore less spare time? Have you ever refused to take an assignment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1223z2m", "is_robot_indexable": true, "report_reasons": null, "author": "Dezireless", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1223z2m/how_long_of_a_take_home_assignment_is_acceptable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1223z2m/how_long_of_a_take_home_assignment_is_acceptable/", "subreddit_subscribers": 862896, "created_utc": 1679788475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI am still a student so I'd like some tips and some ideas or directions I could take. **I am not asking you to do this for me, I just want some ideas. How would you approach this problem?**\n\n# More about the dataset:\n\nThe Y labels are fairly straight forward. Int values between 1 and 4, three samples for each. The X values vary between 0 and very large numbers, sometimes 10\\^18. So we are talking about a dataset with 12 samples, each containing widely variating values for 15000 dimensions. Much of these dimensions do not change too much between one sample and the other: we need to do feature selection.\n\nI know for sure that the dataset has logic, because of how this dataset was obtained. It's from a published paper from a bio lab experiment, the details are not important right now.\n\n# What I have tried so far:\n\n* Pipeline 1: first a PCA, with number of components between 1 and 11. Then, a sklearn [Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html)(norm = 'max'). This is a unit norm normalizer, using the max value as the norm. And then, a SVR with Linear Kernel, and C variating between 0.0001 and 100000.\n\n&amp;#x200B;\n\n    pipe\u00a0=\u00a0make_pipeline(PCA(n_components\u00a0=\u00a0n_dimensions),\u00a0Normalizer(norm='max'),\u00a0SVR(kernel='linear',\u00a0C=c))\n\n&amp;#x200B;\n\n* Pipeline 2: first, I do feature selection with a DecisionTreeRegressor. This outputs 3 features (which I find weird, shouldn't it be 4 I guess?), since I only have 11 samples. Then I normalize the features selected with the Normalizer(norm = 'max') again, just like pipeline1. Then I use a SVR again with Linear Kernel, with C between 0.0001 and 100000.\n\n&amp;#x200B;\n\n    pipe = make_pipeline(SelectFromModel(DecisionTreeRegressor(min_samples_split=1, min_samples_leaf=0.000000001)), Normalizer(norm='max'), SVR(kernel='linear', C=c))\n\nSo all that changes between pipeline 1 and 2 is what I use to reduce the number of dimensions in the problem: one is a PCA, the other is a DecisionTreeRegressor.\n\n# My results:\n\nI am using a Leave One Out test. So I fit for 11 and then test for 1, for each sample.\n\nFor both pipelines, my regressor simply predicts a more or less average value for every sample. It doesn't even try to predict anything, it just guesses in the middle, somewhere between 2 and 3.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nMaybe a SVR is simply not suited for this problem? But I don't think I can train a neural network for this, since I only have 12 samples.\n\nWhat else could I try? Should I invest time in trying new regressors, or is the SVR enough and my problem is actually the feature selector? Or maybe I am messing up the normalization.\n\nAny 2 cents welcome.", "author_fullname": "t2_orxwarav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need some tips and directions on how to approach a regression problem with a very challenging dataset (12 samples, ~15000 dimensions). Give me your 2 cents", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1226752", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679793441.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679793148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am still a student so I&amp;#39;d like some tips and some ideas or directions I could take. &lt;strong&gt;I am not asking you to do this for me, I just want some ideas. How would you approach this problem?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;More about the dataset:&lt;/h1&gt;\n\n&lt;p&gt;The Y labels are fairly straight forward. Int values between 1 and 4, three samples for each. The X values vary between 0 and very large numbers, sometimes 10^18. So we are talking about a dataset with 12 samples, each containing widely variating values for 15000 dimensions. Much of these dimensions do not change too much between one sample and the other: we need to do feature selection.&lt;/p&gt;\n\n&lt;p&gt;I know for sure that the dataset has logic, because of how this dataset was obtained. It&amp;#39;s from a published paper from a bio lab experiment, the details are not important right now.&lt;/p&gt;\n\n&lt;h1&gt;What I have tried so far:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pipeline 1: first a PCA, with number of components between 1 and 11. Then, a sklearn &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\"&gt;Normalizer&lt;/a&gt;(norm = &amp;#39;max&amp;#39;). This is a unit norm normalizer, using the max value as the norm. And then, a SVR with Linear Kernel, and C variating between 0.0001 and 100000.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipe\u00a0=\u00a0make_pipeline(PCA(n_components\u00a0=\u00a0n_dimensions),\u00a0Normalizer(norm=&amp;#39;max&amp;#39;),\u00a0SVR(kernel=&amp;#39;linear&amp;#39;,\u00a0C=c))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pipeline 2: first, I do feature selection with a DecisionTreeRegressor. This outputs 3 features (which I find weird, shouldn&amp;#39;t it be 4 I guess?), since I only have 11 samples. Then I normalize the features selected with the Normalizer(norm = &amp;#39;max&amp;#39;) again, just like pipeline1. Then I use a SVR again with Linear Kernel, with C between 0.0001 and 100000.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipe = make_pipeline(SelectFromModel(DecisionTreeRegressor(min_samples_split=1, min_samples_leaf=0.000000001)), Normalizer(norm=&amp;#39;max&amp;#39;), SVR(kernel=&amp;#39;linear&amp;#39;, C=c))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So all that changes between pipeline 1 and 2 is what I use to reduce the number of dimensions in the problem: one is a PCA, the other is a DecisionTreeRegressor.&lt;/p&gt;\n\n&lt;h1&gt;My results:&lt;/h1&gt;\n\n&lt;p&gt;I am using a Leave One Out test. So I fit for 11 and then test for 1, for each sample.&lt;/p&gt;\n\n&lt;p&gt;For both pipelines, my regressor simply predicts a more or less average value for every sample. It doesn&amp;#39;t even try to predict anything, it just guesses in the middle, somewhere between 2 and 3.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Maybe a SVR is simply not suited for this problem? But I don&amp;#39;t think I can train a neural network for this, since I only have 12 samples.&lt;/p&gt;\n\n&lt;p&gt;What else could I try? Should I invest time in trying new regressors, or is the SVR enough and my problem is actually the feature selector? Or maybe I am messing up the normalization.&lt;/p&gt;\n\n&lt;p&gt;Any 2 cents welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1226752", "is_robot_indexable": true, "report_reasons": null, "author": "perguntando", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1226752/i_need_some_tips_and_directions_on_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1226752/i_need_some_tips_and_directions_on_how_to/", "subreddit_subscribers": 862896, "created_utc": 1679793148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello\n\nI have refrained from outlining my specific example, and am asking this in a more general way. However, for some background, I am  implementing a basic Logistic Regression algorithm in R using glm. I am an Intensive Care physician, so am a keen amateur rather than operating on a professional level.\n\nI ran in to the warning:\n\n`Warning: glm.fit: algorithm did not converge`\n`Warning: fitted probabilities numerically 0 or 1 occurred`\n\nI know this most commonly means that one of my predictors is acting perfectly to allow the algorithm to classify the outcome. However, after examining the variables I couldn't figure which. I created multiple separate logistic regression model-fits (one with each individual predictor variable) to see if I could locate it that way. I could not.\n\nSo, my question is: Can you get this warning with many variables reaching some sort of 'critical mass' that means the algorithm doesn't work? I am unsure why I am getting the warning as no individual predictor seems to be to blame.", "author_fullname": "t2_8v5hm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A general question about an problem with Logistic Regression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121wirk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679772756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I have refrained from outlining my specific example, and am asking this in a more general way. However, for some background, I am  implementing a basic Logistic Regression algorithm in R using glm. I am an Intensive Care physician, so am a keen amateur rather than operating on a professional level.&lt;/p&gt;\n\n&lt;p&gt;I ran in to the warning:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Warning: glm.fit: algorithm did not converge&lt;/code&gt;\n&lt;code&gt;Warning: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I know this most commonly means that one of my predictors is acting perfectly to allow the algorithm to classify the outcome. However, after examining the variables I couldn&amp;#39;t figure which. I created multiple separate logistic regression model-fits (one with each individual predictor variable) to see if I could locate it that way. I could not.&lt;/p&gt;\n\n&lt;p&gt;So, my question is: Can you get this warning with many variables reaching some sort of &amp;#39;critical mass&amp;#39; that means the algorithm doesn&amp;#39;t work? I am unsure why I am getting the warning as no individual predictor seems to be to blame.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121wirk", "is_robot_indexable": true, "report_reasons": null, "author": "e05bf027", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121wirk/a_general_question_about_an_problem_with_logistic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121wirk/a_general_question_about_an_problem_with_logistic/", "subreddit_subscribers": 862896, "created_utc": 1679772756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This thing is like a student forced to write 1000 words on the topic. \n\n**me:**  What is a good random state value to use in general \n\n**Bot:**  \n\nIn machine learning, a random state is used to initialize the random number generator. The random number generator is used to shuffle the data, split the data into training and test sets, initialize the parameters of the model, and make other random decisions during training and evaluation.\n\nChoosing a good random state value is important because it can affect the results of your model. If you use a different random state, you may get different results.\n\nIn general, there is no one \"best\" random state value to use. The value you choose depends on the specific problem you're working on and the specific data you're working with.\n\nHowever, it's a good practice to set a random state to ensure reproducibility of your results. You can choose any integer value for the random state, but it's common to use numbers like 0, 42, or 2022.\n\nIt's also common to use cross-validation techniques like k-fold cross-validation, which helps to reduce the variability of your results due to different random state values.", "author_fullname": "t2_ihiz0a1u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I asked ChatGPT for the best random state value", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122nwrk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679841099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thing is like a student forced to write 1000 words on the topic. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;me:&lt;/strong&gt;  What is a good random state value to use in general &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bot:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;In machine learning, a random state is used to initialize the random number generator. The random number generator is used to shuffle the data, split the data into training and test sets, initialize the parameters of the model, and make other random decisions during training and evaluation.&lt;/p&gt;\n\n&lt;p&gt;Choosing a good random state value is important because it can affect the results of your model. If you use a different random state, you may get different results.&lt;/p&gt;\n\n&lt;p&gt;In general, there is no one &amp;quot;best&amp;quot; random state value to use. The value you choose depends on the specific problem you&amp;#39;re working on and the specific data you&amp;#39;re working with.&lt;/p&gt;\n\n&lt;p&gt;However, it&amp;#39;s a good practice to set a random state to ensure reproducibility of your results. You can choose any integer value for the random state, but it&amp;#39;s common to use numbers like 0, 42, or 2022.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also common to use cross-validation techniques like k-fold cross-validation, which helps to reduce the variability of your results due to different random state values.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122nwrk", "is_robot_indexable": true, "report_reasons": null, "author": "liquidInkRocks", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122nwrk/i_asked_chatgpt_for_the_best_random_state_value/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122nwrk/i_asked_chatgpt_for_the_best_random_state_value/", "subreddit_subscribers": 862896, "created_utc": 1679841099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nI have an interesting problem I thought you guys might be able to help with. I found a couple of similar questions on SO, but the methods mentioned there don't seem to work very well with my data. \n\nI have survey data of compensation percentiles for a certain job, but it only gives me the 10th, 25th, median, 75th, and 90th percentiles. No min/max, SD, mean, etc... \n\nI want to be able to compare any salary with the data and determine its percentile rank, despite the fact I do not know the upper/lower bounds. \n\nExample:\n10th - 184,000\n25th - 222,000\nMedian - 264,000\n75th - 323,000\n90th - 398,000\n\nInputs: 450,000, 100,000, 300,000, etc... \nPercentile ranks: ??? \n\nI heard from a colleague that he has seen this done before, but doesn't know how to do it himself. Ideally, I will be able to solve this in Excel or with VBA. \n\nAny ideas would be appreciated!", "author_fullname": "t2_aj5mt11i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to Calculate the Percentile Rank of a Salary Based on Incomplete Survey Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121w2lv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679771823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I have an interesting problem I thought you guys might be able to help with. I found a couple of similar questions on SO, but the methods mentioned there don&amp;#39;t seem to work very well with my data. &lt;/p&gt;\n\n&lt;p&gt;I have survey data of compensation percentiles for a certain job, but it only gives me the 10th, 25th, median, 75th, and 90th percentiles. No min/max, SD, mean, etc... &lt;/p&gt;\n\n&lt;p&gt;I want to be able to compare any salary with the data and determine its percentile rank, despite the fact I do not know the upper/lower bounds. &lt;/p&gt;\n\n&lt;p&gt;Example:\n10th - 184,000\n25th - 222,000\nMedian - 264,000\n75th - 323,000\n90th - 398,000&lt;/p&gt;\n\n&lt;p&gt;Inputs: 450,000, 100,000, 300,000, etc... \nPercentile ranks: ??? &lt;/p&gt;\n\n&lt;p&gt;I heard from a colleague that he has seen this done before, but doesn&amp;#39;t know how to do it himself. Ideally, I will be able to solve this in Excel or with VBA. &lt;/p&gt;\n\n&lt;p&gt;Any ideas would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121w2lv", "is_robot_indexable": true, "report_reasons": null, "author": "EMoneymaker99", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121w2lv/trying_to_calculate_the_percentile_rank_of_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121w2lv/trying_to_calculate_the_percentile_rank_of_a/", "subreddit_subscribers": 862896, "created_utc": 1679771823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a building a dashboard using Streamlit, Plotly Ex, python, pandas, numpy, scikit-learn, etc. The goal is to allow the audience to interactively explore the data. Different people will be using the dashboard and their data would be different, but they all have some commonalities. I want to build a dashboard that is \"generic\" enough to be applicable for most situations.\n\nThe data can contain categorical and numeric data, but mostly categorical. They all deal with consumer studies. The column of interest is typically some kind of categorical preference question (do you prefer A or B), or (would you buy this or no). There would be some demographic info of the consumer, some numeric columns that might pertain to ratings of a certain feature, and some categorical responses (multiple choice questions). \n\nThere are usually many features compared to number of observations. (ie, you might have only 100 consumers, but you asked them a bunch of questions)\n\nI think pair plots can be useful. Maybe give the user to ability to select some variables of interest, and run pair plots on them. \n\nI also plan to give the user the ability to group by variables and do various aggregations. \n\nSome filtering capability would also be built. (say, only want to see those with age &gt; 30)\n\nI'm also thinking running a random forest on the column of interest (such as preference) and determining the most important features. (not sure if this is a good idea given that the data typically have many features and not enough observations. A typical dataset might have 100 rows, and 40 columns) \n\nI don't plan on doing anything too specific (like stat tests) as those requires an understanding of what they are trying to do. Just want to build something automated and is general enough, and has enough functionalities to be useful, without doing \"too much\" that can cause issues.\n\nPlease advise on what you would or would not include.\n\nThanks", "author_fullname": "t2_6fty441r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a generic interactive business dashboard, what would you include?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121s7p2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a building a dashboard using Streamlit, Plotly Ex, python, pandas, numpy, scikit-learn, etc. The goal is to allow the audience to interactively explore the data. Different people will be using the dashboard and their data would be different, but they all have some commonalities. I want to build a dashboard that is &amp;quot;generic&amp;quot; enough to be applicable for most situations.&lt;/p&gt;\n\n&lt;p&gt;The data can contain categorical and numeric data, but mostly categorical. They all deal with consumer studies. The column of interest is typically some kind of categorical preference question (do you prefer A or B), or (would you buy this or no). There would be some demographic info of the consumer, some numeric columns that might pertain to ratings of a certain feature, and some categorical responses (multiple choice questions). &lt;/p&gt;\n\n&lt;p&gt;There are usually many features compared to number of observations. (ie, you might have only 100 consumers, but you asked them a bunch of questions)&lt;/p&gt;\n\n&lt;p&gt;I think pair plots can be useful. Maybe give the user to ability to select some variables of interest, and run pair plots on them. &lt;/p&gt;\n\n&lt;p&gt;I also plan to give the user the ability to group by variables and do various aggregations. &lt;/p&gt;\n\n&lt;p&gt;Some filtering capability would also be built. (say, only want to see those with age &amp;gt; 30)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also thinking running a random forest on the column of interest (such as preference) and determining the most important features. (not sure if this is a good idea given that the data typically have many features and not enough observations. A typical dataset might have 100 rows, and 40 columns) &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t plan on doing anything too specific (like stat tests) as those requires an understanding of what they are trying to do. Just want to build something automated and is general enough, and has enough functionalities to be useful, without doing &amp;quot;too much&amp;quot; that can cause issues.&lt;/p&gt;\n\n&lt;p&gt;Please advise on what you would or would not include.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121s7p2", "is_robot_indexable": true, "report_reasons": null, "author": "engineheat2", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121s7p2/building_a_generic_interactive_business_dashboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121s7p2/building_a_generic_interactive_business_dashboard/", "subreddit_subscribers": 862896, "created_utc": 1679764140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "General question - A lot of folks I know can't use Github Copilot inside their company for data privacy issues - code cannot leave the companies firewall. Is this an issue for others?", "author_fullname": "t2_7t1fa3mp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Github Copilot allowed inside your company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122cr8d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679810311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;General question - A lot of folks I know can&amp;#39;t use Github Copilot inside their company for data privacy issues - code cannot leave the companies firewall. Is this an issue for others?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122cr8d", "is_robot_indexable": true, "report_reasons": null, "author": "Dry_Win9996", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122cr8d/is_github_copilot_allowed_inside_your_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122cr8d/is_github_copilot_allowed_inside_your_company/", "subreddit_subscribers": 862896, "created_utc": 1679810311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Took over a machine learning model from a colleague. Improved upon it. Now a SDE from same vertical came up with a slightly simpler heuristics approach and got some small improvement over the current model. And it was disclosed on the same day when we were deploying it. Not sure how to react to it. Also spoiled my whole weekend trying to get small improvement over the model. What should me and my team do? Also the improvement I made and by the SDE are still very small to make the stakeholders happy.", "author_fullname": "t2_4jp70y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coping with failure at a project.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_122riho", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679848017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Took over a machine learning model from a colleague. Improved upon it. Now a SDE from same vertical came up with a slightly simpler heuristics approach and got some small improvement over the current model. And it was disclosed on the same day when we were deploying it. Not sure how to react to it. Also spoiled my whole weekend trying to get small improvement over the model. What should me and my team do? Also the improvement I made and by the SDE are still very small to make the stakeholders happy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122riho", "is_robot_indexable": true, "report_reasons": null, "author": "thelostknight99", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122riho/coping_with_failure_at_a_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122riho/coping_with_failure_at_a_project/", "subreddit_subscribers": 862896, "created_utc": 1679848017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am having a hard time understanding the silhouette coefficient...  if I am given a distance matrix as follows:\n\nhttps://preview.redd.it/ieft7an0x3qa1.png?width=481&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cb9707a14f2ce4f8d42313b49f2237f031c661a7\n\nAssuming that the clusters are {P1, P2} and {P3, P4}.  I want to compute the silhouette coefficient for each point, for each cluster and for the overall clustering, how would I do that?  What is the significance/meaning of the coefficient for each case?", "author_fullname": "t2_e6rkj3ul", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Silhouette Coefficient from Distance Matrix", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": true, "media_metadata": {"ieft7an0x3qa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/ieft7an0x3qa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=80d1ddafdb433a557d3f1295090438e6e3ed269d"}, {"y": 78, "x": 216, "u": "https://preview.redd.it/ieft7an0x3qa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe025cecd396874e6dd56aa16ce2591edb2f61ec"}, {"y": 116, "x": 320, "u": "https://preview.redd.it/ieft7an0x3qa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54704b18b5c5fc2e928059db804ca61c7f444d30"}], "s": {"y": 175, "x": 481, "u": "https://preview.redd.it/ieft7an0x3qa1.png?width=481&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cb9707a14f2ce4f8d42313b49f2237f031c661a7"}, "id": "ieft7an0x3qa1"}}, "name": "t3_122qxjf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/PcWQfsY71SSiE2aLbHdcxslqGHTAR0rIJ4duA0f_r64.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679846787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am having a hard time understanding the silhouette coefficient...  if I am given a distance matrix as follows:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ieft7an0x3qa1.png?width=481&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cb9707a14f2ce4f8d42313b49f2237f031c661a7\"&gt;https://preview.redd.it/ieft7an0x3qa1.png?width=481&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cb9707a14f2ce4f8d42313b49f2237f031c661a7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Assuming that the clusters are {P1, P2} and {P3, P4}.  I want to compute the silhouette coefficient for each point, for each cluster and for the overall clustering, how would I do that?  What is the significance/meaning of the coefficient for each case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122qxjf", "is_robot_indexable": true, "report_reasons": null, "author": "skurelowech3", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122qxjf/silhouette_coefficient_from_distance_matrix/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122qxjf/silhouette_coefficient_from_distance_matrix/", "subreddit_subscribers": 862896, "created_utc": 1679846787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi,\nI've gotten two offers. \n\nOnes a customer insight analyst at a large energy and electricity company. It revolves around doing data science, statistical analysis and machine learning on their retail side of the business (vs their generation/wholesale side). \n\nAnother is credit risk manager at a startup, it revolves around developing credit risk methodologies for small business lending and a small part involves around building the machine learning side of credit assessment. Startup size is 6 ppl - not venture backed. \n\nWhat would be the better opportunity given that pay is similar? \n\nMy YOE, 2 yrs out of college.", "author_fullname": "t2_3w4t0q25", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "job offer evaluation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122hk6s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679824779.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI&amp;#39;ve gotten two offers. &lt;/p&gt;\n\n&lt;p&gt;Ones a customer insight analyst at a large energy and electricity company. It revolves around doing data science, statistical analysis and machine learning on their retail side of the business (vs their generation/wholesale side). &lt;/p&gt;\n\n&lt;p&gt;Another is credit risk manager at a startup, it revolves around developing credit risk methodologies for small business lending and a small part involves around building the machine learning side of credit assessment. Startup size is 6 ppl - not venture backed. &lt;/p&gt;\n\n&lt;p&gt;What would be the better opportunity given that pay is similar? &lt;/p&gt;\n\n&lt;p&gt;My YOE, 2 yrs out of college.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122hk6s", "is_robot_indexable": true, "report_reasons": null, "author": "undecidedx10", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122hk6s/job_offer_evaluation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122hk6s/job_offer_evaluation/", "subreddit_subscribers": 862896, "created_utc": 1679824779.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, I've got sort of an unusual research question. Basically, I'd like to perform a comprehensive review of all the literature of a particular topic. To do this, I'd like to use combinations of search terms. For example, I'd conduct a search using terms \"A\" and \"B\", then I'd conduct another search using terms \"A\" and \"C\", then again using \"A\" and \"D\", etc. The problem with this is that there's a decent amount of overlap of search results among these different combinations and there are thousands of search results for each combination so I want to minimize redundancy as much as possible in order to save time. Is there a way for me to conduct an initial search (e.g., A + B) and then conduct each subsequent search (A + C, A + D, etc.) that will only show search results that are NOT included in the initial A + B search?\n\nI'm using OVID Medline as the search database, but I'd be open to any general workaround solutions as well. From my limited knowledge on a possible solution, I was wondering if it's possible to export all the search results, copy them as a list into a column within Excel, and then use the Excel function that can highlight duplicate values. This method would allow me to avoid redundant search results from each search iteration. This isn't an elegant solution imo, but I imagined a possible solution like this. The most ideal solution would be for the database to filter out redundant search results for me automatically.\n\nI can explain or clarify the problem further if that's helpful. Thank you for any help or suggestions with this problem!!", "author_fullname": "t2_179w2x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Literature review - how to filter out redundant search results from similar search iterations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122cqmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679810258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve got sort of an unusual research question. Basically, I&amp;#39;d like to perform a comprehensive review of all the literature of a particular topic. To do this, I&amp;#39;d like to use combinations of search terms. For example, I&amp;#39;d conduct a search using terms &amp;quot;A&amp;quot; and &amp;quot;B&amp;quot;, then I&amp;#39;d conduct another search using terms &amp;quot;A&amp;quot; and &amp;quot;C&amp;quot;, then again using &amp;quot;A&amp;quot; and &amp;quot;D&amp;quot;, etc. The problem with this is that there&amp;#39;s a decent amount of overlap of search results among these different combinations and there are thousands of search results for each combination so I want to minimize redundancy as much as possible in order to save time. Is there a way for me to conduct an initial search (e.g., A + B) and then conduct each subsequent search (A + C, A + D, etc.) that will only show search results that are NOT included in the initial A + B search?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using OVID Medline as the search database, but I&amp;#39;d be open to any general workaround solutions as well. From my limited knowledge on a possible solution, I was wondering if it&amp;#39;s possible to export all the search results, copy them as a list into a column within Excel, and then use the Excel function that can highlight duplicate values. This method would allow me to avoid redundant search results from each search iteration. This isn&amp;#39;t an elegant solution imo, but I imagined a possible solution like this. The most ideal solution would be for the database to filter out redundant search results for me automatically.&lt;/p&gt;\n\n&lt;p&gt;I can explain or clarify the problem further if that&amp;#39;s helpful. Thank you for any help or suggestions with this problem!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122cqmn", "is_robot_indexable": true, "report_reasons": null, "author": "pantaloonsss", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122cqmn/literature_review_how_to_filter_out_redundant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122cqmn/literature_review_how_to_filter_out_redundant/", "subreddit_subscribers": 862896, "created_utc": 1679810258.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For teams responsible for developing, testing and maintaining product recommendation models for e-commerce companies;\n\n~How large is your team?\n~What percent of e-commerce revenue would you attribute to product recommendations?", "author_fullname": "t2_7v3tv87q8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product recommendation teams question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121vs0z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679771212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For teams responsible for developing, testing and maintaining product recommendation models for e-commerce companies;&lt;/p&gt;\n\n&lt;p&gt;~How large is your team?\n~What percent of e-commerce revenue would you attribute to product recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121vs0z", "is_robot_indexable": true, "report_reasons": null, "author": "ilovedataaa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121vs0z/product_recommendation_teams_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121vs0z/product_recommendation_teams_question/", "subreddit_subscribers": 862896, "created_utc": 1679771212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a data set that covers internet data usage of several hundred users over several month period. What is the best approach to analyze? My goal is to retain these customers or upsell. Any suggestions?", "author_fullname": "t2_bco8idf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best approach to analyze a simple data set?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121v2ol", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679769782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data set that covers internet data usage of several hundred users over several month period. What is the best approach to analyze? My goal is to retain these customers or upsell. Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121v2ol", "is_robot_indexable": true, "report_reasons": null, "author": "Environmental_Wind40", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121v2ol/what_is_the_best_approach_to_analyze_a_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121v2ol/what_is_the_best_approach_to_analyze_a_simple/", "subreddit_subscribers": 862896, "created_utc": 1679769782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am confident I am getting  a job offer for company A later this week. I have been interning at a different place, company B, and was told they would consider sending me an offer in a few weeks from now.\n\nI am wondering if I should go to company B now and ask for them to send me an offer now or if I should wait for company A to officially give me an offer first. From what I read online I would only have a few days to a week to make a decision once I am given an offer, so I want to try and make the offers overlap as much as possible.", "author_fullname": "t2_48dmgjj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job offer timeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12279ow", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679795270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am confident I am getting  a job offer for company A later this week. I have been interning at a different place, company B, and was told they would consider sending me an offer in a few weeks from now.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if I should go to company B now and ask for them to send me an offer now or if I should wait for company A to officially give me an offer first. From what I read online I would only have a few days to a week to make a decision once I am given an offer, so I want to try and make the offers overlap as much as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12279ow", "is_robot_indexable": true, "report_reasons": null, "author": "loumani06", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12279ow/job_offer_timeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12279ow/job_offer_timeline/", "subreddit_subscribers": 862896, "created_utc": 1679795270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Find a wide range of learning resources on Web 3.0, Blockchain, AI and Data Science. Get regular career guides and interview preparation guides. Stay tuned with Incubity. \n\n[https://incubity.ambilio.com/learning-center/](https://incubity.ambilio.com/learning-center/)\n\nSome of the recently published articles:\n\n* [Diving Deeper into Decentralized Finance (DeFi)](https://incubity.ambilio.com/diving-deeper-into-decentralized-finance-defi/)\n* [How to build a NFT Marketplace: A Step-by-step Guide](https://incubity.ambilio.com/how-to-build-a-nft-marketplace-a-step-by-step-guide/)\n* [Top 10 Web 3.0 Project Ideas in 2023](https://incubity.ambilio.com/top-10-web-3-0-project-ideas-in-2023/)\n* [Top Stable Diffusion Based Project Ideas in 2023](https://incubity.ambilio.com/top-stable-diffusion-based-project-ideas-in-2023/) \n* [DataOps: The Key to Streamlining Complex Data Pipelines](https://incubity.ambilio.com/dataops-the-key-to-streamlining-complex-data-pipelines/)\n* [Blockchain Developer Interview Questions with Answers](https://incubity.ambilio.com/blockchain-developer-interview-questions-with-answers/)\n\nStay tuned with us through our social media handles and get regular updates.", "author_fullname": "t2_39g49cje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web 3.0 and Data Science Learning Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122ke6z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679833200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Find a wide range of learning resources on Web 3.0, Blockchain, AI and Data Science. Get regular career guides and interview preparation guides. Stay tuned with Incubity. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://incubity.ambilio.com/learning-center/\"&gt;https://incubity.ambilio.com/learning-center/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Some of the recently published articles:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://incubity.ambilio.com/diving-deeper-into-decentralized-finance-defi/\"&gt;Diving Deeper into Decentralized Finance (DeFi)&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://incubity.ambilio.com/how-to-build-a-nft-marketplace-a-step-by-step-guide/\"&gt;How to build a NFT Marketplace: A Step-by-step Guide&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://incubity.ambilio.com/top-10-web-3-0-project-ideas-in-2023/\"&gt;Top 10 Web 3.0 Project Ideas in 2023&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://incubity.ambilio.com/top-stable-diffusion-based-project-ideas-in-2023/\"&gt;Top Stable Diffusion Based Project Ideas in 2023&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://incubity.ambilio.com/dataops-the-key-to-streamlining-complex-data-pipelines/\"&gt;DataOps: The Key to Streamlining Complex Data Pipelines&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://incubity.ambilio.com/blockchain-developer-interview-questions-with-answers/\"&gt;Blockchain Developer Interview Questions with Answers&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Stay tuned with us through our social media handles and get regular updates.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wfWZpvdGoFjX5yHaDRlHDHINTxFX-8qsymGxqPRrH0k.jpg?auto=webp&amp;v=enabled&amp;s=bf3016ea4973a3f9a8b3bab95fc7c7e4a347bbfb", "width": 1284, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/wfWZpvdGoFjX5yHaDRlHDHINTxFX-8qsymGxqPRrH0k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2706464b87674bd01af00f2283311c8b3c715177", "width": 108, "height": 33}, {"url": "https://external-preview.redd.it/wfWZpvdGoFjX5yHaDRlHDHINTxFX-8qsymGxqPRrH0k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77ca14618de3c02970e0dd43db46a27a43491e20", "width": 216, "height": 67}, {"url": "https://external-preview.redd.it/wfWZpvdGoFjX5yHaDRlHDHINTxFX-8qsymGxqPRrH0k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a0da9f9fded4bf2aec9fbdedded873c0bc02756", "width": 320, "height": 99}, {"url": "https://external-preview.redd.it/wfWZpvdGoFjX5yHaDRlHDHINTxFX-8qsymGxqPRrH0k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1fa3c194f7d8b5dafcb998e5723802283b1c0ce4", "width": 640, "height": 199}, {"url": "https://external-preview.redd.it/wfWZpvdGoFjX5yHaDRlHDHINTxFX-8qsymGxqPRrH0k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0479ceee8d14a4a59e3013a0447e403441dad173", "width": 960, "height": 299}, {"url": "https://external-preview.redd.it/wfWZpvdGoFjX5yHaDRlHDHINTxFX-8qsymGxqPRrH0k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=25d362b712aa1eedecf0a40b8beb4a5b49d74be5", "width": 1080, "height": 336}], "variants": {}, "id": "BGsr9jzqj4MVLfGZxfjNZH8vfDYYJNbFUWEFWdr-OEw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122ke6z", "is_robot_indexable": true, "report_reasons": null, "author": "vaibhav05cse", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122ke6z/web_30_and_data_science_learning_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122ke6z/web_30_and_data_science_learning_resources/", "subreddit_subscribers": 862896, "created_utc": 1679833200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone!\n\nTL;DR\n\n\\- Looking for an intensive full-time, in-person, 3-6 month long data program in Canada to become very proficient in coding after graduating from uni this summer. \n\nI'm going to be a graduating with a [B.Sc](https://B.Sc). in Psychology this summer (In Canada) and would like to spend part-time this summer and full-time this Fall (and maybe even Winter) becoming really proficient in coding (thinking Python and SQL). I'm not into the online-learning thing (did enough of that over COVID - hard to stay focused) so I'm trying to figure out a part time online program to learn basics of coding in the summer, and then jump into an in-person intensive data analytics/data science program of some sort in the fall (I'm willing to move for it - somewhere in Canada). \n\nCurrently thinking of doing the WeCloudData data analytics bootcamp then trying to figure out what to do after that in-person (was looking into BrainStation because its the best rated program in Canada for in-person bootcamps). I don't want to pursue career as a data scientist per say, I'm just interested in developing this skillset and see it as a skill that is growing in demand - especially in the humanities. \n\nAny thoughts would be helpful!", "author_fullname": "t2_scu1jkf0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help choosing a coding bootcamp/classes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121v4l2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679769893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;TL;DR&lt;/p&gt;\n\n&lt;p&gt;- Looking for an intensive full-time, in-person, 3-6 month long data program in Canada to become very proficient in coding after graduating from uni this summer. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to be a graduating with a &lt;a href=\"https://B.Sc\"&gt;B.Sc&lt;/a&gt;. in Psychology this summer (In Canada) and would like to spend part-time this summer and full-time this Fall (and maybe even Winter) becoming really proficient in coding (thinking Python and SQL). I&amp;#39;m not into the online-learning thing (did enough of that over COVID - hard to stay focused) so I&amp;#39;m trying to figure out a part time online program to learn basics of coding in the summer, and then jump into an in-person intensive data analytics/data science program of some sort in the fall (I&amp;#39;m willing to move for it - somewhere in Canada). &lt;/p&gt;\n\n&lt;p&gt;Currently thinking of doing the WeCloudData data analytics bootcamp then trying to figure out what to do after that in-person (was looking into BrainStation because its the best rated program in Canada for in-person bootcamps). I don&amp;#39;t want to pursue career as a data scientist per say, I&amp;#39;m just interested in developing this skillset and see it as a skill that is growing in demand - especially in the humanities. &lt;/p&gt;\n\n&lt;p&gt;Any thoughts would be helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121v4l2", "is_robot_indexable": true, "report_reasons": null, "author": "LearningToCode100", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121v4l2/help_choosing_a_coding_bootcampclasses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/121v4l2/help_choosing_a_coding_bootcampclasses/", "subreddit_subscribers": 862896, "created_utc": 1679769893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_sppgx30r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Machine Learning with Python, Getting Started for Data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_121wrga", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7DC5P14iKM8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/121wrga", "height": 200}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TwPjHFympHbiLuuOgGyFADIEsXBoKkGF85TdyaZ-HcQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679773252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/7DC5P14iKM8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?auto=webp&amp;v=enabled&amp;s=2fa34b646cffa3bf70691a05860c888e9fd2449d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0aa8277ab41e4659d1f2641b7da280e0b10cd16b", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1effaa95d1777ee8642cc9a2dbd6bcfcf18d607", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Pc9lDFyxLYvH3VJj95nHxho_7zbB0HSeO_toJWgj1mw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5166cadbfa3780b1e376d3de0770ca8d0be27da5", "width": 320, "height": 240}], "variants": {}, "id": "LEBKQa13ztjayQE2R1vvT6gYphFwdkPbqZeK3aNkFbs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "121wrga", "is_robot_indexable": true, "report_reasons": null, "author": "thetech_learner", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/121wrga/machine_learning_with_python_getting_started_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/7DC5P14iKM8", "subreddit_subscribers": 862896, "created_utc": 1679773252.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7DC5P14iKM8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Machine Learning with Python - Complete Tutorial for Data Science 2023 | Code with Scaler\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7DC5P14iKM8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT-4 can solve most SQL interview questions. In 5 years, do you think Acing a SQL Interview will still be important?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121tjz7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": "", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_6zm7tswg", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "SQL", "selftext": "GPT-4 can write SQL queries and solve most easy &amp; medium SQL interview questions on sites like DataLemur. So I'm curious, will Acing the SQL Interview still be important in 5 years? What about in 10?\n\n[View Poll](https://www.reddit.com/poll/121q7nt)", "author_fullname": "t2_6zm7tswg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT-4 can solve most SQL interview questions. In 5 years, do you think Acing a SQL Interview will still be important?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/SQL", "hidden": false, "pwls": 6, "link_flair_css_class": "h", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121q7nt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "01031502-de87-11e6-b05e-0e1ce54f6a26", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679840715.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679759953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.SQL", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;GPT-4 can write SQL queries and solve most easy &amp;amp; medium SQL interview questions on sites like DataLemur. So I&amp;#39;m curious, will Acing the SQL Interview still be important in 5 years? What about in 10?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/121q7nt\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5cb03a64-2769-11ea-aa04-0ed301f44875", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Author of Ace the Data Science Interview \ud83d\udcd5", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qp8q", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "121q7nt", "is_robot_indexable": true, "report_reasons": null, "author": "NickSinghTechCareers", "discussion_type": null, "num_comments": 65, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1680364753748, "options": [{"text": "SQL Interview Will Be Obsolete", "id": "22238479"}, {"text": "SQL Interviews Might Go Away", "id": "22238480"}, {"text": "SQL Interviews Won't Go Away", "id": "22238481"}, {"text": "Show Results", "id": "22238482"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 2025, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "subreddit_subscribers": 144353, "created_utc": 1679759953.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1679766802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.SQL", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Author | Ace the Data Science Interview", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "121tjz7", "is_robot_indexable": true, "report_reasons": null, "author": "NickSinghTechCareers", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_121q7nt", "author_flair_text_color": "dark", "permalink": "/r/datascience/comments/121tjz7/gpt4_can_solve_most_sql_interview_questions_in_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/", "subreddit_subscribers": 862896, "created_utc": 1679766802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have learnt about python and sql and have just started to get into R language and I found it is pretty easy to comprehend due to the fact that it is pretty similar to python and sql\u2026 discuss about it ?", "author_fullname": "t2_6zmb8t0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "R = Python + SQL (just mu opinion)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_122qqdz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.08, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679846410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have learnt about python and sql and have just started to get into R language and I found it is pretty easy to comprehend due to the fact that it is pretty similar to python and sql\u2026 discuss about it ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122qqdz", "is_robot_indexable": true, "report_reasons": null, "author": "Donfrds", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122qqdz/r_python_sql_just_mu_opinion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122qqdz/r_python_sql_just_mu_opinion/", "subreddit_subscribers": 862896, "created_utc": 1679846410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi r/datascience, I just got my r/microsoft r/AZURE Data Scientist Associate Certificate. Check and verify it on Credly \n\n[https://www.credly.com/badges/a1f5ef1f-687b-4d63-9912-716474debdd6](https://www.credly.com/badges/a1f5ef1f-687b-4d63-9912-716474debdd6)", "author_fullname": "t2_715f2qqtt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Certified Data Scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122lq0h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.04, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679836490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt;, I just got my &lt;a href=\"/r/microsoft\"&gt;r/microsoft&lt;/a&gt; &lt;a href=\"/r/AZURE\"&gt;r/AZURE&lt;/a&gt; Data Scientist Associate Certificate. Check and verify it on Credly &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.credly.com/badges/a1f5ef1f-687b-4d63-9912-716474debdd6\"&gt;https://www.credly.com/badges/a1f5ef1f-687b-4d63-9912-716474debdd6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8LqwhGciqnuwilk6lEU2ZXr8HbVLrxG7X1lk0GckPsE.jpg?auto=webp&amp;v=enabled&amp;s=af8f17cc25a97c591b6f0d0e10af0337e3fe821e", "width": 672, "height": 352}, "resolutions": [{"url": "https://external-preview.redd.it/8LqwhGciqnuwilk6lEU2ZXr8HbVLrxG7X1lk0GckPsE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73fb819c243de557bd677c6e7d14bc7fe86a9590", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/8LqwhGciqnuwilk6lEU2ZXr8HbVLrxG7X1lk0GckPsE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d557d683612b8498f1ed50f6a46d79bbe428cf0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/8LqwhGciqnuwilk6lEU2ZXr8HbVLrxG7X1lk0GckPsE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=affb5c07b7a9539b09f1f5804e45b5922c777b69", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/8LqwhGciqnuwilk6lEU2ZXr8HbVLrxG7X1lk0GckPsE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af4cce2c4269a4ad7f9061b65671ce9635091f85", "width": 640, "height": 335}], "variants": {}, "id": "_Vjgch9k64y-rWvFrsfOV4Vd60LQFu1U0s5JjzdsQiw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122lq0h", "is_robot_indexable": true, "report_reasons": null, "author": "timothynn", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/122lq0h/certified_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/122lq0h/certified_data_scientist/", "subreddit_subscribers": 862896, "created_utc": 1679836490.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}