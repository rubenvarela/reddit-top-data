{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Company recently made the decision to move to snowflake from a mishmash of on-prem databases (mostly sql server). \n\nData science team uses R and spark running on a handful of on prem workstations to build models. \n\nThe IT department is pushing hard to move everything into snowflake and the data science people are complaining that snowpark is half baked. Especially on R programming where they'll still have to take data out of snowflake into some other compute source to run R models, then insert results back into snowflake.\n\nTrying to think of solutions. Should we:\n\n1) convince IT so provision a small databricks account we can use just for running models?\n2) suck it up and convert everything to python, trying to adopt snowpark/anaconda as much as possible?\n3) keep our pile of on prem machines with do modeling by pulling data out of snowflake, computing locally, inserting model results back into snowflake for use in reports, decision dashboards, etc.", "author_fullname": "t2_5iwre", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "machine learning in snowflake, unhappy data scientists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121mm5c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679752633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Company recently made the decision to move to snowflake from a mishmash of on-prem databases (mostly sql server). &lt;/p&gt;\n\n&lt;p&gt;Data science team uses R and spark running on a handful of on prem workstations to build models. &lt;/p&gt;\n\n&lt;p&gt;The IT department is pushing hard to move everything into snowflake and the data science people are complaining that snowpark is half baked. Especially on R programming where they&amp;#39;ll still have to take data out of snowflake into some other compute source to run R models, then insert results back into snowflake.&lt;/p&gt;\n\n&lt;p&gt;Trying to think of solutions. Should we:&lt;/p&gt;\n\n&lt;p&gt;1) convince IT so provision a small databricks account we can use just for running models?\n2) suck it up and convert everything to python, trying to adopt snowpark/anaconda as much as possible?\n3) keep our pile of on prem machines with do modeling by pulling data out of snowflake, computing locally, inserting model results back into snowflake for use in reports, decision dashboards, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121mm5c", "is_robot_indexable": true, "report_reasons": null, "author": "CantGoogleMe", "discussion_type": null, "num_comments": 65, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121mm5c/machine_learning_in_snowflake_unhappy_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121mm5c/machine_learning_in_snowflake_unhappy_data/", "subreddit_subscribers": 94398, "created_utc": 1679752633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As per title. How are you using DuckDB, either privately for passion/side projects, hobbies, or professionally in production environments?\n\nWould love to just hear the innovative ways in which people are using it.", "author_fullname": "t2_4bjsc4oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you using DuckDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121jccb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679744601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per title. How are you using DuckDB, either privately for passion/side projects, hobbies, or professionally in production environments?&lt;/p&gt;\n\n&lt;p&gt;Would love to just hear the innovative ways in which people are using it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121jccb", "is_robot_indexable": true, "report_reasons": null, "author": "jb7834", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121jccb/how_are_you_using_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121jccb/how_are_you_using_duckdb/", "subreddit_subscribers": 94398, "created_utc": 1679744601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, i am in the process of building a new data model. Our existing model is OBT, while it's good for analysts, it's a nightmare for dashboard use cases. I am thinking of building a traditional dimensional Model which I'll use in dashboards and build a OBT on top of it to give to analysts. But I had a few questions :\n\n1. It is said that dimensional models can be slow to process the data. Our current OBT model refreshes 8 times a day and the new model should be refreshed in the same frequency or most probably once every hour.\n\n2. We use databricks, while it could solve some of load time problems mentioned above, I am not really able to find any resources that tells me whether there's omr modelling approach that is more suitable to a spark platform like databricks compared to others.", "author_fullname": "t2_xap78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Platform considerations for modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121lqh0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679750588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, i am in the process of building a new data model. Our existing model is OBT, while it&amp;#39;s good for analysts, it&amp;#39;s a nightmare for dashboard use cases. I am thinking of building a traditional dimensional Model which I&amp;#39;ll use in dashboards and build a OBT on top of it to give to analysts. But I had a few questions :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;It is said that dimensional models can be slow to process the data. Our current OBT model refreshes 8 times a day and the new model should be refreshed in the same frequency or most probably once every hour.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We use databricks, while it could solve some of load time problems mentioned above, I am not really able to find any resources that tells me whether there&amp;#39;s omr modelling approach that is more suitable to a spark platform like databricks compared to others.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121lqh0", "is_robot_indexable": true, "report_reasons": null, "author": "totalsports1", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/121lqh0/platform_considerations_for_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121lqh0/platform_considerations_for_modelling/", "subreddit_subscribers": 94398, "created_utc": 1679750588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am dealing with some data containing user-ids that need to be counted in power-bi. However, due to various potential users in power bi, we have the requirement to not store plain-text user-ids in any table connected to power bi.\nI am wondering what would be the best practice in such a case? I suggested a concept in which user-id get hashed with a salt, and a mapping table between each user-id and its hash is stored in a table which is not available in power-bi.  \nWhat are your thoughts about this approach? Did you had similar requirements? Are there alternatives to solve that problem?", "author_fullname": "t2_8etbdwvd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to encrypt sensitive data from data lake to power bi?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121i1mv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679740981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am dealing with some data containing user-ids that need to be counted in power-bi. However, due to various potential users in power bi, we have the requirement to not store plain-text user-ids in any table connected to power bi.\nI am wondering what would be the best practice in such a case? I suggested a concept in which user-id get hashed with a salt, and a mapping table between each user-id and its hash is stored in a table which is not available in power-bi.&lt;br/&gt;\nWhat are your thoughts about this approach? Did you had similar requirements? Are there alternatives to solve that problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121i1mv", "is_robot_indexable": true, "report_reasons": null, "author": "Remote-Juice2527", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121i1mv/how_to_encrypt_sensitive_data_from_data_lake_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121i1mv/how_to_encrypt_sensitive_data_from_data_lake_to/", "subreddit_subscribers": 94398, "created_utc": 1679740981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\nI\u2019m exploring how to integrate Apache Airflow into our communicable disease unit workflow. We are mostly R users but are starting to use Python more. The basic workflow I have in mind is:\n\nTask 1 - trigger script 1 - pull immunization records from Snowflake &gt;&gt; Task 2 - trigger script 2 - clean and deduplicate, then add new records to historical dataset &gt;&gt; Update R Shiny\n\nI have Airflow up and running via docker, but I\u2019m stuck on how to trigger scripts via bash command. Can I have the script anywhere on our network drive, or does it have to be somewhere within the airflow folder? In the past I\u2019ve used windows task scheduler and .bat files for run scripts overnight, but trying to better automate some of our repetitive tasks.", "author_fullname": "t2_7e2bmavx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Airflow Triggering Python Scripts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121rg90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679762604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI\u2019m exploring how to integrate Apache Airflow into our communicable disease unit workflow. We are mostly R users but are starting to use Python more. The basic workflow I have in mind is:&lt;/p&gt;\n\n&lt;p&gt;Task 1 - trigger script 1 - pull immunization records from Snowflake &amp;gt;&amp;gt; Task 2 - trigger script 2 - clean and deduplicate, then add new records to historical dataset &amp;gt;&amp;gt; Update R Shiny&lt;/p&gt;\n\n&lt;p&gt;I have Airflow up and running via docker, but I\u2019m stuck on how to trigger scripts via bash command. Can I have the script anywhere on our network drive, or does it have to be somewhere within the airflow folder? In the past I\u2019ve used windows task scheduler and .bat files for run scripts overnight, but trying to better automate some of our repetitive tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121rg90", "is_robot_indexable": true, "report_reasons": null, "author": "Senior-Athlete-0", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121rg90/apache_airflow_triggering_python_scripts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121rg90/apache_airflow_triggering_python_scripts/", "subreddit_subscribers": 94398, "created_utc": 1679762604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Interested to hear how (and what) teams are doing for data pipeline documentation. Where is the proper balance between maintainability and enough detail to provide value. What items are you covering the document (source, maybe associated jira details, high level business logic overview, etc...)?  Where are you keeping it github-pages, confluence, G drive (oh the horror)...?\n\nI don't feel code documentation should be included as the code itself should be well commented and clean and with the changes frequent in code bases it'd never be kept up anyway.", "author_fullname": "t2_puuzgu6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline documentation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121s86p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679764741.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interested to hear how (and what) teams are doing for data pipeline documentation. Where is the proper balance between maintainability and enough detail to provide value. What items are you covering the document (source, maybe associated jira details, high level business logic overview, etc...)?  Where are you keeping it github-pages, confluence, G drive (oh the horror)...?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t feel code documentation should be included as the code itself should be well commented and clean and with the changes frequent in code bases it&amp;#39;d never be kept up anyway.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121s86p", "is_robot_indexable": true, "report_reasons": null, "author": "getafterit123", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121s86p/pipeline_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121s86p/pipeline_documentation/", "subreddit_subscribers": 94398, "created_utc": 1679764169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, Redditors!\n\nI applied for a junior DE position at one company. I saw they use Airflow as an orchestration tool and I started learning it recently but can't seem to understand something, so if you guys are willing to help out I would be really grateful.\n\n1. So in ETL, let's say I have a script that extracts data from some source (doesn't matter which one) and that data is now loaded. Where is it loaded? I mean the location? Is it in some repository on the company's cloud or does Airflow has its own place where it temporarily stores it?\n2. Now let's say I have 3 different python scripts (pandas) that perform some transformation each. So transform\\_script1 takes that data, performs transformation and how exactly does it pass it to the transform\\_script2? Is the same for transform\\_script3?\n3. From what I understand, in my dag script, I should specify the downstream or upstream scripts. So it would look something like this:  \nextract\\_script &gt;&gt; transform\\_script1 &gt;&gt; transform\\_script2 &gt;&gt; transform\\_script3 &gt;&gt; load\\_script  \nBut what I fail to understand is how is the data passed between them.\n4. If someone has extra time I would really like to see what transform\\_script(s) look like. I might be asking a lot of your time, but if you have some time I would really appreciate it.\n\nI realize I am a total beginner, but really want this interview to go well. Thank you, guys!", "author_fullname": "t2_2jtk54zi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121ywna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679777841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, Redditors!&lt;/p&gt;\n\n&lt;p&gt;I applied for a junior DE position at one company. I saw they use Airflow as an orchestration tool and I started learning it recently but can&amp;#39;t seem to understand something, so if you guys are willing to help out I would be really grateful.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;So in ETL, let&amp;#39;s say I have a script that extracts data from some source (doesn&amp;#39;t matter which one) and that data is now loaded. Where is it loaded? I mean the location? Is it in some repository on the company&amp;#39;s cloud or does Airflow has its own place where it temporarily stores it?&lt;/li&gt;\n&lt;li&gt;Now let&amp;#39;s say I have 3 different python scripts (pandas) that perform some transformation each. So transform_script1 takes that data, performs transformation and how exactly does it pass it to the transform_script2? Is the same for transform_script3?&lt;/li&gt;\n&lt;li&gt;From what I understand, in my dag script, I should specify the downstream or upstream scripts. So it would look something like this:&lt;br/&gt;\nextract_script &amp;gt;&amp;gt; transform_script1 &amp;gt;&amp;gt; transform_script2 &amp;gt;&amp;gt; transform_script3 &amp;gt;&amp;gt; load_script&lt;br/&gt;\nBut what I fail to understand is how is the data passed between them.&lt;/li&gt;\n&lt;li&gt;If someone has extra time I would really like to see what transform_script(s) look like. I might be asking a lot of your time, but if you have some time I would really appreciate it.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I realize I am a total beginner, but really want this interview to go well. Thank you, guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121ywna", "is_robot_indexable": true, "report_reasons": null, "author": "d_underdog", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/121ywna/airflow_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121ywna/airflow_question/", "subreddit_subscribers": 94398, "created_utc": 1679777841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was asked to migrate 2500 tables from SQL Server to MariaDB and the ETL tool (Cdata sync) doesn\u2019t work for ~20 tables because they\u2019re too big and it keeps timing out. So I tried importing the data for the missing tables by expiring the CSVs through DBeaver then importing them and it still times out. Now I am trying to import the tables using python but my code errors out because of some issue with how it\u2019s reading the data. I don\u2019t even know if that\u2019s worth fixing because it\u2019s going to take forever to read in the tables with ~5 million records and other have even more or about the same. Whenever I use \u201cread_csv\u201d there is an error with tokenizing the data and when I use \u201cwith open\u201d\u2026 it says some error that hit all the data types converted. \n\nI told my manager(s) this and my co worker whom I was told to reach out to help for. I bounced some ideas off of my conworker and he said if I can come up with something in python that would be cool. I was like okay I don\u2019t know what I\u2019m doing is this ok?? What would be cool?? My manager said he\u2019s depending on me to do this and my co worker but idk what I\u2019m doing and when I ask for help they\u2019re like I don\u2019t know?? Like i dont know either what the fuck do you want from me?? I hesitate to download other tools like new ones and start over again. I need to find a way to automate increments of data too so I can\u2019t just manually import them and use the stupid failed ETL tool for others?  Like I don\u2019t know what to say anymore I am stuck and out of ideas?? I don\u2019t know what to say for my standup on Monday.", "author_fullname": "t2_8ewmlf41", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I\u2019m struggling with how to ask for help with my task.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1226vlf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679794333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was asked to migrate 2500 tables from SQL Server to MariaDB and the ETL tool (Cdata sync) doesn\u2019t work for ~20 tables because they\u2019re too big and it keeps timing out. So I tried importing the data for the missing tables by expiring the CSVs through DBeaver then importing them and it still times out. Now I am trying to import the tables using python but my code errors out because of some issue with how it\u2019s reading the data. I don\u2019t even know if that\u2019s worth fixing because it\u2019s going to take forever to read in the tables with ~5 million records and other have even more or about the same. Whenever I use \u201cread_csv\u201d there is an error with tokenizing the data and when I use \u201cwith open\u201d\u2026 it says some error that hit all the data types converted. &lt;/p&gt;\n\n&lt;p&gt;I told my manager(s) this and my co worker whom I was told to reach out to help for. I bounced some ideas off of my conworker and he said if I can come up with something in python that would be cool. I was like okay I don\u2019t know what I\u2019m doing is this ok?? What would be cool?? My manager said he\u2019s depending on me to do this and my co worker but idk what I\u2019m doing and when I ask for help they\u2019re like I don\u2019t know?? Like i dont know either what the fuck do you want from me?? I hesitate to download other tools like new ones and start over again. I need to find a way to automate increments of data too so I can\u2019t just manually import them and use the stupid failed ETL tool for others?  Like I don\u2019t know what to say anymore I am stuck and out of ideas?? I don\u2019t know what to say for my standup on Monday.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1226vlf", "is_robot_indexable": true, "report_reasons": null, "author": "jumpfordespair", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1226vlf/im_struggling_with_how_to_ask_for_help_with_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1226vlf/im_struggling_with_how_to_ask_for_help_with_my/", "subreddit_subscribers": 94398, "created_utc": 1679794333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering if there was a way to go above and beyond in regards do dbt documentation , without buying a software. Ie taking their free html output and making it a lot better ?", "author_fullname": "t2_xo4dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone ever use dbt code and generate docs https://docs.getdbt.com/reference/commands/cmd-docs ; and instead of hosting the documentation website on your own per many blogs \u2014 you every create your own cool webpage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1223bq2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679787113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if there was a way to go above and beyond in regards do dbt documentation , without buying a software. Ie taking their free html output and making it a lot better ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1223bq2", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1223bq2/anyone_ever_use_dbt_code_and_generate_docs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1223bq2/anyone_ever_use_dbt_code_and_generate_docs/", "subreddit_subscribers": 94398, "created_utc": 1679787113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I use RazorSQL to navigate in a DynamoDB database. What's the rationale behind the column order? http://i.stack.imgur.com/Lgcqr.png\n\nI am asking as the import tool uses the same order (which means if I import a CSV file it needs to be in the same ordering as the RazorSQL's column order): http://i.stack.imgur.com/fDyBL.png\n\nI use RazorSQL 6.3.14 with Windows 7 SP1 x64 Ultimate.", "author_fullname": "t2_kprlc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Column order in a DynamoDB table browsed through RazorSQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12237ot", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679786885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use RazorSQL to navigate in a DynamoDB database. What&amp;#39;s the rationale behind the column order? &lt;a href=\"http://i.stack.imgur.com/Lgcqr.png\"&gt;http://i.stack.imgur.com/Lgcqr.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am asking as the import tool uses the same order (which means if I import a CSV file it needs to be in the same ordering as the RazorSQL&amp;#39;s column order): &lt;a href=\"http://i.stack.imgur.com/fDyBL.png\"&gt;http://i.stack.imgur.com/fDyBL.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I use RazorSQL 6.3.14 with Windows 7 SP1 x64 Ultimate.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1HCf9QrSJkHQ0xOEkNIY3cR_7HgL5mGBprd5xwpQl18.png?auto=webp&amp;v=enabled&amp;s=e29d2560fcbf6f1c014341065dce924e6c06b93a", "width": 480, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/1HCf9QrSJkHQ0xOEkNIY3cR_7HgL5mGBprd5xwpQl18.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c399fdb73c0bf1a1159a61a0b0f9135bd69311be", "width": 108, "height": 162}, {"url": "https://external-preview.redd.it/1HCf9QrSJkHQ0xOEkNIY3cR_7HgL5mGBprd5xwpQl18.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a209863b315b2a004a8e9a57fc36c15bc327991e", "width": 216, "height": 324}, {"url": "https://external-preview.redd.it/1HCf9QrSJkHQ0xOEkNIY3cR_7HgL5mGBprd5xwpQl18.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9b497a7303e48a07900f1b9c1ff681cb15a66ca", "width": 320, "height": 480}], "variants": {}, "id": "LHrO0Nr6RjYhrhTbc1QR71XCyuantZcywjo_QIkTKlw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12237ot", "is_robot_indexable": true, "report_reasons": null, "author": "Franck_Dernoncourt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12237ot/column_order_in_a_dynamodb_table_browsed_through/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12237ot/column_order_in_a_dynamodb_table_browsed_through/", "subreddit_subscribers": 94398, "created_utc": 1679786885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using dbt, and I have a use case at the company I work at which I don't think is supported exactly. We use a green/blue deploy strategy for each table. When all the dbt tests for that specific table have completed successfully, I'd like to run a sql snippet to swap/promote it. Kind of like a model post-hook, but not after the model is run, instead for collection after all per-model tests pass. Does anyone have any ideas for a workaround, hack, or a package that might work?\n\nCurrently I'm writing test results to the database and having a second step to parse them, but a single pass option seems like it should be possible.", "author_fullname": "t2_dzfsy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt post-hook for successful tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121shh6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679764674.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using dbt, and I have a use case at the company I work at which I don&amp;#39;t think is supported exactly. We use a green/blue deploy strategy for each table. When all the dbt tests for that specific table have completed successfully, I&amp;#39;d like to run a sql snippet to swap/promote it. Kind of like a model post-hook, but not after the model is run, instead for collection after all per-model tests pass. Does anyone have any ideas for a workaround, hack, or a package that might work?&lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m writing test results to the database and having a second step to parse them, but a single pass option seems like it should be possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "121shh6", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_Again", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121shh6/dbt_posthook_for_successful_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121shh6/dbt_posthook_for_successful_tests/", "subreddit_subscribers": 94398, "created_utc": 1679764674.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone who has tried Polars in AWS Glue using the Shell script option, or whether it is even feasible at all? My work is dirt cheap and wants to minimize costs as much as possible lol. I'll be processing 500mb worth of data and not even sure if the 0.0625 DPU option would even cut it. For context, that is like 0.25 vCPU and 1GB RAM lol.", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars Library in AWS Glue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121ou6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679756928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone who has tried Polars in AWS Glue using the Shell script option, or whether it is even feasible at all? My work is dirt cheap and wants to minimize costs as much as possible lol. I&amp;#39;ll be processing 500mb worth of data and not even sure if the 0.0625 DPU option would even cut it. For context, that is like 0.25 vCPU and 1GB RAM lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121ou6t", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121ou6t/polars_library_in_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121ou6t/polars_library_in_aws_glue/", "subreddit_subscribers": 94398, "created_utc": 1679756928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nNew to ADF, I have an integration landing data in a container on a storage account, I use copy activity with wildcard path (and merge files in settings) to collect all the files every night, I copy them to another storage account where they are stored as parquet. After the copy activity I delete all the files in the first \"landing\" container.\n\nMy next step is to implement a python script I have, that implements scd and then I want to sink the result of that to the database. Two questions:\n\n1. The files arrive as json and I copy to parquet, sometimes it fails since copy activity does not understand what data type a specific column should be. How do I overcome this? Currently I use the advanced editor to read up some mapping from an sql-table where I explicitly pointing out what data type each column should be. However, this is not ideal since this will disregard any attribute on the json-object not defined in the mapping and I want this to be dynamic. I do not want to lose any data in this step.\n2. Where do you guys implement your python scripts on data in adf? What machine will execute it? My only demand is that the script can access the lake and the database. I know about databricks and the notebook activity in adf but spinning up a spark cluster feels a bit dramatic. The biggest parquet files is about 300-400 mb. I do use pyspark in my script though, but since it is pretty small data I think pyspark running on one simple machine would be enough? \n\nI only do the json -&gt; parquet to save storage btw, but maybe completely unnecessary? My goal here is a flow that is as minimalistic and cheap as possible where the only point of the data lake is to have all the data as \"raw\" as possible so I always can reload the database from that. I am completely open to use other tools.\n\n&amp;#x200B;\n\n Thank you in advance!", "author_fullname": "t2_7d6btbfhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ADF copy json to parquet and help on how to implement a python script on files in the lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_121esja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679730692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;New to ADF, I have an integration landing data in a container on a storage account, I use copy activity with wildcard path (and merge files in settings) to collect all the files every night, I copy them to another storage account where they are stored as parquet. After the copy activity I delete all the files in the first &amp;quot;landing&amp;quot; container.&lt;/p&gt;\n\n&lt;p&gt;My next step is to implement a python script I have, that implements scd and then I want to sink the result of that to the database. Two questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The files arrive as json and I copy to parquet, sometimes it fails since copy activity does not understand what data type a specific column should be. How do I overcome this? Currently I use the advanced editor to read up some mapping from an sql-table where I explicitly pointing out what data type each column should be. However, this is not ideal since this will disregard any attribute on the json-object not defined in the mapping and I want this to be dynamic. I do not want to lose any data in this step.&lt;/li&gt;\n&lt;li&gt;Where do you guys implement your python scripts on data in adf? What machine will execute it? My only demand is that the script can access the lake and the database. I know about databricks and the notebook activity in adf but spinning up a spark cluster feels a bit dramatic. The biggest parquet files is about 300-400 mb. I do use pyspark in my script though, but since it is pretty small data I think pyspark running on one simple machine would be enough? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I only do the json -&amp;gt; parquet to save storage btw, but maybe completely unnecessary? My goal here is a flow that is as minimalistic and cheap as possible where the only point of the data lake is to have all the data as &amp;quot;raw&amp;quot; as possible so I always can reload the database from that. I am completely open to use other tools.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "121esja", "is_robot_indexable": true, "report_reasons": null, "author": "pLebesgue", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/121esja/adf_copy_json_to_parquet_and_help_on_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/121esja/adf_copy_json_to_parquet_and_help_on_how_to/", "subreddit_subscribers": 94398, "created_utc": 1679730692.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}