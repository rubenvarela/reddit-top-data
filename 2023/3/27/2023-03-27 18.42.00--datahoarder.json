{"kind": "Listing", "data": {"after": "t3_123nmcz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_d5sfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smr hard drive at 100% active time, copying small files, but still at 100% when file copy is paused. It stays like this until I reset the machine and causes other programs to hang. Is this normal for smr?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 101, "top_awarded_type": null, "hide_score": false, "name": "t3_123g0jl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 121, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 121, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DY6JK2XWJG2bzsLiU-DqFgu_V3jUD-U_8b7yq20u0Rc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679904482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8j8jl62yo8qa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?auto=webp&amp;v=enabled&amp;s=3722830cb77cbeea00cd9571efbaf1bd505548fb", "width": 1610, "height": 1163}, "resolutions": [{"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=452a1042b19fb2d9600aef249e07a006f1d8433a", "width": 108, "height": 78}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe7e15d8aa133955b2f4c3239be38f2885ac5417", "width": 216, "height": 156}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efad0366954076f53ecce0fe01fa12530583dd39", "width": 320, "height": 231}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19bb55f8503cb77b9c676745911b7f1b168072fd", "width": 640, "height": 462}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91e470f5180ff6beaf0571157ded6b1cfe8a8309", "width": 960, "height": 693}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e364e68cbe7403aff77612aea4ac37c68f8ac590", "width": 1080, "height": 780}], "variants": {}, "id": "DB_ZhI18qR7656g_EQRuq-TN-_nrrehZX9OTOGY7RDc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123g0jl", "is_robot_indexable": true, "report_reasons": null, "author": "bluejeans90210", "discussion_type": null, "num_comments": 80, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123g0jl/smr_hard_drive_at_100_active_time_copying_small/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8j8jl62yo8qa1.png", "subreddit_subscribers": 675625, "created_utc": 1679904482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nintendo 3DS and Wii U eShop permanently shutting down in two days", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1230d5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_xlxw6", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/kD1t4xtjieT_weReTKpTttSrPwYP_c1imPsPu1dRXb8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "LeftistGamersUnion", "selftext": "", "author_fullname": "t2_604w21c5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nintendo 3DS and Wii U eShop permanently shutting down in two days", "link_flair_richtext": [], "subreddit_name_prefixed": "r/LeftistGamersUnion", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_122vw4h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 93, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 93, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/kD1t4xtjieT_weReTKpTttSrPwYP_c1imPsPu1dRXb8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1679857231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "digitalspy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.digitalspy.com/tech/a43416801/nintendo-3ds-wii-u-eshop-closing-soon/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?auto=webp&amp;v=enabled&amp;s=d2508aac7dd1c8e0a38e9e1fa8edb0a5798337c9", "width": 1200, "height": 602}, "resolutions": [{"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86d5bdefd8271b02f68c28860888436dfcdc1585", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ea407e90ccf6a3ff8098bc062397ef9a14e674a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=362630d41461db73c14d03628ce6ddd386cb003d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e35bb124bdaaba252694963b25dbede2db1c7aaa", "width": 640, "height": 321}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2cd7f19276031197b5566b5c2a29c2fd2c95806", "width": 960, "height": 481}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db7af360ef66e8fd26c60cd613f8191bb96a937f", "width": 1080, "height": 541}], "variants": {}, "id": "TFaL8O0G456e8qJGa0jZzv4TTXACVycX3ns-KHJEjm4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_ubm2e", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122vw4h", "is_robot_indexable": true, "report_reasons": null, "author": "yuritopiaposadism", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LeftistGamersUnion/comments/122vw4h/nintendo_3ds_and_wii_u_eshop_permanently_shutting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.digitalspy.com/tech/a43416801/nintendo-3ds-wii-u-eshop-closing-soon/", "subreddit_subscribers": 17997, "created_utc": 1679857231.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1679866214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "digitalspy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.digitalspy.com/tech/a43416801/nintendo-3ds-wii-u-eshop-closing-soon/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?auto=webp&amp;v=enabled&amp;s=d2508aac7dd1c8e0a38e9e1fa8edb0a5798337c9", "width": 1200, "height": 602}, "resolutions": [{"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86d5bdefd8271b02f68c28860888436dfcdc1585", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ea407e90ccf6a3ff8098bc062397ef9a14e674a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=362630d41461db73c14d03628ce6ddd386cb003d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e35bb124bdaaba252694963b25dbede2db1c7aaa", "width": 640, "height": 321}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2cd7f19276031197b5566b5c2a29c2fd2c95806", "width": 960, "height": 481}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db7af360ef66e8fd26c60cd613f8191bb96a937f", "width": 1080, "height": 541}], "variants": {}, "id": "TFaL8O0G456e8qJGa0jZzv4TTXACVycX3ns-KHJEjm4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1230d5a", "is_robot_indexable": true, "report_reasons": null, "author": "focus_rising", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_122vw4h", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1230d5a/nintendo_3ds_and_wii_u_eshop_permanently_shutting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.digitalspy.com/tech/a43416801/nintendo-3ds-wii-u-eshop-closing-soon/", "subreddit_subscribers": 675625, "created_utc": 1679866214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I heard backblaze is a very good option for a backing up one\u2019s PC, however, I also noticed that a lot of people complain about the restore process once their drive fails or whatever mishaps happen. Are there any other alternatives that offer a better restore process? I\u2019m fairly new to backing up stuff (lost precious memories before and had to learn it the hard way\u2026) but I always keep a copy of my PC files/data on an external SSD. I definitely want to have more copies of my data, not just in an external drive as everything is prone to failure/degradation/etc. I was thinking of using backblaze and sync.com (one for pure backup and the other for syncing) but I\u2019m undecided on backblaze.", "author_fullname": "t2_4wwjb46d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on backblaze for backing up a PC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1239123", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": 1679886355.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679885669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I heard backblaze is a very good option for a backing up one\u2019s PC, however, I also noticed that a lot of people complain about the restore process once their drive fails or whatever mishaps happen. Are there any other alternatives that offer a better restore process? I\u2019m fairly new to backing up stuff (lost precious memories before and had to learn it the hard way\u2026) but I always keep a copy of my PC files/data on an external SSD. I definitely want to have more copies of my data, not just in an external drive as everything is prone to failure/degradation/etc. I was thinking of using backblaze and sync.com (one for pure backup and the other for syncing) but I\u2019m undecided on backblaze.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1239123", "is_robot_indexable": true, "report_reasons": null, "author": "calpthemcheeks", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1239123/thoughts_on_backblaze_for_backing_up_a_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1239123/thoughts_on_backblaze_for_backing_up_a_pc/", "subreddit_subscribers": 675625, "created_utc": 1679885669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been writing some scripts for my own personal use recently due to an interesting use-case. I have data that I want to store on one machine in one format, and multiple target machines that I want to sync the data to in slightly varying formats. If the source files change, I want to resync and re-transform the files into the proper formats. The result of weeks of polishing these scripts up is this budding new GitHub project of mine, that I am calling (for now, anyway) \"Dumb-Sync\".\n\nMain page and README: [https://github.com/sloshy/dumb-sync](https://github.com/sloshy/dumb-sync)\n\nLicense: LGPL v2.1 (subject to change)\n\nBasically the idea is this: you create a JSON file that describes one or more remote directories you wish to sync to your local machine. From there, you can optionally wire in multiple transformation steps that just exist as bash scripts. Each of these scripts can take any config arguments you want, specified in each JSON config or dynamically from current runtime values (i.e. the current file name, output directory, list of all known remote files). You can set these scripts to be ran once per-file, or once per-directory,  and you can even include your own comparison logic scripts for determining if a transformed file should be synced, kept in case it's removed on a remote system, and so on.\n\nIn the [example transformations directory](https://github.com/sloshy/dumb-sync/tree/main/example_transformations) there is an example config that illustrates the following scenario:\n\n* You have a remote directory full of ZIP files.\n* Inside each ZIP file are BIN/CUE files representing lossless disc images.\n* You want to sync only the ZIP files from that directory to your local machine.\n* Once per-file, you want to extract all of its contents to the current output directory using 7Zip.\n* After they are extracted, you want to re-compress them in the CHD format, which is from the MAME project for creating real-time playable disc images that take up less space.\n* If any files exist afterward that are not meant to be there, delete them on each re-sync.\n\nWith the example config, after each transformation, the local files are only ever re-synced if the remote they are derived from changes. A time offset is stored for your config that displays the latest time a sync was attempted. This might get options to be more granular in the future, but for now it is just a single timestamp for all configs.\n\nThe scripts are still being regularly updated, as I just released this publicly over the weekend, and it will likely gain several more features as time goes on. Just recently I added support for specifying minimum and maximum file size thresholds for syncing. The general idea is that eventually this will encompass most of the meaningful featureset of \"rsync\" which this uses internally, and grow to be even more flexible in ways that people find useful. For example, I would love a way for people to share template configs that other people can plug into their own in order to import scripts and even sync configurations automatically or using parameter options. For now, this mostly just covers my own immediate use cases though.\n\nLet me know what you think and please feel free to report any bugs or feature requests!", "author_fullname": "t2_cb3vo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dumb-Sync: A bash/rsync/jq-based microframework for syncing transformed files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123d8q7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679896800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been writing some scripts for my own personal use recently due to an interesting use-case. I have data that I want to store on one machine in one format, and multiple target machines that I want to sync the data to in slightly varying formats. If the source files change, I want to resync and re-transform the files into the proper formats. The result of weeks of polishing these scripts up is this budding new GitHub project of mine, that I am calling (for now, anyway) &amp;quot;Dumb-Sync&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Main page and README: &lt;a href=\"https://github.com/sloshy/dumb-sync\"&gt;https://github.com/sloshy/dumb-sync&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;License: LGPL v2.1 (subject to change)&lt;/p&gt;\n\n&lt;p&gt;Basically the idea is this: you create a JSON file that describes one or more remote directories you wish to sync to your local machine. From there, you can optionally wire in multiple transformation steps that just exist as bash scripts. Each of these scripts can take any config arguments you want, specified in each JSON config or dynamically from current runtime values (i.e. the current file name, output directory, list of all known remote files). You can set these scripts to be ran once per-file, or once per-directory,  and you can even include your own comparison logic scripts for determining if a transformed file should be synced, kept in case it&amp;#39;s removed on a remote system, and so on.&lt;/p&gt;\n\n&lt;p&gt;In the &lt;a href=\"https://github.com/sloshy/dumb-sync/tree/main/example_transformations\"&gt;example transformations directory&lt;/a&gt; there is an example config that illustrates the following scenario:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You have a remote directory full of ZIP files.&lt;/li&gt;\n&lt;li&gt;Inside each ZIP file are BIN/CUE files representing lossless disc images.&lt;/li&gt;\n&lt;li&gt;You want to sync only the ZIP files from that directory to your local machine.&lt;/li&gt;\n&lt;li&gt;Once per-file, you want to extract all of its contents to the current output directory using 7Zip.&lt;/li&gt;\n&lt;li&gt;After they are extracted, you want to re-compress them in the CHD format, which is from the MAME project for creating real-time playable disc images that take up less space.&lt;/li&gt;\n&lt;li&gt;If any files exist afterward that are not meant to be there, delete them on each re-sync.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;With the example config, after each transformation, the local files are only ever re-synced if the remote they are derived from changes. A time offset is stored for your config that displays the latest time a sync was attempted. This might get options to be more granular in the future, but for now it is just a single timestamp for all configs.&lt;/p&gt;\n\n&lt;p&gt;The scripts are still being regularly updated, as I just released this publicly over the weekend, and it will likely gain several more features as time goes on. Just recently I added support for specifying minimum and maximum file size thresholds for syncing. The general idea is that eventually this will encompass most of the meaningful featureset of &amp;quot;rsync&amp;quot; which this uses internally, and grow to be even more flexible in ways that people find useful. For example, I would love a way for people to share template configs that other people can plug into their own in order to import scripts and even sync configurations automatically or using parameter options. For now, this mostly just covers my own immediate use cases though.&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think and please feel free to report any bugs or feature requests!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?auto=webp&amp;v=enabled&amp;s=1b05a6503cf7321bcabd6fffc6de3e782c98f09b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=781490f75b82b874929b7369ddfe5d7ccba2ed31", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a87e4648dd1ff81e6507801666027f14ee2a4857", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b179abb9c1b8ebacf4a5d02554c50fc8aea793b7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90c0f920565833f809ed411db25ec817ccb187e8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14c75efb92af99897a5b65a689691238eb7de2a4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f71a5e12e386d900f540bf882b8cda4539d8cc9", "width": 1080, "height": 540}], "variants": {}, "id": "YmfaUbVP1XUFLOtPgkgC3giOyD9givyh-uQPhGgTUPI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123d8q7", "is_robot_indexable": true, "report_reasons": null, "author": "Sloshy42", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123d8q7/dumbsync_a_bashrsyncjqbased_microframework_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123d8q7/dumbsync_a_bashrsyncjqbased_microframework_for/", "subreddit_subscribers": 675625, "created_utc": 1679896800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So got my new workstation Thursday.  \n\\- Part of this setup is a 6 x 18tb set of disks connected to the motherboard's SATA connections.  \n\\- I am not using the motherboard's RAID\n\nI initially set up the RAID using linux mdadm.  About 80gb into copy there was a array failure, but upon reboot it was fine again.  Resumed copying and after a hour of copying again it failed once more.  \n\n\nI then ran SMART checks on the disks, everything came back reading 100% fine.  \n\n\nThinking this could be a software issue I wiped the array, deleted all the mdadm configs and installed ZFS and set up a pool with the same disks.  This got me a little further, about two terrabytes in the pool went into a offline state but after a reboot it was fine again.  \n\n\nI have checked in dmesg and found several angry comments about failures on sda and sdb, so I then checked against the zpool when I resumed copying expecting it to fail.  The pool agreed stating that sda and sdb had hundreds of write failures.\n\nI have been running badblocks on the two disks since Saturday night, it's already finishing the second pass on the two disks and so far nothing is showing up as bad, zero blocks failed.\n\nThis is my conundrum, all of the disk checks swear that the disks are perfectly fine, but using them makes the OS/array say otherwise.  So far the only thing I could assume is the cables could be defective?", "author_fullname": "t2_d3vpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mixed messages on a new drive array", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123m7o5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679921854.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So got my new workstation Thursday.&lt;br/&gt;\n- Part of this setup is a 6 x 18tb set of disks connected to the motherboard&amp;#39;s SATA connections.&lt;br/&gt;\n- I am not using the motherboard&amp;#39;s RAID&lt;/p&gt;\n\n&lt;p&gt;I initially set up the RAID using linux mdadm.  About 80gb into copy there was a array failure, but upon reboot it was fine again.  Resumed copying and after a hour of copying again it failed once more.  &lt;/p&gt;\n\n&lt;p&gt;I then ran SMART checks on the disks, everything came back reading 100% fine.  &lt;/p&gt;\n\n&lt;p&gt;Thinking this could be a software issue I wiped the array, deleted all the mdadm configs and installed ZFS and set up a pool with the same disks.  This got me a little further, about two terrabytes in the pool went into a offline state but after a reboot it was fine again.  &lt;/p&gt;\n\n&lt;p&gt;I have checked in dmesg and found several angry comments about failures on sda and sdb, so I then checked against the zpool when I resumed copying expecting it to fail.  The pool agreed stating that sda and sdb had hundreds of write failures.&lt;/p&gt;\n\n&lt;p&gt;I have been running badblocks on the two disks since Saturday night, it&amp;#39;s already finishing the second pass on the two disks and so far nothing is showing up as bad, zero blocks failed.&lt;/p&gt;\n\n&lt;p&gt;This is my conundrum, all of the disk checks swear that the disks are perfectly fine, but using them makes the OS/array say otherwise.  So far the only thing I could assume is the cables could be defective?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123m7o5", "is_robot_indexable": true, "report_reasons": null, "author": "TheIllusioneer", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123m7o5/mixed_messages_on_a_new_drive_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123m7o5/mixed_messages_on_a_new_drive_array/", "subreddit_subscribers": 675625, "created_utc": 1679921854.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone successfully ripped video files (movies, TV shows, etc) onto a BD-R disc and played it on a Blu-Ray player or console?\n\nAny recommendations on best software for ripping/menu creation/misc?\n\nDoes DRM prevent this from working?\n\nI imagine that most people in this sub are doing the opposite. Ripping the Blu's onto PC. I don't know where else to go for this information.", "author_fullname": "t2_8zpyfcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping to Blu-Ray discs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123apwr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679889739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone successfully ripped video files (movies, TV shows, etc) onto a BD-R disc and played it on a Blu-Ray player or console?&lt;/p&gt;\n\n&lt;p&gt;Any recommendations on best software for ripping/menu creation/misc?&lt;/p&gt;\n\n&lt;p&gt;Does DRM prevent this from working?&lt;/p&gt;\n\n&lt;p&gt;I imagine that most people in this sub are doing the opposite. Ripping the Blu&amp;#39;s onto PC. I don&amp;#39;t know where else to go for this information.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123apwr", "is_robot_indexable": true, "report_reasons": null, "author": "HunteHorseman", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123apwr/ripping_to_bluray_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123apwr/ripping_to_bluray_discs/", "subreddit_subscribers": 675625, "created_utc": 1679889739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I may end up getting a bunch of HDDs, possibly 50+, not sure of the size. The last ones were 500 GB I am sure these ones are going to be larger , possibly 1 tb plus.  The PCs are ones that I am planning to give away to local charity but may strip down a few of them to personal use.  Or repurpose. This is PCs from local college.\n\nHere is my dilema.\n\nI currently have 20 HDDs 111,599 GB various sizes. External, internal, Enclosure etc. I currently have room for 3 more HDDS in an enclosure.  And have an  option for quick HDD swaps with one external device that I have.\n\nAll these hard drives are used for backup, movies, pictures, picture archives etc.\n\nAny suggestions on expanding for room for multiple HDD additions.", "author_fullname": "t2_f1bbee6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding New HDDs to my 20 HDD collection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123829e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679883446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I may end up getting a bunch of HDDs, possibly 50+, not sure of the size. The last ones were 500 GB I am sure these ones are going to be larger , possibly 1 tb plus.  The PCs are ones that I am planning to give away to local charity but may strip down a few of them to personal use.  Or repurpose. This is PCs from local college.&lt;/p&gt;\n\n&lt;p&gt;Here is my dilema.&lt;/p&gt;\n\n&lt;p&gt;I currently have 20 HDDs 111,599 GB various sizes. External, internal, Enclosure etc. I currently have room for 3 more HDDS in an enclosure.  And have an  option for quick HDD swaps with one external device that I have.&lt;/p&gt;\n\n&lt;p&gt;All these hard drives are used for backup, movies, pictures, picture archives etc.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions on expanding for room for multiple HDD additions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123829e", "is_robot_indexable": true, "report_reasons": null, "author": "Monkeydu2", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123829e/adding_new_hdds_to_my_20_hdd_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123829e/adding_new_hdds_to_my_20_hdd_collection/", "subreddit_subscribers": 675625, "created_utc": 1679883446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I'm transferring about 8tb from an internal 8tb SSD to a new QNAP NAS that's running two 16tb HDD's set up as a single RAID1 volume. \n\nOver the course of the several days that the files have been copying, I'm getting some intermittent error messages regarding the transfer. Here's a few examples: \n\n&amp;#x200B;\n\n&gt;10:48:11: Created: 230324-144811-493-737.db  \n10:48:58: Copying...  \n10:48:58: Source: F:\\\\  \n10:48:58: Target: \\\\\\\\NASStorage\\\\SSDDATA  \n12:48:24: F:\\\\Games\\\\Arcade\\\\MAME 0.250 EXTRAs\\\\covers\\_SL.zip  \n12:48:24: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392  \n12:48:24: ------------  \n12:48:56: F:\\\\Games\\\\Arcade\\\\MAME 0.250 EXTRAs\\\\devices.zip  \n12:48:56: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392  \n12:48:56: ------------  \n13:06:23: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\3countb\\\\3countb.txt  \n13:06:23: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392  \n13:06:23: ------------  \n13:08:15: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\iron\\\\01 Demo.mp3  \n13:08:15: Error opening source file. The system cannot find the file specified. Code: 2  \n13:08:15: ------------  \n13:08:55: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\mslug3\\\\07 First Contracts.mp3  \n13:08:55: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392  \n13:08:55: ------------  \n13:08:55: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\mslug3\\\\08 Escape.mp3  \n13:08:55: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392  \n13:08:55: ------------  \n13:09:53: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\vendetta\\\\12 Bay Area (Stage 4).mp3  \n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2  \n13:09:53: ------------  \n13:09:53: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\vendetta\\\\13 The Armageddon (Final Boss).mp3  \n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2  \n13:09:53: ------------\n\n So far, it appears to only be throwing errors with files in the MAME directory.  \n\n\nI navigated over to F:\\\\Games\\\\Arcade\\\\MAME 0.250 EXTRAs and tried to open covers\\_SL.zip with 7zip and it throws the same error. Weird. I'm not sure how these files became corrupted. So TeraCopy won't even copy them over if it's discovered the archives are corrupted? I guess that's fine since I don't have any use for corrupted files.  \n\n\nNow I just navigated to F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\3countb and 3countb.txt doesn't even exist (even if viewing hidden files)...what the hell? \n\n I also just navigated over to the directory for these errors:  \n\n\nF:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\vendetta\\\\12 Bay Area (Stage 4).mp3  \n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2  \n13:09:53: ------------  \n13:09:53: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\vendetta\\\\13 The Armageddon (Final Boss).mp3  \n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2  \n\n\nThose files are also missing from the directory...  \n\n\nBut tracks 1 through 11 and 14 on, are all there.  \n\n\nI don't get it. They \\*were\\* there until TeraCopy went to copy them and then they...disappeared? Teracopy...removed them? Either that, or Teracopy somehow knows they're supposed to be there and is expecting them, and they've always not been there. I'm assuming that's highly unlikely. \n\nSo...the files were there enough for Teracopy to see them and try to copy them, and then they immediately stopped existing, and then Teracopy logged the error in the...what?  \n\n\nThis is getting really strange now.", "author_fullname": "t2_96z9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Teracopy deleting my files during transfer? Something really strange is going on here...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1234afa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679874678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m transferring about 8tb from an internal 8tb SSD to a new QNAP NAS that&amp;#39;s running two 16tb HDD&amp;#39;s set up as a single RAID1 volume. &lt;/p&gt;\n\n&lt;p&gt;Over the course of the several days that the files have been copying, I&amp;#39;m getting some intermittent error messages regarding the transfer. Here&amp;#39;s a few examples: &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;10:48:11: Created: 230324-144811-493-737.db&lt;br/&gt;\n10:48:58: Copying...&lt;br/&gt;\n10:48:58: Source: F:\\&lt;br/&gt;\n10:48:58: Target: \\\\NASStorage\\SSDDATA&lt;br/&gt;\n12:48:24: F:\\Games\\Arcade\\MAME 0.250 EXTRAs\\covers_SL.zip&lt;br/&gt;\n12:48:24: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392&lt;br/&gt;\n12:48:24: ------------&lt;br/&gt;\n12:48:56: F:\\Games\\Arcade\\MAME 0.250 EXTRAs\\devices.zip&lt;br/&gt;\n12:48:56: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392&lt;br/&gt;\n12:48:56: ------------&lt;br/&gt;\n13:06:23: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\3countb\\3countb.txt&lt;br/&gt;\n13:06:23: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392&lt;br/&gt;\n13:06:23: ------------&lt;br/&gt;\n13:08:15: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\iron\\01 Demo.mp3&lt;br/&gt;\n13:08:15: Error opening source file. The system cannot find the file specified. Code: 2&lt;br/&gt;\n13:08:15: ------------&lt;br/&gt;\n13:08:55: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\mslug3\\07 First Contracts.mp3&lt;br/&gt;\n13:08:55: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392&lt;br/&gt;\n13:08:55: ------------&lt;br/&gt;\n13:08:55: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\mslug3\\08 Escape.mp3&lt;br/&gt;\n13:08:55: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392&lt;br/&gt;\n13:08:55: ------------&lt;br/&gt;\n13:09:53: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\vendetta\\12 Bay Area (Stage 4).mp3&lt;br/&gt;\n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2&lt;br/&gt;\n13:09:53: ------------&lt;br/&gt;\n13:09:53: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\vendetta\\13 The Armageddon (Final Boss).mp3&lt;br/&gt;\n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2&lt;br/&gt;\n13:09:53: ------------&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;So far, it appears to only be throwing errors with files in the MAME directory.  &lt;/p&gt;\n\n&lt;p&gt;I navigated over to F:\\Games\\Arcade\\MAME 0.250 EXTRAs and tried to open covers_SL.zip with 7zip and it throws the same error. Weird. I&amp;#39;m not sure how these files became corrupted. So TeraCopy won&amp;#39;t even copy them over if it&amp;#39;s discovered the archives are corrupted? I guess that&amp;#39;s fine since I don&amp;#39;t have any use for corrupted files.  &lt;/p&gt;\n\n&lt;p&gt;Now I just navigated to F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\3countb and 3countb.txt doesn&amp;#39;t even exist (even if viewing hidden files)...what the hell? &lt;/p&gt;\n\n&lt;p&gt;I also just navigated over to the directory for these errors:  &lt;/p&gt;\n\n&lt;p&gt;F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\vendetta\\12 Bay Area (Stage 4).mp3&lt;br/&gt;\n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2&lt;br/&gt;\n13:09:53: ------------&lt;br/&gt;\n13:09:53: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\vendetta\\13 The Armageddon (Final Boss).mp3&lt;br/&gt;\n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2  &lt;/p&gt;\n\n&lt;p&gt;Those files are also missing from the directory...  &lt;/p&gt;\n\n&lt;p&gt;But tracks 1 through 11 and 14 on, are all there.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t get it. They *were* there until TeraCopy went to copy them and then they...disappeared? Teracopy...removed them? Either that, or Teracopy somehow knows they&amp;#39;re supposed to be there and is expecting them, and they&amp;#39;ve always not been there. I&amp;#39;m assuming that&amp;#39;s highly unlikely. &lt;/p&gt;\n\n&lt;p&gt;So...the files were there enough for Teracopy to see them and try to copy them, and then they immediately stopped existing, and then Teracopy logged the error in the...what?  &lt;/p&gt;\n\n&lt;p&gt;This is getting really strange now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1234afa", "is_robot_indexable": true, "report_reasons": null, "author": "ultranothing", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1234afa/is_teracopy_deleting_my_files_during_transfer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1234afa/is_teracopy_deleting_my_files_during_transfer/", "subreddit_subscribers": 675625, "created_utc": 1679874678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone know of a tutorial for for creating a Winrar command line process for a periodic full folder back up for windows scheduler?  \nI've looked around the help file but not as straight forward as i would wish.", "author_fullname": "t2_12sxeu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tutorial for for creating Winrar command line back up for a folder for windows scheduler", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122wnb3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679858832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know of a tutorial for for creating a Winrar command line process for a periodic full folder back up for windows scheduler?&lt;br/&gt;\nI&amp;#39;ve looked around the help file but not as straight forward as i would wish.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "122wnb3", "is_robot_indexable": true, "report_reasons": null, "author": "ctles", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/122wnb3/tutorial_for_for_creating_winrar_command_line/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/122wnb3/tutorial_for_for_creating_winrar_command_line/", "subreddit_subscribers": 675625, "created_utc": 1679858832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a license for it but it's starting to get on my nerves. The backups are excruciatingly slow, like it's taking a whole afternoon to incorporate a 10 GB incremental backup image into the main one. It doesn't need to be free but I don't want any subscription based services. Thank you for your help.", "author_fullname": "t2_8muaq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any alternatives to Macrium Reflect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_123t7el", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679936485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a license for it but it&amp;#39;s starting to get on my nerves. The backups are excruciatingly slow, like it&amp;#39;s taking a whole afternoon to incorporate a 10 GB incremental backup image into the main one. It doesn&amp;#39;t need to be free but I don&amp;#39;t want any subscription based services. Thank you for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123t7el", "is_robot_indexable": true, "report_reasons": null, "author": "nicktheone", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123t7el/any_alternatives_to_macrium_reflect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123t7el/any_alternatives_to_macrium_reflect/", "subreddit_subscribers": 675625, "created_utc": 1679936485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I recently had a hard disk crash. Fortunately I had a local backup but it was a few months old. I have been using Idrive for a couple years. Well I went to restore the more recent files that aren't on my local backup from Idrive but there are lots and lots of files missing. Like the folder structure is intact but the folders are just empty! So basically I've lost who knows what.\n\nAre there any reliable cloud backup providers you recommend? I've currently got 10 TB personal plan with IDrive and running out of space.\n\nThanks", "author_fullname": "t2_a3875", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better alternatives to Idrive? That are reliable.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12339yp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679872366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I recently had a hard disk crash. Fortunately I had a local backup but it was a few months old. I have been using Idrive for a couple years. Well I went to restore the more recent files that aren&amp;#39;t on my local backup from Idrive but there are lots and lots of files missing. Like the folder structure is intact but the folders are just empty! So basically I&amp;#39;ve lost who knows what.&lt;/p&gt;\n\n&lt;p&gt;Are there any reliable cloud backup providers you recommend? I&amp;#39;ve currently got 10 TB personal plan with IDrive and running out of space.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12339yp", "is_robot_indexable": true, "report_reasons": null, "author": "roofoo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12339yp/better_alternatives_to_idrive_that_are_reliable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12339yp/better_alternatives_to_idrive_that_are_reliable/", "subreddit_subscribers": 675625, "created_utc": 1679872366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I don't know if that's the right place to post such a question, but I figured that maybe someone here has had the same issue I'm having. I stored around 650gb of data on an illimited google drive archive offered by my university, but I recently got an email telling me that this is going to change and that they are going to shrink the space available to students. So I bought an external 2tb SSD and decided to download everything I have on google drive there. I tried using google takeout but I think they made it very frustrating on purpouse: they split your data in multiple zip files and it's almost certain that you will get some error downloading the zip files. Long story short I managed to download only 6 zip files out of 16. So I'm looking for alternatives. I often used google colab to download content from another shared drive to my personal drive, and I was wondering if it was possibile to download folders from google drive to an external drive using it. That way I could just write the script, launch it and go to sleep. I'm also opened to basically anything else that would allow me to move my data out from google drive.  \n\n\nThanks in advance for your help. I apologize for my english (I'm not an english speaker) and if it wasn't the right place to post it.", "author_fullname": "t2_5a0gnoys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using google colab to download the entirety of google drive to external ssd", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_122y6w5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679861942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I don&amp;#39;t know if that&amp;#39;s the right place to post such a question, but I figured that maybe someone here has had the same issue I&amp;#39;m having. I stored around 650gb of data on an illimited google drive archive offered by my university, but I recently got an email telling me that this is going to change and that they are going to shrink the space available to students. So I bought an external 2tb SSD and decided to download everything I have on google drive there. I tried using google takeout but I think they made it very frustrating on purpouse: they split your data in multiple zip files and it&amp;#39;s almost certain that you will get some error downloading the zip files. Long story short I managed to download only 6 zip files out of 16. So I&amp;#39;m looking for alternatives. I often used google colab to download content from another shared drive to my personal drive, and I was wondering if it was possibile to download folders from google drive to an external drive using it. That way I could just write the script, launch it and go to sleep. I&amp;#39;m also opened to basically anything else that would allow me to move my data out from google drive.  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help. I apologize for my english (I&amp;#39;m not an english speaker) and if it wasn&amp;#39;t the right place to post it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "122y6w5", "is_robot_indexable": true, "report_reasons": null, "author": "L_ESIMIO", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/122y6w5/using_google_colab_to_download_the_entirety_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/122y6w5/using_google_colab_to_download_the_entirety_of/", "subreddit_subscribers": 675625, "created_utc": 1679861942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am searching amazon, looks like some great deals until I read the reviews. Alot of these advertise USB 3.0 but have read/write speeds of USB 2.0. Curious as to your experiences, there has to be a USB drive that has the sweet spot of value &amp; speed. Maybe not the absolute fastest but close to the speed it advertises and a good price. Anyone?", "author_fullname": "t2_ay71t455", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for best speed &amp; value USB 3.0 or 3.1 flash drive 128 or 256gb", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123r3mm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679932312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am searching amazon, looks like some great deals until I read the reviews. Alot of these advertise USB 3.0 but have read/write speeds of USB 2.0. Curious as to your experiences, there has to be a USB drive that has the sweet spot of value &amp;amp; speed. Maybe not the absolute fastest but close to the speed it advertises and a good price. Anyone?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123r3mm", "is_robot_indexable": true, "report_reasons": null, "author": "SunnyDay20212", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123r3mm/looking_for_best_speed_value_usb_30_or_31_flash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123r3mm/looking_for_best_speed_value_usb_30_or_31_flash/", "subreddit_subscribers": 675625, "created_utc": 1679932312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was wondering if this even exists. I'd like to connect a few 3.5in HDD to my mini server via a couple sata ports. But I'm powering it with a PicoPSU which doesn't have enough power to power the additional HDDs. What I do have is a bunch of 12v 6/8 pin connectors via a server PSU.\n\nAre there any sata backplanes that take 12v 6/8 pin PCIE power connectors?", "author_fullname": "t2_ef5xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a 12v powered sata backplane", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123eyqc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679901734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if this even exists. I&amp;#39;d like to connect a few 3.5in HDD to my mini server via a couple sata ports. But I&amp;#39;m powering it with a PicoPSU which doesn&amp;#39;t have enough power to power the additional HDDs. What I do have is a bunch of 12v 6/8 pin connectors via a server PSU.&lt;/p&gt;\n\n&lt;p&gt;Are there any sata backplanes that take 12v 6/8 pin PCIE power connectors?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123eyqc", "is_robot_indexable": true, "report_reasons": null, "author": "FallingSnowStar", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123eyqc/looking_for_a_12v_powered_sata_backplane/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123eyqc/looking_for_a_12v_powered_sata_backplane/", "subreddit_subscribers": 675625, "created_utc": 1679901734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Moving some videos from my wifes portable harddrive to my server, and almost all of the videos have spots where they freeze frame and play max volume static for about one second before continuing on and doing it again.\n\n\nEverything else on here seems OK, no issues with pictures, music, or other files.\n\nAny suggestions as to the best effort to recover these? (I'm thinking some kind of utility that will just cut out the static/frozen bits)", "author_fullname": "t2_4upw0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Videos recovered off a portable harddrive pause and play loud static every few seconds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1236vpm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679880757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Moving some videos from my wifes portable harddrive to my server, and almost all of the videos have spots where they freeze frame and play max volume static for about one second before continuing on and doing it again.&lt;/p&gt;\n\n&lt;p&gt;Everything else on here seems OK, no issues with pictures, music, or other files.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions as to the best effort to recover these? (I&amp;#39;m thinking some kind of utility that will just cut out the static/frozen bits)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1236vpm", "is_robot_indexable": true, "report_reasons": null, "author": "Doggins", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1236vpm/videos_recovered_off_a_portable_harddrive_pause/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1236vpm/videos_recovered_off_a_portable_harddrive_pause/", "subreddit_subscribers": 675625, "created_utc": 1679880757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "shit, how do i even start this\n\ntoday a friend of mine gave me her phone to just save pictures on her external hard drive\n\nit was an SP Armor A80 1TB drive (never heard of this company called Silicon Power probably terrible)\n\nanyways I connect it to a windows PC, the USB controller recognizes the drive, drive spins\u2026 everythings great\n\nI then start to copy the DCIM folder, but I get some error and the file explorer freezes so I end task, then I delete the messed up copied file from the drive and try to recopy just the images instead\n\nNext thing you know I replug the drive and nothing is happening, it shows the Silicon Power drive logo in diskmgmt.msc, but the drive is not actually readable\n\nI then go on linux and type lsblk, nope the drive isnt showing\n\nTyping dmesg I see that it spins the disk, then a bunch of errors showing not responding show up and thats that\n\n\nI decided ok, maybe I should remove it from the external case and plug into my PC internally using SATA cables.\n\nNope same issue\n\nSo now im SOL, I want to recover everything as Im not sure what I did wrong, and it would break my heart knowing I just lost a good friend of mine all of her photos and videos", "author_fullname": "t2_6n2n1f8zk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "recovery of 1TB external hard drive from a friend", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1232q97", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679871160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;shit, how do i even start this&lt;/p&gt;\n\n&lt;p&gt;today a friend of mine gave me her phone to just save pictures on her external hard drive&lt;/p&gt;\n\n&lt;p&gt;it was an SP Armor A80 1TB drive (never heard of this company called Silicon Power probably terrible)&lt;/p&gt;\n\n&lt;p&gt;anyways I connect it to a windows PC, the USB controller recognizes the drive, drive spins\u2026 everythings great&lt;/p&gt;\n\n&lt;p&gt;I then start to copy the DCIM folder, but I get some error and the file explorer freezes so I end task, then I delete the messed up copied file from the drive and try to recopy just the images instead&lt;/p&gt;\n\n&lt;p&gt;Next thing you know I replug the drive and nothing is happening, it shows the Silicon Power drive logo in diskmgmt.msc, but the drive is not actually readable&lt;/p&gt;\n\n&lt;p&gt;I then go on linux and type lsblk, nope the drive isnt showing&lt;/p&gt;\n\n&lt;p&gt;Typing dmesg I see that it spins the disk, then a bunch of errors showing not responding show up and thats that&lt;/p&gt;\n\n&lt;p&gt;I decided ok, maybe I should remove it from the external case and plug into my PC internally using SATA cables.&lt;/p&gt;\n\n&lt;p&gt;Nope same issue&lt;/p&gt;\n\n&lt;p&gt;So now im SOL, I want to recover everything as Im not sure what I did wrong, and it would break my heart knowing I just lost a good friend of mine all of her photos and videos&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1232q97", "is_robot_indexable": true, "report_reasons": null, "author": "helloidkwhatthisis1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1232q97/recovery_of_1tb_external_hard_drive_from_a_friend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1232q97/recovery_of_1tb_external_hard_drive_from_a_friend/", "subreddit_subscribers": 675625, "created_utc": 1679871160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got a raidz array (1 disk of redundancy, the ZFS equivalent of RAID 5) with 5x 4TB drives. One of them seems to have failed.\n\nIt's certainly not unreasonable that the hardware just wore out. It's got a power-on time of about 6 years, as do the other drives in my array, and the SMART short self-test shows a failure.\n\nThe failed drive is a Western Digital Red Pro WDC WD4001FFSX-68JNUN0\n\nI'm running all five drives off of a used 3ware 9000 series HBA card I got for cheap 7 years ago. My 16TB array currently has 10TB used, and it's on an ubuntu server in an ATX desktop case.\n\nZFS has saved my data (as long as I don't have another problem), so that's great. But it's clearly time to retire this drive pool. What should I replace it with? \n\n&amp;#x200B;\n\nAre there any gotchas with newer drives that I should be aware of, that weren't true of hard drives 7 years ago?", "author_fullname": "t2_l13ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drive failed in raidz array. What should I replace it with?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1232875", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679870992.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679870066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a raidz array (1 disk of redundancy, the ZFS equivalent of RAID 5) with 5x 4TB drives. One of them seems to have failed.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s certainly not unreasonable that the hardware just wore out. It&amp;#39;s got a power-on time of about 6 years, as do the other drives in my array, and the SMART short self-test shows a failure.&lt;/p&gt;\n\n&lt;p&gt;The failed drive is a Western Digital Red Pro WDC WD4001FFSX-68JNUN0&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running all five drives off of a used 3ware 9000 series HBA card I got for cheap 7 years ago. My 16TB array currently has 10TB used, and it&amp;#39;s on an ubuntu server in an ATX desktop case.&lt;/p&gt;\n\n&lt;p&gt;ZFS has saved my data (as long as I don&amp;#39;t have another problem), so that&amp;#39;s great. But it&amp;#39;s clearly time to retire this drive pool. What should I replace it with? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Are there any gotchas with newer drives that I should be aware of, that weren&amp;#39;t true of hard drives 7 years ago?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1232875", "is_robot_indexable": true, "report_reasons": null, "author": "squigish", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1232875/drive_failed_in_raidz_array_what_should_i_replace/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1232875/drive_failed_in_raidz_array_what_should_i_replace/", "subreddit_subscribers": 675625, "created_utc": 1679870066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have poorly managed my data for years and now have about 4-5TB of data on a single hard drive. I want to start by copying it to an external HDD in order to provide some redundancy while I put together a more modern solution.\n\nThe data currently is on a Windows 7 system and I figured that Windows file transfer is not the best/safest/most efficient way to ensure my data is appropriately transferred. What suggestions would you have to move it to an external drive until I can build an UnRaid system or something similar. TY!", "author_fullname": "t2_aw54b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to copy/clone/backup 4TB....how?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123260w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679869933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have poorly managed my data for years and now have about 4-5TB of data on a single hard drive. I want to start by copying it to an external HDD in order to provide some redundancy while I put together a more modern solution.&lt;/p&gt;\n\n&lt;p&gt;The data currently is on a Windows 7 system and I figured that Windows file transfer is not the best/safest/most efficient way to ensure my data is appropriately transferred. What suggestions would you have to move it to an external drive until I can build an UnRaid system or something similar. TY!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123260w", "is_robot_indexable": true, "report_reasons": null, "author": "delnith", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123260w/need_to_copyclonebackup_4tbhow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123260w/need_to_copyclonebackup_4tbhow/", "subreddit_subscribers": 675625, "created_utc": 1679869933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What books/sub-topics should I study if I'm interested in enterprise storage? This is a topic not well discussed when people talk about computer books. I have a ton of books on programming, but none on disks/file systems, etc.", "author_fullname": "t2_13g9il", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What books/sub-topics should I study if I'm interested in enterprise storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1231lp8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679868725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What books/sub-topics should I study if I&amp;#39;m interested in enterprise storage? This is a topic not well discussed when people talk about computer books. I have a ton of books on programming, but none on disks/file systems, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1231lp8", "is_robot_indexable": true, "report_reasons": null, "author": "linuxman1929", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1231lp8/what_bookssubtopics_should_i_study_if_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1231lp8/what_bookssubtopics_should_i_study_if_im/", "subreddit_subscribers": 675625, "created_utc": 1679868725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried using fx or vxtwitter but if a tweet is deleted the image/video that was in it becomes invalid, I wanted to know a way to leave it archived without these problems", "author_fullname": "t2_nnj9ml6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is there any way to archive a tweet with discord?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_123uw7w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679939972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried using fx or vxtwitter but if a tweet is deleted the image/video that was in it becomes invalid, I wanted to know a way to leave it archived without these problems&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123uw7w", "is_robot_indexable": true, "report_reasons": null, "author": "SaiaExitt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123uw7w/is_there_any_way_to_archive_a_tweet_with_discord/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123uw7w/is_there_any_way_to_archive_a_tweet_with_discord/", "subreddit_subscribers": 675625, "created_utc": 1679939972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_169tozec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have about 20k files like this. How do I mass rename to .mp3? I'm on Windows, thank you!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": true, "name": "t3_123utbe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/l3_beH1XxRwgqozLpli8wgsXzLM0DDSrPUScKrjad48.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679939801.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/4gdgtgmtlbqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?auto=webp&amp;v=enabled&amp;s=1e3253bbaed71a2b1ba6bf7be95a9eb8eca7af0c", "width": 651, "height": 101}, "resolutions": [{"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6300da0768860dd6a90e89a2ef4322728f7cc53a", "width": 108, "height": 16}, {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f64122edf79fbe09a74027e590f24484541f614", "width": 216, "height": 33}, {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44c0f63d601b12cb9ea2c51468c1ca798ba239ff", "width": 320, "height": 49}, {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0352d6ec78931b63f442f1a61aed1b510451d074", "width": 640, "height": 99}], "variants": {}, "id": "WCFNRn7Ih8SraNX3YYZNWYsv6-sBSavYoJby_KhdbvA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123utbe", "is_robot_indexable": true, "report_reasons": null, "author": "AutomaticInitiative", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123utbe/i_have_about_20k_files_like_this_how_do_i_mass/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/4gdgtgmtlbqa1.png", "subreddit_subscribers": 675625, "created_utc": 1679939801.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone!!!!!  \nI KNOW - i messed up. I should have backed up (in plaintext) but hear me out.   \n\n\nI'm in a bit of a bind and could use your expertise in decrypting a .json file that contains **all** of my bookmarks from a private bookmark extension. Unfortunately, the password to access the extension is no longer valid due to a bug in the extension, which has locked me out of accessing it. while opening/unlocking the extension theres a bug if firefox closes it can \"corrupt\" the decryption of the file causing your password to never work again. Weird... i know.\n\nThankfully, I can still open the .json in notepad and view the encrypted text, but I'm not sure how to reverse engineer it to get a password that will work. I'm hoping someone here might be able to help me out with this.  **I should also mention that the bookmark extension I used is** ***open-source*****,  which means the code is available for anyone to see and audit. If that  helps with finding a solution to my problem, please let me know. (**[**https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/**](https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/)**) according to the FAQ - it used** [**https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto**](https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto) \n\nIf that doesn't work, I'm also curious if it's possible to \"rollback\" my PC/Firefox so that I can revert the files to their state from a few hours ago. If anyone has experience with this, any advice would be appreciated.  \n\n\nor heck, maybe even re-engineer the addon to show the password/ allow me to click the \"export as plain text\" option in the backups.\n\nThank you in advance for any help you can provide.", "author_fullname": "t2_maupq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help decrypting a .json file containing my private bookmarks. Huge list of tons of things saved over the years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_123sy9k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679935958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!!!!!&lt;br/&gt;\nI KNOW - i messed up. I should have backed up (in plaintext) but hear me out.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in a bit of a bind and could use your expertise in decrypting a .json file that contains &lt;strong&gt;all&lt;/strong&gt; of my bookmarks from a private bookmark extension. Unfortunately, the password to access the extension is no longer valid due to a bug in the extension, which has locked me out of accessing it. while opening/unlocking the extension theres a bug if firefox closes it can &amp;quot;corrupt&amp;quot; the decryption of the file causing your password to never work again. Weird... i know.&lt;/p&gt;\n\n&lt;p&gt;Thankfully, I can still open the .json in notepad and view the encrypted text, but I&amp;#39;m not sure how to reverse engineer it to get a password that will work. I&amp;#39;m hoping someone here might be able to help me out with this.  &lt;strong&gt;I should also mention that the bookmark extension I used is&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;open-source&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;,  which means the code is available for anyone to see and audit. If that  helps with finding a solution to my problem, please let me know. (&lt;/strong&gt;&lt;a href=\"https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/\"&gt;&lt;strong&gt;https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;) according to the FAQ - it used&lt;/strong&gt; &lt;a href=\"https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto\"&gt;&lt;strong&gt;https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;If that doesn&amp;#39;t work, I&amp;#39;m also curious if it&amp;#39;s possible to &amp;quot;rollback&amp;quot; my PC/Firefox so that I can revert the files to their state from a few hours ago. If anyone has experience with this, any advice would be appreciated.  &lt;/p&gt;\n\n&lt;p&gt;or heck, maybe even re-engineer the addon to show the password/ allow me to click the &amp;quot;export as plain text&amp;quot; option in the backups.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for any help you can provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?auto=webp&amp;v=enabled&amp;s=857980c7cbe0664da13e89a9f9d2077b6a36034b", "width": 350, "height": 525}, "resolutions": [{"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8df7dfdb53b40222072c38663ff3a7c5ba814e8", "width": 108, "height": 162}, {"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea7cc113736f27bb02b0184181dbffef1b0d4a62", "width": 216, "height": 324}, {"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb4eda9c727d02b73ab314a521f44f0231e911dd", "width": 320, "height": 480}], "variants": {}, "id": "1pf_pu1xk_lkxMMadhSke0hAPmCHs5IrzQRU0RoxoFc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123sy9k", "is_robot_indexable": true, "report_reasons": null, "author": "InhaleMC", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123sy9k/need_help_decrypting_a_json_file_containing_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123sy9k/need_help_decrypting_a_json_file_containing_my/", "subreddit_subscribers": 675625, "created_utc": 1679935958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone! I'm new to being a hoarder of data but my job enabled me to get a dl380 HP server and enclosure which has the following:\n\n1. 2 x 300GB drives in a RAID 1 array holding my windows server 2012 os\n2. 10 x 300GB drives in RAID 6 as a local cloud\n3. 12 x 2TB drives IN RAID 6 holding my library\n\nI am looking to create a 3-2-1 backup plan for the 20tb array as I've seen preached here! I am interested in setting up a physical backup at my job and a cloud backup. My thoughts are as follows:\n\nSetup a RAID 1 using 2 x 22TB wd gold drives in a server at my job OR use more smaller drives to create another RAID 6 array for my physical backup, I'm interested in opinions on this as I plan on keeping my library ideally for the rest of my life and don't know what the best long-term route would be here?\n\nFor cloud.. is backblaze really reliable? I was looking into 20tb using Google drive, onedrive or Dropbox but those are very expensive. I saw backblaze which appealed to me because of the unlimited storage but I've seen lots of hit or miss reviews. I don't have a full 20tb worth of data, just 2-5tb but plan to add more. \n\nI'm just looking for insight on my plan, better alternatives and opinions on my setup are welcome! I just don't want to lose everything", "author_fullname": "t2_jfddw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backups for new library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123rlg9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679933242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! I&amp;#39;m new to being a hoarder of data but my job enabled me to get a dl380 HP server and enclosure which has the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;2 x 300GB drives in a RAID 1 array holding my windows server 2012 os&lt;/li&gt;\n&lt;li&gt;10 x 300GB drives in RAID 6 as a local cloud&lt;/li&gt;\n&lt;li&gt;12 x 2TB drives IN RAID 6 holding my library&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I am looking to create a 3-2-1 backup plan for the 20tb array as I&amp;#39;ve seen preached here! I am interested in setting up a physical backup at my job and a cloud backup. My thoughts are as follows:&lt;/p&gt;\n\n&lt;p&gt;Setup a RAID 1 using 2 x 22TB wd gold drives in a server at my job OR use more smaller drives to create another RAID 6 array for my physical backup, I&amp;#39;m interested in opinions on this as I plan on keeping my library ideally for the rest of my life and don&amp;#39;t know what the best long-term route would be here?&lt;/p&gt;\n\n&lt;p&gt;For cloud.. is backblaze really reliable? I was looking into 20tb using Google drive, onedrive or Dropbox but those are very expensive. I saw backblaze which appealed to me because of the unlimited storage but I&amp;#39;ve seen lots of hit or miss reviews. I don&amp;#39;t have a full 20tb worth of data, just 2-5tb but plan to add more. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just looking for insight on my plan, better alternatives and opinions on my setup are welcome! I just don&amp;#39;t want to lose everything&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123rlg9", "is_robot_indexable": true, "report_reasons": null, "author": "Th3_L1Nx", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123rlg9/backups_for_new_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123rlg9/backups_for_new_library/", "subreddit_subscribers": 675625, "created_utc": 1679933242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there. I'm travelling without access to a computer, so I intend to use my Android phone to upload my photos and videos from sd cards/external drive via OTG to the cloud.\n\nThe problem I have is that I can't seem to find a good cloud provider that wouldn't cause issues with managing duplicates. What I tried so far:\n\n- Google Drive, OneDrive, IceDrive, pCloud - when I perform a dump upload of my sd card and it fails (for example because of bad Internet connection), retrying it doesn't detect which files have already been uploaded. The issue with those providers is that when you try to upload a file that's already on the cloud, they don't ask whether you'd want to replace it or rename it, they upload it without a warning and give the file a new name. This is a very undesirable behaviour as I'd end up with multiple duplicates if I just try to select all files and upload them in bulk. What's ridiculous is that the behaviour in the Web apps of most of those providers is different the there's an option to choose the desired behaviour. Not in the app, though.\n\n- iDrive - the app sees which files have already been uploaded, but I'm not able to upload from other places than the phone internal storage...\n\n- MEGA - has duplicate detection working, but whenever I try to upload a bulk of files, the app would crash. It seems to be that the app is caching the files on phone memory before uploading them, and for bulk uploads, this simply cannot work. I get a warning from the system that it's running out of memory, and the app keeps on crashing until I wipe its memory.\n\nIs there any cloud that would have a mechanism to prevent the creation of duplicates working in an Android app?", "author_fullname": "t2_3e6lr021", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud storage for bulk uploads on Android", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123r91a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679934627.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679932610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there. I&amp;#39;m travelling without access to a computer, so I intend to use my Android phone to upload my photos and videos from sd cards/external drive via OTG to the cloud.&lt;/p&gt;\n\n&lt;p&gt;The problem I have is that I can&amp;#39;t seem to find a good cloud provider that wouldn&amp;#39;t cause issues with managing duplicates. What I tried so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Google Drive, OneDrive, IceDrive, pCloud - when I perform a dump upload of my sd card and it fails (for example because of bad Internet connection), retrying it doesn&amp;#39;t detect which files have already been uploaded. The issue with those providers is that when you try to upload a file that&amp;#39;s already on the cloud, they don&amp;#39;t ask whether you&amp;#39;d want to replace it or rename it, they upload it without a warning and give the file a new name. This is a very undesirable behaviour as I&amp;#39;d end up with multiple duplicates if I just try to select all files and upload them in bulk. What&amp;#39;s ridiculous is that the behaviour in the Web apps of most of those providers is different the there&amp;#39;s an option to choose the desired behaviour. Not in the app, though.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;iDrive - the app sees which files have already been uploaded, but I&amp;#39;m not able to upload from other places than the phone internal storage...&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;MEGA - has duplicate detection working, but whenever I try to upload a bulk of files, the app would crash. It seems to be that the app is caching the files on phone memory before uploading them, and for bulk uploads, this simply cannot work. I get a warning from the system that it&amp;#39;s running out of memory, and the app keeps on crashing until I wipe its memory.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is there any cloud that would have a mechanism to prevent the creation of duplicates working in an Android app?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123r91a", "is_robot_indexable": true, "report_reasons": null, "author": "Pramus", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123r91a/cloud_storage_for_bulk_uploads_on_android/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123r91a/cloud_storage_for_bulk_uploads_on_android/", "subreddit_subscribers": 675625, "created_utc": 1679932610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI'm looking for a list or references of old negative scanners (I mean cheap second hand).  \nThat allow at least 4 35mm films strip to be scanned at the same time.\n\nCan any of you help me?\n\nThanks", "author_fullname": "t2_exg3en8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanner negative that scan at least 4 35 mm film strip", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123nmcz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679925346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a list or references of old negative scanners (I mean cheap second hand).&lt;br/&gt;\nThat allow at least 4 35mm films strip to be scanned at the same time.&lt;/p&gt;\n\n&lt;p&gt;Can any of you help me?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123nmcz", "is_robot_indexable": true, "report_reasons": null, "author": "mnemonickus", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123nmcz/scanner_negative_that_scan_at_least_4_35_mm_film/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123nmcz/scanner_negative_that_scan_at_least_4_35_mm_film/", "subreddit_subscribers": 675625, "created_utc": 1679925346.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}