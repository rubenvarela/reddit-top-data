{"kind": "Listing", "data": {"after": "t3_1232875", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1sg4r0ip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data hoarding is older than we thought! MAD Magazine 215 from 1980", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 58, "top_awarded_type": null, "hide_score": false, "name": "t3_123w808", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 512, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 512, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DyWr-OUsjo-LrfIC3NbmVrq_4GrjuCEyIO7Qwki-2pY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679942659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/pgh70h2dubqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/pgh70h2dubqa1.png?auto=webp&amp;v=enabled&amp;s=d3c24d6005948561a54e232c18a44c894e698e61", "width": 979, "height": 407}, "resolutions": [{"url": "https://preview.redd.it/pgh70h2dubqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6228b30804ff7a35cc7f7fbc4a0bb032bef8537", "width": 108, "height": 44}, {"url": "https://preview.redd.it/pgh70h2dubqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8ecc378bb39266037437737a214d2764ad11fd8", "width": 216, "height": 89}, {"url": "https://preview.redd.it/pgh70h2dubqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a404abd8dce6591fcd9acfc33e8e307a186bb2c", "width": 320, "height": 133}, {"url": "https://preview.redd.it/pgh70h2dubqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3edad980b0b337de7d0b6081147155e3dd60d719", "width": 640, "height": 266}, {"url": "https://preview.redd.it/pgh70h2dubqa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f609192eeef539443b05919a7cd1c41a0d0b105", "width": 960, "height": 399}], "variants": {}, "id": "4aZpRgFzCZQYsqoLMMYIkU25nRUwKGJQW7vwOeeqhwQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123w808", "is_robot_indexable": true, "report_reasons": null, "author": "Hong-Hong-Hang-Hang", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123w808/data_hoarding_is_older_than_we_thought_mad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/pgh70h2dubqa1.png", "subreddit_subscribers": 675646, "created_utc": 1679942659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_d5sfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smr hard drive at 100% active time, copying small files, but still at 100% when file copy is paused. It stays like this until I reset the machine and causes other programs to hang. Is this normal for smr?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 101, "top_awarded_type": null, "hide_score": false, "name": "t3_123g0jl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 132, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 132, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DY6JK2XWJG2bzsLiU-DqFgu_V3jUD-U_8b7yq20u0Rc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679904482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8j8jl62yo8qa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?auto=webp&amp;v=enabled&amp;s=3722830cb77cbeea00cd9571efbaf1bd505548fb", "width": 1610, "height": 1163}, "resolutions": [{"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=452a1042b19fb2d9600aef249e07a006f1d8433a", "width": 108, "height": 78}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe7e15d8aa133955b2f4c3239be38f2885ac5417", "width": 216, "height": 156}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efad0366954076f53ecce0fe01fa12530583dd39", "width": 320, "height": 231}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19bb55f8503cb77b9c676745911b7f1b168072fd", "width": 640, "height": 462}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91e470f5180ff6beaf0571157ded6b1cfe8a8309", "width": 960, "height": 693}, {"url": "https://preview.redd.it/8j8jl62yo8qa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e364e68cbe7403aff77612aea4ac37c68f8ac590", "width": 1080, "height": 780}], "variants": {}, "id": "DB_ZhI18qR7656g_EQRuq-TN-_nrrehZX9OTOGY7RDc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123g0jl", "is_robot_indexable": true, "report_reasons": null, "author": "bluejeans90210", "discussion_type": null, "num_comments": 96, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123g0jl/smr_hard_drive_at_100_active_time_copying_small/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8j8jl62yo8qa1.png", "subreddit_subscribers": 675646, "created_utc": 1679904482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nintendo 3DS and Wii U eShop permanently shutting down in two days", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1230d5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_xlxw6", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/kD1t4xtjieT_weReTKpTttSrPwYP_c1imPsPu1dRXb8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "LeftistGamersUnion", "selftext": "", "author_fullname": "t2_604w21c5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nintendo 3DS and Wii U eShop permanently shutting down in two days", "link_flair_richtext": [], "subreddit_name_prefixed": "r/LeftistGamersUnion", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_122vw4h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 93, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 93, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/kD1t4xtjieT_weReTKpTttSrPwYP_c1imPsPu1dRXb8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1679857231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "digitalspy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.digitalspy.com/tech/a43416801/nintendo-3ds-wii-u-eshop-closing-soon/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?auto=webp&amp;v=enabled&amp;s=d2508aac7dd1c8e0a38e9e1fa8edb0a5798337c9", "width": 1200, "height": 602}, "resolutions": [{"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86d5bdefd8271b02f68c28860888436dfcdc1585", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ea407e90ccf6a3ff8098bc062397ef9a14e674a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=362630d41461db73c14d03628ce6ddd386cb003d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e35bb124bdaaba252694963b25dbede2db1c7aaa", "width": 640, "height": 321}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2cd7f19276031197b5566b5c2a29c2fd2c95806", "width": 960, "height": 481}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db7af360ef66e8fd26c60cd613f8191bb96a937f", "width": 1080, "height": 541}], "variants": {}, "id": "TFaL8O0G456e8qJGa0jZzv4TTXACVycX3ns-KHJEjm4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_ubm2e", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "122vw4h", "is_robot_indexable": true, "report_reasons": null, "author": "yuritopiaposadism", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LeftistGamersUnion/comments/122vw4h/nintendo_3ds_and_wii_u_eshop_permanently_shutting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.digitalspy.com/tech/a43416801/nintendo-3ds-wii-u-eshop-closing-soon/", "subreddit_subscribers": 17999, "created_utc": 1679857231.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1679866214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "digitalspy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.digitalspy.com/tech/a43416801/nintendo-3ds-wii-u-eshop-closing-soon/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?auto=webp&amp;v=enabled&amp;s=d2508aac7dd1c8e0a38e9e1fa8edb0a5798337c9", "width": 1200, "height": 602}, "resolutions": [{"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86d5bdefd8271b02f68c28860888436dfcdc1585", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ea407e90ccf6a3ff8098bc062397ef9a14e674a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=362630d41461db73c14d03628ce6ddd386cb003d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e35bb124bdaaba252694963b25dbede2db1c7aaa", "width": 640, "height": 321}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2cd7f19276031197b5566b5c2a29c2fd2c95806", "width": 960, "height": 481}, {"url": "https://external-preview.redd.it/mo9GmKJ4svW4RoFz9VciEDjW97SLWE0DdCcQGBJsBV8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db7af360ef66e8fd26c60cd613f8191bb96a937f", "width": 1080, "height": 541}], "variants": {}, "id": "TFaL8O0G456e8qJGa0jZzv4TTXACVycX3ns-KHJEjm4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1230d5a", "is_robot_indexable": true, "report_reasons": null, "author": "focus_rising", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_122vw4h", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1230d5a/nintendo_3ds_and_wii_u_eshop_permanently_shutting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.digitalspy.com/tech/a43416801/nintendo-3ds-wii-u-eshop-closing-soon/", "subreddit_subscribers": 675646, "created_utc": 1679866214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I heard backblaze is a very good option for a backing up one\u2019s PC, however, I also noticed that a lot of people complain about the restore process once their drive fails or whatever mishaps happen. Are there any other alternatives that offer a better restore process? I\u2019m fairly new to backing up stuff (lost precious memories before and had to learn it the hard way\u2026) but I always keep a copy of my PC files/data on an external SSD. I definitely want to have more copies of my data, not just in an external drive as everything is prone to failure/degradation/etc. I was thinking of using backblaze and sync.com (one for pure backup and the other for syncing) but I\u2019m undecided on backblaze.", "author_fullname": "t2_4wwjb46d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on backblaze for backing up a PC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1239123", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": 1679886355.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679885669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I heard backblaze is a very good option for a backing up one\u2019s PC, however, I also noticed that a lot of people complain about the restore process once their drive fails or whatever mishaps happen. Are there any other alternatives that offer a better restore process? I\u2019m fairly new to backing up stuff (lost precious memories before and had to learn it the hard way\u2026) but I always keep a copy of my PC files/data on an external SSD. I definitely want to have more copies of my data, not just in an external drive as everything is prone to failure/degradation/etc. I was thinking of using backblaze and sync.com (one for pure backup and the other for syncing) but I\u2019m undecided on backblaze.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1239123", "is_robot_indexable": true, "report_reasons": null, "author": "calpthemcheeks", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1239123/thoughts_on_backblaze_for_backing_up_a_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1239123/thoughts_on_backblaze_for_backing_up_a_pc/", "subreddit_subscribers": 675646, "created_utc": 1679885669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been writing some scripts for my own personal use recently due to an interesting use-case. I have data that I want to store on one machine in one format, and multiple target machines that I want to sync the data to in slightly varying formats. If the source files change, I want to resync and re-transform the files into the proper formats. The result of weeks of polishing these scripts up is this budding new GitHub project of mine, that I am calling (for now, anyway) \"Dumb-Sync\".\n\nMain page and README: [https://github.com/sloshy/dumb-sync](https://github.com/sloshy/dumb-sync)\n\nLicense: LGPL v2.1 (subject to change)\n\nBasically the idea is this: you create a JSON file that describes one or more remote directories you wish to sync to your local machine. From there, you can optionally wire in multiple transformation steps that just exist as bash scripts. Each of these scripts can take any config arguments you want, specified in each JSON config or dynamically from current runtime values (i.e. the current file name, output directory, list of all known remote files). You can set these scripts to be ran once per-file, or once per-directory,  and you can even include your own comparison logic scripts for determining if a transformed file should be synced, kept in case it's removed on a remote system, and so on.\n\nIn the [example transformations directory](https://github.com/sloshy/dumb-sync/tree/main/example_transformations) there is an example config that illustrates the following scenario:\n\n* You have a remote directory full of ZIP files.\n* Inside each ZIP file are BIN/CUE files representing lossless disc images.\n* You want to sync only the ZIP files from that directory to your local machine.\n* Once per-file, you want to extract all of its contents to the current output directory using 7Zip.\n* After they are extracted, you want to re-compress them in the CHD format, which is from the MAME project for creating real-time playable disc images that take up less space.\n* If any files exist afterward that are not meant to be there, delete them on each re-sync.\n\nWith the example config, after each transformation, the local files are only ever re-synced if the remote they are derived from changes. A time offset is stored for your config that displays the latest time a sync was attempted. This might get options to be more granular in the future, but for now it is just a single timestamp for all configs.\n\nThe scripts are still being regularly updated, as I just released this publicly over the weekend, and it will likely gain several more features as time goes on. Just recently I added support for specifying minimum and maximum file size thresholds for syncing. The general idea is that eventually this will encompass most of the meaningful featureset of \"rsync\" which this uses internally, and grow to be even more flexible in ways that people find useful. For example, I would love a way for people to share template configs that other people can plug into their own in order to import scripts and even sync configurations automatically or using parameter options. For now, this mostly just covers my own immediate use cases though.\n\nLet me know what you think and please feel free to report any bugs or feature requests!", "author_fullname": "t2_cb3vo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dumb-Sync: A bash/rsync/jq-based microframework for syncing transformed files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123d8q7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679896800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been writing some scripts for my own personal use recently due to an interesting use-case. I have data that I want to store on one machine in one format, and multiple target machines that I want to sync the data to in slightly varying formats. If the source files change, I want to resync and re-transform the files into the proper formats. The result of weeks of polishing these scripts up is this budding new GitHub project of mine, that I am calling (for now, anyway) &amp;quot;Dumb-Sync&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Main page and README: &lt;a href=\"https://github.com/sloshy/dumb-sync\"&gt;https://github.com/sloshy/dumb-sync&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;License: LGPL v2.1 (subject to change)&lt;/p&gt;\n\n&lt;p&gt;Basically the idea is this: you create a JSON file that describes one or more remote directories you wish to sync to your local machine. From there, you can optionally wire in multiple transformation steps that just exist as bash scripts. Each of these scripts can take any config arguments you want, specified in each JSON config or dynamically from current runtime values (i.e. the current file name, output directory, list of all known remote files). You can set these scripts to be ran once per-file, or once per-directory,  and you can even include your own comparison logic scripts for determining if a transformed file should be synced, kept in case it&amp;#39;s removed on a remote system, and so on.&lt;/p&gt;\n\n&lt;p&gt;In the &lt;a href=\"https://github.com/sloshy/dumb-sync/tree/main/example_transformations\"&gt;example transformations directory&lt;/a&gt; there is an example config that illustrates the following scenario:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You have a remote directory full of ZIP files.&lt;/li&gt;\n&lt;li&gt;Inside each ZIP file are BIN/CUE files representing lossless disc images.&lt;/li&gt;\n&lt;li&gt;You want to sync only the ZIP files from that directory to your local machine.&lt;/li&gt;\n&lt;li&gt;Once per-file, you want to extract all of its contents to the current output directory using 7Zip.&lt;/li&gt;\n&lt;li&gt;After they are extracted, you want to re-compress them in the CHD format, which is from the MAME project for creating real-time playable disc images that take up less space.&lt;/li&gt;\n&lt;li&gt;If any files exist afterward that are not meant to be there, delete them on each re-sync.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;With the example config, after each transformation, the local files are only ever re-synced if the remote they are derived from changes. A time offset is stored for your config that displays the latest time a sync was attempted. This might get options to be more granular in the future, but for now it is just a single timestamp for all configs.&lt;/p&gt;\n\n&lt;p&gt;The scripts are still being regularly updated, as I just released this publicly over the weekend, and it will likely gain several more features as time goes on. Just recently I added support for specifying minimum and maximum file size thresholds for syncing. The general idea is that eventually this will encompass most of the meaningful featureset of &amp;quot;rsync&amp;quot; which this uses internally, and grow to be even more flexible in ways that people find useful. For example, I would love a way for people to share template configs that other people can plug into their own in order to import scripts and even sync configurations automatically or using parameter options. For now, this mostly just covers my own immediate use cases though.&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think and please feel free to report any bugs or feature requests!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?auto=webp&amp;v=enabled&amp;s=1b05a6503cf7321bcabd6fffc6de3e782c98f09b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=781490f75b82b874929b7369ddfe5d7ccba2ed31", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a87e4648dd1ff81e6507801666027f14ee2a4857", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b179abb9c1b8ebacf4a5d02554c50fc8aea793b7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90c0f920565833f809ed411db25ec817ccb187e8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14c75efb92af99897a5b65a689691238eb7de2a4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aJioNeJB76VbUiQr-bqwKSf0O2T-AzsFU-R2nmzxvic.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f71a5e12e386d900f540bf882b8cda4539d8cc9", "width": 1080, "height": 540}], "variants": {}, "id": "YmfaUbVP1XUFLOtPgkgC3giOyD9givyh-uQPhGgTUPI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123d8q7", "is_robot_indexable": true, "report_reasons": null, "author": "Sloshy42", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123d8q7/dumbsync_a_bashrsyncjqbased_microframework_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123d8q7/dumbsync_a_bashrsyncjqbased_microframework_for/", "subreddit_subscribers": 675646, "created_utc": 1679896800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So got my new workstation Thursday.- Part of this setup is a 6 x 18tb set of disks connected to the motherboard's SATA connections.- I am not using the motherboard's RAID\n\nI initially set up the RAID using linux mdadm.  About 80gb into copy there was a array failure, but upon reboot it was fine again.  Resumed copying and after a hour of copying again it failed once more.\n\nI then ran SMART checks on the disks, everything came back reading 100% fine.\n\nThinking this could be a software issue I wiped the array, deleted all the mdadm configs and installed ZFS and set up a pool with the same disks.  This got me a little further, about two terrabytes in the pool went into a offline state but after a reboot it was fine again.\n\nI have checked in dmesg and found several angry comments about failures on sda and sdb, so I then checked against the zpool when I resumed copying expecting it to fail.  The pool agreed stating that sda and sdb had hundreds of write failures.\n\nI have been running badblocks on the two disks since Saturday night, it's already finishing the second pass on the two disks and so far nothing is showing up as bad, zero blocks failed.\n\nThis is my conundrum, all of the disk checks swear that the disks are perfectly fine, but using them makes the OS/array say otherwise.  So far the only thing I could assume is the cables could be defective?  \n\n\nUPDATE: So have been running a zfs cluster without sda and sdb which were erroring.  So far it's been running steady all morning/afternoon, nearly 10tb copied without complaint in dmesg.  Thoughts: if this is a voltage/cable issue, not running the two could be tested by creating a new cluster with sda-sdd, using the two it complained about, but still just a four disk cluster.  If it passes then it's more likely a power issue not a physical cable issue.  Alternately I was thinking of making a two disk array with them and copying from my NVME stick, seeing if it could cause it to still error out.", "author_fullname": "t2_d3vpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mixed messages on a new drive array", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123m7o5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679951191.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679921854.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So got my new workstation Thursday.- Part of this setup is a 6 x 18tb set of disks connected to the motherboard&amp;#39;s SATA connections.- I am not using the motherboard&amp;#39;s RAID&lt;/p&gt;\n\n&lt;p&gt;I initially set up the RAID using linux mdadm.  About 80gb into copy there was a array failure, but upon reboot it was fine again.  Resumed copying and after a hour of copying again it failed once more.&lt;/p&gt;\n\n&lt;p&gt;I then ran SMART checks on the disks, everything came back reading 100% fine.&lt;/p&gt;\n\n&lt;p&gt;Thinking this could be a software issue I wiped the array, deleted all the mdadm configs and installed ZFS and set up a pool with the same disks.  This got me a little further, about two terrabytes in the pool went into a offline state but after a reboot it was fine again.&lt;/p&gt;\n\n&lt;p&gt;I have checked in dmesg and found several angry comments about failures on sda and sdb, so I then checked against the zpool when I resumed copying expecting it to fail.  The pool agreed stating that sda and sdb had hundreds of write failures.&lt;/p&gt;\n\n&lt;p&gt;I have been running badblocks on the two disks since Saturday night, it&amp;#39;s already finishing the second pass on the two disks and so far nothing is showing up as bad, zero blocks failed.&lt;/p&gt;\n\n&lt;p&gt;This is my conundrum, all of the disk checks swear that the disks are perfectly fine, but using them makes the OS/array say otherwise.  So far the only thing I could assume is the cables could be defective?  &lt;/p&gt;\n\n&lt;p&gt;UPDATE: So have been running a zfs cluster without sda and sdb which were erroring.  So far it&amp;#39;s been running steady all morning/afternoon, nearly 10tb copied without complaint in dmesg.  Thoughts: if this is a voltage/cable issue, not running the two could be tested by creating a new cluster with sda-sdd, using the two it complained about, but still just a four disk cluster.  If it passes then it&amp;#39;s more likely a power issue not a physical cable issue.  Alternately I was thinking of making a two disk array with them and copying from my NVME stick, seeing if it could cause it to still error out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123m7o5", "is_robot_indexable": true, "report_reasons": null, "author": "TheIllusioneer", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123m7o5/mixed_messages_on_a_new_drive_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123m7o5/mixed_messages_on_a_new_drive_array/", "subreddit_subscribers": 675646, "created_utc": 1679921854.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_169tozec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have about 20k files like this. How do I mass rename to .mp3? I'm on Windows, thank you!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": false, "name": "t3_123utbe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/l3_beH1XxRwgqozLpli8wgsXzLM0DDSrPUScKrjad48.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679939801.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/4gdgtgmtlbqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?auto=webp&amp;v=enabled&amp;s=1e3253bbaed71a2b1ba6bf7be95a9eb8eca7af0c", "width": 651, "height": 101}, "resolutions": [{"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6300da0768860dd6a90e89a2ef4322728f7cc53a", "width": 108, "height": 16}, {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f64122edf79fbe09a74027e590f24484541f614", "width": 216, "height": 33}, {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44c0f63d601b12cb9ea2c51468c1ca798ba239ff", "width": 320, "height": 49}, {"url": "https://preview.redd.it/4gdgtgmtlbqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0352d6ec78931b63f442f1a61aed1b510451d074", "width": 640, "height": 99}], "variants": {}, "id": "WCFNRn7Ih8SraNX3YYZNWYsv6-sBSavYoJby_KhdbvA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123utbe", "is_robot_indexable": true, "report_reasons": null, "author": "AutomaticInitiative", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123utbe/i_have_about_20k_files_like_this_how_do_i_mass/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/4gdgtgmtlbqa1.png", "subreddit_subscribers": 675646, "created_utc": 1679939801.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone successfully ripped video files (movies, TV shows, etc) onto a BD-R disc and played it on a Blu-Ray player or console?\n\nAny recommendations on best software for ripping/menu creation/misc?\n\nDoes DRM prevent this from working?\n\nI imagine that most people in this sub are doing the opposite. Ripping the Blu's onto PC. I don't know where else to go for this information.", "author_fullname": "t2_8zpyfcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping to Blu-Ray discs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123apwr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679889739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone successfully ripped video files (movies, TV shows, etc) onto a BD-R disc and played it on a Blu-Ray player or console?&lt;/p&gt;\n\n&lt;p&gt;Any recommendations on best software for ripping/menu creation/misc?&lt;/p&gt;\n\n&lt;p&gt;Does DRM prevent this from working?&lt;/p&gt;\n\n&lt;p&gt;I imagine that most people in this sub are doing the opposite. Ripping the Blu&amp;#39;s onto PC. I don&amp;#39;t know where else to go for this information.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123apwr", "is_robot_indexable": true, "report_reasons": null, "author": "HunteHorseman", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123apwr/ripping_to_bluray_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123apwr/ripping_to_bluray_discs/", "subreddit_subscribers": 675646, "created_utc": 1679889739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I may end up getting a bunch of HDDs, possibly 50+, not sure of the size. The last ones were 500 GB I am sure these ones are going to be larger , possibly 1 tb plus.  The PCs are ones that I am planning to give away to local charity but may strip down a few of them to personal use.  Or repurpose. This is PCs from local college.\n\nHere is my dilema.\n\nI currently have 20 HDDs 111,599 GB various sizes. External, internal, Enclosure etc. I currently have room for 3 more HDDS in an enclosure.  And have an  option for quick HDD swaps with one external device that I have.\n\nAll these hard drives are used for backup, movies, pictures, picture archives etc.\n\nAny suggestions on expanding for room for multiple HDD additions.", "author_fullname": "t2_f1bbee6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding New HDDs to my 20 HDD collection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123829e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679883446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I may end up getting a bunch of HDDs, possibly 50+, not sure of the size. The last ones were 500 GB I am sure these ones are going to be larger , possibly 1 tb plus.  The PCs are ones that I am planning to give away to local charity but may strip down a few of them to personal use.  Or repurpose. This is PCs from local college.&lt;/p&gt;\n\n&lt;p&gt;Here is my dilema.&lt;/p&gt;\n\n&lt;p&gt;I currently have 20 HDDs 111,599 GB various sizes. External, internal, Enclosure etc. I currently have room for 3 more HDDS in an enclosure.  And have an  option for quick HDD swaps with one external device that I have.&lt;/p&gt;\n\n&lt;p&gt;All these hard drives are used for backup, movies, pictures, picture archives etc.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions on expanding for room for multiple HDD additions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123829e", "is_robot_indexable": true, "report_reasons": null, "author": "Monkeydu2", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123829e/adding_new_hdds_to_my_20_hdd_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123829e/adding_new_hdds_to_my_20_hdd_collection/", "subreddit_subscribers": 675646, "created_utc": 1679883446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\"Counter-Strike 2 will replace CS:GO once the game officially launches, a departure from previous updates to the Counter-Strike franchise which saw multiple versions of the game \u2014 and esports scenes \u2014 run simultaneously.\"\n\nhttps://esportsinsider.com/2023/03/counter-strike-2\n\nValve is calling this more of an \"upgrade\" than anything, but the fact that the original CS:GO will no longer be available doesn't sit right with me... \n\nFor those of you that don't know, CS:GO is a Source 1 game which is now free to play. Source is the engine Half Life 2 was built on. \n\nCS2 is based on Source 2, which is a newer engine released by Valve in 2015. \n\nHope some hero out there gets CS:GO working independently of Steam so that it can be preserved in it's current state. \n\nAlso: RIP to all the poor gamers out there who can barely run CS:GO as is, let alone CS2.", "author_fullname": "t2_cqz2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Counter Strike 2 is a replacement for Global Offensive, and CS:GO will no longer be available after CS2 drops.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123v9iw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679940706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Counter-Strike 2 will replace CS:GO once the game officially launches, a departure from previous updates to the Counter-Strike franchise which saw multiple versions of the game \u2014 and esports scenes \u2014 run simultaneously.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://esportsinsider.com/2023/03/counter-strike-2\"&gt;https://esportsinsider.com/2023/03/counter-strike-2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Valve is calling this more of an &amp;quot;upgrade&amp;quot; than anything, but the fact that the original CS:GO will no longer be available doesn&amp;#39;t sit right with me... &lt;/p&gt;\n\n&lt;p&gt;For those of you that don&amp;#39;t know, CS:GO is a Source 1 game which is now free to play. Source is the engine Half Life 2 was built on. &lt;/p&gt;\n\n&lt;p&gt;CS2 is based on Source 2, which is a newer engine released by Valve in 2015. &lt;/p&gt;\n\n&lt;p&gt;Hope some hero out there gets CS:GO working independently of Steam so that it can be preserved in it&amp;#39;s current state. &lt;/p&gt;\n\n&lt;p&gt;Also: RIP to all the poor gamers out there who can barely run CS:GO as is, let alone CS2.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?auto=webp&amp;v=enabled&amp;s=52f8e57846df3d9d3adc539d3629cf2028afb797", "width": 1920, "height": 949}, "resolutions": [{"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f19d470bbb7e8f395e8238e8d56a24ec47b71913", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c73209fe0cb0da492e73b1ef77114aa67e9ba442", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3b18ae3ee5cc11acdee1ba635a6dee92e81e025", "width": 320, "height": 158}, {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5550f65eedfc990833c8148d537b3818f48988ec", "width": 640, "height": 316}, {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc0a55a67c606d5fe64a579a4a07b58b926c8ae0", "width": 960, "height": 474}, {"url": "https://external-preview.redd.it/FK01XJhrKhcWQra6bDATukNexVbZh7kHcmfUrwucvr0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1eab0a37d6733ff85e95311b7d49adf8308a5f3", "width": 1080, "height": 533}], "variants": {}, "id": "TYX9lXDOlTmcK5BrzHpz7T7XsXL1CaeKuHxzopEmf_g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123v9iw", "is_robot_indexable": true, "report_reasons": null, "author": "blackletum", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123v9iw/counter_strike_2_is_a_replacement_for_global/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123v9iw/counter_strike_2_is_a_replacement_for_global/", "subreddit_subscribers": 675646, "created_utc": 1679940706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title, broadly. I'll be crossposting.\n\nI understand that this is only tangentially related to data hoarding, but it is related to my \"Spent $405 for a single show\" flair.\n\nEssentially, the BD for Avatar: The Last Airbender is HUGE. Like, \u2248 420GB (including DVD extras)^(1)\n\nIf it were up to me, I'd encode using AV1, but I'd like to also maybe \"release\" this (if you catch my drift) and apparently nobody uses that. So I'm left with HEVC. But holy SHIET there's a lot of discourse on this. [Scenerules](https://scenerules.org/t.html?id=2020_X265.nfo) has their, well, rules, [redditors](https://www.reddit.com/r/x265/comments/e08tfu/ultimate_encoding_test_results_for_animation/) have opinions, doom9 has discussions from [2015](https://forum.doom9.org/showthread.php?t=172458) and [2017](https://forum.doom9.org/showthread.php?t=174679) for 4k, there's a pretty nice Azumanga Daioh BDrip using [these](https://bin.theindex.moe/?177c7f606a37cd6a#9Es7QtMRBoBbsihRgLYuv8cqKRuB2xZN7XSs28QGyVzr) settings [(alt)](https://pastebin.com/hPbNmhns).\n\nSo many choices. Does anybody have any tips? Any guides you usually follow? I'm going for something like \"more or less visually lossless\" and \"not immediately obvious it's been compressed\" at the very least, though ideally I'd love to apply some kind of sharpening(?)/de-ionizing(?) filter to make it look better (the BD has a lot of film grain, and it's obviously a 480p source)\n\nThank you *so* much in advance.\n\n&amp;#x200B;\n\n^(1) I started this journey because I couldn't find a release online with quality I was happy with (I have since found releases that are probably good enough, but I digress). I'd like to make my own encode, not only for myself, but for everyone else unhappy with the quality on public trackers.", "author_fullname": "t2_1hutcmww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Transcoding] Overwhelmed with options, how can I best encode 1080p with x265?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123vp5h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679942289.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679941603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title, broadly. I&amp;#39;ll be crossposting.&lt;/p&gt;\n\n&lt;p&gt;I understand that this is only tangentially related to data hoarding, but it is related to my &amp;quot;Spent $405 for a single show&amp;quot; flair.&lt;/p&gt;\n\n&lt;p&gt;Essentially, the BD for Avatar: The Last Airbender is HUGE. Like, \u2248 420GB (including DVD extras)&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;\n\n&lt;p&gt;If it were up to me, I&amp;#39;d encode using AV1, but I&amp;#39;d like to also maybe &amp;quot;release&amp;quot; this (if you catch my drift) and apparently nobody uses that. So I&amp;#39;m left with HEVC. But holy SHIET there&amp;#39;s a lot of discourse on this. &lt;a href=\"https://scenerules.org/t.html?id=2020_X265.nfo\"&gt;Scenerules&lt;/a&gt; has their, well, rules, &lt;a href=\"https://www.reddit.com/r/x265/comments/e08tfu/ultimate_encoding_test_results_for_animation/\"&gt;redditors&lt;/a&gt; have opinions, doom9 has discussions from &lt;a href=\"https://forum.doom9.org/showthread.php?t=172458\"&gt;2015&lt;/a&gt; and &lt;a href=\"https://forum.doom9.org/showthread.php?t=174679\"&gt;2017&lt;/a&gt; for 4k, there&amp;#39;s a pretty nice Azumanga Daioh BDrip using &lt;a href=\"https://bin.theindex.moe/?177c7f606a37cd6a#9Es7QtMRBoBbsihRgLYuv8cqKRuB2xZN7XSs28QGyVzr\"&gt;these&lt;/a&gt; settings &lt;a href=\"https://pastebin.com/hPbNmhns\"&gt;(alt)&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;So many choices. Does anybody have any tips? Any guides you usually follow? I&amp;#39;m going for something like &amp;quot;more or less visually lossless&amp;quot; and &amp;quot;not immediately obvious it&amp;#39;s been compressed&amp;quot; at the very least, though ideally I&amp;#39;d love to apply some kind of sharpening(?)/de-ionizing(?) filter to make it look better (the BD has a lot of film grain, and it&amp;#39;s obviously a 480p source)&lt;/p&gt;\n\n&lt;p&gt;Thank you &lt;em&gt;so&lt;/em&gt; much in advance.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; I started this journey because I couldn&amp;#39;t find a release online with quality I was happy with (I have since found releases that are probably good enough, but I digress). I&amp;#39;d like to make my own encode, not only for myself, but for everyone else unhappy with the quality on public trackers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Spent $405 for a single show", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "123vp5h", "is_robot_indexable": true, "report_reasons": null, "author": "General-Stryker", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/123vp5h/transcoding_overwhelmed_with_options_how_can_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123vp5h/transcoding_overwhelmed_with_options_how_can_i/", "subreddit_subscribers": 675646, "created_utc": 1679941603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried using fx or vxtwitter but if a tweet is deleted the image/video that was in it becomes invalid, I wanted to know a way to leave it archived without these problems", "author_fullname": "t2_nnj9ml6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is there any way to archive a tweet with discord?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123uw7w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679939972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried using fx or vxtwitter but if a tweet is deleted the image/video that was in it becomes invalid, I wanted to know a way to leave it archived without these problems&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123uw7w", "is_robot_indexable": true, "report_reasons": null, "author": "SaiaExitt", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123uw7w/is_there_any_way_to_archive_a_tweet_with_discord/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123uw7w/is_there_any_way_to_archive_a_tweet_with_discord/", "subreddit_subscribers": 675646, "created_utc": 1679939972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I'm transferring about 8tb from an internal 8tb SSD to a new QNAP NAS that's running two 16tb HDD's set up as a single RAID1 volume. \n\nOver the course of the several days that the files have been copying, I'm getting some intermittent error messages regarding the transfer. Here's a few examples: \n\n&amp;#x200B;\n\n&gt;10:48:11: Created: 230324-144811-493-737.db  \n10:48:58: Copying...  \n10:48:58: Source: F:\\\\  \n10:48:58: Target: \\\\\\\\NASStorage\\\\SSDDATA  \n12:48:24: F:\\\\Games\\\\Arcade\\\\MAME 0.250 EXTRAs\\\\covers\\_SL.zip  \n12:48:24: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392  \n12:48:24: ------------  \n12:48:56: F:\\\\Games\\\\Arcade\\\\MAME 0.250 EXTRAs\\\\devices.zip  \n12:48:56: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392  \n12:48:56: ------------  \n13:06:23: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\3countb\\\\3countb.txt  \n13:06:23: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392  \n13:06:23: ------------  \n13:08:15: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\iron\\\\01 Demo.mp3  \n13:08:15: Error opening source file. The system cannot find the file specified. Code: 2  \n13:08:15: ------------  \n13:08:55: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\mslug3\\\\07 First Contracts.mp3  \n13:08:55: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392  \n13:08:55: ------------  \n13:08:55: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\mslug3\\\\08 Escape.mp3  \n13:08:55: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392  \n13:08:55: ------------  \n13:09:53: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\vendetta\\\\12 Bay Area (Stage 4).mp3  \n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2  \n13:09:53: ------------  \n13:09:53: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\vendetta\\\\13 The Armageddon (Final Boss).mp3  \n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2  \n13:09:53: ------------\n\n So far, it appears to only be throwing errors with files in the MAME directory.  \n\n\nI navigated over to F:\\\\Games\\\\Arcade\\\\MAME 0.250 EXTRAs and tried to open covers\\_SL.zip with 7zip and it throws the same error. Weird. I'm not sure how these files became corrupted. So TeraCopy won't even copy them over if it's discovered the archives are corrupted? I guess that's fine since I don't have any use for corrupted files.  \n\n\nNow I just navigated to F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\3countb and 3countb.txt doesn't even exist (even if viewing hidden files)...what the hell? \n\n I also just navigated over to the directory for these errors:  \n\n\nF:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\vendetta\\\\12 Bay Area (Stage 4).mp3  \n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2  \n13:09:53: ------------  \n13:09:53: F:\\\\Games\\\\Arcade\\\\MAME 0.250 Multimedia\\\\soundtrack\\\\vendetta\\\\13 The Armageddon (Final Boss).mp3  \n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2  \n\n\nThose files are also missing from the directory...  \n\n\nBut tracks 1 through 11 and 14 on, are all there.  \n\n\nI don't get it. They \\*were\\* there until TeraCopy went to copy them and then they...disappeared? Teracopy...removed them? Either that, or Teracopy somehow knows they're supposed to be there and is expecting them, and they've always not been there. I'm assuming that's highly unlikely. \n\nSo...the files were there enough for Teracopy to see them and try to copy them, and then they immediately stopped existing, and then Teracopy logged the error in the...what?  \n\n\nThis is getting really strange now.", "author_fullname": "t2_96z9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Teracopy deleting my files during transfer? Something really strange is going on here...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1234afa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679874678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m transferring about 8tb from an internal 8tb SSD to a new QNAP NAS that&amp;#39;s running two 16tb HDD&amp;#39;s set up as a single RAID1 volume. &lt;/p&gt;\n\n&lt;p&gt;Over the course of the several days that the files have been copying, I&amp;#39;m getting some intermittent error messages regarding the transfer. Here&amp;#39;s a few examples: &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;10:48:11: Created: 230324-144811-493-737.db&lt;br/&gt;\n10:48:58: Copying...&lt;br/&gt;\n10:48:58: Source: F:\\&lt;br/&gt;\n10:48:58: Target: \\\\NASStorage\\SSDDATA&lt;br/&gt;\n12:48:24: F:\\Games\\Arcade\\MAME 0.250 EXTRAs\\covers_SL.zip&lt;br/&gt;\n12:48:24: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392&lt;br/&gt;\n12:48:24: ------------&lt;br/&gt;\n12:48:56: F:\\Games\\Arcade\\MAME 0.250 EXTRAs\\devices.zip&lt;br/&gt;\n12:48:56: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392&lt;br/&gt;\n12:48:56: ------------&lt;br/&gt;\n13:06:23: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\3countb\\3countb.txt&lt;br/&gt;\n13:06:23: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392&lt;br/&gt;\n13:06:23: ------------&lt;br/&gt;\n13:08:15: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\iron\\01 Demo.mp3&lt;br/&gt;\n13:08:15: Error opening source file. The system cannot find the file specified. Code: 2&lt;br/&gt;\n13:08:15: ------------&lt;br/&gt;\n13:08:55: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\mslug3\\07 First Contracts.mp3&lt;br/&gt;\n13:08:55: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392&lt;br/&gt;\n13:08:55: ------------&lt;br/&gt;\n13:08:55: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\mslug3\\08 Escape.mp3&lt;br/&gt;\n13:08:55: Error opening source file. The file or directory is corrupted and unreadable. Code: 1392&lt;br/&gt;\n13:08:55: ------------&lt;br/&gt;\n13:09:53: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\vendetta\\12 Bay Area (Stage 4).mp3&lt;br/&gt;\n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2&lt;br/&gt;\n13:09:53: ------------&lt;br/&gt;\n13:09:53: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\vendetta\\13 The Armageddon (Final Boss).mp3&lt;br/&gt;\n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2&lt;br/&gt;\n13:09:53: ------------&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;So far, it appears to only be throwing errors with files in the MAME directory.  &lt;/p&gt;\n\n&lt;p&gt;I navigated over to F:\\Games\\Arcade\\MAME 0.250 EXTRAs and tried to open covers_SL.zip with 7zip and it throws the same error. Weird. I&amp;#39;m not sure how these files became corrupted. So TeraCopy won&amp;#39;t even copy them over if it&amp;#39;s discovered the archives are corrupted? I guess that&amp;#39;s fine since I don&amp;#39;t have any use for corrupted files.  &lt;/p&gt;\n\n&lt;p&gt;Now I just navigated to F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\3countb and 3countb.txt doesn&amp;#39;t even exist (even if viewing hidden files)...what the hell? &lt;/p&gt;\n\n&lt;p&gt;I also just navigated over to the directory for these errors:  &lt;/p&gt;\n\n&lt;p&gt;F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\vendetta\\12 Bay Area (Stage 4).mp3&lt;br/&gt;\n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2&lt;br/&gt;\n13:09:53: ------------&lt;br/&gt;\n13:09:53: F:\\Games\\Arcade\\MAME 0.250 Multimedia\\soundtrack\\vendetta\\13 The Armageddon (Final Boss).mp3&lt;br/&gt;\n13:09:53: Error opening source file. The system cannot find the file specified. Code: 2  &lt;/p&gt;\n\n&lt;p&gt;Those files are also missing from the directory...  &lt;/p&gt;\n\n&lt;p&gt;But tracks 1 through 11 and 14 on, are all there.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t get it. They *were* there until TeraCopy went to copy them and then they...disappeared? Teracopy...removed them? Either that, or Teracopy somehow knows they&amp;#39;re supposed to be there and is expecting them, and they&amp;#39;ve always not been there. I&amp;#39;m assuming that&amp;#39;s highly unlikely. &lt;/p&gt;\n\n&lt;p&gt;So...the files were there enough for Teracopy to see them and try to copy them, and then they immediately stopped existing, and then Teracopy logged the error in the...what?  &lt;/p&gt;\n\n&lt;p&gt;This is getting really strange now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1234afa", "is_robot_indexable": true, "report_reasons": null, "author": "ultranothing", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1234afa/is_teracopy_deleting_my_files_during_transfer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1234afa/is_teracopy_deleting_my_files_during_transfer/", "subreddit_subscribers": 675646, "created_utc": 1679874678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a license for it but it's starting to get on my nerves. The backups are excruciatingly slow, like it's taking a whole afternoon to incorporate a 10 GB incremental backup image into the main one. It doesn't need to be free but I don't want any subscription based services. Thank you for your help.", "author_fullname": "t2_8muaq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any alternatives to Macrium Reflect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123t7el", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679936485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a license for it but it&amp;#39;s starting to get on my nerves. The backups are excruciatingly slow, like it&amp;#39;s taking a whole afternoon to incorporate a 10 GB incremental backup image into the main one. It doesn&amp;#39;t need to be free but I don&amp;#39;t want any subscription based services. Thank you for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123t7el", "is_robot_indexable": true, "report_reasons": null, "author": "nicktheone", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123t7el/any_alternatives_to_macrium_reflect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123t7el/any_alternatives_to_macrium_reflect/", "subreddit_subscribers": 675646, "created_utc": 1679936485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone!!!!!  \nI KNOW - i messed up. I should have backed up (in plaintext) but hear me out.   \n\n\nI'm in a bit of a bind and could use your expertise in decrypting a .json file that contains **all** of my bookmarks from a private bookmark extension. Unfortunately, the password to access the extension is no longer valid due to a bug in the extension, which has locked me out of accessing it. while opening/unlocking the extension theres a bug if firefox closes it can \"corrupt\" the decryption of the file causing your password to never work again. Weird... i know.\n\nThankfully, I can still open the .json in notepad and view the encrypted text, but I'm not sure how to reverse engineer it to get a password that will work. I'm hoping someone here might be able to help me out with this.  **I should also mention that the bookmark extension I used is** ***open-source*****,  which means the code is available for anyone to see and audit. If that  helps with finding a solution to my problem, please let me know. (**[**https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/**](https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/)**) according to the FAQ - it used** [**https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto**](https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto) \n\nIf that doesn't work, I'm also curious if it's possible to \"rollback\" my PC/Firefox so that I can revert the files to their state from a few hours ago. If anyone has experience with this, any advice would be appreciated.  \n\n\nor heck, maybe even re-engineer the addon to show the password/ allow me to click the \"export as plain text\" option in the backups.\n\nThank you in advance for any help you can provide.", "author_fullname": "t2_maupq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help decrypting a .json file containing my private bookmarks. Huge list of tons of things saved over the years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123sy9k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679935958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!!!!!&lt;br/&gt;\nI KNOW - i messed up. I should have backed up (in plaintext) but hear me out.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in a bit of a bind and could use your expertise in decrypting a .json file that contains &lt;strong&gt;all&lt;/strong&gt; of my bookmarks from a private bookmark extension. Unfortunately, the password to access the extension is no longer valid due to a bug in the extension, which has locked me out of accessing it. while opening/unlocking the extension theres a bug if firefox closes it can &amp;quot;corrupt&amp;quot; the decryption of the file causing your password to never work again. Weird... i know.&lt;/p&gt;\n\n&lt;p&gt;Thankfully, I can still open the .json in notepad and view the encrypted text, but I&amp;#39;m not sure how to reverse engineer it to get a password that will work. I&amp;#39;m hoping someone here might be able to help me out with this.  &lt;strong&gt;I should also mention that the bookmark extension I used is&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;open-source&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;,  which means the code is available for anyone to see and audit. If that  helps with finding a solution to my problem, please let me know. (&lt;/strong&gt;&lt;a href=\"https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/\"&gt;&lt;strong&gt;https://addons.mozilla.org/en-US/firefox/addon/webext-private-bookmarks/&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;) according to the FAQ - it used&lt;/strong&gt; &lt;a href=\"https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto\"&gt;&lt;strong&gt;https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;If that doesn&amp;#39;t work, I&amp;#39;m also curious if it&amp;#39;s possible to &amp;quot;rollback&amp;quot; my PC/Firefox so that I can revert the files to their state from a few hours ago. If anyone has experience with this, any advice would be appreciated.  &lt;/p&gt;\n\n&lt;p&gt;or heck, maybe even re-engineer the addon to show the password/ allow me to click the &amp;quot;export as plain text&amp;quot; option in the backups.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for any help you can provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?auto=webp&amp;v=enabled&amp;s=857980c7cbe0664da13e89a9f9d2077b6a36034b", "width": 350, "height": 525}, "resolutions": [{"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8df7dfdb53b40222072c38663ff3a7c5ba814e8", "width": 108, "height": 162}, {"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea7cc113736f27bb02b0184181dbffef1b0d4a62", "width": 216, "height": 324}, {"url": "https://external-preview.redd.it/_cRO7XqAmmD_FlkKGLpfkSv-bkNQe38vbPIauY1tRtY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb4eda9c727d02b73ab314a521f44f0231e911dd", "width": 320, "height": 480}], "variants": {}, "id": "1pf_pu1xk_lkxMMadhSke0hAPmCHs5IrzQRU0RoxoFc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123sy9k", "is_robot_indexable": true, "report_reasons": null, "author": "InhaleMC", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123sy9k/need_help_decrypting_a_json_file_containing_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123sy9k/need_help_decrypting_a_json_file_containing_my/", "subreddit_subscribers": 675646, "created_utc": 1679935958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I recently had a hard disk crash. Fortunately I had a local backup but it was a few months old. I have been using Idrive for a couple years. Well I went to restore the more recent files that aren't on my local backup from Idrive but there are lots and lots of files missing. Like the folder structure is intact but the folders are just empty! So basically I've lost who knows what.\n\nAre there any reliable cloud backup providers you recommend? I've currently got 10 TB personal plan with IDrive and running out of space.\n\nThanks", "author_fullname": "t2_a3875", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better alternatives to Idrive? That are reliable.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12339yp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679872366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I recently had a hard disk crash. Fortunately I had a local backup but it was a few months old. I have been using Idrive for a couple years. Well I went to restore the more recent files that aren&amp;#39;t on my local backup from Idrive but there are lots and lots of files missing. Like the folder structure is intact but the folders are just empty! So basically I&amp;#39;ve lost who knows what.&lt;/p&gt;\n\n&lt;p&gt;Are there any reliable cloud backup providers you recommend? I&amp;#39;ve currently got 10 TB personal plan with IDrive and running out of space.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12339yp", "is_robot_indexable": true, "report_reasons": null, "author": "roofoo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12339yp/better_alternatives_to_idrive_that_are_reliable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12339yp/better_alternatives_to_idrive_that_are_reliable/", "subreddit_subscribers": 675646, "created_utc": 1679872366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "shit, how do i even start this\n\ntoday a friend of mine gave me her phone to just save pictures on her external hard drive\n\nit was an SP Armor A80 1TB drive (never heard of this company called Silicon Power probably terrible)\n\nanyways I connect it to a windows PC, the USB controller recognizes the drive, drive spins\u2026 everythings great\n\nI then start to copy the DCIM folder, but I get some error and the file explorer freezes so I end task, then I delete the messed up copied file from the drive and try to recopy just the images instead\n\nNext thing you know I replug the drive and nothing is happening, it shows the Silicon Power drive logo in diskmgmt.msc, but the drive is not actually readable\n\nI then go on linux and type lsblk, nope the drive isnt showing\n\nTyping dmesg I see that it spins the disk, then a bunch of errors showing not responding show up and thats that\n\n\nI decided ok, maybe I should remove it from the external case and plug into my PC internally using SATA cables.\n\nNope same issue\n\nSo now im SOL, I want to recover everything as Im not sure what I did wrong, and it would break my heart knowing I just lost a good friend of mine all of her photos and videos", "author_fullname": "t2_6n2n1f8zk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "recovery of 1TB external hard drive from a friend", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1232q97", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679871160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;shit, how do i even start this&lt;/p&gt;\n\n&lt;p&gt;today a friend of mine gave me her phone to just save pictures on her external hard drive&lt;/p&gt;\n\n&lt;p&gt;it was an SP Armor A80 1TB drive (never heard of this company called Silicon Power probably terrible)&lt;/p&gt;\n\n&lt;p&gt;anyways I connect it to a windows PC, the USB controller recognizes the drive, drive spins\u2026 everythings great&lt;/p&gt;\n\n&lt;p&gt;I then start to copy the DCIM folder, but I get some error and the file explorer freezes so I end task, then I delete the messed up copied file from the drive and try to recopy just the images instead&lt;/p&gt;\n\n&lt;p&gt;Next thing you know I replug the drive and nothing is happening, it shows the Silicon Power drive logo in diskmgmt.msc, but the drive is not actually readable&lt;/p&gt;\n\n&lt;p&gt;I then go on linux and type lsblk, nope the drive isnt showing&lt;/p&gt;\n\n&lt;p&gt;Typing dmesg I see that it spins the disk, then a bunch of errors showing not responding show up and thats that&lt;/p&gt;\n\n&lt;p&gt;I decided ok, maybe I should remove it from the external case and plug into my PC internally using SATA cables.&lt;/p&gt;\n\n&lt;p&gt;Nope same issue&lt;/p&gt;\n\n&lt;p&gt;So now im SOL, I want to recover everything as Im not sure what I did wrong, and it would break my heart knowing I just lost a good friend of mine all of her photos and videos&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1232q97", "is_robot_indexable": true, "report_reasons": null, "author": "helloidkwhatthisis1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1232q97/recovery_of_1tb_external_hard_drive_from_a_friend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1232q97/recovery_of_1tb_external_hard_drive_from_a_friend/", "subreddit_subscribers": 675646, "created_utc": 1679871160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What books/sub-topics should I study if I'm interested in enterprise storage? This is a topic not well discussed when people talk about computer books. I have a ton of books on programming, but none on disks/file systems, etc.", "author_fullname": "t2_13g9il", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What books/sub-topics should I study if I'm interested in enterprise storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1231lp8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679868725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What books/sub-topics should I study if I&amp;#39;m interested in enterprise storage? This is a topic not well discussed when people talk about computer books. I have a ton of books on programming, but none on disks/file systems, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1231lp8", "is_robot_indexable": true, "report_reasons": null, "author": "linuxman1929", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1231lp8/what_bookssubtopics_should_i_study_if_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1231lp8/what_bookssubtopics_should_i_study_if_im/", "subreddit_subscribers": 675646, "created_utc": 1679868725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nJust wanted to give you all an update on the Dim Project - we're still actively working on it and making progress toward our goal of creating an incredible self-hosted media experience.\n\nWe would like to invite you to fill out our survey to help us better understand your thoughts and opinions on self-hosted media and what can be improved. Your input will help us make better decisions and develop strategies to create an innovative experience.\n\nSo, if you have a few minutes to spare, please take the time to fill out our survey. We appreciate your support and look forward to sharing our progress with you all.\n\nSurvey link: [https://forms.gle/uuFvuUb6FHQenY2Y9](https://forms.gle/uuFvuUb6FHQenY2Y9)\n\nGithub: [https://github.com/Dusk-Labs/dim](https://github.com/Dusk-Labs/dim)\n\nThank you from the team at [Dusk Labs](https://dusklabs.io/)!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4220hzdprbqa1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=128bd58d43159c98942fc318c6eb28986139fbaa", "author_fullname": "t2_2dwfny8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Dim Media Manager is still active - Help us by filling out our survey!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4220hzdprbqa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a7247e8caebf2731905effba8913c5e707b0f1d"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d228ae62967ec38e5205f7f31d89d3f4d139af4"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4dc44fee647f1f90969f00bf01828c9a2f9ba8b"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e324cb7cc5b65847a135ca81217f48967e98744"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d62b7d2321819fff35eb962fea9bc471fb56c9f6"}, {"y": 675, "x": 1080, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9c2c115f0d0885a0289ae4bf68af85d9d1da836"}], "s": {"y": 900, "x": 1440, "u": "https://preview.redd.it/4220hzdprbqa1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=128bd58d43159c98942fc318c6eb28986139fbaa"}, "id": "4220hzdprbqa1"}}, "name": "t3_123vrk8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DW6LK6_DZMfFMM1as4beqfY-F6KtygzEK1LY1tBXbQU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1679941739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Just wanted to give you all an update on the Dim Project - we&amp;#39;re still actively working on it and making progress toward our goal of creating an incredible self-hosted media experience.&lt;/p&gt;\n\n&lt;p&gt;We would like to invite you to fill out our survey to help us better understand your thoughts and opinions on self-hosted media and what can be improved. Your input will help us make better decisions and develop strategies to create an innovative experience.&lt;/p&gt;\n\n&lt;p&gt;So, if you have a few minutes to spare, please take the time to fill out our survey. We appreciate your support and look forward to sharing our progress with you all.&lt;/p&gt;\n\n&lt;p&gt;Survey link: &lt;a href=\"https://forms.gle/uuFvuUb6FHQenY2Y9\"&gt;https://forms.gle/uuFvuUb6FHQenY2Y9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/Dusk-Labs/dim\"&gt;https://github.com/Dusk-Labs/dim&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you from the team at &lt;a href=\"https://dusklabs.io/\"&gt;Dusk Labs&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4220hzdprbqa1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=128bd58d43159c98942fc318c6eb28986139fbaa\"&gt;https://preview.redd.it/4220hzdprbqa1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=128bd58d43159c98942fc318c6eb28986139fbaa&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?auto=webp&amp;v=enabled&amp;s=21620f0372507a6acc0f7982c5182a89cd6b1b7b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83ab4e94efe55fe1a599012612b3424e37cfb5b5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=387cac8ea6ecdd5403e898e1ef2c470867d4bb30", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68a83a5874284ed7aff41c11299c83c1df25a24f", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba7a406398df7441f0f347a9c323bc134b7d70f1", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=127394252ea2e44854b7e7d33f838cd2395b846f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/n328L3l_64cZ_AVASgP8AEVgTW09Xvmd-_XfpQWZZgk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33679000502df4016a4e7b01628b5a88040a819e", "width": 1080, "height": 567}], "variants": {}, "id": "jOfA63H5Nm6IY4ygjSZkFNqIYJzt-g4hTEZ3ut1zAgI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123vrk8", "is_robot_indexable": true, "report_reasons": null, "author": "HinaCh4n", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123vrk8/the_dim_media_manager_is_still_active_help_us_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123vrk8/the_dim_media_manager_is_still_active_help_us_by/", "subreddit_subscribers": 675646, "created_utc": 1679941739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone! I'm new to being a hoarder of data but my job enabled me to get a dl380 HP server and enclosure which has the following:\n\n1. 2 x 300GB drives in a RAID 1 array holding my windows server 2012 os\n2. 10 x 300GB drives in RAID 6 as a local cloud\n3. 12 x 2TB drives IN RAID 6 holding my library\n\nI am looking to create a 3-2-1 backup plan for the 20tb array as I've seen preached here! I am interested in setting up a physical backup at my job and a cloud backup. My thoughts are as follows:\n\nSetup a RAID 1 using 2 x 22TB wd gold drives in a server at my job OR use more smaller drives to create another RAID 6 array for my physical backup, I'm interested in opinions on this as I plan on keeping my library ideally for the rest of my life and don't know what the best long-term route would be here?\n\nFor cloud.. is backblaze really reliable? I was looking into 20tb using Google drive, onedrive or Dropbox but those are very expensive. I saw backblaze which appealed to me because of the unlimited storage but I've seen lots of hit or miss reviews. I don't have a full 20tb worth of data, just 2-5tb but plan to add more. \n\nI'm just looking for insight on my plan, better alternatives and opinions on my setup are welcome! I just don't want to lose everything", "author_fullname": "t2_jfddw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backups for new library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123rlg9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679933242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! I&amp;#39;m new to being a hoarder of data but my job enabled me to get a dl380 HP server and enclosure which has the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;2 x 300GB drives in a RAID 1 array holding my windows server 2012 os&lt;/li&gt;\n&lt;li&gt;10 x 300GB drives in RAID 6 as a local cloud&lt;/li&gt;\n&lt;li&gt;12 x 2TB drives IN RAID 6 holding my library&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I am looking to create a 3-2-1 backup plan for the 20tb array as I&amp;#39;ve seen preached here! I am interested in setting up a physical backup at my job and a cloud backup. My thoughts are as follows:&lt;/p&gt;\n\n&lt;p&gt;Setup a RAID 1 using 2 x 22TB wd gold drives in a server at my job OR use more smaller drives to create another RAID 6 array for my physical backup, I&amp;#39;m interested in opinions on this as I plan on keeping my library ideally for the rest of my life and don&amp;#39;t know what the best long-term route would be here?&lt;/p&gt;\n\n&lt;p&gt;For cloud.. is backblaze really reliable? I was looking into 20tb using Google drive, onedrive or Dropbox but those are very expensive. I saw backblaze which appealed to me because of the unlimited storage but I&amp;#39;ve seen lots of hit or miss reviews. I don&amp;#39;t have a full 20tb worth of data, just 2-5tb but plan to add more. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just looking for insight on my plan, better alternatives and opinions on my setup are welcome! I just don&amp;#39;t want to lose everything&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123rlg9", "is_robot_indexable": true, "report_reasons": null, "author": "Th3_L1Nx", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123rlg9/backups_for_new_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123rlg9/backups_for_new_library/", "subreddit_subscribers": 675646, "created_utc": 1679933242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there. I'm travelling without access to a computer, so I intend to use my Android phone to upload my photos and videos from sd cards/external drive via OTG to the cloud.\n\nThe problem I have is that I can't seem to find a good cloud provider that wouldn't cause issues with managing duplicates. What I tried so far:\n\n- Google Drive, OneDrive, IceDrive, pCloud - when I perform a dump upload of my sd card and it fails (for example because of bad Internet connection), retrying it doesn't detect which files have already been uploaded. The issue with those providers is that when you try to upload a file that's already on the cloud, they don't ask whether you'd want to replace it or rename it, they upload it without a warning and give the file a new name. This is a very undesirable behaviour as I'd end up with multiple duplicates if I just try to select all files and upload them in bulk. What's ridiculous is that the behaviour in the Web apps of most of those providers is different the there's an option to choose the desired behaviour. Not in the app, though.\n\n- iDrive - the app sees which files have already been uploaded, but I'm not able to upload from other places than the phone internal storage...\n\n- MEGA - has duplicate detection working, but whenever I try to upload a bulk of files, the app would crash. It seems to be that the app is caching the files on phone memory before uploading them, and for bulk uploads, this simply cannot work. I get a warning from the system that it's running out of memory, and the app keeps on crashing until I wipe its memory.\n\nIs there any cloud that would have a mechanism to prevent the creation of duplicates working in an Android app?", "author_fullname": "t2_3e6lr021", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud storage for bulk uploads on Android", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123r91a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679934627.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679932610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there. I&amp;#39;m travelling without access to a computer, so I intend to use my Android phone to upload my photos and videos from sd cards/external drive via OTG to the cloud.&lt;/p&gt;\n\n&lt;p&gt;The problem I have is that I can&amp;#39;t seem to find a good cloud provider that wouldn&amp;#39;t cause issues with managing duplicates. What I tried so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Google Drive, OneDrive, IceDrive, pCloud - when I perform a dump upload of my sd card and it fails (for example because of bad Internet connection), retrying it doesn&amp;#39;t detect which files have already been uploaded. The issue with those providers is that when you try to upload a file that&amp;#39;s already on the cloud, they don&amp;#39;t ask whether you&amp;#39;d want to replace it or rename it, they upload it without a warning and give the file a new name. This is a very undesirable behaviour as I&amp;#39;d end up with multiple duplicates if I just try to select all files and upload them in bulk. What&amp;#39;s ridiculous is that the behaviour in the Web apps of most of those providers is different the there&amp;#39;s an option to choose the desired behaviour. Not in the app, though.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;iDrive - the app sees which files have already been uploaded, but I&amp;#39;m not able to upload from other places than the phone internal storage...&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;MEGA - has duplicate detection working, but whenever I try to upload a bulk of files, the app would crash. It seems to be that the app is caching the files on phone memory before uploading them, and for bulk uploads, this simply cannot work. I get a warning from the system that it&amp;#39;s running out of memory, and the app keeps on crashing until I wipe its memory.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is there any cloud that would have a mechanism to prevent the creation of duplicates working in an Android app?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123r91a", "is_robot_indexable": true, "report_reasons": null, "author": "Pramus", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123r91a/cloud_storage_for_bulk_uploads_on_android/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123r91a/cloud_storage_for_bulk_uploads_on_android/", "subreddit_subscribers": 675646, "created_utc": 1679932610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I feel like NVMe and SSD are gradually replacing other types of storage media, with HDDs reserved for large sizes and cold storage in data centers. \n\nWhat do you think of a good strategy for long-term storage of important data (tax documents, photos etc)?\n\nStored properly, M-disks promise storage for several decades. Thoughts on using optical media for archival of limited data? \n\nExperience with durability of DVDs and Blue Rays? \n\nOther concerns are availability of a reader, and that presumably some of the well known brands such as verbatim have sold business to other companies that may not maintain the same quality.\n\nReading comments on DVDs on   r/datahoarder, I see mixed opinions. But most of the posts I have read are old.", "author_fullname": "t2_l1vjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optical media such as M-Disk for archival storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123gvpm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679908486.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679907204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like NVMe and SSD are gradually replacing other types of storage media, with HDDs reserved for large sizes and cold storage in data centers. &lt;/p&gt;\n\n&lt;p&gt;What do you think of a good strategy for long-term storage of important data (tax documents, photos etc)?&lt;/p&gt;\n\n&lt;p&gt;Stored properly, M-disks promise storage for several decades. Thoughts on using optical media for archival of limited data? &lt;/p&gt;\n\n&lt;p&gt;Experience with durability of DVDs and Blue Rays? &lt;/p&gt;\n\n&lt;p&gt;Other concerns are availability of a reader, and that presumably some of the well known brands such as verbatim have sold business to other companies that may not maintain the same quality.&lt;/p&gt;\n\n&lt;p&gt;Reading comments on DVDs on   &lt;a href=\"/r/datahoarder\"&gt;r/datahoarder&lt;/a&gt;, I see mixed opinions. But most of the posts I have read are old.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123gvpm", "is_robot_indexable": true, "report_reasons": null, "author": "chaplin2", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123gvpm/optical_media_such_as_mdisk_for_archival_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123gvpm/optical_media_such_as_mdisk_for_archival_storage/", "subreddit_subscribers": 675646, "created_utc": 1679907204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was wondering if this even exists. I'd like to connect a few 3.5in HDD to my mini server via a couple sata ports. But I'm powering it with a PicoPSU which doesn't have enough power to power the additional HDDs. What I do have is a bunch of 12v 6/8 pin connectors via a server PSU.\n\nAre there any sata backplanes that take 12v 6/8 pin PCIE power connectors?", "author_fullname": "t2_ef5xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a 12v powered sata backplane", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_123eyqc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679901734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if this even exists. I&amp;#39;d like to connect a few 3.5in HDD to my mini server via a couple sata ports. But I&amp;#39;m powering it with a PicoPSU which doesn&amp;#39;t have enough power to power the additional HDDs. What I do have is a bunch of 12v 6/8 pin connectors via a server PSU.&lt;/p&gt;\n\n&lt;p&gt;Are there any sata backplanes that take 12v 6/8 pin PCIE power connectors?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "123eyqc", "is_robot_indexable": true, "report_reasons": null, "author": "FallingSnowStar", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/123eyqc/looking_for_a_12v_powered_sata_backplane/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/123eyqc/looking_for_a_12v_powered_sata_backplane/", "subreddit_subscribers": 675646, "created_utc": 1679901734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Moving some videos from my wifes portable harddrive to my server, and almost all of the videos have spots where they freeze frame and play max volume static for about one second before continuing on and doing it again.\n\n\nEverything else on here seems OK, no issues with pictures, music, or other files.\n\nAny suggestions as to the best effort to recover these? (I'm thinking some kind of utility that will just cut out the static/frozen bits)", "author_fullname": "t2_4upw0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Videos recovered off a portable harddrive pause and play loud static every few seconds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1236vpm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679880757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Moving some videos from my wifes portable harddrive to my server, and almost all of the videos have spots where they freeze frame and play max volume static for about one second before continuing on and doing it again.&lt;/p&gt;\n\n&lt;p&gt;Everything else on here seems OK, no issues with pictures, music, or other files.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions as to the best effort to recover these? (I&amp;#39;m thinking some kind of utility that will just cut out the static/frozen bits)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1236vpm", "is_robot_indexable": true, "report_reasons": null, "author": "Doggins", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1236vpm/videos_recovered_off_a_portable_harddrive_pause/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1236vpm/videos_recovered_off_a_portable_harddrive_pause/", "subreddit_subscribers": 675646, "created_utc": 1679880757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got a raidz array (1 disk of redundancy, the ZFS equivalent of RAID 5) with 5x 4TB drives. One of them seems to have failed.\n\nIt's certainly not unreasonable that the hardware just wore out. It's got a power-on time of about 6 years, as do the other drives in my array, and the SMART short self-test shows a failure.\n\nThe failed drive is a Western Digital Red Pro WDC WD4001FFSX-68JNUN0\n\nI'm running all five drives off of a used 3ware 9000 series HBA card I got for cheap 7 years ago. My 16TB array currently has 10TB used, and it's on an ubuntu server in an ATX desktop case.\n\nZFS has saved my data (as long as I don't have another problem), so that's great. But it's clearly time to retire this drive pool. What should I replace it with? \n\n&amp;#x200B;\n\nAre there any gotchas with newer drives that I should be aware of, that weren't true of hard drives 7 years ago?", "author_fullname": "t2_l13ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drive failed in raidz array. What should I replace it with?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1232875", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1679870992.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679870066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a raidz array (1 disk of redundancy, the ZFS equivalent of RAID 5) with 5x 4TB drives. One of them seems to have failed.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s certainly not unreasonable that the hardware just wore out. It&amp;#39;s got a power-on time of about 6 years, as do the other drives in my array, and the SMART short self-test shows a failure.&lt;/p&gt;\n\n&lt;p&gt;The failed drive is a Western Digital Red Pro WDC WD4001FFSX-68JNUN0&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running all five drives off of a used 3ware 9000 series HBA card I got for cheap 7 years ago. My 16TB array currently has 10TB used, and it&amp;#39;s on an ubuntu server in an ATX desktop case.&lt;/p&gt;\n\n&lt;p&gt;ZFS has saved my data (as long as I don&amp;#39;t have another problem), so that&amp;#39;s great. But it&amp;#39;s clearly time to retire this drive pool. What should I replace it with? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Are there any gotchas with newer drives that I should be aware of, that weren&amp;#39;t true of hard drives 7 years ago?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1232875", "is_robot_indexable": true, "report_reasons": null, "author": "squigish", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1232875/drive_failed_in_raidz_array_what_should_i_replace/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1232875/drive_failed_in_raidz_array_what_should_i_replace/", "subreddit_subscribers": 675646, "created_utc": 1679870066.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}