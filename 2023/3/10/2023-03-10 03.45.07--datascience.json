{"kind": "Listing", "data": {"after": "t3_11mzcle", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "&amp;#x200B;\n\n[Visualization of polarized echo chambers](https://preview.redd.it/45ac0obl3oma1.png?width=405&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f17e0082e619641616b6929ee6d19c5e7372dd5f)\n\nOn the one hand, this study is a fascinating data science use case because it shows that it seems possible to narrow down disinformation clusters in social networks purely based on user interaction (shares). And that without viewing the content!\n\n[https://www.researchgate.net/publication/368961767](https://www.researchgate.net/publication/368961767)\n\nThe paper presents a method to identify these problematic interaction patterns providing data science contributions to improve social media platforms.\n\nOn the other hand, this study also reveals worrying conditions: The study recorded and analyzed the German Twitter stream over two months. More than 6.7 million accounts and 75.5 million interactions (including 33 million retweets) were recorded. The analysis shows a reasonably stable \"red echo chamber\" that spreads disinformation and reinforces prejudice and polarization in the network. This echo chamber, which consists of about 66,000 accounts, focuses on only a few topics, such as Anti-Covid, Right-wing Populism, and pro-Russian narratives, which is probably reinforced by Kremlin-initiated troll accounts. In the same way, there is a \"blue chamber\", but its content range is much more multilayered, and its communication pattern is much more inconspicuous. Since Twitter provided only about 1% of the data stream, these echo chambers are likely to be much more prominent in reality.", "author_fullname": "t2_2dyqc8ht", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "This Data Science study identified a German echo chamber of 66K accounts mainly focused on Anti-Covid, Right-wing Populism, and pro-Russian narratives likely reinforced by Kremlin-orchestrated Troll-Accounts just by looking at retweets!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 65, "top_awarded_type": null, "hide_score": false, "media_metadata": {"45ac0obl3oma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/45ac0obl3oma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c6b8fe001d3e087e55fe26740041e5907851e83"}, {"y": 100, "x": 216, "u": "https://preview.redd.it/45ac0obl3oma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f79712ce496ecbd1e5bbe30ca918f4770eb0e29"}, {"y": 149, "x": 320, "u": "https://preview.redd.it/45ac0obl3oma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8857bb5b7b88bb568f25d34bce5b7d1c914be95"}], "s": {"y": 189, "x": 405, "u": "https://preview.redd.it/45ac0obl3oma1.png?width=405&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f17e0082e619641616b6929ee6d19c5e7372dd5f"}, "id": "45ac0obl3oma1"}}, "name": "t3_11mm7uc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 534, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 534, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/sX5bEiGI5_RTAp0C-X3TAJ7YDSG8a3GcBy4P8IQ6raM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678347763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/45ac0obl3oma1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=f17e0082e619641616b6929ee6d19c5e7372dd5f\"&gt;Visualization of polarized echo chambers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On the one hand, this study is a fascinating data science use case because it shows that it seems possible to narrow down disinformation clusters in social networks purely based on user interaction (shares). And that without viewing the content!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.researchgate.net/publication/368961767\"&gt;https://www.researchgate.net/publication/368961767&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The paper presents a method to identify these problematic interaction patterns providing data science contributions to improve social media platforms.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, this study also reveals worrying conditions: The study recorded and analyzed the German Twitter stream over two months. More than 6.7 million accounts and 75.5 million interactions (including 33 million retweets) were recorded. The analysis shows a reasonably stable &amp;quot;red echo chamber&amp;quot; that spreads disinformation and reinforces prejudice and polarization in the network. This echo chamber, which consists of about 66,000 accounts, focuses on only a few topics, such as Anti-Covid, Right-wing Populism, and pro-Russian narratives, which is probably reinforced by Kremlin-initiated troll accounts. In the same way, there is a &amp;quot;blue chamber&amp;quot;, but its content range is much more multilayered, and its communication pattern is much more inconspicuous. Since Twitter provided only about 1% of the data stream, these echo chambers are likely to be much more prominent in reality.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mm7uc", "is_robot_indexable": true, "report_reasons": null, "author": "nkode", "discussion_type": null, "num_comments": 84, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mm7uc/this_data_science_study_identified_a_german_echo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mm7uc/this_data_science_study_identified_a_german_echo/", "subreddit_subscribers": 855570, "created_utc": 1678347763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Here's an interview with Chris Deotte, Quadruple Kaggle Grandmaster at NVIDIA. \n\nIn this episode, Chris shares valuable insights on topics such as crafting a strong data science resume, achieving grandmaster status on Kaggle (even quadruple), working at NVIDIA, and how to approach current data science challenges. Learn more about Kaggle, the data science world, and NVIDIA through the fascinating story of Chris Deotte. (and win an RTX 4080 thanks to NVIDIA GTC collaboration!)\n\nListen to this week's episode on your favorite platform: \n\n[https://youtu.be/NjGnnG3evmE](https://youtu.be/NjGnnG3evmE)\n\n[https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690](https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690)\n\n[https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt](https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt)", "author_fullname": "t2_c14wpji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An interview with Chris Deotte, Quadruple Kaggle Grandmaster and Data Scientist at NVIDIA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mhlju", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678333053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s an interview with Chris Deotte, Quadruple Kaggle Grandmaster at NVIDIA. &lt;/p&gt;\n\n&lt;p&gt;In this episode, Chris shares valuable insights on topics such as crafting a strong data science resume, achieving grandmaster status on Kaggle (even quadruple), working at NVIDIA, and how to approach current data science challenges. Learn more about Kaggle, the data science world, and NVIDIA through the fascinating story of Chris Deotte. (and win an RTX 4080 thanks to NVIDIA GTC collaboration!)&lt;/p&gt;\n\n&lt;p&gt;Listen to this week&amp;#39;s episode on your favorite platform: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/NjGnnG3evmE\"&gt;https://youtu.be/NjGnnG3evmE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690\"&gt;https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt\"&gt;https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QxJKr_LjR-d7tcAYXMrRQlHXi-xte4UZ7w3cUdJ87oQ.jpg?auto=webp&amp;v=enabled&amp;s=ad217e9e22b59124cccaca919e7b15b6fc857a91", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/QxJKr_LjR-d7tcAYXMrRQlHXi-xte4UZ7w3cUdJ87oQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10242c52b7d287c934dd8b5522605ee7cdb6a0f7", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QxJKr_LjR-d7tcAYXMrRQlHXi-xte4UZ7w3cUdJ87oQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97716c0df33be5eac5d14335dd05fd87fff78ef1", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QxJKr_LjR-d7tcAYXMrRQlHXi-xte4UZ7w3cUdJ87oQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d0668d76a843f731972b74487c091737af85436", "width": 320, "height": 240}], "variants": {}, "id": "NkclBFNszCSm9L1egqxSalhI4ESIMbvPkls0EYHxLvI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mhlju", "is_robot_indexable": true, "report_reasons": null, "author": "OnlyProggingForFun", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mhlju/an_interview_with_chris_deotte_quadruple_kaggle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mhlju/an_interview_with_chris_deotte_quadruple_kaggle/", "subreddit_subscribers": 855570, "created_utc": 1678333053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m mentoring a junior data scientist on my team and we\u2019ve been going through the importance of exploratory data analysis. Most of the techniques I use are ones I\u2019ve just picked up or learned over time, but does anyone have any favorite resources (books, blog posts, videos, etc.) that put all these EDA best practices together? Like if you have x type of data, these are some recommended first steps for EDA? We\u2019re currently working on a lot of text data but more general EDA tips would be great as well.", "author_fullname": "t2_femmyhh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Favorite resources for EDA?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mz57q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678385283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m mentoring a junior data scientist on my team and we\u2019ve been going through the importance of exploratory data analysis. Most of the techniques I use are ones I\u2019ve just picked up or learned over time, but does anyone have any favorite resources (books, blog posts, videos, etc.) that put all these EDA best practices together? Like if you have x type of data, these are some recommended first steps for EDA? We\u2019re currently working on a lot of text data but more general EDA tips would be great as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mz57q", "is_robot_indexable": true, "report_reasons": null, "author": "umnosorry", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mz57q/favorite_resources_for_eda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mz57q/favorite_resources_for_eda/", "subreddit_subscribers": 855570, "created_utc": 1678385283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_up87re", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've created an open-source geospatial library to manage and query your geospatial data efficiently. The same approach has been tested with applications up to a scale of ~48m requests per day and worked like a charm. If you like, you can Star the repository to help it grow. Feedback is most welcome", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_11n874l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7sw2SFBK8h6sfgJBKmpiN3fbl-xares0PD0hDA1_Gpc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678405897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/thegeekyasian/geo-assist", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ExoOumP3oTI-PBNNeKOXbuZ0P9SipKBzgqUbJ6ny_a4.jpg?auto=webp&amp;v=enabled&amp;s=962eb85a878b7c076c090c6e73dbc234b3354667", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ExoOumP3oTI-PBNNeKOXbuZ0P9SipKBzgqUbJ6ny_a4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75a98942cf9e7f8625d03e765ca6a843df205356", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ExoOumP3oTI-PBNNeKOXbuZ0P9SipKBzgqUbJ6ny_a4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb9b90de6d821b228e6525cc77d9e83c3e7d0f4d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ExoOumP3oTI-PBNNeKOXbuZ0P9SipKBzgqUbJ6ny_a4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47753fbe20bed55a5921496ffc24611af6d4d184", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ExoOumP3oTI-PBNNeKOXbuZ0P9SipKBzgqUbJ6ny_a4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e51f71613f7022981b92498b35c049c8df8abb16", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ExoOumP3oTI-PBNNeKOXbuZ0P9SipKBzgqUbJ6ny_a4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=785ae62dfffe175eb4de31ca81714b3ec5910730", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ExoOumP3oTI-PBNNeKOXbuZ0P9SipKBzgqUbJ6ny_a4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83fd151918222deb803c5bb23a4f7bf9813d41c2", "width": 1080, "height": 540}], "variants": {}, "id": "1EJgFSvJ_7YZ2UEqBC9nuAehnkt8d8KzeWNoH_LsgQw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n874l", "is_robot_indexable": true, "report_reasons": null, "author": "thegeekyasian", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n874l/ive_created_an_opensource_geospatial_library_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/thegeekyasian/geo-assist", "subreddit_subscribers": 855570, "created_utc": 1678405897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "By areas, I mean CV, NLP, Time series, Audio classification, and others. Do you try to have some basic knowledge of every areas?", "author_fullname": "t2_tuwq1ase", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When you are building portfolio, are you doing projects from more areas, or do you focus on one?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mz2ca", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678385101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;By areas, I mean CV, NLP, Time series, Audio classification, and others. Do you try to have some basic knowledge of every areas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mz2ca", "is_robot_indexable": true, "report_reasons": null, "author": "No_Philosophy_8520", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mz2ca/when_you_are_building_portfolio_are_you_doing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mz2ca/when_you_are_building_portfolio_are_you_doing/", "subreddit_subscribers": 855570, "created_utc": 1678385101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I am working with a high-dimensional dataset that contains approximately 300 columns and 10,000 rows. The dataset is characterized by a significant number of columns with missing values. To gain a better understanding of the missing data patterns, I generated a nullity matrix, which revealed the following:   \n\n[nullity matrix](https://preview.redd.it/6bhzfkds2qma1.png?width=2074&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=68c05b771136c8e20f2b2a39d7bd0fbd15a3950a)\n\n After careful consideration, I have devised a plan to divide the data into two datasets. The first dataset will consist of the first 5000 rows, while the second dataset will encompass the remaining rows after columns with a high number of missing values have been dropped. It is worth noting that the test dataset exhibits the same pattern of missing values, and will therefore undergo the same treatment. Subsequently, I will train my model using the first dataset to predict the outcomes of the first portion of the test data, and then repeat the process using the second dataset for the remaining portion of the test data. Is this approach valid, or is there a concern that important information will be lost? Do you have any suggestions for alternative, more optimal solutions for this particular challenge?", "author_fullname": "t2_7uwwf65z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dealing with a lot of missing values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 55, "top_awarded_type": null, "hide_score": false, "media_metadata": {"6bhzfkds2qma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdb5bebc1d165d70a832076654f3a7abf1f76970"}, {"y": 85, "x": 216, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c89d13eb9dc5c2781ba7cb82e25d9105c46ff5d"}, {"y": 126, "x": 320, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89947b0fdead6015965d641b52eaa672a95a05c5"}, {"y": 252, "x": 640, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efb348a7397231cefb27c03771c0de886de6eb3a"}, {"y": 378, "x": 960, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d41d281c1a3be7b98132ebf631891c7e8203aaa5"}, {"y": 425, "x": 1080, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1bf1b4135b3ff68b23bf181e04612b2e2c08bc9"}], "s": {"y": 817, "x": 2074, "u": "https://preview.redd.it/6bhzfkds2qma1.png?width=2074&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=68c05b771136c8e20f2b2a39d7bd0fbd15a3950a"}, "id": "6bhzfkds2qma1"}}, "name": "t3_11mtoua", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5Ai5OJ2cloS7xUaRLt83UptWtJpolBjcb2l13ISbhLM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678372315.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a high-dimensional dataset that contains approximately 300 columns and 10,000 rows. The dataset is characterized by a significant number of columns with missing values. To gain a better understanding of the missing data patterns, I generated a nullity matrix, which revealed the following:   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6bhzfkds2qma1.png?width=2074&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=68c05b771136c8e20f2b2a39d7bd0fbd15a3950a\"&gt;nullity matrix&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After careful consideration, I have devised a plan to divide the data into two datasets. The first dataset will consist of the first 5000 rows, while the second dataset will encompass the remaining rows after columns with a high number of missing values have been dropped. It is worth noting that the test dataset exhibits the same pattern of missing values, and will therefore undergo the same treatment. Subsequently, I will train my model using the first dataset to predict the outcomes of the first portion of the test data, and then repeat the process using the second dataset for the remaining portion of the test data. Is this approach valid, or is there a concern that important information will be lost? Do you have any suggestions for alternative, more optimal solutions for this particular challenge?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mtoua", "is_robot_indexable": true, "report_reasons": null, "author": "Hamdi_bks", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mtoua/dealing_with_a_lot_of_missing_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mtoua/dealing_with_a_lot_of_missing_values/", "subreddit_subscribers": 855570, "created_utc": 1678372315.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How much importance does recuruiters/interviewers give to Github portfolio for Data Science jobs?", "author_fullname": "t2_3it27rt0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Github Portfolio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mjsxa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678339641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How much importance does recuruiters/interviewers give to Github portfolio for Data Science jobs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mjsxa", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Opinion_5729", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mjsxa/github_portfolio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mjsxa/github_portfolio/", "subreddit_subscribers": 855570, "created_utc": 1678339641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Asking for a friend. Do you know of good data science programs for Masters or PhD that focus on climate science. If someone wants to do research on applying data science techniques to study climate change, what would be your advice for them ?", "author_fullname": "t2_9y3xn48g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data science graduate programs with focus on climate science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n5k9x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678399933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Asking for a friend. Do you know of good data science programs for Masters or PhD that focus on climate science. If someone wants to do research on applying data science techniques to study climate change, what would be your advice for them ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n5k9x", "is_robot_indexable": true, "report_reasons": null, "author": "AsohkaTano", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n5k9x/data_science_graduate_programs_with_focus_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n5k9x/data_science_graduate_programs_with_focus_on/", "subreddit_subscribers": 855570, "created_utc": 1678399933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently finishing the first year of my undergrad and I have concluded that I have two options if I want to finish school within 4 years:\n\n1. An honors bachelor's in data science \n2. An honors bachelor's in statistics with a minor in comp sci\n\nOf the two options provided which would better prepare me for a data science job (or anything related such as MS engineer, data engineer, etc.) post-graduation?   \n\n\nAs well, I have been looking into different paid courses such as Dataquest, Data Camp, and 365Datascience, and was looking for some input as to if these are worth it to try and get ahead, to possibly land an early internship in my second or third-year of study.\n\nI have been especially curious about 365Datascience and wanted to hear everyone's thoughts on it if you have taken the courses they provide.\n\nThanks in advance!", "author_fullname": "t2_9i713ffq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice needed for an undergraduate degree path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n9t1p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678409842.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently finishing the first year of my undergrad and I have concluded that I have two options if I want to finish school within 4 years:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;An honors bachelor&amp;#39;s in data science &lt;/li&gt;\n&lt;li&gt;An honors bachelor&amp;#39;s in statistics with a minor in comp sci&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Of the two options provided which would better prepare me for a data science job (or anything related such as MS engineer, data engineer, etc.) post-graduation?   &lt;/p&gt;\n\n&lt;p&gt;As well, I have been looking into different paid courses such as Dataquest, Data Camp, and 365Datascience, and was looking for some input as to if these are worth it to try and get ahead, to possibly land an early internship in my second or third-year of study.&lt;/p&gt;\n\n&lt;p&gt;I have been especially curious about 365Datascience and wanted to hear everyone&amp;#39;s thoughts on it if you have taken the courses they provide.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n9t1p", "is_robot_indexable": true, "report_reasons": null, "author": "Routine-Race3913", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n9t1p/advice_needed_for_an_undergraduate_degree_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n9t1p/advice_needed_for_an_undergraduate_degree_path/", "subreddit_subscribers": 855570, "created_utc": 1678409842.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am bombarded with videos of fights and violence. And I don\u2019t even interact with these videos - it\u2019s being forced on me.\n\nIs anyone else experiencing this?", "author_fullname": "t2_vg8jg1rc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is Twitters algorithm promoting violence?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n8anr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678406131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am bombarded with videos of fights and violence. And I don\u2019t even interact with these videos - it\u2019s being forced on me.&lt;/p&gt;\n\n&lt;p&gt;Is anyone else experiencing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n8anr", "is_robot_indexable": true, "report_reasons": null, "author": "dataentryadmin", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n8anr/why_is_twitters_algorithm_promoting_violence/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n8anr/why_is_twitters_algorithm_promoting_violence/", "subreddit_subscribers": 855570, "created_utc": 1678406131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Looking for some free tools to get quick insights", "author_fullname": "t2_65chay6oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good tools for exploratory data analysis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n52b0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678398839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some free tools to get quick insights&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n52b0", "is_robot_indexable": true, "report_reasons": null, "author": "Mona_Labs", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n52b0/what_are_some_good_tools_for_exploratory_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n52b0/what_are_some_good_tools_for_exploratory_data/", "subreddit_subscribers": 855570, "created_utc": 1678398839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all!\n\nI'm currently working with time series data. My manager wants me to use a \"simple\" model that is explainable. He said to start off with tree models, so I went with XGBoost having seen it being used for time series. I'm new to time series though, so I'm a bit confused as to how some things work.\n\nMy question is, upon train/test split, do I have to use the tail end of the dataset for the test set?\n\nIt doesn't seem to me like that makes a huge amount of sense for an XGBoost. Does the XGBoost model really take into account the order of the data points?", "author_fullname": "t2_9a5zvrr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "XGBoost for time series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11moqft", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678357333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working with time series data. My manager wants me to use a &amp;quot;simple&amp;quot; model that is explainable. He said to start off with tree models, so I went with XGBoost having seen it being used for time series. I&amp;#39;m new to time series though, so I&amp;#39;m a bit confused as to how some things work.&lt;/p&gt;\n\n&lt;p&gt;My question is, upon train/test split, do I have to use the tail end of the dataset for the test set?&lt;/p&gt;\n\n&lt;p&gt;It doesn&amp;#39;t seem to me like that makes a huge amount of sense for an XGBoost. Does the XGBoost model really take into account the order of the data points?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11moqft", "is_robot_indexable": true, "report_reasons": null, "author": "No_Storm_1500", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11moqft/xgboost_for_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11moqft/xgboost_for_time_series/", "subreddit_subscribers": 855570, "created_utc": 1678357333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello I'm looking for an open dataset for demand forecasting that has sales data, product data and price information for a number of different products, possibly across categories and including seasonality information. \n\nI'm also looking for instructive papers for baseline models and techniques used in demand forecasting or planning space. \n\nI've looked at lots of datasets online - kaggle, AWS, etc and I can't find a good one that fits my needs for FREE. Any help would be appreciated!", "author_fullname": "t2_bk9hp9qn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "demand forecasting datasets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n8bnt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678406201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello I&amp;#39;m looking for an open dataset for demand forecasting that has sales data, product data and price information for a number of different products, possibly across categories and including seasonality information. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also looking for instructive papers for baseline models and techniques used in demand forecasting or planning space. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked at lots of datasets online - kaggle, AWS, etc and I can&amp;#39;t find a good one that fits my needs for FREE. Any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n8bnt", "is_robot_indexable": true, "report_reasons": null, "author": "perfectlylonely13", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n8bnt/demand_forecasting_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n8bnt/demand_forecasting_datasets/", "subreddit_subscribers": 855570, "created_utc": 1678406201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "TL;DR\nGive me your best resources about this topic. Guides, implementation, but most importantly, I would like to receive some resources that let me understand all the parameters I could change and how it affects the learning.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nI have recently find out a possible approach for my problem (predictive maintenance) could be tackled using LSTM Autoencoders.\nThe idea is to learn normal representation of  my data (that is multi variate, I have more than 20 features), and then test some time intervals to see if the MAE or some other measure increase over a certain chosen threshold. \n\nSome doubts:\n-Historical data for my sensors cover more than 1 month and time steps are every 5 seconds.\nI would like to learn short temporal patterns since I am not sure if LSTM can handle large temporal series. So my sample is made of 180 timesteps(15 minutes in total). How to improve this?\n-Also how to handle the encoder decoder architecture? I mean since I have 10000 samples of 180 timesteps and each sample has multiple features per timestep, what can be the structure of the LSTM? \n-Also feel free to post every good resources you have used in the past about this", "author_fullname": "t2_i6lyoywf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LSTM AutoEncoders", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n6z1q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678403061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR\nGive me your best resources about this topic. Guides, implementation, but most importantly, I would like to receive some resources that let me understand all the parameters I could change and how it affects the learning.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014&lt;/p&gt;\n\n&lt;p&gt;I have recently find out a possible approach for my problem (predictive maintenance) could be tackled using LSTM Autoencoders.\nThe idea is to learn normal representation of  my data (that is multi variate, I have more than 20 features), and then test some time intervals to see if the MAE or some other measure increase over a certain chosen threshold. &lt;/p&gt;\n\n&lt;p&gt;Some doubts:\n-Historical data for my sensors cover more than 1 month and time steps are every 5 seconds.\nI would like to learn short temporal patterns since I am not sure if LSTM can handle large temporal series. So my sample is made of 180 timesteps(15 minutes in total). How to improve this?\n-Also how to handle the encoder decoder architecture? I mean since I have 10000 samples of 180 timesteps and each sample has multiple features per timestep, what can be the structure of the LSTM? \n-Also feel free to post every good resources you have used in the past about this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n6z1q", "is_robot_indexable": true, "report_reasons": null, "author": "ginotherhino1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n6z1q/lstm_autoencoders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n6z1q/lstm_autoencoders/", "subreddit_subscribers": 855570, "created_utc": 1678403061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi r/datascience community,\n\nI have been invited to give a lecture on \"*data science in practice*\" for a class of MSc. business analytics students. While I plan on covering some technical aspects of the field, I believe that it might be worthwhile to focus more on the non-technical insights and tricks that one picks up after some time in industry. The things you don't learn on Kaggle.\n\nThis got me thinking: *What are the important non-technical lessons to you guys*? Perhaps it's the importance of effective communication skills, the ability to work collaboratively with colleagues from diverse backgrounds, or the value of prioritizing business needs over technical perfectionism.\n\nI'm hoping to compile a list of key takeaways to share with the class (and this subreddit), and your insights would be incredibly valuable. Whether you're a seasoned data scientist or just starting out, I'd love to hear your thoughts on this topic.\n\nTo start the conversation off, I have considered the following two points:\n\n\\- **How to choose DS projects:** For example, I find it helpful to score the potential project on \"feasibility\", \"scalability\", \"maintainability\" and \"business importance\". \n\n\\- **Understanding the differences in** ***data science maturity*** across sectors and organizations. For many organisations, the barriers to success with DS/ML/AI are strategic alignment, data availability/integrity etc. or executive sponsorship/buy-in - not the technical know-how.\n\nLooking forward to hearing your thoughts.\n\n\ud83d\udcf7\ud83d\udcf7", "author_fullname": "t2_26n50qtm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What non-technical lessons are important for aspiring data scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n1xgt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678391695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt; community,&lt;/p&gt;\n\n&lt;p&gt;I have been invited to give a lecture on &amp;quot;&lt;em&gt;data science in practice&lt;/em&gt;&amp;quot; for a class of MSc. business analytics students. While I plan on covering some technical aspects of the field, I believe that it might be worthwhile to focus more on the non-technical insights and tricks that one picks up after some time in industry. The things you don&amp;#39;t learn on Kaggle.&lt;/p&gt;\n\n&lt;p&gt;This got me thinking: &lt;em&gt;What are the important non-technical lessons to you guys&lt;/em&gt;? Perhaps it&amp;#39;s the importance of effective communication skills, the ability to work collaboratively with colleagues from diverse backgrounds, or the value of prioritizing business needs over technical perfectionism.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping to compile a list of key takeaways to share with the class (and this subreddit), and your insights would be incredibly valuable. Whether you&amp;#39;re a seasoned data scientist or just starting out, I&amp;#39;d love to hear your thoughts on this topic.&lt;/p&gt;\n\n&lt;p&gt;To start the conversation off, I have considered the following two points:&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;How to choose DS projects:&lt;/strong&gt; For example, I find it helpful to score the potential project on &amp;quot;feasibility&amp;quot;, &amp;quot;scalability&amp;quot;, &amp;quot;maintainability&amp;quot; and &amp;quot;business importance&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Understanding the differences in&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;data science maturity&lt;/em&gt;&lt;/strong&gt; across sectors and organizations. For many organisations, the barriers to success with DS/ML/AI are strategic alignment, data availability/integrity etc. or executive sponsorship/buy-in - not the technical know-how.&lt;/p&gt;\n\n&lt;p&gt;Looking forward to hearing your thoughts.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcf7\ud83d\udcf7&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n1xgt", "is_robot_indexable": true, "report_reasons": null, "author": "Academy-", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n1xgt/what_nontechnical_lessons_are_important_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n1xgt/what_nontechnical_lessons_are_important_for/", "subreddit_subscribers": 855570, "created_utc": 1678391695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_1zz7cefa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automating operations on column-based data across multiple files using the Codex Davinci model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11msd2v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/yl1xdh05upma1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/yl1xdh05upma1/DASH_96.mp4", "dash_url": "https://v.redd.it/yl1xdh05upma1/DASHPlaylist.mpd?a=1681011907%2CZmFiYTViYjk3NmMyNDgyNzRmYWU2ZmUwZDFlZTg5NDQ3ZTc5YWEwMzMzYjFlNTdhMjhhNmQzZDliNjQwMDk3Ng%3D%3D&amp;v=1&amp;f=sd", "duration": 19, "hls_url": "https://v.redd.it/yl1xdh05upma1/HLSPlaylist.m3u8?a=1681011907%2CNGZjZTkwNzk2NWIzZDc3YmM2OGM5YzhmOGM2N2Q3MGQ5ZDhjODYxNjdkZjBlNTM1OThkNjA1YmRkY2M4OGZkMA%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/s3J_Y0ezmKHiH2pX98m0wBU4FKskQlwwJWUkTGBqnLc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678368970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/yl1xdh05upma1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a08af611da44165a74d1c8e4a6ba151b3cfe8225", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=173556aff19a963730bb5c13645b06e0c87d2888", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8c9e849efa9fd3666c8c68dcf92fb8a5ba962ea2", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7df886a3ceab54f2068538fe5828182fd267d7b0", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=32ef900b4ae0be5098352535f6183c1558a595c2", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5d00980b8129c1e403c574d5a1f4fd9296db4e19", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/zWzZYky2xe72zhpvlhfgSk3FnUP4VDziPj6VjOZ-h40.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6b0e9f98d5bf740c90650d6f66966a5f320ba66a", "width": 1080, "height": 607}], "variants": {}, "id": "sD1ViAqbGBOUiDB-PrxLdzGJ96XYSJnGoymD9gu3uww"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11msd2v", "is_robot_indexable": true, "report_reasons": null, "author": "kijubikal", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11msd2v/automating_operations_on_columnbased_data_across/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/yl1xdh05upma1", "subreddit_subscribers": 855570, "created_utc": 1678368970.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/yl1xdh05upma1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/yl1xdh05upma1/DASH_96.mp4", "dash_url": "https://v.redd.it/yl1xdh05upma1/DASHPlaylist.mpd?a=1681011907%2CZmFiYTViYjk3NmMyNDgyNzRmYWU2ZmUwZDFlZTg5NDQ3ZTc5YWEwMzMzYjFlNTdhMjhhNmQzZDliNjQwMDk3Ng%3D%3D&amp;v=1&amp;f=sd", "duration": 19, "hls_url": "https://v.redd.it/yl1xdh05upma1/HLSPlaylist.m3u8?a=1681011907%2CNGZjZTkwNzk2NWIzZDc3YmM2OGM5YzhmOGM2N2Q3MGQ5ZDhjODYxNjdkZjBlNTM1OThkNjA1YmRkY2M4OGZkMA%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello people,\n\nI look for reference books for cleaning time series datasets. can anyone help me with this request.\n\nthanks", "author_fullname": "t2_eikjje19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "time series data cleaning reference books", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mrkmz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678366811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello people,&lt;/p&gt;\n\n&lt;p&gt;I look for reference books for cleaning time series datasets. can anyone help me with this request.&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mrkmz", "is_robot_indexable": true, "report_reasons": null, "author": "jorgecormane", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mrkmz/time_series_data_cleaning_reference_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mrkmz/time_series_data_cleaning_reference_books/", "subreddit_subscribers": 855570, "created_utc": 1678366811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_kgkprqpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Leading Technology Event - Big Data &amp; AI World 8-9 March, 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 52, "top_awarded_type": null, "hide_score": false, "name": "t3_11mqftc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/05_goNF3kF_nE5zdGN6lwz4PVQTtvoB0uDyUhHCwjxo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678363345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dasca.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.dasca.org/newsroom/discover-the-intelligent-future-at-big-data-ai-world-2023", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?auto=webp&amp;v=enabled&amp;s=de57f865cc5e42243d5235a4715c71954269bf68", "width": 1600, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adefb8b91ad01ebd34517d171d938b01bf5b8c66", "width": 108, "height": 40}, {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=130ce02b62410f3ed6685924e4fad97aa13a0f04", "width": 216, "height": 81}, {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6ce1c7599c569b518259aecb7cb2d35a806b26b", "width": 320, "height": 120}, {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72ee3e8af1bf03361ac42916120b0d876e4d67ae", "width": 640, "height": 240}, {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=436adb0f6d9229cb591781aede5f372aab9eaaba", "width": 960, "height": 360}, {"url": "https://external-preview.redd.it/ZgSJAptt01NNKV_Cl-8-8PbzYtGuKaGM66kIRUpeXO8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f94cf3246648acceb7000a1bea76ef45d413850", "width": 1080, "height": 405}], "variants": {}, "id": "aFdnRqHZhCc_vlA3H683TQXP5YJ0qnajroSr9Kd4K0M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mqftc", "is_robot_indexable": true, "report_reasons": null, "author": "Emily-joe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mqftc/leading_technology_event_big_data_ai_world_89/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dasca.org/newsroom/discover-the-intelligent-future-at-big-data-ai-world-2023", "subreddit_subscribers": 855570, "created_utc": 1678363345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_akn4dziw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What major would go well with a data science minor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11nbb1m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678413637.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11nbb1m", "is_robot_indexable": true, "report_reasons": null, "author": "dahnzii", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11nbb1m/what_major_would_go_well_with_a_data_science_minor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11nbb1m/what_major_would_go_well_with_a_data_science_minor/", "subreddit_subscribers": 855570, "created_utc": 1678413637.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently going to be graduating with a masters in computer science and really worried about the future in data science. I focused on machine learning and while I have done very well in school, I don\u2019t really have the work experience that some of these job postings ask for. Since I am graduating in May, I can\u2019t get an internship or anything. I tried getting an internship last year, applied to 100 and didn\u2019t get any offers. \n\nWould be doing my own projects on the side be enough to gain experience? Is trying Kaggle Competitions a better route? I am honestly so discouraged about the market\u2026", "author_fullname": "t2_v66nvlfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not sure where to go from here.. (graduating from school soon)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11navpn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678412549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently going to be graduating with a masters in computer science and really worried about the future in data science. I focused on machine learning and while I have done very well in school, I don\u2019t really have the work experience that some of these job postings ask for. Since I am graduating in May, I can\u2019t get an internship or anything. I tried getting an internship last year, applied to 100 and didn\u2019t get any offers. &lt;/p&gt;\n\n&lt;p&gt;Would be doing my own projects on the side be enough to gain experience? Is trying Kaggle Competitions a better route? I am honestly so discouraged about the market\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11navpn", "is_robot_indexable": true, "report_reasons": null, "author": "suduko6029", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11navpn/not_sure_where_to_go_from_here_graduating_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11navpn/not_sure_where_to_go_from_here_graduating_from/", "subreddit_subscribers": 855570, "created_utc": 1678412549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Has anyone written a Roadmap of Udemy courses a self taught person could use to learn Data Science and that way be prepared for the job market?\n\nThank you for your input, used google, searched a few forums and have not been able to find one.", "author_fullname": "t2_3luaynwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Udemy Roadmap request", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n4he4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678397522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone written a Roadmap of Udemy courses a self taught person could use to learn Data Science and that way be prepared for the job market?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your input, used google, searched a few forums and have not been able to find one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n4he4", "is_robot_indexable": true, "report_reasons": null, "author": "LightDarkCloud", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n4he4/udemy_roadmap_request/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n4he4/udemy_roadmap_request/", "subreddit_subscribers": 855570, "created_utc": 1678397522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Not sure if \u201cheterogeneous\u201d is the right word here but I\u2019m doing KRR where features that appear to be more suited to a power/log transform, some more suited for centering to 0 mean and unit variance. If I\u2019m manually choosing \u201chow\u201d to encode each feature, maybe I should upgrade to a fancier model, like a a neural network with VAE? Curious to hear opinions on this.", "author_fullname": "t2_1k2rzmpm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mixed Feature Transformers for highly heterogeneous data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n1yol", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678391771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if \u201cheterogeneous\u201d is the right word here but I\u2019m doing KRR where features that appear to be more suited to a power/log transform, some more suited for centering to 0 mean and unit variance. If I\u2019m manually choosing \u201chow\u201d to encode each feature, maybe I should upgrade to a fancier model, like a a neural network with VAE? Curious to hear opinions on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n1yol", "is_robot_indexable": true, "report_reasons": null, "author": "vent2012", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n1yol/mixed_feature_transformers_for_highly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n1yol/mixed_feature_transformers_for_highly/", "subreddit_subscribers": 855570, "created_utc": 1678391771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone, I am a 12th grader in a data science class and I chose my project to be on how data science can be used to learn aspects of history. I'm wondering what you all think about this, I've read some articles that were beneficial but I was interested in what you all had to say.\n\nthank you!", "author_fullname": "t2_62ubafd4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can data science be used to learn aspects of history?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n0d4p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678388101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I am a 12th grader in a data science class and I chose my project to be on how data science can be used to learn aspects of history. I&amp;#39;m wondering what you all think about this, I&amp;#39;ve read some articles that were beneficial but I was interested in what you all had to say.&lt;/p&gt;\n\n&lt;p&gt;thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11n0d4p", "is_robot_indexable": true, "report_reasons": null, "author": "smellykenma", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11n0d4p/how_can_data_science_be_used_to_learn_aspects_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11n0d4p/how_can_data_science_be_used_to_learn_aspects_of/", "subreddit_subscribers": 855570, "created_utc": 1678388101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The project I'm currently working on uses (relatively) small tables / DB sizes. We have a setup that is appropriate for our current size of team, size of data, ease of use to maintain, ease of use to extract, etc.\n\nWe are going to be expanding our data size by something like 4x-20x in the next year or two.\n\nWe want to find examples of companies doing similar data work to us, and seeing how they managed their data.\n\nOur main setup is:\n\n1. Raw data (in the thousands of records, with thousands of datapoints, so relatively small all things considered). This is not expanding much.\n2. Raw data processing pipelines. This is where the 4x-20x is coming from. We can edit the processing pipeline to give us different output features.\n3. Post processing Feature values. We have a database of post processing features so we don't need to process the raw data each time we do a model training run.\n\nThe setup as described above must be quite common - storing post processed feature values. Then combining different sets of features to do model training runs.\n\nAre there any good resources on how more established companies manage this setup?", "author_fullname": "t2_80ont", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog posts detailing ML companies data infrastructure / DB schema?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mzo1f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678386478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The project I&amp;#39;m currently working on uses (relatively) small tables / DB sizes. We have a setup that is appropriate for our current size of team, size of data, ease of use to maintain, ease of use to extract, etc.&lt;/p&gt;\n\n&lt;p&gt;We are going to be expanding our data size by something like 4x-20x in the next year or two.&lt;/p&gt;\n\n&lt;p&gt;We want to find examples of companies doing similar data work to us, and seeing how they managed their data.&lt;/p&gt;\n\n&lt;p&gt;Our main setup is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Raw data (in the thousands of records, with thousands of datapoints, so relatively small all things considered). This is not expanding much.&lt;/li&gt;\n&lt;li&gt;Raw data processing pipelines. This is where the 4x-20x is coming from. We can edit the processing pipeline to give us different output features.&lt;/li&gt;\n&lt;li&gt;Post processing Feature values. We have a database of post processing features so we don&amp;#39;t need to process the raw data each time we do a model training run.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The setup as described above must be quite common - storing post processed feature values. Then combining different sets of features to do model training runs.&lt;/p&gt;\n\n&lt;p&gt;Are there any good resources on how more established companies manage this setup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mzo1f", "is_robot_indexable": true, "report_reasons": null, "author": "Cwlrs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mzo1f/blog_posts_detailing_ml_companies_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mzo1f/blog_posts_detailing_ml_companies_data/", "subreddit_subscribers": 855570, "created_utc": 1678386478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Have you ever wanted to use handy scikit-learn functionalities with your neural networks, but couldn\u2019t because TensorFlow models are not compatible with the scikit-learn API?\n\nI\u2019m excited to introduce one-line wrappers for TensorFlow/Keras models that enable you to use TensorFlow models within scikit-learn workflows with features like Pipeline, GridSearch, and more.\n\n[Swap in one line of code to use keras\\/TF models with scikit-learn.](https://preview.redd.it/jebe5i446rma1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8afe3c365de5c2810bfb2deaea840e7447b59f3e)\n\nTransformers are extremely popular for modeling text nowadays with GPT3, ChatGPT, Bard, PaLM, FLAN excelling for conversational AI and other Transformers like T5 &amp; BERT excelling for text classification. Scikit-learn offers a broadly useful suite of features for classifier models, but these are hard to use with Transformers. However not if you use these wrappers we developed, which only require changing one line of code to make your existing Tensorflow/Keras model compatible with scikit-learn\u2019s rich ecosystem!\n\nAll you have to do is swap `keras.Model` \u2192 `KerasWrapperModel`, or `keras.Sequential` \u2192 `KerasSequentialWrapper`. The wrapper objects have all the same methods as their keras counterparts, plus you can use them with tons of awesome scikit-learn methods.\n\nYou can find a demo jupyter notebook and read more about the wrappers [here](https://cleanlab.ai/blog/transformer-sklearn/) :)", "author_fullname": "t2_s0qucgfk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Training Transformer Networks in Scikit-Learn?!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"jebe5i446rma1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/jebe5i446rma1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b54c29c6cb01e512465dcd303a36d2f7edf905e0"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/jebe5i446rma1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d08d3f6b881fa8b74ffc2f95a820c62ada76729b"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/jebe5i446rma1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e8b15bf5d72de07a2d23a804518f9ba1ab43ede"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/jebe5i446rma1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ded45299e716979802c2c3aecd270d2412cf343"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/jebe5i446rma1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=787b99c95bb4e9ae19f07aca494b01ff89c4914f"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/jebe5i446rma1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8afe3c365de5c2810bfb2deaea840e7447b59f3e"}, "id": "jebe5i446rma1"}}, "name": "t3_11mzcle", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/v1Fd8XNkRgEWk-CFj66FA0QCVOqelXOeu69e1-2kd78.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678385751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you ever wanted to use handy scikit-learn functionalities with your neural networks, but couldn\u2019t because TensorFlow models are not compatible with the scikit-learn API?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m excited to introduce one-line wrappers for TensorFlow/Keras models that enable you to use TensorFlow models within scikit-learn workflows with features like Pipeline, GridSearch, and more.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jebe5i446rma1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8afe3c365de5c2810bfb2deaea840e7447b59f3e\"&gt;Swap in one line of code to use keras/TF models with scikit-learn.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Transformers are extremely popular for modeling text nowadays with GPT3, ChatGPT, Bard, PaLM, FLAN excelling for conversational AI and other Transformers like T5 &amp;amp; BERT excelling for text classification. Scikit-learn offers a broadly useful suite of features for classifier models, but these are hard to use with Transformers. However not if you use these wrappers we developed, which only require changing one line of code to make your existing Tensorflow/Keras model compatible with scikit-learn\u2019s rich ecosystem!&lt;/p&gt;\n\n&lt;p&gt;All you have to do is swap &lt;code&gt;keras.Model&lt;/code&gt; \u2192 &lt;code&gt;KerasWrapperModel&lt;/code&gt;, or &lt;code&gt;keras.Sequential&lt;/code&gt; \u2192 &lt;code&gt;KerasSequentialWrapper&lt;/code&gt;. The wrapper objects have all the same methods as their keras counterparts, plus you can use them with tons of awesome scikit-learn methods.&lt;/p&gt;\n\n&lt;p&gt;You can find a demo jupyter notebook and read more about the wrappers &lt;a href=\"https://cleanlab.ai/blog/transformer-sklearn/\"&gt;here&lt;/a&gt; :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11mzcle", "is_robot_indexable": true, "report_reasons": null, "author": "cmauck10", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11mzcle/training_transformer_networks_in_scikitlearn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11mzcle/training_transformer_networks_in_scikitlearn/", "subreddit_subscribers": 855570, "created_utc": 1678385751.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}