{"kind": "Listing", "data": {"after": "t3_11nahid", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just a lighter post for a change. \n\nI'll go first, I'd sell my car for an api access to the myFitnessPal data.\n\nSo many interesting findings out of it. What do people eat? Per country? Age group? How does it relate to their fitness goal? Such a gold mine.", "author_fullname": "t2_btfgm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What dataset would you pay good money to get your hands on?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mtgx0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 114, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 114, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678372796.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678371766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just a lighter post for a change. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll go first, I&amp;#39;d sell my car for an api access to the myFitnessPal data.&lt;/p&gt;\n\n&lt;p&gt;So many interesting findings out of it. What do people eat? Per country? Age group? How does it relate to their fitness goal? Such a gold mine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11mtgx0", "is_robot_indexable": true, "report_reasons": null, "author": "holiquetal", "discussion_type": null, "num_comments": 78, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mtgx0/what_dataset_would_you_pay_good_money_to_get_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mtgx0/what_dataset_would_you_pay_good_money_to_get_your/", "subreddit_subscribers": 92547, "created_utc": 1678371766.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "ConduKtor released a Free Kafka Sandbox : [https://www.conduktor.io/blog/kafka-playground-two-free-kafka-clusters-without-operational-hassles/](https://www.conduktor.io/blog/kafka-playground-two-free-kafka-clusters-without-operational-hassles/)", "author_fullname": "t2_244osg1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Conduktor : Free Kafka Clusters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11my5wf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678383020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ConduKtor released a Free Kafka Sandbox : &lt;a href=\"https://www.conduktor.io/blog/kafka-playground-two-free-kafka-clusters-without-operational-hassles/\"&gt;https://www.conduktor.io/blog/kafka-playground-two-free-kafka-clusters-without-operational-hassles/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UeTrVgO8vf8Mc4OkREi9zHGKcid9AWDjo-bKADQkl7M.jpg?auto=webp&amp;v=enabled&amp;s=a33e037885a6e5b5df57ccbccad8cba64db1dbcc", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/UeTrVgO8vf8Mc4OkREi9zHGKcid9AWDjo-bKADQkl7M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3d7bf51724e9b73189d11a5847034ee2d8c660e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/UeTrVgO8vf8Mc4OkREi9zHGKcid9AWDjo-bKADQkl7M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=925865149ac549f3fbb66f9a5c4057da27e19b3c", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/UeTrVgO8vf8Mc4OkREi9zHGKcid9AWDjo-bKADQkl7M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09ae372ad6339dee7484d94d503346962f2cf32d", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/UeTrVgO8vf8Mc4OkREi9zHGKcid9AWDjo-bKADQkl7M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d41aa75cacb21ff468847ae553da6e9befe78d4", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/UeTrVgO8vf8Mc4OkREi9zHGKcid9AWDjo-bKADQkl7M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69967e6df8b770d4a7be0ef2790777c47b549d4a", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/UeTrVgO8vf8Mc4OkREi9zHGKcid9AWDjo-bKADQkl7M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=225b959cf772c69c62a45eabdf2674d5185cda34", "width": 1080, "height": 607}], "variants": {}, "id": "Yn3kzGTLqppmgdktyy6rac2phoJwHdSWpc-PYWYbD1Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11my5wf", "is_robot_indexable": true, "report_reasons": null, "author": "vainamoinen_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11my5wf/conduktor_free_kafka_clusters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11my5wf/conduktor_free_kafka_clusters/", "subreddit_subscribers": 92547, "created_utc": 1678383020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi redditors!\n\nToday I'm delighted to bring you the latest 0.6 release of UI for Apache Kafka, packed with new features and enhancements!\n\nThis version offers:\n- A configuration wizard that simplifies cluster setup (right in web UI!). Now we can launch the app via AWS AMI image and setup a cluster on the go\n- Integration with OpenDataDiscovery Platform to gain deeper insight into your metadata changes\n- Support for protobuf imports &amp; file references\n\nOther minor, yet significant, enhancements include:\n- Embedded Avro embedded serde plugin\n- Improved ISR display on Topic overview (now you can view it per partition!)\n\nAnd a cherry on top? Now we\u2019re able to work around kafka ACL errors so you won\u2019t need to confront pesky permission issues when using the app.\n\nDon\u2019t wait, the update is already available on github &amp; @ AWS Marketplace!\n\nFull changelog: https://github.com/provectus/kafka-ui/releases/tag/v0.6.0\nThanks to everyone who just started and continued to contribute!\nIn the next release, we'll focus a bit on expanding our RBAC possibilities (support for LDAP and universal OAuth providers) and some Wizard features!", "author_fullname": "t2_ttr4u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A 0.6 release of UI for Apache Kafka w/ cluster configuration wizard &amp; ODD Platform integration is out!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mxpfx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678381958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi redditors!&lt;/p&gt;\n\n&lt;p&gt;Today I&amp;#39;m delighted to bring you the latest 0.6 release of UI for Apache Kafka, packed with new features and enhancements!&lt;/p&gt;\n\n&lt;p&gt;This version offers:\n- A configuration wizard that simplifies cluster setup (right in web UI!). Now we can launch the app via AWS AMI image and setup a cluster on the go\n- Integration with OpenDataDiscovery Platform to gain deeper insight into your metadata changes\n- Support for protobuf imports &amp;amp; file references&lt;/p&gt;\n\n&lt;p&gt;Other minor, yet significant, enhancements include:\n- Embedded Avro embedded serde plugin\n- Improved ISR display on Topic overview (now you can view it per partition!)&lt;/p&gt;\n\n&lt;p&gt;And a cherry on top? Now we\u2019re able to work around kafka ACL errors so you won\u2019t need to confront pesky permission issues when using the app.&lt;/p&gt;\n\n&lt;p&gt;Don\u2019t wait, the update is already available on github &amp;amp; @ AWS Marketplace!&lt;/p&gt;\n\n&lt;p&gt;Full changelog: &lt;a href=\"https://github.com/provectus/kafka-ui/releases/tag/v0.6.0\"&gt;https://github.com/provectus/kafka-ui/releases/tag/v0.6.0&lt;/a&gt;\nThanks to everyone who just started and continued to contribute!\nIn the next release, we&amp;#39;ll focus a bit on expanding our RBAC possibilities (support for LDAP and universal OAuth providers) and some Wizard features!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZRuSQVebRubrwpdeF5dYo_X8B1Zzxa-Q4OFredy3t7s.jpg?auto=webp&amp;v=enabled&amp;s=732eccf249c9781fa450f3a492a67c6432c650d2", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ZRuSQVebRubrwpdeF5dYo_X8B1Zzxa-Q4OFredy3t7s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cf90c40c1eb7d27f2eb4d06f69cc13bd30e34e3", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ZRuSQVebRubrwpdeF5dYo_X8B1Zzxa-Q4OFredy3t7s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f7ac8f7f01040c109a95d00ac0fc4ac31fb01dd", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ZRuSQVebRubrwpdeF5dYo_X8B1Zzxa-Q4OFredy3t7s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be45d1fc9504af97e8876c63e9bd1eeb74301123", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ZRuSQVebRubrwpdeF5dYo_X8B1Zzxa-Q4OFredy3t7s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=120f4f2ae2398315c6057a2d2893770b17a802ba", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ZRuSQVebRubrwpdeF5dYo_X8B1Zzxa-Q4OFredy3t7s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8936e8568e791d0f5724d5505f59f209da626bc0", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ZRuSQVebRubrwpdeF5dYo_X8B1Zzxa-Q4OFredy3t7s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74e4e9522d03e9302f188ec6d2eeff3e1bb9993e", "width": 1080, "height": 540}], "variants": {}, "id": "-Hc6UQ7nHvrI_2PX2xCAyvlgRvCKUvnwgnk9eTW7IEI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11mxpfx", "is_robot_indexable": true, "report_reasons": null, "author": "Haarolean", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mxpfx/a_06_release_of_ui_for_apache_kafka_w_cluster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mxpfx/a_06_release_of_ui_for_apache_kafka_w_cluster/", "subreddit_subscribers": 92547, "created_utc": 1678381958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Source**: Application databases hosted on AWS\n\n**Target**: S3 (data lake raw zone) plus any other sources that might come in future\n\n**AWS DMS**: Use CDC to dump most of the source data to S3 \n\n**S3 to Redshift** (nightly run): Using glue move only selected tables from S3 to redshift and then use stored procs to move data within different schemas.\n\n* Schema-1 (silver zone) : contains latest data similar to source database \n* Schema-2 (silver zone) : Apply data quality rules and track history (SCD-2) on most tables by comparing what is in schema-1 vs schema-2\n* Schema-3 (Gold Zone - DW): Apply business logic and create star schema (dimensions and fact tables)\n* Schema-4 (Gold Zone - DM): Create summarized tables and/or views for end users and bi apps\n\nUse Glue jobs and step functions to call stored procs. Glue jobs are used because lamda functions timeout after 15 min and some stored proc can run longer than 15 min.", "author_fullname": "t2_2gldxsu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your thoughts on this data warehouse architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ncza8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678417946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;: Application databases hosted on AWS&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Target&lt;/strong&gt;: S3 (data lake raw zone) plus any other sources that might come in future&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;AWS DMS&lt;/strong&gt;: Use CDC to dump most of the source data to S3 &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;S3 to Redshift&lt;/strong&gt; (nightly run): Using glue move only selected tables from S3 to redshift and then use stored procs to move data within different schemas.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Schema-1 (silver zone) : contains latest data similar to source database &lt;/li&gt;\n&lt;li&gt;Schema-2 (silver zone) : Apply data quality rules and track history (SCD-2) on most tables by comparing what is in schema-1 vs schema-2&lt;/li&gt;\n&lt;li&gt;Schema-3 (Gold Zone - DW): Apply business logic and create star schema (dimensions and fact tables)&lt;/li&gt;\n&lt;li&gt;Schema-4 (Gold Zone - DM): Create summarized tables and/or views for end users and bi apps&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Use Glue jobs and step functions to call stored procs. Glue jobs are used because lamda functions timeout after 15 min and some stored proc can run longer than 15 min.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11ncza8", "is_robot_indexable": true, "report_reasons": null, "author": "rocks4in", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ncza8/what_are_your_thoughts_on_this_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ncza8/what_are_your_thoughts_on_this_data_warehouse/", "subreddit_subscribers": 92547, "created_utc": 1678417946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9asbozcb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ChatGPT Is Now Finding Bugs in Databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11nbk7a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tI_DUgy-Ba5ecSEGYBB7sdUDGcYpcoVmBf3WUPRg7ZI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678414269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "starrocks.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://starrocks.medium.com/chatgpt-is-now-finding-bugs-in-databases-5d3b4f113da5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8r9ChZbqLIBrx_PcdFCWewuEEXT8M_HHeHu0W65aDMY.jpg?auto=webp&amp;v=enabled&amp;s=09ff5b4a713abafc8e8926a0ab4db095c5fcbcab", "width": 953, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/8r9ChZbqLIBrx_PcdFCWewuEEXT8M_HHeHu0W65aDMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c81dbfe3a089440bd142a3eb695287e8087726c6", "width": 108, "height": 145}, {"url": "https://external-preview.redd.it/8r9ChZbqLIBrx_PcdFCWewuEEXT8M_HHeHu0W65aDMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=788091db64bee9326c71e8f08f4ed4c2318c3338", "width": 216, "height": 290}, {"url": "https://external-preview.redd.it/8r9ChZbqLIBrx_PcdFCWewuEEXT8M_HHeHu0W65aDMY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=784db6c40e30cc28da92b5e69b54302e84580b31", "width": 320, "height": 429}, {"url": "https://external-preview.redd.it/8r9ChZbqLIBrx_PcdFCWewuEEXT8M_HHeHu0W65aDMY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5476d623560a3559e3a279d71e456b641449d38", "width": 640, "height": 859}], "variants": {}, "id": "JkesKKCc0p3YgENQ9kdHDn0D9nTpE3_wTMj1nG-WALY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11nbk7a", "is_robot_indexable": true, "report_reasons": null, "author": "Embarrassed-Cod-2756", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nbk7a/chatgpt_is_now_finding_bugs_in_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://starrocks.medium.com/chatgpt-is-now-finding-bugs-in-databases-5d3b4f113da5", "subreddit_subscribers": 92547, "created_utc": 1678414269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've never worked in the nonprofit space before but have an offer to be a DE at a nonprofit that I'm strongly considering taking, so I was wondering if anyone here has experience related to that. \n\nI mostly am curious about people's experiences (likes/dislikes, nuances and caveats, etc), tech stack, and any nonprofit specific things that I might want to keep in mind.", "author_fullname": "t2_f4y2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone here a DE at a nonprofit org?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n56yd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678399137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never worked in the nonprofit space before but have an offer to be a DE at a nonprofit that I&amp;#39;m strongly considering taking, so I was wondering if anyone here has experience related to that. &lt;/p&gt;\n\n&lt;p&gt;I mostly am curious about people&amp;#39;s experiences (likes/dislikes, nuances and caveats, etc), tech stack, and any nonprofit specific things that I might want to keep in mind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11n56yd", "is_robot_indexable": true, "report_reasons": null, "author": "SOG_clearbell", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11n56yd/anyone_here_a_de_at_a_nonprofit_org/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11n56yd/anyone_here_a_de_at_a_nonprofit_org/", "subreddit_subscribers": 92547, "created_utc": 1678399137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4ymkgdql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Build and deploy apps with DuckDB and Streamlit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_11nkdp4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QlqxSIiNhc9o57yCFMI1_JsInJ3t2UJvwnIg_vEDhRA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678440545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@octavianzarzu/build-and-deploy-apps-with-duckdb-and-streamlit-in-under-one-hour-852cd31cccce", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ftoaitwUxjx8z0oEL4RIR90NuNSPCYJ8Tb4YZo3gwok.jpg?auto=webp&amp;v=enabled&amp;s=ab504e3b22179692e6cbcb666d3e709d27ece6b5", "width": 1200, "height": 636}, "resolutions": [{"url": "https://external-preview.redd.it/ftoaitwUxjx8z0oEL4RIR90NuNSPCYJ8Tb4YZo3gwok.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7798108f936008ee81689f2dad76e46e5587c5ca", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/ftoaitwUxjx8z0oEL4RIR90NuNSPCYJ8Tb4YZo3gwok.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a90bdd937aca9627492c305079dc70744c690a67", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/ftoaitwUxjx8z0oEL4RIR90NuNSPCYJ8Tb4YZo3gwok.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a5013efc2de509fc7046528e105de7ab5a9a240", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/ftoaitwUxjx8z0oEL4RIR90NuNSPCYJ8Tb4YZo3gwok.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d83b22b837a900f06600ef04b51ece232db01df4", "width": 640, "height": 339}, {"url": "https://external-preview.redd.it/ftoaitwUxjx8z0oEL4RIR90NuNSPCYJ8Tb4YZo3gwok.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=562fdece3ea7aa7e2ce4779bf3b4496f70809297", "width": 960, "height": 508}, {"url": "https://external-preview.redd.it/ftoaitwUxjx8z0oEL4RIR90NuNSPCYJ8Tb4YZo3gwok.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7e3baf3135eb6dd61c0bacc842f77613a908f75", "width": 1080, "height": 572}], "variants": {}, "id": "S1zTOAlU0QO_pC_5XR0CLdwNSh1QZ4NdAhVh35Si71s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11nkdp4", "is_robot_indexable": true, "report_reasons": null, "author": "SnooBeans3890", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nkdp4/build_and_deploy_apps_with_duckdb_and_streamlit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@octavianzarzu/build-and-deploy-apps-with-duckdb-and-streamlit-in-under-one-hour-852cd31cccce", "subreddit_subscribers": 92547, "created_utc": 1678440545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nWe use a combination of Glue and Redshift for our ETL workloads. What I\u2019ve seen in Athena using Iceberg has had great performance and cost in some basic tests.\n\nWondering if anyone who is using Athena for production work loads has any wisdom to impart. I\u2019m also curious if the performance is consistent/reliable. \n\nThanks!", "author_fullname": "t2_g2snr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Athena production ETL workloads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nayyc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678412775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;We use a combination of Glue and Redshift for our ETL workloads. What I\u2019ve seen in Athena using Iceberg has had great performance and cost in some basic tests.&lt;/p&gt;\n\n&lt;p&gt;Wondering if anyone who is using Athena for production work loads has any wisdom to impart. I\u2019m also curious if the performance is consistent/reliable. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nayyc", "is_robot_indexable": true, "report_reasons": null, "author": "Zomgojira", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nayyc/aws_athena_production_etl_workloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nayyc/aws_athena_production_etl_workloads/", "subreddit_subscribers": 92547, "created_utc": 1678412775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have just joined a company as a Junior Data Engineer, I have 3 years industry experience in software dev, systems administration and DevSecOps but this is my first DE role. To begin my company want me to complete the Google Professional Data Engineer cert. I would be able to dedicate my full working week each week to this. \n\nHow doable do you think this is and what sort of time frame should I be looking at? TIA", "author_fullname": "t2_3fxv004y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Professional Data Engineer Certification without prior data engineering experience - how possible is it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nnj4x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678450433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have just joined a company as a Junior Data Engineer, I have 3 years industry experience in software dev, systems administration and DevSecOps but this is my first DE role. To begin my company want me to complete the Google Professional Data Engineer cert. I would be able to dedicate my full working week each week to this. &lt;/p&gt;\n\n&lt;p&gt;How doable do you think this is and what sort of time frame should I be looking at? TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nnj4x", "is_robot_indexable": true, "report_reasons": null, "author": "J1010H", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nnj4x/google_professional_data_engineer_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nnj4x/google_professional_data_engineer_certification/", "subreddit_subscribers": 92547, "created_utc": 1678450433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI need to run many steps etl, which joins and does complicated transformations on multiple datasets with 10-100B records and as fast as possible.\n\nI want to run it on single server because of cost and simplicity. I am currently using Postgres, but everybody is saying it is slow, and I also found some queries can't be run in parallel on multiple cores, so I am looking at other options.\n\nI tried clickhouse, but my understanding is that it expects all data to fit in the memory in many cases, specifically it can't join two very large tables, and also I encountered some other corner-cases, like for example doing rank() over partitions, it just dies with OOM.\n\nI have duckdb on my list to try, but maybe anyone can recommend any other tools? It should be stable, reliable, open-source and support advanced SQL.\n\nThank you!", "author_fullname": "t2_svw2lyu4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Engine recommendation for heavy and fast etl on single server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n5jky", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678399887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I need to run many steps etl, which joins and does complicated transformations on multiple datasets with 10-100B records and as fast as possible.&lt;/p&gt;\n\n&lt;p&gt;I want to run it on single server because of cost and simplicity. I am currently using Postgres, but everybody is saying it is slow, and I also found some queries can&amp;#39;t be run in parallel on multiple cores, so I am looking at other options.&lt;/p&gt;\n\n&lt;p&gt;I tried clickhouse, but my understanding is that it expects all data to fit in the memory in many cases, specifically it can&amp;#39;t join two very large tables, and also I encountered some other corner-cases, like for example doing rank() over partitions, it just dies with OOM.&lt;/p&gt;\n\n&lt;p&gt;I have duckdb on my list to try, but maybe anyone can recommend any other tools? It should be stable, reliable, open-source and support advanced SQL.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11n5jky", "is_robot_indexable": true, "report_reasons": null, "author": "FirstOrderCat", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11n5jky/engine_recommendation_for_heavy_and_fast_etl_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11n5jky/engine_recommendation_for_heavy_and_fast_etl_on/", "subreddit_subscribers": 92547, "created_utc": 1678399887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, we are trying to cut cost in our company. We are trying to reduce the number of nodes in our cluster.\n\nWe have decided on only keeping the recent data that would be 6 months and deleting all the records before that.\n\n  \nI want to develop an efficient solution or architecture to implement this feature. I am thinking of designing a script using python.\n\n&amp;#x200B;\n\nI have thought of two solutions :\n\n* Getting a data range and create a date list and delete data on day by day basis and at the end running a vaccum and analyze.\n* Moving all the required records to a new table and dropping the table.\n\n&amp;#x200B;\n\nOther Noes:\n\n* Table size is around 40gb and 40M records.\n* Daily elt jobs are running which sync the tables, so putting a halt on the etl jobs for the specific table would be a good idea or the delete command won't hinder the upsert on the table.", "author_fullname": "t2_l38csc3b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deleting data efficiently from Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nn64i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678449444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, we are trying to cut cost in our company. We are trying to reduce the number of nodes in our cluster.&lt;/p&gt;\n\n&lt;p&gt;We have decided on only keeping the recent data that would be 6 months and deleting all the records before that.&lt;/p&gt;\n\n&lt;p&gt;I want to develop an efficient solution or architecture to implement this feature. I am thinking of designing a script using python.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have thought of two solutions :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Getting a data range and create a date list and delete data on day by day basis and at the end running a vaccum and analyze.&lt;/li&gt;\n&lt;li&gt;Moving all the required records to a new table and dropping the table.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Other Noes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Table size is around 40gb and 40M records.&lt;/li&gt;\n&lt;li&gt;Daily elt jobs are running which sync the tables, so putting a halt on the etl jobs for the specific table would be a good idea or the delete command won&amp;#39;t hinder the upsert on the table.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nn64i", "is_robot_indexable": true, "report_reasons": null, "author": "AdSure744", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nn64i/deleting_data_efficiently_from_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nn64i/deleting_data_efficiently_from_redshift/", "subreddit_subscribers": 92547, "created_utc": 1678449444.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I've started a new GCP project for a part-time client a few months ago and I had to build a lot of code to extract order information from Amazon.\n\nThey were in a bit of a \"red alert\" because their previous agency had left after doing a very bad job and their business was growing and they needed something to take control of their data and get better insights on how the business was going. When I started I was pretty much left with a GCP project containing a VM and a BigQuery data warehouse that was a mess.\n\nThe VM had a NiFi instance installed where the ETL was done but it kept crashing all the time, while the BigQuery data warehouse contained numerous datasets and tables like \"sales\\_1\", \"sales\\_2\", and \"orders\\_view1\" (which was actually a table not a view), \"orders-view2\" etc.\n\nThe problem was that their API tokens were expiring and they were hitting throttling limits for their endpoints (each endpoint has a different limit), so I ended up doing most of the Python coding work to get all the data that they needed and save it to BigQuery.\n\nBecause at some point they want to do ML and predictions using their data (but they don't have any idea, yet), I created a Cloud Storage bucket where I save all of the response data I get from doing the API requests, following the \"data lake\" principles, so that later we can process them with Spark or Databricks or something similar.\n\nHowever, apart from building new functionality I had to fix a lot of the old code that the previous agency left behind, views and scheduled query jobs that were failing, because most of them were with SELECT \\* and when we added new columns with data to certain tables, those jobs failed, etc.\n\nAnyway, I've ended up building an architecture which I feel is a bit of a Frankenstein, and I am looking for some help in finding, maybe, better services to run them on and a combination that will easily scale.\n\n[Bad architecture](https://preview.redd.it/mivsz45d6wma1.jpg?width=734&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=695d8d484046d7b40eeab24c416db1e998a9132a)\n\nI will explain below the purpose of each service:\n\n1. VM (a compute Engine VM where all the Python code that reads data from the API is stored). This won't really scale since we're not allowed to call the same endpoint multiple times from different services and we only download the data once every 24h. I plan to go down to 1h increments, but the \"problem\" is that the business just can't react as fast to hour-level changing data. And this is orders and sales information and they don't havea a sharp drop-off, so daily is fine for now. *(I would love to look into streaming one day, but idk if it's necessary or what the advantages for the business team would be, since they don't care &lt;right now&gt; about new data coming in much faster).*\n2. Firestore - most of the API endpoints have a parameter called \"last\\_updated\\_on\", which I use to only take in new data from when the previous API call was made. The \"previous\" and \"current\" timestamps for when I called the endpoints are stored in Firestore. Each endpoints has its own collection and a \"last\\_executed\" field in a document with metadata, from where I get this information and use it in my API call. Then, once the call is successfully completed, I overwrite the \"last\\_executed\" date with the timestamp of when I started my latest call. (**1a in my diagram**)\n3. PubSub - some API endpoints depend on data returned by previous endpoints. For example: ***Orders*** and ***OrderDetails*** are 2 different endpoints. ***Orders*** takes the \"last\\_updated\\_on\" parameter, whereas **OrderDetails** takes an OrderID, which I get from the response of ***Orders*** (I actually get all orders that were updated in the last 24 hours). I save all these OrderID's into PubSub, and call ***OrderDetails*** again, with each OrderID. (**1b in my diagram**)\n4. Cloud Storage - I save all of the JSON response files to Cloud Storage to also have the \"raw\" data I get from the API and so that I can also reference back to the raw data, if I find any strange things in the BigQuery data warehouse (sometimes it happens).\n5. BigQuery - where all the API data from the Amazon Service and other 3rd party Services are saved.\n\nEach of these API calls and ETL processing jobs are being triggered by Cron jobs inside the VM, fact which I find quite annoying. I was thinking of implementing Prefect, because even a Small Cloud Composer instance would 4x our spending bill, and there's not a lot of benefit on top.\n\nAll the data download and ETL processing is done inside the VM, which runs 24/7. I don't know if moving to Cloud Functions would necessarily be better (sadly never built one before, and I've extremely stretched with time in my day-to-day), and also I don't know how to build an architecture that would scale, tbh.\n\nI'm looking for any kind of advice that would help me figure out a better architecture, because I don't want the code debt to pile up too much along the way. I know the current architecture is quite \"cringy\", but I'm not sure which way to go next.\n\nImprovements I was thinking about, but don't know if it's a good idea:\n\n\\- I was thinking of Google Workflows to trigger my jobs, but I don't know what are the limitations of those\n\n\\- Move some of the code to Cloud Run, although the current implementation works and the costs / month aren't that high, since we're running a very small custom instance and saving an extra $15-$20 on a $30 per month bill is not worth the development effort, for now.\n\n\\- Keep the API download in the VM and move the ETL part in Databricks, since I know I can schedule, monitor, debug and make code changes to the ETL much faster there, if necessary. Also, job scheduling in Databricks is readily available so I don't have to do much there to make sure this runs fine.\n\n\\- Get rid of Firestore, because I feel I'm using it for the wrong reasons, but idk what other service I can use. Maybe Redis?\n\n\\- Change PubSub with something else, since Idk if the pattern I'm using is a \"publisher-subscriber\" pattern. I feel like it kind of is, but I'm not a very good software engineer, so I'm not sure.\n\nAnyway, any roast, feedback, idea or guidance is very much appreciated. Sadly I have nobody around me who to ask for feedback on this, since I am the only dev.", "author_fullname": "t2_v4yctwkv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please help me improve my architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mivsz45d6wma1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 84, "x": 108, "u": "https://preview.redd.it/mivsz45d6wma1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7f2f62ae135897d3c5456b51643e3926e6606cd"}, {"y": 168, "x": 216, "u": "https://preview.redd.it/mivsz45d6wma1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=873be1041ed552c25c96befc2fb4369e16c1b55a"}, {"y": 249, "x": 320, "u": "https://preview.redd.it/mivsz45d6wma1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c6712e3540d31df6067c79edb84934b147e0e32"}, {"y": 499, "x": 640, "u": "https://preview.redd.it/mivsz45d6wma1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2856e2125b5c8eb9ab007eab0646a35e27a0c5e7"}], "s": {"y": 573, "x": 734, "u": "https://preview.redd.it/mivsz45d6wma1.jpg?width=734&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=695d8d484046d7b40eeab24c416db1e998a9132a"}, "id": "mivsz45d6wma1"}}, "name": "t3_11nlsw1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jc7lfBbC__JCpVtUTVaXmVj4SqKQ515JvKh3XEaUwFU.jpg", "edited": 1678445742.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678445279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;ve started a new GCP project for a part-time client a few months ago and I had to build a lot of code to extract order information from Amazon.&lt;/p&gt;\n\n&lt;p&gt;They were in a bit of a &amp;quot;red alert&amp;quot; because their previous agency had left after doing a very bad job and their business was growing and they needed something to take control of their data and get better insights on how the business was going. When I started I was pretty much left with a GCP project containing a VM and a BigQuery data warehouse that was a mess.&lt;/p&gt;\n\n&lt;p&gt;The VM had a NiFi instance installed where the ETL was done but it kept crashing all the time, while the BigQuery data warehouse contained numerous datasets and tables like &amp;quot;sales_1&amp;quot;, &amp;quot;sales_2&amp;quot;, and &amp;quot;orders_view1&amp;quot; (which was actually a table not a view), &amp;quot;orders-view2&amp;quot; etc.&lt;/p&gt;\n\n&lt;p&gt;The problem was that their API tokens were expiring and they were hitting throttling limits for their endpoints (each endpoint has a different limit), so I ended up doing most of the Python coding work to get all the data that they needed and save it to BigQuery.&lt;/p&gt;\n\n&lt;p&gt;Because at some point they want to do ML and predictions using their data (but they don&amp;#39;t have any idea, yet), I created a Cloud Storage bucket where I save all of the response data I get from doing the API requests, following the &amp;quot;data lake&amp;quot; principles, so that later we can process them with Spark or Databricks or something similar.&lt;/p&gt;\n\n&lt;p&gt;However, apart from building new functionality I had to fix a lot of the old code that the previous agency left behind, views and scheduled query jobs that were failing, because most of them were with SELECT * and when we added new columns with data to certain tables, those jobs failed, etc.&lt;/p&gt;\n\n&lt;p&gt;Anyway, I&amp;#39;ve ended up building an architecture which I feel is a bit of a Frankenstein, and I am looking for some help in finding, maybe, better services to run them on and a combination that will easily scale.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mivsz45d6wma1.jpg?width=734&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=695d8d484046d7b40eeab24c416db1e998a9132a\"&gt;Bad architecture&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I will explain below the purpose of each service:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;VM (a compute Engine VM where all the Python code that reads data from the API is stored). This won&amp;#39;t really scale since we&amp;#39;re not allowed to call the same endpoint multiple times from different services and we only download the data once every 24h. I plan to go down to 1h increments, but the &amp;quot;problem&amp;quot; is that the business just can&amp;#39;t react as fast to hour-level changing data. And this is orders and sales information and they don&amp;#39;t havea a sharp drop-off, so daily is fine for now. &lt;em&gt;(I would love to look into streaming one day, but idk if it&amp;#39;s necessary or what the advantages for the business team would be, since they don&amp;#39;t care &amp;lt;right now&amp;gt; about new data coming in much faster).&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Firestore - most of the API endpoints have a parameter called &amp;quot;last_updated_on&amp;quot;, which I use to only take in new data from when the previous API call was made. The &amp;quot;previous&amp;quot; and &amp;quot;current&amp;quot; timestamps for when I called the endpoints are stored in Firestore. Each endpoints has its own collection and a &amp;quot;last_executed&amp;quot; field in a document with metadata, from where I get this information and use it in my API call. Then, once the call is successfully completed, I overwrite the &amp;quot;last_executed&amp;quot; date with the timestamp of when I started my latest call. (&lt;strong&gt;1a in my diagram&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;PubSub - some API endpoints depend on data returned by previous endpoints. For example: &lt;strong&gt;&lt;em&gt;Orders&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;OrderDetails&lt;/em&gt;&lt;/strong&gt; are 2 different endpoints. &lt;strong&gt;&lt;em&gt;Orders&lt;/em&gt;&lt;/strong&gt; takes the &amp;quot;last_updated_on&amp;quot; parameter, whereas &lt;strong&gt;OrderDetails&lt;/strong&gt; takes an OrderID, which I get from the response of &lt;strong&gt;&lt;em&gt;Orders&lt;/em&gt;&lt;/strong&gt; (I actually get all orders that were updated in the last 24 hours). I save all these OrderID&amp;#39;s into PubSub, and call &lt;strong&gt;&lt;em&gt;OrderDetails&lt;/em&gt;&lt;/strong&gt; again, with each OrderID. (&lt;strong&gt;1b in my diagram&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;Cloud Storage - I save all of the JSON response files to Cloud Storage to also have the &amp;quot;raw&amp;quot; data I get from the API and so that I can also reference back to the raw data, if I find any strange things in the BigQuery data warehouse (sometimes it happens).&lt;/li&gt;\n&lt;li&gt;BigQuery - where all the API data from the Amazon Service and other 3rd party Services are saved.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Each of these API calls and ETL processing jobs are being triggered by Cron jobs inside the VM, fact which I find quite annoying. I was thinking of implementing Prefect, because even a Small Cloud Composer instance would 4x our spending bill, and there&amp;#39;s not a lot of benefit on top.&lt;/p&gt;\n\n&lt;p&gt;All the data download and ETL processing is done inside the VM, which runs 24/7. I don&amp;#39;t know if moving to Cloud Functions would necessarily be better (sadly never built one before, and I&amp;#39;ve extremely stretched with time in my day-to-day), and also I don&amp;#39;t know how to build an architecture that would scale, tbh.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for any kind of advice that would help me figure out a better architecture, because I don&amp;#39;t want the code debt to pile up too much along the way. I know the current architecture is quite &amp;quot;cringy&amp;quot;, but I&amp;#39;m not sure which way to go next.&lt;/p&gt;\n\n&lt;p&gt;Improvements I was thinking about, but don&amp;#39;t know if it&amp;#39;s a good idea:&lt;/p&gt;\n\n&lt;p&gt;- I was thinking of Google Workflows to trigger my jobs, but I don&amp;#39;t know what are the limitations of those&lt;/p&gt;\n\n&lt;p&gt;- Move some of the code to Cloud Run, although the current implementation works and the costs / month aren&amp;#39;t that high, since we&amp;#39;re running a very small custom instance and saving an extra $15-$20 on a $30 per month bill is not worth the development effort, for now.&lt;/p&gt;\n\n&lt;p&gt;- Keep the API download in the VM and move the ETL part in Databricks, since I know I can schedule, monitor, debug and make code changes to the ETL much faster there, if necessary. Also, job scheduling in Databricks is readily available so I don&amp;#39;t have to do much there to make sure this runs fine.&lt;/p&gt;\n\n&lt;p&gt;- Get rid of Firestore, because I feel I&amp;#39;m using it for the wrong reasons, but idk what other service I can use. Maybe Redis?&lt;/p&gt;\n\n&lt;p&gt;- Change PubSub with something else, since Idk if the pattern I&amp;#39;m using is a &amp;quot;publisher-subscriber&amp;quot; pattern. I feel like it kind of is, but I&amp;#39;m not a very good software engineer, so I&amp;#39;m not sure.&lt;/p&gt;\n\n&lt;p&gt;Anyway, any roast, feedback, idea or guidance is very much appreciated. Sadly I have nobody around me who to ask for feedback on this, since I am the only dev.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nlsw1", "is_robot_indexable": true, "report_reasons": null, "author": "jack-in-the-sack", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nlsw1/please_help_me_improve_my_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nlsw1/please_help_me_improve_my_architecture/", "subreddit_subscribers": 92547, "created_utc": 1678445279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2fea44un", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Save $ on public S3 buckets using VPC endpoints via SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n5hb6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1678399760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "iasql.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://iasql.com/blog/save-s3-vpc/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11n5hb6", "is_robot_indexable": true, "report_reasons": null, "author": "g0_g6t_1t", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11n5hb6/save_on_public_s3_buckets_using_vpc_endpoints_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://iasql.com/blog/save-s3-vpc/", "subreddit_subscribers": 92547, "created_utc": 1678399760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys\nMy current data pipeline is a simple one using S3 and AWS Glue/ python\nIm ingesting like 10 files daily for ETL,  only a small percentage of each file actually gives new data that needs to be transformed while the rest is old data that can be ignored. (Unfortunately, I cant do anything to these files from the source side)\n\nWhat Im doing is to basically build up a dynamodb table of all the recorded that have ever been processed but this is not ideal in the long runand I hope to find a better solution.\n\nGlue Bookmark feature  doesnt seem to be able to actually track new partition records (?) so Im wondering if there is anything I should look either with my current stack or even a to replace it with something else as a better solution.?\nThank you", "author_fullname": "t2_cx3t0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data pipeline stack and incremental change tracking?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n00ip", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678387289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys\nMy current data pipeline is a simple one using S3 and AWS Glue/ python\nIm ingesting like 10 files daily for ETL,  only a small percentage of each file actually gives new data that needs to be transformed while the rest is old data that can be ignored. (Unfortunately, I cant do anything to these files from the source side)&lt;/p&gt;\n\n&lt;p&gt;What Im doing is to basically build up a dynamodb table of all the recorded that have ever been processed but this is not ideal in the long runand I hope to find a better solution.&lt;/p&gt;\n\n&lt;p&gt;Glue Bookmark feature  doesnt seem to be able to actually track new partition records (?) so Im wondering if there is anything I should look either with my current stack or even a to replace it with something else as a better solution.?\nThank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11n00ip", "is_robot_indexable": true, "report_reasons": null, "author": "duyth", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11n00ip/data_pipeline_stack_and_incremental_change/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11n00ip/data_pipeline_stack_and_incremental_change/", "subreddit_subscribers": 92547, "created_utc": 1678387289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_mnzkoz4n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Deploy a Prefect Agent with Plural", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_11mzvny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zc4I7IVZmgMm8Mg7hNlskWQuStKy_LlQlyQAilSOakU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678386967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "plural.sh", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.plural.sh/blog/how-to-deploy-a-prefect-agent-with-plural/?utm_medium=social&amp;utm_source=Reddit&amp;utm_campaign=prefect", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6fviuLh0GFn7opChaP1OGktI-9IozipU6DF3hPrCsFk.jpg?auto=webp&amp;v=enabled&amp;s=b3a25fe61d85d605d6eaadd705b334c85a355681", "width": 2200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/6fviuLh0GFn7opChaP1OGktI-9IozipU6DF3hPrCsFk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3ca560cde3b258619c24fd6957cc14bf5a4ef04", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/6fviuLh0GFn7opChaP1OGktI-9IozipU6DF3hPrCsFk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36084796a523d4dc45768e67050322c282bee93c", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/6fviuLh0GFn7opChaP1OGktI-9IozipU6DF3hPrCsFk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=185eb5ba1bd2e20ba3ccbbda0ed06aa2753da40d", "width": 320, "height": 174}, {"url": "https://external-preview.redd.it/6fviuLh0GFn7opChaP1OGktI-9IozipU6DF3hPrCsFk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5c91fb280ff5f7346ea020df77a818f176b5cce", "width": 640, "height": 349}, {"url": "https://external-preview.redd.it/6fviuLh0GFn7opChaP1OGktI-9IozipU6DF3hPrCsFk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e38549030fe512c918a2183c5e71845b3d45ce7a", "width": 960, "height": 523}, {"url": "https://external-preview.redd.it/6fviuLh0GFn7opChaP1OGktI-9IozipU6DF3hPrCsFk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed3e0f83baf3f0a98574236d938187dad867b317", "width": 1080, "height": 589}], "variants": {}, "id": "m-b1uQLr37zASH4qBSY1AGAvPrtxE3hYrPSvQSNyOn4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11mzvny", "is_robot_indexable": true, "report_reasons": null, "author": "techdatanerd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mzvny/how_to_deploy_a_prefect_agent_with_plural/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.plural.sh/blog/how-to-deploy-a-prefect-agent-with-plural/?utm_medium=social&amp;utm_source=Reddit&amp;utm_campaign=prefect", "subreddit_subscribers": 92547, "created_utc": 1678386967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'd argue the main difference is their goals and, more specifically, the targets they serve:\n\n* data analysts' insights support humans to make decisions\n* data scientists' iterations improve algorithms to take decisions\n\nAnd supporting both of these roles as a data engineer is very similar:\n\n* facilitate the exploration of data \n* \"productionize\" the models they've designed (feeding viz or other tools (via rETL) or ML)\n* improve their workflows (collaborative tooling, CI/CD, ...)\n\nOnly the actual tools used change, mainly because they're still being seen as very separate verticals in the data tooling landscape.\n\n&amp;#x200B;\n\nWhat do you think??", "author_fullname": "t2_hizqfv2o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The difference between data scientists &amp; analysts and what it changes for data engineers...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nmrai", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678448246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d argue the main difference is their goals and, more specifically, the targets they serve:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;data analysts&amp;#39; insights support humans to make decisions&lt;/li&gt;\n&lt;li&gt;data scientists&amp;#39; iterations improve algorithms to take decisions&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;And supporting both of these roles as a data engineer is very similar:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;facilitate the exploration of data &lt;/li&gt;\n&lt;li&gt;&amp;quot;productionize&amp;quot; the models they&amp;#39;ve designed (feeding viz or other tools (via rETL) or ML)&lt;/li&gt;\n&lt;li&gt;improve their workflows (collaborative tooling, CI/CD, ...)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Only the actual tools used change, mainly because they&amp;#39;re still being seen as very separate verticals in the data tooling landscape.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What do you think??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11nmrai", "is_robot_indexable": true, "report_reasons": null, "author": "briceluu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nmrai/the_difference_between_data_scientists_analysts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nmrai/the_difference_between_data_scientists_analysts/", "subreddit_subscribers": 92547, "created_utc": 1678448246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to get a sense of what the market is looking to like.\n\nI got 3% bonus and 3% bonus, imo, it's not bad not good, so okay !!", "author_fullname": "t2_vtx6qjs2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much raise and bonus are you getting this year ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11npctm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678455239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to get a sense of what the market is looking to like.&lt;/p&gt;\n\n&lt;p&gt;I got 3% bonus and 3% bonus, imo, it&amp;#39;s not bad not good, so okay !!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11npctm", "is_robot_indexable": true, "report_reasons": null, "author": "Budget_Assignment457", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11npctm/how_much_raise_and_bonus_are_you_getting_this_year/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11npctm/how_much_raise_and_bonus_are_you_getting_this_year/", "subreddit_subscribers": 92547, "created_utc": 1678455239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Having difficulties connecting to a mysql db hosted on an azure virtual machine. Any tips will help!", "author_fullname": "t2_45uul41f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone use Matillion? How would I connect to a MySQL database hosted on an Azure Virtual Machine?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n454f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678396749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Having difficulties connecting to a mysql db hosted on an azure virtual machine. Any tips will help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11n454f", "is_robot_indexable": true, "report_reasons": null, "author": "obaid_alandavid", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11n454f/anyone_use_matillion_how_would_i_connect_to_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11n454f/anyone_use_matillion_how_would_i_connect_to_a/", "subreddit_subscribers": 92547, "created_utc": 1678396749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anyone give me the links to some good practice exams for the Microsoft dp-203 exam? I took an online course on Udemy already but want too happy with the practice exams that came with the course.", "author_fullname": "t2_bz1aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dp-203 practice exams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n22dp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678392020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone give me the links to some good practice exams for the Microsoft dp-203 exam? I took an online course on Udemy already but want too happy with the practice exams that came with the course.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11n22dp", "is_robot_indexable": true, "report_reasons": null, "author": "Im_probably_naked", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11n22dp/dp203_practice_exams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11n22dp/dp203_practice_exams/", "subreddit_subscribers": 92547, "created_utc": 1678392020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used flyway before? I don't think I fully understand its use cases. My understanding is if I wanted to deploy tables and views and track schema changes, I could just use dbt. Does flyway offer any additional functionality?", "author_fullname": "t2_vsf3ige5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt vs flyway", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11n0dz4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678388151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used flyway before? I don&amp;#39;t think I fully understand its use cases. My understanding is if I wanted to deploy tables and views and track schema changes, I could just use dbt. Does flyway offer any additional functionality?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11n0dz4", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated_Ad_1108", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11n0dz4/dbt_vs_flyway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11n0dz4/dbt_vs_flyway/", "subreddit_subscribers": 92547, "created_utc": 1678388151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can dbt be developed into a GUI tool in future?", "author_fullname": "t2_khph1234", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can dbt be a GUI tool in future?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mvy1i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678377804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can dbt be developed into a GUI tool in future?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11mvy1i", "is_robot_indexable": true, "report_reasons": null, "author": "misc0007", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mvy1i/can_dbt_be_a_gui_tool_in_future/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mvy1i/can_dbt_be_a_gui_tool_in_future/", "subreddit_subscribers": 92547, "created_utc": 1678377804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve created a free, open source tool for data analysts / engineers / scientists \u2013 the [Data Scratchpad](https://www.datascratchpad.com/). It\u2019s a lightweight desktop application that helps inspect, profile and analyse data sources. It allows visual inspection of bytes, code points and parsed data, and handles a range of text encodings with endianness.\n\nIt uses a simple scripting language (and comes with documentation!) to quickly and easily interrogate data files and streams. Here\u2019s a [demo video](https://youtu.be/wRr8s9Sr0dk).\n\nIf you work with data sources, you need a detailed understanding of the content and this tool can help. It also comes with a set of efficient statistical analysis commands and helps you sketch analysis ideas before doing thorough insight work.\n\nDownload and have a play with it - [https://www.datascratchpad.com/](https://www.datascratchpad.com/)\n\nPlease let me know any feedback, and share it with your fellow data professionals :)\n\nThanks!", "author_fullname": "t2_6r8adbwwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inspecting, profiling and analysing data sources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11mv8qn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678376121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve created a free, open source tool for data analysts / engineers / scientists \u2013 the &lt;a href=\"https://www.datascratchpad.com/\"&gt;Data Scratchpad&lt;/a&gt;. It\u2019s a lightweight desktop application that helps inspect, profile and analyse data sources. It allows visual inspection of bytes, code points and parsed data, and handles a range of text encodings with endianness.&lt;/p&gt;\n\n&lt;p&gt;It uses a simple scripting language (and comes with documentation!) to quickly and easily interrogate data files and streams. Here\u2019s a &lt;a href=\"https://youtu.be/wRr8s9Sr0dk\"&gt;demo video&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;If you work with data sources, you need a detailed understanding of the content and this tool can help. It also comes with a set of efficient statistical analysis commands and helps you sketch analysis ideas before doing thorough insight work.&lt;/p&gt;\n\n&lt;p&gt;Download and have a play with it - &lt;a href=\"https://www.datascratchpad.com/\"&gt;https://www.datascratchpad.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please let me know any feedback, and share it with your fellow data professionals :)&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11mv8qn", "is_robot_indexable": true, "report_reasons": null, "author": "DataScratchpad", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mv8qn/inspecting_profiling_and_analysing_data_sources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11mv8qn/inspecting_profiling_and_analysing_data_sources/", "subreddit_subscribers": 92547, "created_utc": 1678376121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_tyl6qdc4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Basic terminology and practices related to graph databases and graph modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_11mtvay", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0NyBuTvjPeHdF2zlTcWj35dnzBArL7lxVTmudJsnhgc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678372775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memgraph.com/docs/memgraph/tutorials/graph-modeling", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vci22rOGVsZwM2Ocn4TunsfGbmDVBiBNVipfps98DdE.jpg?auto=webp&amp;v=enabled&amp;s=0c12c7cf7697648578ce443559891c478718afb3", "width": 1271, "height": 753}, "resolutions": [{"url": "https://external-preview.redd.it/vci22rOGVsZwM2Ocn4TunsfGbmDVBiBNVipfps98DdE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce9b74a24b9079589434bfd4611f2019117ba329", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/vci22rOGVsZwM2Ocn4TunsfGbmDVBiBNVipfps98DdE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b612795dd9da70f294873f910ce191b1ac1eff2", "width": 216, "height": 127}, {"url": "https://external-preview.redd.it/vci22rOGVsZwM2Ocn4TunsfGbmDVBiBNVipfps98DdE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b3b0aae3eb8cc9da91228e565f4e6a05f778826", "width": 320, "height": 189}, {"url": "https://external-preview.redd.it/vci22rOGVsZwM2Ocn4TunsfGbmDVBiBNVipfps98DdE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9578636a8b892429b11c8c938a3a32b05c6485cc", "width": 640, "height": 379}, {"url": "https://external-preview.redd.it/vci22rOGVsZwM2Ocn4TunsfGbmDVBiBNVipfps98DdE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=624c663a4b6acf911b63a21759c9263ccc97e636", "width": 960, "height": 568}, {"url": "https://external-preview.redd.it/vci22rOGVsZwM2Ocn4TunsfGbmDVBiBNVipfps98DdE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90d0e20fe36c4a3d4f0566a7d0adecbae5a7624e", "width": 1080, "height": 639}], "variants": {}, "id": "TnQxzN00bkfwVrypCHCgQfvGiU84UpN0z0SoGxlTEEE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11mtvay", "is_robot_indexable": true, "report_reasons": null, "author": "Zealousideal_Plan591", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11mtvay/basic_terminology_and_practices_related_to_graph/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memgraph.com/docs/memgraph/tutorials/graph-modeling", "subreddit_subscribers": 92547, "created_utc": 1678372775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_53k5q2o1s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Join the Intellus Data Race an win Joker travel vouchers!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11no2hp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1678451884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datarace.intellus.group", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datarace.intellus.group/?utm_source=Reddit_organisch&amp;utm_medium=video_destinations&amp;utm_campaign=DataRace&amp;utm_id=DataRace", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11no2hp", "is_robot_indexable": true, "report_reasons": null, "author": "intellusgroup", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11no2hp/join_the_intellus_data_race_an_win_joker_travel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datarace.intellus.group/?utm_source=Reddit_organisch&amp;utm_medium=video_destinations&amp;utm_campaign=DataRace&amp;utm_id=DataRace", "subreddit_subscribers": 92547, "created_utc": 1678451884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody just got access to our new redshift data warehouse using Tableau and other tools. I am the only one with access and want to know what my next steps would be. \n\nFor more context the ware house is synced to our financial niche CRM daily and grabs data from there. We are having a hard time building a proper flow. We think that the structure of our niche CRM was copy pasted due to the way the connections are set up when I look at a diagram of the tables. Need help knowing best way to reorganize their mess visually so I can start to understand their keys and connections. \n\nAny advice is great.", "author_fullname": "t2_cdom6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just Got Access to RedShift Data in Tableau and using DBeaver - Next Steps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11nahid", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678411541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody just got access to our new redshift data warehouse using Tableau and other tools. I am the only one with access and want to know what my next steps would be. &lt;/p&gt;\n\n&lt;p&gt;For more context the ware house is synced to our financial niche CRM daily and grabs data from there. We are having a hard time building a proper flow. We think that the structure of our niche CRM was copy pasted due to the way the connections are set up when I look at a diagram of the tables. Need help knowing best way to reorganize their mess visually so I can start to understand their keys and connections. &lt;/p&gt;\n\n&lt;p&gt;Any advice is great.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11nahid", "is_robot_indexable": true, "report_reasons": null, "author": "FXAFrank", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11nahid/just_got_access_to_redshift_data_in_tableau_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11nahid/just_got_access_to_redshift_data_in_tableau_and/", "subreddit_subscribers": 92547, "created_utc": 1678411541.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}