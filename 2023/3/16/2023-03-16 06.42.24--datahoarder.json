{"kind": "Listing", "data": {"after": "t3_11s0z7l", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_e8a0y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The EU\u2019s new \"Data Act\" will let the user of a tech products (like wearables) access all the data it generates. Imagine all the personal data we will have access to!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_11rqpak", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 1930, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 1930, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Nr3h4sW5bBJlWUQmL8gIvspqbGLVdxv5ezUIHgoeB4k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678870346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "en.wikipedia.org", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://en.wikipedia.org/wiki/Data_Act_(European_Union)", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SPPIEcA2742_ySVEqHxfmwLfDH3ipVnOOaL90EKngSQ.jpg?auto=webp&amp;v=enabled&amp;s=e88e6f60f22d0cab0f3db8145b840d8ad5feeaab", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/SPPIEcA2742_ySVEqHxfmwLfDH3ipVnOOaL90EKngSQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efa1893c06db61cbd7f67aba61727784e8d86a03", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/SPPIEcA2742_ySVEqHxfmwLfDH3ipVnOOaL90EKngSQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fe6312eaf130c49c30837a4a1866c4808cc11a8", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/SPPIEcA2742_ySVEqHxfmwLfDH3ipVnOOaL90EKngSQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2261f14b9625c6a55aaa0202b2f6c3c7bb0d39fc", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/SPPIEcA2742_ySVEqHxfmwLfDH3ipVnOOaL90EKngSQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02594d8509a0336d1cb1252f89f4a692745c1790", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/SPPIEcA2742_ySVEqHxfmwLfDH3ipVnOOaL90EKngSQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae990ec7207465cd55b4c7e9107ce9556725547b", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/SPPIEcA2742_ySVEqHxfmwLfDH3ipVnOOaL90EKngSQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35e67657270f24fa7fe380b70e27736d4828df36", "width": 1080, "height": 720}], "variants": {}, "id": "7AiId35ZsoVusfgBuQuEI8kyBuFJH4kdDxA_vl4KdtM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11rqpak", "is_robot_indexable": true, "report_reasons": null, "author": "anonboxis", "discussion_type": null, "num_comments": 130, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11rqpak/the_eus_new_data_act_will_let_the_user_of_a_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://en.wikipedia.org/wiki/Data_Act_(European_Union)", "subreddit_subscribers": 673386, "created_utc": 1678870346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had my suspicions Scott Adams would get himself cancelled eventually, so last November I put together a quick Python script to archive all the comics from [dilbert.com](https://dilbert.com)Found out this week it had gone offline, so using the Wayback machine I was able to fill in the last few missing months.12,384 comics spanning nearly 34 years.  \n\n\nFinally up - [https://pastebin.com/YayLHMMZ](https://pastebin.com/YayLHMMZ)", "author_fullname": "t2_zgva5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Complete Dilbert Comic Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11rtliq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678905177.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678879661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had my suspicions Scott Adams would get himself cancelled eventually, so last November I put together a quick Python script to archive all the comics from &lt;a href=\"https://dilbert.com\"&gt;dilbert.com&lt;/a&gt;Found out this week it had gone offline, so using the Wayback machine I was able to fill in the last few missing months.12,384 comics spanning nearly 34 years.  &lt;/p&gt;\n\n&lt;p&gt;Finally up - &lt;a href=\"https://pastebin.com/YayLHMMZ\"&gt;https://pastebin.com/YayLHMMZ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m5p8e0xltmK8uybL8IDXUZDnE2QzcBCBVtQCjjNBe0U.jpg?auto=webp&amp;v=enabled&amp;s=45d3181b6f29232b9ddc87592c70bd91663d1755", "width": 375, "height": 375}, "resolutions": [{"url": "https://external-preview.redd.it/m5p8e0xltmK8uybL8IDXUZDnE2QzcBCBVtQCjjNBe0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f522f68d51475df75adee26cc00d933f096959f8", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/m5p8e0xltmK8uybL8IDXUZDnE2QzcBCBVtQCjjNBe0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc82ee634f23339a3450fa62efa2397ecb8b8629", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/m5p8e0xltmK8uybL8IDXUZDnE2QzcBCBVtQCjjNBe0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68f7631e352e6d199ffeb26553c7aef6610d29f8", "width": 320, "height": 320}], "variants": {}, "id": "jd4H4NBVFOwiNBXS2Gq3dixYrzH2btiCfY1mqmTJpTQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "44TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11rtliq", "is_robot_indexable": true, "report_reasons": null, "author": "MikeS159", "discussion_type": null, "num_comments": 99, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11rtliq/complete_dilbert_comic_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11rtliq/complete_dilbert_comic_archive/", "subreddit_subscribers": 673386, "created_utc": 1678879661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dockerhub is sunsetting their free tier of hosting. While the communication isn't totally clear yet, it sounds like there'll be a very large amount of images that'll be forever purged, having ripple effects all over the industry.\n\n[https://github.com/docker/hub-feedback/issues/2314](https://github.com/docker/hub-feedback/issues/2314#issuecomment-1470418355)", "author_fullname": "t2_340acr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dockerhub to (likely?) delete a lot of organizations.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s2s15", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678900612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dockerhub is sunsetting their free tier of hosting. While the communication isn&amp;#39;t totally clear yet, it sounds like there&amp;#39;ll be a very large amount of images that&amp;#39;ll be forever purged, having ripple effects all over the industry.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/docker/hub-feedback/issues/2314#issuecomment-1470418355\"&gt;https://github.com/docker/hub-feedback/issues/2314&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ND7ZM59EeHethLmSqgN9LW2D8a36uX9h3BquQqCYToQ.jpg?auto=webp&amp;v=enabled&amp;s=befe2b3438deee4a8ce16350db16bef8f90cb147", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ND7ZM59EeHethLmSqgN9LW2D8a36uX9h3BquQqCYToQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9bdb5cca78eb36c8f23d18aafaf43e913cb68255", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ND7ZM59EeHethLmSqgN9LW2D8a36uX9h3BquQqCYToQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c40d6503f1049ba3fc0e4f7d24f40efce9b0644", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ND7ZM59EeHethLmSqgN9LW2D8a36uX9h3BquQqCYToQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3235264c2b5ebdead8710714335bb7b2578ca69", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ND7ZM59EeHethLmSqgN9LW2D8a36uX9h3BquQqCYToQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cc97b97fec2343e0a8b563230fd72d16c9c8577", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ND7ZM59EeHethLmSqgN9LW2D8a36uX9h3BquQqCYToQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f57c44875b50f07ac7bd6518117d8e0bb441baa9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ND7ZM59EeHethLmSqgN9LW2D8a36uX9h3BquQqCYToQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=252e3f9ce8cba80935f1dab8e33cd73b7718af07", "width": 1080, "height": 540}], "variants": {}, "id": "Ed-HlA65xjhXhCz-8XJjVNcA_YTfNYQwyM5s1tlHEzM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "110TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s2s15", "is_robot_indexable": true, "report_reasons": null, "author": "nicolasnoble", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11s2s15/dockerhub_to_likely_delete_a_lot_of_organizations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s2s15/dockerhub_to_likely_delete_a_lot_of_organizations/", "subreddit_subscribers": 673386, "created_utc": 1678900612.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a ton of copies of the same pictures, videos, and data all over multiple 1TB and 2 TB hard drives, and I want to create one giant archive of everything without duplicates. So I was thinking of Shucking 2 14TB Easystores, putting them into raid 1 internally in my PC, and possibly doing Primocache with either a 512 GB NVMe drive or a 480 SSD as a cache drive for the raid. \n\nThen make a copy of every file onto the 14 TB Drive and run a Deduplication software on that drive. I would leave a copy of the original files on the old drives. Then go through the tedious task of comparing duplicates and getting them organized. Then backing those organized files up to a NAS.\n\nWould having the cache be worth it? I could also buy a few smaller 8TB HGST drives and attach them via SAS and run in Raid 10 if that would be faster. Any suggestions on how to make this as quick as possible?", "author_fullname": "t2_foutj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a better setup for this? Setting up drives for Deduplication task. 14TBx2 WD Drives and an NVMe or SSD as a cache. Would that speed things up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sc4z9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678920698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a ton of copies of the same pictures, videos, and data all over multiple 1TB and 2 TB hard drives, and I want to create one giant archive of everything without duplicates. So I was thinking of Shucking 2 14TB Easystores, putting them into raid 1 internally in my PC, and possibly doing Primocache with either a 512 GB NVMe drive or a 480 SSD as a cache drive for the raid. &lt;/p&gt;\n\n&lt;p&gt;Then make a copy of every file onto the 14 TB Drive and run a Deduplication software on that drive. I would leave a copy of the original files on the old drives. Then go through the tedious task of comparing duplicates and getting them organized. Then backing those organized files up to a NAS.&lt;/p&gt;\n\n&lt;p&gt;Would having the cache be worth it? I could also buy a few smaller 8TB HGST drives and attach them via SAS and run in Raid 10 if that would be faster. Any suggestions on how to make this as quick as possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11sc4z9", "is_robot_indexable": true, "report_reasons": null, "author": "Photographer_Rob", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11sc4z9/is_there_a_better_setup_for_this_setting_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11sc4z9/is_there_a_better_setup_for_this_setting_up/", "subreddit_subscribers": 673386, "created_utc": 1678920698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for any thoughts/advice on my plan to overhaul the archive system for a commercial photographer I work for.  This community is full of some really smart/experienced folks so I value the opinions. Thanks in advance.\n\nTLDR; Planning to purchase a Synology DS1821+, fill it with 8x 16TB Ironwolf Pro/EXOS, and set it up with B2 backup. Is this a good use case for a small office where mostly only one person will be accessing the data, and even then the access will mostly be for cold storage?\n\nMore info than your probably require:\n\nThis particular photographer currently has about 52TB of data in his archive, which is currently backed up by using a number of 4-8TB hard drives, which three copies for each set.   One is the active drive on my desk, one lives in the fire safe here on site, and one lives at my house.  I try to bring all these drives together once a month to get everything current, but in reality the syncs occur less frequently.\n\nMost of this data is essentially in cold storage and doesn't change very much, if at all over time.  We do need to have it easily accessible all the time, which has become harder as the number of HDD sets has grown.  As we shoot new jobs, the work will be done and then the entire catalog of RAW files, TIFFS, PSDs and whatever else will be added to the current set of archive drives.  Once that set fills up, I buy three new drives and start the process again.  This method has been okay so far, and is infinitely expandable, but it also becomes more and more unwieldy as the data sets grow.  I originally started doing these with 1TB drives, which I then consolidated into 4TB drives, and now I have 9 sets of 4TB drives.\n\nI also have a couple of G-RAID style 2-bay RAID drives which I use for misc. items.  These have a matching unit that also makes the trip back and forth from my house (ideally) monthly.  These are currently bursting full which is what is spurring me to want to overhaul the whole system.\n\nGenerally there will only be one user accessing the NAS at any one time, and I don't plan on using it for anything like VM or even as a volume for doing live work from, or something like editing 4K footage from, so I don't imagine I need much in terms of horsepower or throughput.  I do plan to get the 10GBe expansion card.\n\nThe plan:\n\nI want to purchase a NAS in order to have one centralized volume where I can access the archive both locally and remotely and add to the archive as needed. I need something large enough to hold the current archive, as well as contain everything for the foreseeable future.  (If I could get 10 years of use out of this unit I would be pretty happy with that)\n\nAfter doing some research, I'm planning to purchase the DS1821+ along with 8x 16TB drives (Either Ironwolf Pro or EXOS most likely) and set the unit up with SHR-2 for maximum redundancy.  This would provide 96TB of usable space, with approx 45TB of free space based on our current archive. I also like the ability of being able to get Synology Expansion units down the road if I end up using that free space faster than expected.\n\nI also want to setup scheduled cloud backups so I can stop doing the off site backup drives requiring manual backups.  Backblaze B2 seems the obvious choice for this.  The initial backup for 50TB of stuff is going to be a chore in and of itself.  With a quick speed test (23.3Mbps Upload) and some napkin math, that puts things at about 4 days per TB.  So that's a long \\*\\*\\*\\*ing time to get our first complete backup to B2.  I'll have to look into upgrading our upload speed here at the office.\n\nI know this forum is a fan of (what I consider to be) more bespoke/complicated RAID systems and softwares.  I'm not uber technical, and not really familiar with Linux/Unix etc.  Lowest cost is not my primary concern.  When I floated the idea of spending 3000-3500 on a backup unit along with $3000/year cost for cloud backups, my boss gave me a thumbs up.  I also don't plan on staying at this job forever, so I need something that I can essentially setup easily and walk away from, with minimal training involved for whoever replaces me down the road.\n\nQuestions:\n\n**Is this setup a good plan for our use case and needs?** Again primary concerns are, Stability, Ease of use &amp; Economical capacity for a single user to access what is essentially cold storage.\n\n**Is there any reason to spend more money on anything above the IronWolf Pro drives?**  the EXOs are not unreasonably more money and I would consider them but I'm skeptical of seeing any real world performance difference.  Again, speed of read/write for the NAS isn't my highest priority.\n\n**What are the primary upgrades I should look at putting into the DS1821+?**  My current thoughts are to add the 10GBe adapter, and perhaps a small M.2 SSD which I believe the unit can use as cache?  I know people commonly upgrade the RAM in the unit, but I feel like I wouldn't see any benefit from this.\n\n**Is the DX517 Expansion unit stable enough to be considered a good addition down the road?**   I did read recently that you don't want to add the DX517 to your existing pool, which makes sense since it's not attached.  Would this be better considered for a separate pool of storage?\n\n**Is SHR-2 stable enough for serious archival use?** I understand the fact that a single NAS/RAID unit is not a sufficient backup.  I'm more asking if the system is stable enough for me not to worry about it crashing/corrupting the whole unit and me having to restore from cloud backup.\n\n**~~Does B2 have any offline options to seed your initial backup (like the Amazon Snowball?) when we're talking about \\~50TB of data?~~**\n\nThanks again for your time and consideration if you've made it this far.", "author_fullname": "t2_3v6zzk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice before overhauling archive system for photographer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sa32l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678931153.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678916126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for any thoughts/advice on my plan to overhaul the archive system for a commercial photographer I work for.  This community is full of some really smart/experienced folks so I value the opinions. Thanks in advance.&lt;/p&gt;\n\n&lt;p&gt;TLDR; Planning to purchase a Synology DS1821+, fill it with 8x 16TB Ironwolf Pro/EXOS, and set it up with B2 backup. Is this a good use case for a small office where mostly only one person will be accessing the data, and even then the access will mostly be for cold storage?&lt;/p&gt;\n\n&lt;p&gt;More info than your probably require:&lt;/p&gt;\n\n&lt;p&gt;This particular photographer currently has about 52TB of data in his archive, which is currently backed up by using a number of 4-8TB hard drives, which three copies for each set.   One is the active drive on my desk, one lives in the fire safe here on site, and one lives at my house.  I try to bring all these drives together once a month to get everything current, but in reality the syncs occur less frequently.&lt;/p&gt;\n\n&lt;p&gt;Most of this data is essentially in cold storage and doesn&amp;#39;t change very much, if at all over time.  We do need to have it easily accessible all the time, which has become harder as the number of HDD sets has grown.  As we shoot new jobs, the work will be done and then the entire catalog of RAW files, TIFFS, PSDs and whatever else will be added to the current set of archive drives.  Once that set fills up, I buy three new drives and start the process again.  This method has been okay so far, and is infinitely expandable, but it also becomes more and more unwieldy as the data sets grow.  I originally started doing these with 1TB drives, which I then consolidated into 4TB drives, and now I have 9 sets of 4TB drives.&lt;/p&gt;\n\n&lt;p&gt;I also have a couple of G-RAID style 2-bay RAID drives which I use for misc. items.  These have a matching unit that also makes the trip back and forth from my house (ideally) monthly.  These are currently bursting full which is what is spurring me to want to overhaul the whole system.&lt;/p&gt;\n\n&lt;p&gt;Generally there will only be one user accessing the NAS at any one time, and I don&amp;#39;t plan on using it for anything like VM or even as a volume for doing live work from, or something like editing 4K footage from, so I don&amp;#39;t imagine I need much in terms of horsepower or throughput.  I do plan to get the 10GBe expansion card.&lt;/p&gt;\n\n&lt;p&gt;The plan:&lt;/p&gt;\n\n&lt;p&gt;I want to purchase a NAS in order to have one centralized volume where I can access the archive both locally and remotely and add to the archive as needed. I need something large enough to hold the current archive, as well as contain everything for the foreseeable future.  (If I could get 10 years of use out of this unit I would be pretty happy with that)&lt;/p&gt;\n\n&lt;p&gt;After doing some research, I&amp;#39;m planning to purchase the DS1821+ along with 8x 16TB drives (Either Ironwolf Pro or EXOS most likely) and set the unit up with SHR-2 for maximum redundancy.  This would provide 96TB of usable space, with approx 45TB of free space based on our current archive. I also like the ability of being able to get Synology Expansion units down the road if I end up using that free space faster than expected.&lt;/p&gt;\n\n&lt;p&gt;I also want to setup scheduled cloud backups so I can stop doing the off site backup drives requiring manual backups.  Backblaze B2 seems the obvious choice for this.  The initial backup for 50TB of stuff is going to be a chore in and of itself.  With a quick speed test (23.3Mbps Upload) and some napkin math, that puts things at about 4 days per TB.  So that&amp;#39;s a long ****ing time to get our first complete backup to B2.  I&amp;#39;ll have to look into upgrading our upload speed here at the office.&lt;/p&gt;\n\n&lt;p&gt;I know this forum is a fan of (what I consider to be) more bespoke/complicated RAID systems and softwares.  I&amp;#39;m not uber technical, and not really familiar with Linux/Unix etc.  Lowest cost is not my primary concern.  When I floated the idea of spending 3000-3500 on a backup unit along with $3000/year cost for cloud backups, my boss gave me a thumbs up.  I also don&amp;#39;t plan on staying at this job forever, so I need something that I can essentially setup easily and walk away from, with minimal training involved for whoever replaces me down the road.&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is this setup a good plan for our use case and needs?&lt;/strong&gt; Again primary concerns are, Stability, Ease of use &amp;amp; Economical capacity for a single user to access what is essentially cold storage.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there any reason to spend more money on anything above the IronWolf Pro drives?&lt;/strong&gt;  the EXOs are not unreasonably more money and I would consider them but I&amp;#39;m skeptical of seeing any real world performance difference.  Again, speed of read/write for the NAS isn&amp;#39;t my highest priority.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What are the primary upgrades I should look at putting into the DS1821+?&lt;/strong&gt;  My current thoughts are to add the 10GBe adapter, and perhaps a small M.2 SSD which I believe the unit can use as cache?  I know people commonly upgrade the RAM in the unit, but I feel like I wouldn&amp;#39;t see any benefit from this.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is the DX517 Expansion unit stable enough to be considered a good addition down the road?&lt;/strong&gt;   I did read recently that you don&amp;#39;t want to add the DX517 to your existing pool, which makes sense since it&amp;#39;s not attached.  Would this be better considered for a separate pool of storage?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is SHR-2 stable enough for serious archival use?&lt;/strong&gt; I understand the fact that a single NAS/RAID unit is not a sufficient backup.  I&amp;#39;m more asking if the system is stable enough for me not to worry about it crashing/corrupting the whole unit and me having to restore from cloud backup.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;del&gt;Does B2 have any offline options to seed your initial backup (like the Amazon Snowball?) when we&amp;#39;re talking about ~50TB of data?&lt;/del&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks again for your time and consideration if you&amp;#39;ve made it this far.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11sa32l", "is_robot_indexable": true, "report_reasons": null, "author": "RebelliousBristles", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11sa32l/looking_for_advice_before_overhauling_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11sa32l/looking_for_advice_before_overhauling_archive/", "subreddit_subscribers": 673386, "created_utc": 1678916126.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My guess would be yes since it is under load, current and spinning but does the slower transfer and thus slower head movement offer any advantage?  I am talking about a single drive not an array.  Thanks.  To be clear I am talking about the link to the data being the bottleneck", "author_fullname": "t2_3gqtv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a slower large file transfer worse for drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sfbpw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678928383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My guess would be yes since it is under load, current and spinning but does the slower transfer and thus slower head movement offer any advantage?  I am talking about a single drive not an array.  Thanks.  To be clear I am talking about the link to the data being the bottleneck&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "66 TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11sfbpw", "is_robot_indexable": true, "report_reasons": null, "author": "cyril0", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11sfbpw/is_a_slower_large_file_transfer_worse_for_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11sfbpw/is_a_slower_large_file_transfer_worse_for_drives/", "subreddit_subscribers": 673386, "created_utc": 1678928383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_68usyre0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FYI If you wondered how efficient storage spaces uses different size hard disks 16TB 14TB 6TB 5TB=9.08 TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_11s1197", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hymtZKtW08swIXLQOdcNF0yTga5WoYXl7T3KtdX83VA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678896850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/y6tarxyfgxna1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/y6tarxyfgxna1.jpg?auto=webp&amp;v=enabled&amp;s=4b2892d7e2846bb67da9427ef9c17b00e865598a", "width": 1366, "height": 729}, "resolutions": [{"url": "https://preview.redd.it/y6tarxyfgxna1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6dfad438105847099e986c749dffc2c2c3c9c3b", "width": 108, "height": 57}, {"url": "https://preview.redd.it/y6tarxyfgxna1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63a3e94d4781cd8e4d42fd4f7ffa8bb17d83cb66", "width": 216, "height": 115}, {"url": "https://preview.redd.it/y6tarxyfgxna1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=384edcf60062f1ce68d53ec7e320bf4a9306c531", "width": 320, "height": 170}, {"url": "https://preview.redd.it/y6tarxyfgxna1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cbb9251a11ad1381ca4372773b84ac7503581a9", "width": 640, "height": 341}, {"url": "https://preview.redd.it/y6tarxyfgxna1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=603f15eb74bd3639faa60431c980fd3a5bfa56e0", "width": 960, "height": 512}, {"url": "https://preview.redd.it/y6tarxyfgxna1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20acbe89feb8c46e5988cd19e35347952f963a9a", "width": 1080, "height": 576}], "variants": {}, "id": "0OFDxoJTJU1o5cI_Tcby_MreeRKaqiw4FSOo2ovwmjE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s1197", "is_robot_indexable": true, "report_reasons": null, "author": "frozenPizza83", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11s1197/fyi_if_you_wondered_how_efficient_storage_spaces/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/y6tarxyfgxna1.jpg", "subreddit_subscribers": 673386, "created_utc": 1678896850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was under the impression for a long time now that all 3.5\" external hard drives would need an external power supply due to to the 12v power requirements of 3.5\" HDDs.\n\nHow do modern external drives like WD Passport 4TB manage this just through USB alone? I tried to google this and found absolutely nothing.", "author_fullname": "t2_cc458uu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is it that modern WD Passport drives don't need external power?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sjezm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678938858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was under the impression for a long time now that all 3.5&amp;quot; external hard drives would need an external power supply due to to the 12v power requirements of 3.5&amp;quot; HDDs.&lt;/p&gt;\n\n&lt;p&gt;How do modern external drives like WD Passport 4TB manage this just through USB alone? I tried to google this and found absolutely nothing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11sjezm", "is_robot_indexable": true, "report_reasons": null, "author": "SleepingAndy", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11sjezm/how_is_it_that_modern_wd_passport_drives_dont/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11sjezm/how_is_it_that_modern_wd_passport_drives_dont/", "subreddit_subscribers": 673386, "created_utc": 1678938858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was wondering what software resources you use to test newly purchased recertified hard drives and verify that they're good before utilizing them either for a NAS or cold storage.   \n\n\nThanks!", "author_fullname": "t2_atmah", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Refurbished/Recertified Hard Drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s4rel", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678904763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering what software resources you use to test newly purchased recertified hard drives and verify that they&amp;#39;re good before utilizing them either for a NAS or cold storage.   &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB TrueNAS ZFS ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s4rel", "is_robot_indexable": true, "report_reasons": null, "author": "nks12345", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11s4rel/refurbishedrecertified_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s4rel/refurbishedrecertified_hard_drives/", "subreddit_subscribers": 673386, "created_utc": 1678904763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_bhm3v2x5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A PCI to 4x u.2 adapter for under 20 bucks, where is the catch. Or is it a straight scam?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11rrjvk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/q_PZR4qyNZXGlCHVodl4DVjQQxdEkCYwbMob1N-O0ck.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678873180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zgnxpc9xzwna1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zgnxpc9xzwna1.png?auto=webp&amp;v=enabled&amp;s=e1aeee07f3167336f45e85b2da36ba0262f806c8", "width": 1080, "height": 2287}, "resolutions": [{"url": "https://preview.redd.it/zgnxpc9xzwna1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86ceadcf840c1470016989c88a40b41c8bf2a88a", "width": 108, "height": 216}, {"url": "https://preview.redd.it/zgnxpc9xzwna1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a6186acf0ea53ee29786232ad3520149b046580", "width": 216, "height": 432}, {"url": "https://preview.redd.it/zgnxpc9xzwna1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=487c8eda87a7034625bc6a921e50c687405f47d0", "width": 320, "height": 640}, {"url": "https://preview.redd.it/zgnxpc9xzwna1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb9839d4d2a296681202e92707f4f80c499dd9b0", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/zgnxpc9xzwna1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29707dcc655c2faa41b1e442f1085dbf51125caf", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/zgnxpc9xzwna1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abf59c0171f102ae9572f27c7ad0166cb457921e", "width": 1080, "height": 2160}], "variants": {}, "id": "DMfRVcb8JUtEDTxS26iNJ63EBHxS32cMUw2koyPUGFw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11rrjvk", "is_robot_indexable": true, "report_reasons": null, "author": "ZeltTent", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11rrjvk/a_pci_to_4x_u2_adapter_for_under_20_bucks_where/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zgnxpc9xzwna1.png", "subreddit_subscribers": 673386, "created_utc": 1678873180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a mountain of paper documents that I am trying to digitise. I set up paperless-ngx with a Brother ADS 1200 scanner that I bought lightly used on ebay. It's an entry level model but a beast: it can do 25 double sided pages a minute.\n\nIt often misfeeds on certain types of document though, particularly stacks of amex statements that have a sharply creased fold where they were originally folded. For these, I have to manually space each sheet in the stack so the folds don't align and even then it can take a couple of tries.\n\nI have read everything I can find on ADF scanners but I've never owned one before so I don't know if I have unrealistic expectations. Would upgrading to something like a Fujitsu scansnap s1500 help? (That model is elderly but apparently still works well and has ultrasonic detection of misfeeds)", "author_fullname": "t2_148qkv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ADF scanner for paperless - folded pages", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11rr23h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678871523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a mountain of paper documents that I am trying to digitise. I set up paperless-ngx with a Brother ADS 1200 scanner that I bought lightly used on ebay. It&amp;#39;s an entry level model but a beast: it can do 25 double sided pages a minute.&lt;/p&gt;\n\n&lt;p&gt;It often misfeeds on certain types of document though, particularly stacks of amex statements that have a sharply creased fold where they were originally folded. For these, I have to manually space each sheet in the stack so the folds don&amp;#39;t align and even then it can take a couple of tries.&lt;/p&gt;\n\n&lt;p&gt;I have read everything I can find on ADF scanners but I&amp;#39;ve never owned one before so I don&amp;#39;t know if I have unrealistic expectations. Would upgrading to something like a Fujitsu scansnap s1500 help? (That model is elderly but apparently still works well and has ultrasonic detection of misfeeds)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11rr23h", "is_robot_indexable": true, "report_reasons": null, "author": "wellknownname", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11rr23h/adf_scanner_for_paperless_folded_pages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11rr23h/adf_scanner_for_paperless_folded_pages/", "subreddit_subscribers": 673386, "created_utc": 1678871523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a relatively large file from my Google takeout history from old conversations that were had via hangouts. When I downloaded the takeout zip folders it gave me 2 different files. One that's hangouts.json and another is messages.json. I can use either the Hangon or Jessiecar96 pages to view the hangouts.json file no problem. Comes out clean.\n\nHowever, the messages.json file has to be on a different format because those programs won't recognize it. Does anybody know a resource I can use to edit that file into a readable organized format? Any help would be greatly appreciated!", "author_fullname": "t2_e0njq2q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "JSON Reader for Google messages.json file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sk005", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678940610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a relatively large file from my Google takeout history from old conversations that were had via hangouts. When I downloaded the takeout zip folders it gave me 2 different files. One that&amp;#39;s hangouts.json and another is messages.json. I can use either the Hangon or Jessiecar96 pages to view the hangouts.json file no problem. Comes out clean.&lt;/p&gt;\n\n&lt;p&gt;However, the messages.json file has to be on a different format because those programs won&amp;#39;t recognize it. Does anybody know a resource I can use to edit that file into a readable organized format? Any help would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11sk005", "is_robot_indexable": true, "report_reasons": null, "author": "pppc4life", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11sk005/json_reader_for_google_messagesjson_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11sk005/json_reader_for_google_messagesjson_file/", "subreddit_subscribers": 673386, "created_utc": 1678940610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know where I can download my old MFC recordings from?\nThere used to be sites that used to store them for a few but they\u2019ve all been removed. Want to reuse for OF. Not bothered how you guys share them but just looking for a copy for me", "author_fullname": "t2_bme4vmrm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MFC archive 2015 to 2016", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11siwi1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678937389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know where I can download my old MFC recordings from?\nThere used to be sites that used to store them for a few but they\u2019ve all been removed. Want to reuse for OF. Not bothered how you guys share them but just looking for a copy for me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11siwi1", "is_robot_indexable": true, "report_reasons": null, "author": "Character_Station944", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11siwi1/mfc_archive_2015_to_2016/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11siwi1/mfc_archive_2015_to_2016/", "subreddit_subscribers": 673386, "created_utc": 1678937389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently got an 8TB WD-Black external hard drive, mainly using it for plex server.\nJust downloaded crystal disk info and it\u2019s showing HD temperature is 58 degrees Celsius.\nIs that normal ? Room temperature is 22 C. \nThat temperature was recorded while nothing is being played on my plex server.\nShould I contact WD ?", "author_fullname": "t2_jiwyg458", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External hard drive temperature", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sfj9e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678928875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently got an 8TB WD-Black external hard drive, mainly using it for plex server.\nJust downloaded crystal disk info and it\u2019s showing HD temperature is 58 degrees Celsius.\nIs that normal ? Room temperature is 22 C. \nThat temperature was recorded while nothing is being played on my plex server.\nShould I contact WD ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11sfj9e", "is_robot_indexable": true, "report_reasons": null, "author": "Lilstitious__", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11sfj9e/external_hard_drive_temperature/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11sfj9e/external_hard_drive_temperature/", "subreddit_subscribers": 673386, "created_utc": 1678928875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\n\nI want to share with you a tool that I recently created and that I'm sure will be very helpful to you. It's called [localbackup](https://github.com/eduardolat/localbackup) and it allows you to easily back up your folders to your local hard drive.\n\nThe best thing about localbackup is that it's very versatile and you can use it from the command line. Here's a very simple example command you can use:\n\n`npx localbackup --source /path/to/source --destdir /path/to/backups-dest --keeplast 10`\n\nThis command tells localbackup to back up the folder in the directory \"/path/to/source\" and save it to the directory \"/path/to/backups-dest\". It also tells it to keep only the last 10 backups and delete the older ones.\n\nAs you can see, localbackup is a very useful and easy-to-use tool (no installation required if run with node). If you want to make sure your important files are always backed up, I recommend you give it a try. And if you like it, don't forget to give it a star on Github:\n\n[https://github.com/eduardolat/localbackup](https://github.com/eduardolat/localbackup)\n\nYou can use it to:\n\n* Backup docker volumes\n* Back up CMS files\n* Backup SQLite databases\n* Etc\n\nThanks for reading!", "author_fullname": "t2_6l27monh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "localbackup: Easily and Quickly Backup Your Important Files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ser8y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678927017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I want to share with you a tool that I recently created and that I&amp;#39;m sure will be very helpful to you. It&amp;#39;s called &lt;a href=\"https://github.com/eduardolat/localbackup\"&gt;localbackup&lt;/a&gt; and it allows you to easily back up your folders to your local hard drive.&lt;/p&gt;\n\n&lt;p&gt;The best thing about localbackup is that it&amp;#39;s very versatile and you can use it from the command line. Here&amp;#39;s a very simple example command you can use:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;npx localbackup --source /path/to/source --destdir /path/to/backups-dest --keeplast 10&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This command tells localbackup to back up the folder in the directory &amp;quot;/path/to/source&amp;quot; and save it to the directory &amp;quot;/path/to/backups-dest&amp;quot;. It also tells it to keep only the last 10 backups and delete the older ones.&lt;/p&gt;\n\n&lt;p&gt;As you can see, localbackup is a very useful and easy-to-use tool (no installation required if run with node). If you want to make sure your important files are always backed up, I recommend you give it a try. And if you like it, don&amp;#39;t forget to give it a star on Github:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/eduardolat/localbackup\"&gt;https://github.com/eduardolat/localbackup&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You can use it to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Backup docker volumes&lt;/li&gt;\n&lt;li&gt;Back up CMS files&lt;/li&gt;\n&lt;li&gt;Backup SQLite databases&lt;/li&gt;\n&lt;li&gt;Etc&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MF4L9CFUdyfTEb2c4UVlLwlDD5zFhU4g1FAWPTJvNiM.jpg?auto=webp&amp;v=enabled&amp;s=a08b00f96c820899508c1c11ba0c925605cb7f3b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/MF4L9CFUdyfTEb2c4UVlLwlDD5zFhU4g1FAWPTJvNiM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70a2341a28ba8b2587f98f121a0d1888216f8dce", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/MF4L9CFUdyfTEb2c4UVlLwlDD5zFhU4g1FAWPTJvNiM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f457dca61c9d80dd00f1b9e4827ab58a0b269d59", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/MF4L9CFUdyfTEb2c4UVlLwlDD5zFhU4g1FAWPTJvNiM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=624078c450215860ad3839ef27d0f3e45a37bb3d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/MF4L9CFUdyfTEb2c4UVlLwlDD5zFhU4g1FAWPTJvNiM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d2dffc9c979c66f69cf73115400b9370b2aeddc", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/MF4L9CFUdyfTEb2c4UVlLwlDD5zFhU4g1FAWPTJvNiM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de9f843f35f972df7b87232f53b75267dccf8c27", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/MF4L9CFUdyfTEb2c4UVlLwlDD5zFhU4g1FAWPTJvNiM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64205564ef00ab9081aa9dab0faccf6f20ca6d62", "width": 1080, "height": 540}], "variants": {}, "id": "wgu2PLNuSfqa518oGA-_QYK7eguyA1rbewPj9WsiEXY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11ser8y", "is_robot_indexable": true, "report_reasons": null, "author": "EduardoDevop", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11ser8y/localbackup_easily_and_quickly_backup_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11ser8y/localbackup_easily_and_quickly_backup_your/", "subreddit_subscribers": 673386, "created_utc": 1678927017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can implementation based on following concept work well?\n\nBackground: so far a set of internal SATA disks + internal SATA to USB docking station/(s) as one of backup plan major pillars. Possibly the point of time achieved one can consider stepwise switch to NVMe PCIe-SSD + matching USB-enclosure.\nMajor requirement: it must be possible for disks to plug in or remove from enclosure in short time. This must be quick and of effort as low as possible.\nFirst target: setup in USB 20Gbps class.\n\nIf to take SSD of sufficient read/write performance yet as well as heatsink installed during its original production, then to use it in metal enclosure without applying  additional thermal pads nor adhering to enclosure - can stable transfer speed at 20Gbps level be permanently achieved?", "author_fullname": "t2_2xb7mnp0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "USB 20GB-class setup, enclosure + internal SSD + \u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s9h5r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678916799.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678914813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can implementation based on following concept work well?&lt;/p&gt;\n\n&lt;p&gt;Background: so far a set of internal SATA disks + internal SATA to USB docking station/(s) as one of backup plan major pillars. Possibly the point of time achieved one can consider stepwise switch to NVMe PCIe-SSD + matching USB-enclosure.\nMajor requirement: it must be possible for disks to plug in or remove from enclosure in short time. This must be quick and of effort as low as possible.\nFirst target: setup in USB 20Gbps class.&lt;/p&gt;\n\n&lt;p&gt;If to take SSD of sufficient read/write performance yet as well as heatsink installed during its original production, then to use it in metal enclosure without applying  additional thermal pads nor adhering to enclosure - can stable transfer speed at 20Gbps level be permanently achieved?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s9h5r", "is_robot_indexable": true, "report_reasons": null, "author": "Biyeuy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11s9h5r/usb_20gbclass_setup_enclosure_internal_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s9h5r/usb_20gbclass_setup_enclosure_internal_ssd/", "subreddit_subscribers": 673386, "created_utc": 1678914813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone I was hoping someone could help me find out what is going on. \n\nI recently replaced a failed drive on my storage pool and everything was fine until I checked it today. It's telling me that the storage pool had reduced resiliency but the drive status seemed to be fine. I've checked with Seagate's own SeaTools and CrystalDiskInfo and drives report to be ok on there as well.\n\nThe pool is fine and I can use it with no problems whatsoever.\n\n[What storage spaces reports](https://i.imgur.com/8upr6zy.png)\n\nAny ideas on what could be happening here?", "author_fullname": "t2_985fd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows Storage Spaces showing error when drives are fine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s8bl5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678912293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone I was hoping someone could help me find out what is going on. &lt;/p&gt;\n\n&lt;p&gt;I recently replaced a failed drive on my storage pool and everything was fine until I checked it today. It&amp;#39;s telling me that the storage pool had reduced resiliency but the drive status seemed to be fine. I&amp;#39;ve checked with Seagate&amp;#39;s own SeaTools and CrystalDiskInfo and drives report to be ok on there as well.&lt;/p&gt;\n\n&lt;p&gt;The pool is fine and I can use it with no problems whatsoever.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/8upr6zy.png\"&gt;What storage spaces reports&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any ideas on what could be happening here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mPGqBiytHMMyhhRdH1ZG0C3kvDQHMRUCx0pnqnKLgMo.png?auto=webp&amp;v=enabled&amp;s=0157b3290b39baa2b61f54c1ad1cdca7ac91b47c", "width": 606, "height": 632}, "resolutions": [{"url": "https://external-preview.redd.it/mPGqBiytHMMyhhRdH1ZG0C3kvDQHMRUCx0pnqnKLgMo.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b10158571f8c15fcf9946a28dd62bde5eee9d8c7", "width": 108, "height": 112}, {"url": "https://external-preview.redd.it/mPGqBiytHMMyhhRdH1ZG0C3kvDQHMRUCx0pnqnKLgMo.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccd351f75147e0193b9960d0cb8cc9d6a0cd814b", "width": 216, "height": 225}, {"url": "https://external-preview.redd.it/mPGqBiytHMMyhhRdH1ZG0C3kvDQHMRUCx0pnqnKLgMo.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e226fbb82c2e03dfeca789c31653a6ecd29a788", "width": 320, "height": 333}], "variants": {}, "id": "qRLhmgwzNi1IzcMWOVzNB0Aw2bi5XeYbKPdLxWuyxqc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s8bl5", "is_robot_indexable": true, "report_reasons": null, "author": "majority_taco", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11s8bl5/windows_storage_spaces_showing_error_when_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s8bl5/windows_storage_spaces_showing_error_when_drives/", "subreddit_subscribers": 673386, "created_utc": 1678912293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " (Crossposted from r/storage)\n\nSo, I just picked up an Orico 3559RU3 drive enclosure, and I want to set it up in a RAID array. I (currently) have 4 drives that I am going to use:\n\n\\- 1 (brand new) Seagate IronWolf NAS 12TB Drive\n\n&amp;#x200B;\n\n\\- 2 10 TB Seagate IronWolf NAS Drives  \n\n\n\\- 1 6 TB Seagate Barracuda Drive (oldest and not a NAS drive, I want to replace it when I have the chance, or use it as internal storage/swap for the desktop)\n\nI want to set them up in a RAID configuration. I have been using Stablebit Drivepool for my \"RAID,\" setup for the last few years, with no issues, just going to go back to a Linux install on my desktop, and it doesn't look like Drivepool is available on Linux (and I would rather not run it with WINE or in a VM, just in case...), so might as well set it up as a hardware RAID.\n\nI have trimmed my data down so it will all fit on one 10 TB drive, removed duplication, removed the 6 TB, and am in the process of removing the 2nd 10 TB drive from the pool.\n\nWhat I would like to do, is install the 6, 10, and 12 TB drives in the Orico, build the array, transfer the data from the pool, wipe the second 10 TB drive, physically install it in the Orico, add it to the array, and then be able to access all of the storage without having to format them...as I'd rather not have to download 10 TB of data from Backblaze if I can help it.\n\n1) Is this possible, or will I have to rebuild the array upon adding drives to the Orico?  \n\n\n2) Do I need to have all of the same size drives (or lose capacity to the smallest) in order to RAID them?  \n\n\n3) What RAID configuration would be your recommendation for this setup? Or just run it in a normal configuration and just run regular backups/Backblaze backups?  \n\n\nThanks!!!", "author_fullname": "t2_5hvr5vj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a RAID5 Array in a Orico 3559RU3 drive enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s78to", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678909956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Crossposted from &lt;a href=\"/r/storage\"&gt;r/storage&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;So, I just picked up an Orico 3559RU3 drive enclosure, and I want to set it up in a RAID array. I (currently) have 4 drives that I am going to use:&lt;/p&gt;\n\n&lt;p&gt;- 1 (brand new) Seagate IronWolf NAS 12TB Drive&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- 2 10 TB Seagate IronWolf NAS Drives  &lt;/p&gt;\n\n&lt;p&gt;- 1 6 TB Seagate Barracuda Drive (oldest and not a NAS drive, I want to replace it when I have the chance, or use it as internal storage/swap for the desktop)&lt;/p&gt;\n\n&lt;p&gt;I want to set them up in a RAID configuration. I have been using Stablebit Drivepool for my &amp;quot;RAID,&amp;quot; setup for the last few years, with no issues, just going to go back to a Linux install on my desktop, and it doesn&amp;#39;t look like Drivepool is available on Linux (and I would rather not run it with WINE or in a VM, just in case...), so might as well set it up as a hardware RAID.&lt;/p&gt;\n\n&lt;p&gt;I have trimmed my data down so it will all fit on one 10 TB drive, removed duplication, removed the 6 TB, and am in the process of removing the 2nd 10 TB drive from the pool.&lt;/p&gt;\n\n&lt;p&gt;What I would like to do, is install the 6, 10, and 12 TB drives in the Orico, build the array, transfer the data from the pool, wipe the second 10 TB drive, physically install it in the Orico, add it to the array, and then be able to access all of the storage without having to format them...as I&amp;#39;d rather not have to download 10 TB of data from Backblaze if I can help it.&lt;/p&gt;\n\n&lt;p&gt;1) Is this possible, or will I have to rebuild the array upon adding drives to the Orico?  &lt;/p&gt;\n\n&lt;p&gt;2) Do I need to have all of the same size drives (or lose capacity to the smallest) in order to RAID them?  &lt;/p&gt;\n\n&lt;p&gt;3) What RAID configuration would be your recommendation for this setup? Or just run it in a normal configuration and just run regular backups/Backblaze backups?  &lt;/p&gt;\n\n&lt;p&gt;Thanks!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s78to", "is_robot_indexable": true, "report_reasons": null, "author": "justice-faye-dazzle", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11s78to/setting_up_a_raid5_array_in_a_orico_3559ru3_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s78to/setting_up_a_raid5_array_in_a_orico_3559ru3_drive/", "subreddit_subscribers": 673386, "created_utc": 1678909956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to backup some old SMS/MMS messages from an older Android phone. I was able to use SMS Backup and Restore to create and XML file but I can't figure out a way to covert it into anything viewable. Has anyone found a good way to back up old text conversations?", "author_fullname": "t2_1o4nzu5c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SMS/MMS Backup of Old Android", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s665z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678907693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to backup some old SMS/MMS messages from an older Android phone. I was able to use SMS Backup and Restore to create and XML file but I can&amp;#39;t figure out a way to covert it into anything viewable. Has anyone found a good way to back up old text conversations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s665z", "is_robot_indexable": true, "report_reasons": null, "author": "FBIMichaelScarn", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11s665z/smsmms_backup_of_old_android/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s665z/smsmms_backup_of_old_android/", "subreddit_subscribers": 673386, "created_utc": 1678907693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I purchased two 8TB HDDs to replace my existing 4TB HDD. I'm currently running it 3.5TB with no redundancy nor backup (the data is not that essential and it's easily replaceable). However, I'd like to at least have some redundancy to avoid the inconvenience of potential drive failure.\n\nI'm planning to run the two 8TB drives as a mirrored space with Microsoft Storage Spaces. My question is, what is the best way to transfer the data to the new drives? Surely it's something better than copy paste. I'm also open to feedback on this setup.\n\nCurrent setup:\nWindows 11 PC\n1TB SSD as main drive/scratch\n2TB SSD as fast data drive (gaming)\n4TB HDD overflow games/media\n\nPotential new setup:\nWindows 11 PC\n1TB SSD as main drive/scratch\n2TB SSD as fast data drive (gaming)\n(2) 8TB HDD with redundancy for overflow games/media/windows backups/file history", "author_fullname": "t2_6j2puixb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copy data to new storage space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s5tp2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678906970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I purchased two 8TB HDDs to replace my existing 4TB HDD. I&amp;#39;m currently running it 3.5TB with no redundancy nor backup (the data is not that essential and it&amp;#39;s easily replaceable). However, I&amp;#39;d like to at least have some redundancy to avoid the inconvenience of potential drive failure.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to run the two 8TB drives as a mirrored space with Microsoft Storage Spaces. My question is, what is the best way to transfer the data to the new drives? Surely it&amp;#39;s something better than copy paste. I&amp;#39;m also open to feedback on this setup.&lt;/p&gt;\n\n&lt;p&gt;Current setup:\nWindows 11 PC\n1TB SSD as main drive/scratch\n2TB SSD as fast data drive (gaming)\n4TB HDD overflow games/media&lt;/p&gt;\n\n&lt;p&gt;Potential new setup:\nWindows 11 PC\n1TB SSD as main drive/scratch\n2TB SSD as fast data drive (gaming)\n(2) 8TB HDD with redundancy for overflow games/media/windows backups/file history&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s5tp2", "is_robot_indexable": true, "report_reasons": null, "author": "NoTaro3850", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11s5tp2/copy_data_to_new_storage_space/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s5tp2/copy_data_to_new_storage_space/", "subreddit_subscribers": 673386, "created_utc": 1678906970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all. Recently I bought a couple of brand new Exos X18 16TB &amp; Skyhawk AI 8TB drives and installed on a PC workstation to be used as separate drives, not Raid.  \n\n\nChecking with CrystalDiskInfo I noticed that both the brand new Skyhawk &amp; Exos X18 had high Read Errors score against 0 hours of usage, which also checked out on SeaTools. I wrote to the Seagate support and they basically told me to use the \"fix\" utility on SeaTool. On the other had, I found out google searching that tose Raw digits are actually equivalent to 0 if there are up to 8 digits, but two of my four drives have 9 digits Raw Read Error Rate (still waiting Seagate reply on this).  \n\n\n**Question 1: did you encounter a similar situation with brand new drives / is this normal?**\n\n&amp;#x200B;\n\nDoing more research about this I also encountered this article: [**https://realhardwarereviews.com/seagate-exos-x18-review/5/**](https://realhardwarereviews.com/seagate-exos-x18-review/5/)\n\nwhere they point out the fact that NAS/Enterprise disks have a very low Error Recovery Control time as errors are recovered by Raid parity; but if you, like me, are using these drives as separate regular PC drives, there's obiouvsly no Raid so if a read error occur and the ERC times out, Windows would mark the file as corrupted. I shall think that this scenario would be rare, but:  \n\n\n**Question 2: how likely windows will corrupt files using the Exos and Skyhawk as regular PC drives, no Raid?**\n\n&amp;#x200B;\n\nAny suggestion and info appreciated. Thanks!", "author_fullname": "t2_hraaio63", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about Seagate Exos X18 &amp; Skyhawk AI Read Error Rate &amp; ERC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s59jw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678905807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all. Recently I bought a couple of brand new Exos X18 16TB &amp;amp; Skyhawk AI 8TB drives and installed on a PC workstation to be used as separate drives, not Raid.  &lt;/p&gt;\n\n&lt;p&gt;Checking with CrystalDiskInfo I noticed that both the brand new Skyhawk &amp;amp; Exos X18 had high Read Errors score against 0 hours of usage, which also checked out on SeaTools. I wrote to the Seagate support and they basically told me to use the &amp;quot;fix&amp;quot; utility on SeaTool. On the other had, I found out google searching that tose Raw digits are actually equivalent to 0 if there are up to 8 digits, but two of my four drives have 9 digits Raw Read Error Rate (still waiting Seagate reply on this).  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 1: did you encounter a similar situation with brand new drives / is this normal?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Doing more research about this I also encountered this article: &lt;a href=\"https://realhardwarereviews.com/seagate-exos-x18-review/5/\"&gt;&lt;strong&gt;https://realhardwarereviews.com/seagate-exos-x18-review/5/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;where they point out the fact that NAS/Enterprise disks have a very low Error Recovery Control time as errors are recovered by Raid parity; but if you, like me, are using these drives as separate regular PC drives, there&amp;#39;s obiouvsly no Raid so if a read error occur and the ERC times out, Windows would mark the file as corrupted. I shall think that this scenario would be rare, but:  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 2: how likely windows will corrupt files using the Exos and Skyhawk as regular PC drives, no Raid?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any suggestion and info appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aj7lD-M4sbs4RdjigLQOg8_3JZv_HuOATSxMqY0iRVU.jpg?auto=webp&amp;v=enabled&amp;s=c8a62464128be623e4b598159cdd940b7cb193e1", "width": 2052, "height": 1360}, "resolutions": [{"url": "https://external-preview.redd.it/aj7lD-M4sbs4RdjigLQOg8_3JZv_HuOATSxMqY0iRVU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d19d076b661adb91772904f8fa8764f878a5e65", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/aj7lD-M4sbs4RdjigLQOg8_3JZv_HuOATSxMqY0iRVU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ca7bdcf6070c25db9962325656649363fb6b3c2", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/aj7lD-M4sbs4RdjigLQOg8_3JZv_HuOATSxMqY0iRVU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2692d9280c3fb98e62cb69a7ca7b4a5fdd74c10", "width": 320, "height": 212}, {"url": "https://external-preview.redd.it/aj7lD-M4sbs4RdjigLQOg8_3JZv_HuOATSxMqY0iRVU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54aaaba01ed74e9e625ceef5f31b55289a5f8b8e", "width": 640, "height": 424}, {"url": "https://external-preview.redd.it/aj7lD-M4sbs4RdjigLQOg8_3JZv_HuOATSxMqY0iRVU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=112761eac99d26f251a77d39c6fb2c03a0ba71cf", "width": 960, "height": 636}, {"url": "https://external-preview.redd.it/aj7lD-M4sbs4RdjigLQOg8_3JZv_HuOATSxMqY0iRVU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e4bf3c49173d734e20c00cde53bd898a26047629", "width": 1080, "height": 715}], "variants": {}, "id": "5b6jEKVPwgLlQAXcQxnUdW1QbnSJv6XwJu1fOzNP-XM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s59jw", "is_robot_indexable": true, "report_reasons": null, "author": "LewisTollani1967", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11s59jw/questions_about_seagate_exos_x18_skyhawk_ai_read/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s59jw/questions_about_seagate_exos_x18_skyhawk_ai_read/", "subreddit_subscribers": 673386, "created_utc": 1678905807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After getting advices from knowledged people here, I was settle with buying 2 of WD New My Passport Gen3 5TB.\nIt was really cheap compare to any options, specially my usual one, LaCie Mobile Drive 5TB.\n\nI have been using 4 of them without problems, but recently I noticed WD ones makes more rumbling and noise than LaCie ones. Both of WD has different, level of noise and rumbling too.\n\nI checked health of those, and it seems fine. And I suspect it happens because LaCie ones use aluminium rather then plastic, so makes it less noise and rumble. It\u2019s slightly bigger and thicker too.\n\nBut after google search, and searching in this reddit, it\u2019s really confusing if it\u2019s harmful for the drive or not.\nSome said it doesn\u2019t matter if general health is good, some said it\u2019s bad for HDD and that\u2019s why you shouldn\u2019t stack those while it\u2019s running, and some people said getting 2.5 inch external drive is bad idea in the first place.\n\nSo, please understand if I asking really basic stuff here but, which information should I trust? Do I need to concern about noise/rumbling and make my future upgrade choice more carefully? Is it bad or okay to stack external drives? I am really confused by all of these.", "author_fullname": "t2_hidwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please understand, but I am really confused about HDD noise/rumbling.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s51fk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678905332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After getting advices from knowledged people here, I was settle with buying 2 of WD New My Passport Gen3 5TB.\nIt was really cheap compare to any options, specially my usual one, LaCie Mobile Drive 5TB.&lt;/p&gt;\n\n&lt;p&gt;I have been using 4 of them without problems, but recently I noticed WD ones makes more rumbling and noise than LaCie ones. Both of WD has different, level of noise and rumbling too.&lt;/p&gt;\n\n&lt;p&gt;I checked health of those, and it seems fine. And I suspect it happens because LaCie ones use aluminium rather then plastic, so makes it less noise and rumble. It\u2019s slightly bigger and thicker too.&lt;/p&gt;\n\n&lt;p&gt;But after google search, and searching in this reddit, it\u2019s really confusing if it\u2019s harmful for the drive or not.\nSome said it doesn\u2019t matter if general health is good, some said it\u2019s bad for HDD and that\u2019s why you shouldn\u2019t stack those while it\u2019s running, and some people said getting 2.5 inch external drive is bad idea in the first place.&lt;/p&gt;\n\n&lt;p&gt;So, please understand if I asking really basic stuff here but, which information should I trust? Do I need to concern about noise/rumbling and make my future upgrade choice more carefully? Is it bad or okay to stack external drives? I am really confused by all of these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s51fk", "is_robot_indexable": true, "report_reasons": null, "author": "Winial", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11s51fk/please_understand_but_i_am_really_confused_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s51fk/please_understand_but_i_am_really_confused_about/", "subreddit_subscribers": 673386, "created_utc": 1678905332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So first of all, I might be asking this in the wrong place, but I figured you guys would probably get me started in the right direction for now\u2026\n\nI am a film DIT (digital image technician) for my university. I backup fellow student projects from time to time. Most are around 100 gigs per day. \n\nI currently use an HDD, and there is so much downtime on set just waiting for me to finish. I need an upgrade.\n\nI was looking at the 4TB T7, but my MacBook doesn\u2019t support 3.2 gen 2x2, so I wouldn\u2019t be able to use to the full gig/s transfer speed. It gets capped at around 700mbps. Lots of videos on this.\n\nI also found a thunderbolt 3 drive (which my computer does support) and that gives roughly 3 gb/s read, and 2gb/s write. Drives like this include Sandisk G40, or fantom.\n\nI would be very interested in getting one, but the price increase is stiff, especially for higher storage.\n\nLet\u2019s say my budget is around $500, what would be my best choice to balance storage and speed. Thanks for the help!", "author_fullname": "t2_4cmd3nkb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a thunderbolt 3 external SSD for MacBook.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s4ze9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678905217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So first of all, I might be asking this in the wrong place, but I figured you guys would probably get me started in the right direction for now\u2026&lt;/p&gt;\n\n&lt;p&gt;I am a film DIT (digital image technician) for my university. I backup fellow student projects from time to time. Most are around 100 gigs per day. &lt;/p&gt;\n\n&lt;p&gt;I currently use an HDD, and there is so much downtime on set just waiting for me to finish. I need an upgrade.&lt;/p&gt;\n\n&lt;p&gt;I was looking at the 4TB T7, but my MacBook doesn\u2019t support 3.2 gen 2x2, so I wouldn\u2019t be able to use to the full gig/s transfer speed. It gets capped at around 700mbps. Lots of videos on this.&lt;/p&gt;\n\n&lt;p&gt;I also found a thunderbolt 3 drive (which my computer does support) and that gives roughly 3 gb/s read, and 2gb/s write. Drives like this include Sandisk G40, or fantom.&lt;/p&gt;\n\n&lt;p&gt;I would be very interested in getting one, but the price increase is stiff, especially for higher storage.&lt;/p&gt;\n\n&lt;p&gt;Let\u2019s say my budget is around $500, what would be my best choice to balance storage and speed. Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s4ze9", "is_robot_indexable": true, "report_reasons": null, "author": "GLOFISH2000", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11s4ze9/looking_for_a_thunderbolt_3_external_ssd_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s4ze9/looking_for_a_thunderbolt_3_external_ssd_for/", "subreddit_subscribers": 673386, "created_utc": 1678905217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I mean, \"uncommon\", it's just not full width paragraphs, it's more complex. But it's not more complex than any webpage of today, you know, lot of divs separated in space, who have div in them in yet another structure....\n\nIt's easy to just retreive the original HTML as it have the perfect structure and text order described. Problem is that I try to convert a book, not a webpage, so I don't have that magical HTML file with all paragraphs in linear order despite the presentation being organic\n\nI have a PDF of a book that I try to OCR to convert to EPUB. Without talking about the problem of character recogniton, which is a whole other chapter. I have a problem about the order of paragraphs, of structures. The OCR mix up all the paragraphs so it doesn't have any sense anymore, it's unreadable. I know OCR tools have option to read non linear paragraph. But I mean, identifying non linear paragraph and putting them in order is another task that it seem to fail. And it'll be too difficult to do it manually, the book is big. But if you have a tool where I can define some pattern to follow, and not page per page, maybe I can give it a try. As long as I don't have neither to define each page which model to follow\n\n&amp;#x200B;\n\nIt'll be way easier in another world without author rights, where original docx or tex could be shared, with original tables and so on, but you know. At least retro engineering is a thing and it has permitted to create things too\n\n&amp;#x200B;\n\nMaybe even better, if any tool could create any organized file automatically with OCRing one (for exaple recreating docx with tables and so on). That base file would be even better to work on than the original picture", "author_fullname": "t2_3bqukjsf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Uncommon text structure OCR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s1fba", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678897974.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678897695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mean, &amp;quot;uncommon&amp;quot;, it&amp;#39;s just not full width paragraphs, it&amp;#39;s more complex. But it&amp;#39;s not more complex than any webpage of today, you know, lot of divs separated in space, who have div in them in yet another structure....&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s easy to just retreive the original HTML as it have the perfect structure and text order described. Problem is that I try to convert a book, not a webpage, so I don&amp;#39;t have that magical HTML file with all paragraphs in linear order despite the presentation being organic&lt;/p&gt;\n\n&lt;p&gt;I have a PDF of a book that I try to OCR to convert to EPUB. Without talking about the problem of character recogniton, which is a whole other chapter. I have a problem about the order of paragraphs, of structures. The OCR mix up all the paragraphs so it doesn&amp;#39;t have any sense anymore, it&amp;#39;s unreadable. I know OCR tools have option to read non linear paragraph. But I mean, identifying non linear paragraph and putting them in order is another task that it seem to fail. And it&amp;#39;ll be too difficult to do it manually, the book is big. But if you have a tool where I can define some pattern to follow, and not page per page, maybe I can give it a try. As long as I don&amp;#39;t have neither to define each page which model to follow&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;ll be way easier in another world without author rights, where original docx or tex could be shared, with original tables and so on, but you know. At least retro engineering is a thing and it has permitted to create things too&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Maybe even better, if any tool could create any organized file automatically with OCRing one (for exaple recreating docx with tables and so on). That base file would be even better to work on than the original picture&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s1fba", "is_robot_indexable": true, "report_reasons": null, "author": "ybhi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11s1fba/uncommon_text_structure_ocr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s1fba/uncommon_text_structure_ocr/", "subreddit_subscribers": 673386, "created_utc": 1678897695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking to move a bunch of my files/folders around on the hard drives I have hooked up to my computer at the moment (I think there's 22 right now lol).  Is there an app that will let me sort all this stuff out and it will move and put them away to each drive (even if it takes a week of moving)??  Or even an app that lets me queue up moves so I can set it and go, and not have them all going at the same time, or having to come back to move the next batch of files??  I got everything all over the place, so would be nice to just have only Movies or TV Shows on each drive, the Switch games or whatever on others.\n\n&amp;#x200B;\n\nThanks for any insight one can provide!", "author_fullname": "t2_wea6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Program to Sort Hard Drives and Queue up moves?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s0z7l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678896726.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to move a bunch of my files/folders around on the hard drives I have hooked up to my computer at the moment (I think there&amp;#39;s 22 right now lol).  Is there an app that will let me sort all this stuff out and it will move and put them away to each drive (even if it takes a week of moving)??  Or even an app that lets me queue up moves so I can set it and go, and not have them all going at the same time, or having to come back to move the next batch of files??  I got everything all over the place, so would be nice to just have only Movies or TV Shows on each drive, the Switch games or whatever on others.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for any insight one can provide!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "11s0z7l", "is_robot_indexable": true, "report_reasons": null, "author": "Sovikos", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11s0z7l/program_to_sort_hard_drives_and_queue_up_moves/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11s0z7l/program_to_sort_hard_drives_and_queue_up_moves/", "subreddit_subscribers": 673386, "created_utc": 1678896726.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}