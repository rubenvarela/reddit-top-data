{"kind": "Listing", "data": {"after": "t3_11sszvk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've got the below synopsis of dbt...\n\n \n\n1. Modularity and Reusability: DBT allows for the creation of reusable and modular data models, which can be easily shared across different projects and teams. This makes it easier to maintain consistency across the organization and reduces the duplication of effort.\n2. Version Control: DBT integrates with version control systems like Git, allowing data engineers to track changes to the data pipeline over time. This feature is crucial for maintaining data integrity and auditability.\n3. Testing and Validation: DBT includes built-in testing and validation features, which enable data engineers to test data pipelines and ensure that the output is accurate and consistent with the input data.\n4. Collaboration: DBT's collaborative features allow multiple team members to work on the same project simultaneously, which increases productivity and reduces the likelihood of errors.\n\n&amp;#x200B;\n\n...but I still don't get what the point of it is. What am I missing?", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm not getting it...what's the point of DBT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s8x0q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678913595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got the below synopsis of dbt...&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Modularity and Reusability: DBT allows for the creation of reusable and modular data models, which can be easily shared across different projects and teams. This makes it easier to maintain consistency across the organization and reduces the duplication of effort.&lt;/li&gt;\n&lt;li&gt;Version Control: DBT integrates with version control systems like Git, allowing data engineers to track changes to the data pipeline over time. This feature is crucial for maintaining data integrity and auditability.&lt;/li&gt;\n&lt;li&gt;Testing and Validation: DBT includes built-in testing and validation features, which enable data engineers to test data pipelines and ensure that the output is accurate and consistent with the input data.&lt;/li&gt;\n&lt;li&gt;Collaboration: DBT&amp;#39;s collaborative features allow multiple team members to work on the same project simultaneously, which increases productivity and reduces the likelihood of errors.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;...but I still don&amp;#39;t get what the point of it is. What am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11s8x0q", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 107, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11s8x0q/im_not_getting_itwhats_the_point_of_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11s8x0q/im_not_getting_itwhats_the_point_of_dbt/", "subreddit_subscribers": 93289, "created_utc": 1678913595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I see this sentiment a lot in this sub. Seems to be mainly from analytics engineer types who are focused on data modeling inside the warehouse. The reasoning is normally along the lines of \"tools like fivetran and airbyte have connectors for everything, so no need to write integrations for anything, we can just deploy those and get on with the real work\"\n\nWhile on some level this is true, i really do feel like its missing a big part of the picture. For one it doesn't consider streaming and real time data but that's a debate for another time. The big problem with the above vision for me is it's overly focused on just lifting and shifting data from OLTP prod systems, with no consideration for things like data quality, schema validation, schema evolution, data contracts etc. To me it's overly coupled to the specific OLTP technologies used. Shouldn't we be looking to see OLTP systems wrapped in an interface that doesn't rely on internal implementation details?\n\nCurious to hear other people's thoughts on this as it's something that's been bothering me lately.", "author_fullname": "t2_18xb5x05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"ingestion is a solved problem\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sq68k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678962424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see this sentiment a lot in this sub. Seems to be mainly from analytics engineer types who are focused on data modeling inside the warehouse. The reasoning is normally along the lines of &amp;quot;tools like fivetran and airbyte have connectors for everything, so no need to write integrations for anything, we can just deploy those and get on with the real work&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;While on some level this is true, i really do feel like its missing a big part of the picture. For one it doesn&amp;#39;t consider streaming and real time data but that&amp;#39;s a debate for another time. The big problem with the above vision for me is it&amp;#39;s overly focused on just lifting and shifting data from OLTP prod systems, with no consideration for things like data quality, schema validation, schema evolution, data contracts etc. To me it&amp;#39;s overly coupled to the specific OLTP technologies used. Shouldn&amp;#39;t we be looking to see OLTP systems wrapped in an interface that doesn&amp;#39;t rely on internal implementation details?&lt;/p&gt;\n\n&lt;p&gt;Curious to hear other people&amp;#39;s thoughts on this as it&amp;#39;s something that&amp;#39;s been bothering me lately.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sq68k", "is_robot_indexable": true, "report_reasons": null, "author": "the-data-scientist", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sq68k/ingestion_is_a_solved_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sq68k/ingestion_is_a_solved_problem/", "subreddit_subscribers": 93289, "created_utc": 1678962424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I love using dbt, I'm not going to stop using it any time soon.  I can easily explain where it lives in the Modern Data Stack / ELT style of doing data engineering AND I can explain the features and benefits.\n\nBUT, in a meeting earlier today I was asked what are the alternatives to dbt? \n\n&amp;#x200B;\n\nAs in ... for E &amp; L you can use FiveTran or Hevo or Stitch, for database you can use AWS RedShift or GBQ or Snowflake, but for T ..... I can only think of dbt - is there any alternative?   (and to define that a bit better - any software that can transform data in SQL, cloud-based)", "author_fullname": "t2_353ucr1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt - what are the alternatives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sintx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678936710.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I love using dbt, I&amp;#39;m not going to stop using it any time soon.  I can easily explain where it lives in the Modern Data Stack / ELT style of doing data engineering AND I can explain the features and benefits.&lt;/p&gt;\n\n&lt;p&gt;BUT, in a meeting earlier today I was asked what are the alternatives to dbt? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As in ... for E &amp;amp; L you can use FiveTran or Hevo or Stitch, for database you can use AWS RedShift or GBQ or Snowflake, but for T ..... I can only think of dbt - is there any alternative?   (and to define that a bit better - any software that can transform data in SQL, cloud-based)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sintx", "is_robot_indexable": true, "report_reasons": null, "author": "cmcau", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sintx/dbt_what_are_the_alternatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sintx/dbt_what_are_the_alternatives/", "subreddit_subscribers": 93289, "created_utc": 1678936710.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The programming language that we'd use is Python, and I have worked with Python and SQL but never with async structures.", "author_fullname": "t2_q0hjyvx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recently joined a DE team and I've been asked to study async, multiprocessing, queuing, and Kafka. Can anybody tell me how to proceed and also share resources that I can use.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s4t6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678904866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The programming language that we&amp;#39;d use is Python, and I have worked with Python and SQL but never with async structures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11s4t6s", "is_robot_indexable": true, "report_reasons": null, "author": "Ready--Aim--Fire", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11s4t6s/recently_joined_a_de_team_and_ive_been_asked_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11s4t6s/recently_joined_a_de_team_and_ive_been_asked_to/", "subreddit_subscribers": 93289, "created_utc": 1678904866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering what the community has used for moving petabytes (say 100+) of data from a clustered of databases to another (lets say from Mysql &amp; Postgres to snowflake)?\n\n&amp;#x200B;\n\nin my old life I'd use standard Microsoft tech using BCP wrapped in Powershell, honestly I was quite divorced from the whole process since it was cookie cutter templates that my old team developed.\n\nWhat I've seen so far -\n\n&amp;#x200B;\n\n* JDBC connectors to incrementally load data - works well if you only have a few TBs of data and can switch to a CDC approach to avoid reloading in future.\n* BCP/ Bulk flat file exports to Blob / S3 then stage that into Snowflake / Synapse / BQ / Redshift.\n* off-the shelf solutions for cloud owned databases (Aurora RDS -&gt; Redshift, Datafactory(SQL Server) -&gt; Synapse / Databricks).\n* EL tools like Airbyte, Fivetran, Stitch?\n\n&amp;#x200B;\n\nwith regards to EL tools - are they using some magic to lift and shift massive amounts of data - or is it all JDBC with throttling as you don't want to kill the databases? Looking at the source code for Airbyte I know it's a incremental batched process to persist data into memory and move it to an external location to stage.\n\nhappy to hear some stories from the more experience folk.", "author_fullname": "t2_7iuhjtv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving Petabytes of Data from DBs to a DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11spizb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678962352.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678960288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering what the community has used for moving petabytes (say 100+) of data from a clustered of databases to another (lets say from Mysql &amp;amp; Postgres to snowflake)?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;in my old life I&amp;#39;d use standard Microsoft tech using BCP wrapped in Powershell, honestly I was quite divorced from the whole process since it was cookie cutter templates that my old team developed.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve seen so far -&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;JDBC connectors to incrementally load data - works well if you only have a few TBs of data and can switch to a CDC approach to avoid reloading in future.&lt;/li&gt;\n&lt;li&gt;BCP/ Bulk flat file exports to Blob / S3 then stage that into Snowflake / Synapse / BQ / Redshift.&lt;/li&gt;\n&lt;li&gt;off-the shelf solutions for cloud owned databases (Aurora RDS -&amp;gt; Redshift, Datafactory(SQL Server) -&amp;gt; Synapse / Databricks).&lt;/li&gt;\n&lt;li&gt;EL tools like Airbyte, Fivetran, Stitch?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;with regards to EL tools - are they using some magic to lift and shift massive amounts of data - or is it all JDBC with throttling as you don&amp;#39;t want to kill the databases? Looking at the source code for Airbyte I know it&amp;#39;s a incremental batched process to persist data into memory and move it to an external location to stage.&lt;/p&gt;\n\n&lt;p&gt;happy to hear some stories from the more experience folk.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11spizb", "is_robot_indexable": true, "report_reasons": null, "author": "Omar_88", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11spizb/moving_petabytes_of_data_from_dbs_to_a_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11spizb/moving_petabytes_of_data_from_dbs_to_a_dwh/", "subreddit_subscribers": 93289, "created_utc": 1678960288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I watched a video on breaking into big tech as a data engineer and in the video they mention a few things you should know. One thing you should know is how to build \"high quality pipelines\", but that's very vague. So, what are the characteristics of a high quality data pipeline?", "author_fullname": "t2_yabvd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "High Quality Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s8iqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678912734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I watched a video on breaking into big tech as a data engineer and in the video they mention a few things you should know. One thing you should know is how to build &amp;quot;high quality pipelines&amp;quot;, but that&amp;#39;s very vague. So, what are the characteristics of a high quality data pipeline?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11s8iqn", "is_robot_indexable": true, "report_reasons": null, "author": "phantomxxone", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11s8iqn/high_quality_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11s8iqn/high_quality_pipelines/", "subreddit_subscribers": 93289, "created_utc": 1678912734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My coworker and I are tasked with building out our data platform at our company. Our company has never had a data warehouse - just reporting directly against production database. It was decided not by us that we will be using Azure tools since we are already mostly a Microsoft shop. So we are using ADF, Azure Data Lake, Azure SQL servers/databases for the Data Warehouse, etc. We are not data engineers or even data analysts per se, but do work in these areas and are increasingly doing more.\n\nThe idea is to use ADF as simply an orchestrator as much as possible. Any transformations will be done on the SQL server side via stored procedures (it was clear after browsing this and other forums/sites that ADF can be a pain for transformations). We have a very small team which is very comfortable in SQL, so we are thinking this will play to our strengths.\n\nOur company does not deal with the amount of data where we need robust heavily-engineered processes. We don't even know what those even look like anyway.\n\nHere is how we are approaching the design at a high level. All steps are orchestrated via ADF.\n\n1. **Extract** from source and **Load** to Data Lake as CSV files\n2. **Extract** from Data Lake and **Load** to SQL staging table\n3. Call a stored procedure in SQL to do the following: \n   1. **Extract**/select data from staging table\n   2. **Transform** the data as needed\n   3. **Load**/insert the data into production table in the Warehouse\n\nTo be honest, my coworker and I are not really sure we will even realize the benefit of using the Data Lake given our use case, but again it was decided not by us that we will use the Data Lake.\n\nAny and all feedback, advice, tips, etc. is welcome and greatly appreciated. If this is a garbage approach, please tell me it is, but then also provide suggestions on what you would do instead.\n\nThank you in advance!", "author_fullname": "t2_dycud4bp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELT Design - Request for Feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s9h3v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678914809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My coworker and I are tasked with building out our data platform at our company. Our company has never had a data warehouse - just reporting directly against production database. It was decided not by us that we will be using Azure tools since we are already mostly a Microsoft shop. So we are using ADF, Azure Data Lake, Azure SQL servers/databases for the Data Warehouse, etc. We are not data engineers or even data analysts per se, but do work in these areas and are increasingly doing more.&lt;/p&gt;\n\n&lt;p&gt;The idea is to use ADF as simply an orchestrator as much as possible. Any transformations will be done on the SQL server side via stored procedures (it was clear after browsing this and other forums/sites that ADF can be a pain for transformations). We have a very small team which is very comfortable in SQL, so we are thinking this will play to our strengths.&lt;/p&gt;\n\n&lt;p&gt;Our company does not deal with the amount of data where we need robust heavily-engineered processes. We don&amp;#39;t even know what those even look like anyway.&lt;/p&gt;\n\n&lt;p&gt;Here is how we are approaching the design at a high level. All steps are orchestrated via ADF.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Extract&lt;/strong&gt; from source and &lt;strong&gt;Load&lt;/strong&gt; to Data Lake as CSV files&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Extract&lt;/strong&gt; from Data Lake and &lt;strong&gt;Load&lt;/strong&gt; to SQL staging table&lt;/li&gt;\n&lt;li&gt;Call a stored procedure in SQL to do the following: \n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Extract&lt;/strong&gt;/select data from staging table&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Transform&lt;/strong&gt; the data as needed&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Load&lt;/strong&gt;/insert the data into production table in the Warehouse&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To be honest, my coworker and I are not really sure we will even realize the benefit of using the Data Lake given our use case, but again it was decided not by us that we will use the Data Lake.&lt;/p&gt;\n\n&lt;p&gt;Any and all feedback, advice, tips, etc. is welcome and greatly appreciated. If this is a garbage approach, please tell me it is, but then also provide suggestions on what you would do instead.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11s9h3v", "is_robot_indexable": true, "report_reasons": null, "author": "armurphy1907", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11s9h3v/elt_design_request_for_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11s9h3v/elt_design_request_for_feedback/", "subreddit_subscribers": 93289, "created_utc": 1678914809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Every now and then I find myself writing practically the same data connector that does the same exhaustive traversing:\n\nfor example, maybe something like:\n\n    GET /orgs\n    GET /orgs/1/users\n    GET /orgs/1/users/100\n    GET /orgs/1/users/100/events\n    GET /orgs/1/users/100/events?page=2\n    GET /orgs/1/users/100/events?page=3\n    GET /orgs/1/users/200/events\n    GET /orgs/1/users/200/events?page=2\n\nI've always written dirty one-offs to get where I need to go quickly, but with the feeling I'm reinventing the wheel each time. Does anyone know of any mature python libraries that abstract away the minutia of exhaustively iterating rest endpoints? Something where you might pass something like an endpoints definition dict and it runs with it?", "author_fullname": "t2_udio63vp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Library for Iterating over REST endpoints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s6zyu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678909437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every now and then I find myself writing practically the same data connector that does the same exhaustive traversing:&lt;/p&gt;\n\n&lt;p&gt;for example, maybe something like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;GET /orgs\nGET /orgs/1/users\nGET /orgs/1/users/100\nGET /orgs/1/users/100/events\nGET /orgs/1/users/100/events?page=2\nGET /orgs/1/users/100/events?page=3\nGET /orgs/1/users/200/events\nGET /orgs/1/users/200/events?page=2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;ve always written dirty one-offs to get where I need to go quickly, but with the feeling I&amp;#39;m reinventing the wheel each time. Does anyone know of any mature python libraries that abstract away the minutia of exhaustively iterating rest endpoints? Something where you might pass something like an endpoints definition dict and it runs with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11s6zyu", "is_robot_indexable": true, "report_reasons": null, "author": "unsupported-subquery", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11s6zyu/python_library_for_iterating_over_rest_endpoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11s6zyu/python_library_for_iterating_over_rest_endpoints/", "subreddit_subscribers": 93289, "created_utc": 1678909437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For reference, here is the JD they used (same for normal and intern roles lol) https://boards.greenhouse.io/icapitalnetwork/jobs/6628661002\n\nI\u2019m very excited to have been accepted because I haven\u2019t worked with a lot of industry stuff before like AWS, Snowflake, etc\n\nFinding housing in NYC is so difficult and expensive lol\n\nAny advice on how to make the most of my internship? I already know python and sql very well for reference", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I make the most of my internship", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s7x46", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678911423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For reference, here is the JD they used (same for normal and intern roles lol) &lt;a href=\"https://boards.greenhouse.io/icapitalnetwork/jobs/6628661002\"&gt;https://boards.greenhouse.io/icapitalnetwork/jobs/6628661002&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I\u2019m very excited to have been accepted because I haven\u2019t worked with a lot of industry stuff before like AWS, Snowflake, etc&lt;/p&gt;\n\n&lt;p&gt;Finding housing in NYC is so difficult and expensive lol&lt;/p&gt;\n\n&lt;p&gt;Any advice on how to make the most of my internship? I already know python and sql very well for reference&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3UEUo51yjDeVtTnn4soxeRZQ1WuOG8NL6fhMG_X5lXo.jpg?auto=webp&amp;v=enabled&amp;s=a54f2cd3057f25902a68be3bd4e4cc8c3e6187ad", "width": 430, "height": 102}, "resolutions": [{"url": "https://external-preview.redd.it/3UEUo51yjDeVtTnn4soxeRZQ1WuOG8NL6fhMG_X5lXo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f4cfee524f77741a3d0575d2800665d0fd906d5", "width": 108, "height": 25}, {"url": "https://external-preview.redd.it/3UEUo51yjDeVtTnn4soxeRZQ1WuOG8NL6fhMG_X5lXo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb81672a91f70f0c786ba9549cdc7e59bcd40d90", "width": 216, "height": 51}, {"url": "https://external-preview.redd.it/3UEUo51yjDeVtTnn4soxeRZQ1WuOG8NL6fhMG_X5lXo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8205db9ffaba6dde27c79aac470593c67b3e4241", "width": 320, "height": 75}], "variants": {}, "id": "4H1lO3enS-406D_brUYyFcz09gQJmJ3-OV8Q6vyIyF8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11s7x46", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11s7x46/how_can_i_make_the_most_of_my_internship/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11s7x46/how_can_i_make_the_most_of_my_internship/", "subreddit_subscribers": 93289, "created_utc": 1678911423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've stumbled upon a European job site for data jobs and I've gone through some job ads for the DACH region, mainly for Big 4 companies.\n\nIs this salary really typical for managers with 5+ years of experience? Or is this one of these job ads from companies that hope to employ cheap immigrants?\n\nEven assuming that it's the bottom amount (\"from\"), I find the salary shockingly low. (I know the realities of the countries, including the cost of living, quite well, just haven't worked there in the area of data).\n\n&amp;#x200B;\n\nhttps://preview.redd.it/s78wh3vezyna1.png?width=672&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=011409db5c60b76f9c7d3a45c79db136bdbfae55", "author_fullname": "t2_ti6b0o4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data jobs in Germany, Austria, Switzerland (DACH region)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "media_metadata": {"s78wh3vezyna1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/s78wh3vezyna1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8269605643affc833877e97a5e6680904292c2a1"}, {"y": 109, "x": 216, "u": "https://preview.redd.it/s78wh3vezyna1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=149c06be929caba3d0e2e544ca76db0b8edb3207"}, {"y": 162, "x": 320, "u": "https://preview.redd.it/s78wh3vezyna1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1208ab937277c4af9c0677fb080733f3c12ccd5f"}, {"y": 325, "x": 640, "u": "https://preview.redd.it/s78wh3vezyna1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adc31a5bf4bb8528cd23c3b27bbbe163118fd554"}], "s": {"y": 342, "x": 672, "u": "https://preview.redd.it/s78wh3vezyna1.png?width=672&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=011409db5c60b76f9c7d3a45c79db136bdbfae55"}, "id": "s78wh3vezyna1"}}, "name": "t3_11s9shm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/j0E9DAKCQiuBUsnL3ljT41f8MIiVmml_6deVYKLSDwc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678915500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve stumbled upon a European job site for data jobs and I&amp;#39;ve gone through some job ads for the DACH region, mainly for Big 4 companies.&lt;/p&gt;\n\n&lt;p&gt;Is this salary really typical for managers with 5+ years of experience? Or is this one of these job ads from companies that hope to employ cheap immigrants?&lt;/p&gt;\n\n&lt;p&gt;Even assuming that it&amp;#39;s the bottom amount (&amp;quot;from&amp;quot;), I find the salary shockingly low. (I know the realities of the countries, including the cost of living, quite well, just haven&amp;#39;t worked there in the area of data).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s78wh3vezyna1.png?width=672&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=011409db5c60b76f9c7d3a45c79db136bdbfae55\"&gt;https://preview.redd.it/s78wh3vezyna1.png?width=672&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=011409db5c60b76f9c7d3a45c79db136bdbfae55&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11s9shm", "is_robot_indexable": true, "report_reasons": null, "author": "user2401372", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11s9shm/data_jobs_in_germany_austria_switzerland_dach/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11s9shm/data_jobs_in_germany_austria_switzerland_dach/", "subreddit_subscribers": 93289, "created_utc": 1678915500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. Me and some colleagues are working on various script files (mainly written in R and Python) whose goal is to perform some ELT tasks and generate some data analysis. \n\nI was wondering how do u manage your documentation for these types of scripts:\n\n- We have code documentation (comments) inside the script to describe exactly what tasks are performed by the code\n\n- We have a more \u00ab\u00a0business \u00bb documentation, which consists in one Readme file per script\n\n- everything is well versionned using Git. \n\nAll of the things I described work well, but I would need A more \u00abhigh level\u00bb documentation, which could be a sort of user friendly documentation website with the list of all scripts, the business description (sourcing from readmes), and links to github scripts \u00b4 codes. \n\nDo tools like confluence allow us to do these tasks or do you have any orges recommandations ?\n\nThank u :)", "author_fullname": "t2_ce3go0rt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scripts documentation ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sbho0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678919214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. Me and some colleagues are working on various script files (mainly written in R and Python) whose goal is to perform some ELT tasks and generate some data analysis. &lt;/p&gt;\n\n&lt;p&gt;I was wondering how do u manage your documentation for these types of scripts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;We have code documentation (comments) inside the script to describe exactly what tasks are performed by the code&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We have a more \u00ab\u00a0business \u00bb documentation, which consists in one Readme file per script&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;everything is well versionned using Git. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All of the things I described work well, but I would need A more \u00abhigh level\u00bb documentation, which could be a sort of user friendly documentation website with the list of all scripts, the business description (sourcing from readmes), and links to github scripts \u00b4 codes. &lt;/p&gt;\n\n&lt;p&gt;Do tools like confluence allow us to do these tasks or do you have any orges recommandations ?&lt;/p&gt;\n\n&lt;p&gt;Thank u :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11sbho0", "is_robot_indexable": true, "report_reasons": null, "author": "nad_pub", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sbho0/scripts_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sbho0/scripts_documentation/", "subreddit_subscribers": 93289, "created_utc": 1678919214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "***Edit****: clarifying the question as it is not specified*\n\n*Which of the 3 options listed below ( Msc, courses with certs, boot camps ) is more relevant to gain required experience/knowledge for a data engineer role , for someone with software engineering experience ?*\n\n\\---\n\nHi everyone,\n\nFirst of all, sorry if that post sounds like it has been treated already multiple time, but I would like extra thoughts.\n\nAs a software developer with 12 y experience, I want to switch to a data engineer role. I don't have a degree nor a certification.\n\nWith the data tech ecosystem having grown so fast, I think I need to learn other things to get into DE area but I cannot manage to see what is the most relevant for me, and the list of possibilities is quite big.\n\nLet say that the main goal is be more attractive to companies for that role and to have some good theorical + some basic hands on experience on some of the new cool tools used by data engineers.\n\nFor a more practical experience I m thinking of building pet projects on my own, or contributing to open source projects.\n\n**I see 3 categories ( maybe there are more ) for the learning aspect, and I would like thoughts about the most relevant one.**\n\n1. **Msc online** are very expensives, I can afford them, but would the return of investement be worth ?\n2. **courses/ learning path** giving certification on [https://www.coursera.org/](https://www.coursera.org/), [https://www.dataquest.io/](https://www.dataquest.io/) and others.\n3. **data boot camps** they seem to be in the middle between the first two options.\n\n&amp;#x200B;\n\nmy background is, I guess, a factor that plays a role in the decision, so I leave it here as well.\n\nI have a quite strong experience in the java ecosystem plus some good sql / nosql db experience.\n\nWith some experience in python ( not in big data field ), scala, elasticsearch, kafka, hadoop, aws cli/sdk for s3, elasticbeanstalk, lambdas and dynamodb.\n\nNon professional experience in kafka-stream, reactive programming.\n\nI m also confortable building CI/CD pipeline with github actions and aws cli.\n\n&amp;#x200B;\n\nThanks for the ones who reach that part of the post without missing a line ;)", "author_fullname": "t2_nj7sjynk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career switch advice , choosing the right learning form", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s8xbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678985589.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678913616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Edit&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: clarifying the question as it is not specified&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Which of the 3 options listed below ( Msc, courses with certs, boot camps ) is more relevant to gain required experience/knowledge for a data engineer role , for someone with software engineering experience ?&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;First of all, sorry if that post sounds like it has been treated already multiple time, but I would like extra thoughts.&lt;/p&gt;\n\n&lt;p&gt;As a software developer with 12 y experience, I want to switch to a data engineer role. I don&amp;#39;t have a degree nor a certification.&lt;/p&gt;\n\n&lt;p&gt;With the data tech ecosystem having grown so fast, I think I need to learn other things to get into DE area but I cannot manage to see what is the most relevant for me, and the list of possibilities is quite big.&lt;/p&gt;\n\n&lt;p&gt;Let say that the main goal is be more attractive to companies for that role and to have some good theorical + some basic hands on experience on some of the new cool tools used by data engineers.&lt;/p&gt;\n\n&lt;p&gt;For a more practical experience I m thinking of building pet projects on my own, or contributing to open source projects.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I see 3 categories ( maybe there are more ) for the learning aspect, and I would like thoughts about the most relevant one.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Msc online&lt;/strong&gt; are very expensives, I can afford them, but would the return of investement be worth ?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;courses/ learning path&lt;/strong&gt; giving certification on &lt;a href=\"https://www.coursera.org/\"&gt;https://www.coursera.org/&lt;/a&gt;, &lt;a href=\"https://www.dataquest.io/\"&gt;https://www.dataquest.io/&lt;/a&gt; and others.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;data boot camps&lt;/strong&gt; they seem to be in the middle between the first two options.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;my background is, I guess, a factor that plays a role in the decision, so I leave it here as well.&lt;/p&gt;\n\n&lt;p&gt;I have a quite strong experience in the java ecosystem plus some good sql / nosql db experience.&lt;/p&gt;\n\n&lt;p&gt;With some experience in python ( not in big data field ), scala, elasticsearch, kafka, hadoop, aws cli/sdk for s3, elasticbeanstalk, lambdas and dynamodb.&lt;/p&gt;\n\n&lt;p&gt;Non professional experience in kafka-stream, reactive programming.&lt;/p&gt;\n\n&lt;p&gt;I m also confortable building CI/CD pipeline with github actions and aws cli.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for the ones who reach that part of the post without missing a line ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?auto=webp&amp;v=enabled&amp;s=27d1fbfa41af08ab445125a5830ec1430f088fbe", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b3ed072c56ec3ae0f01b63b31f88e9dcfedd755", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6aac48616e1f390d96fa9ec298d73476bb0c10a0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d3c1980b3aaf1c175d31609ebfcb55b32519959", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a9b5b8270e6b546ed7251d47ac6e5acf7403cd1f", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07b9aa4e8ce45c44a350f4ee3e281f47dbc30394", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=642d9f69a16a0baabc5c6d9c77a1a7de26ad4e49", "width": 1080, "height": 565}], "variants": {}, "id": "nnmvZitIdtl3UJSov7ZcIqMwnFG7OvGcVvkNPR4Vsrc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11s8xbk", "is_robot_indexable": true, "report_reasons": null, "author": "Educational-Feed5480", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11s8xbk/career_switch_advice_choosing_the_right_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11s8xbk/career_switch_advice_choosing_the_right_learning/", "subreddit_subscribers": 93289, "created_utc": 1678913616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have any experience with qwak? and can shed light on the pros cons, and how it stands vs other platforms?  \nThe background is that I'm a DS trying to improve the data engineering infrastructure in my company, the data is time series and audio from &gt; 100K sensors. We are looking to improve our infrastructure, and someone suggested this platform.  The desired infra would allow the DSs to create new features seamlessly, there is no need for real-time processing, and daily jobs are good enough. Hope this made some sense :)  \n\n\nI don't have much experience with the engineering side, and any suggestions would be appreciated.", "author_fullname": "t2_1ioyllpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any experience with qwak?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s6sj3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678908979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any experience with qwak? and can shed light on the pros cons, and how it stands vs other platforms?&lt;br/&gt;\nThe background is that I&amp;#39;m a DS trying to improve the data engineering infrastructure in my company, the data is time series and audio from &amp;gt; 100K sensors. We are looking to improve our infrastructure, and someone suggested this platform.  The desired infra would allow the DSs to create new features seamlessly, there is no need for real-time processing, and daily jobs are good enough. Hope this made some sense :)  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have much experience with the engineering side, and any suggestions would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11s6sj3", "is_robot_indexable": true, "report_reasons": null, "author": "zoshka", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11s6sj3/any_experience_with_qwak/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11s6sj3/any_experience_with_qwak/", "subreddit_subscribers": 93289, "created_utc": 1678908979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all. I recently started a position at a manufacturer where the entire IT and data process is done through SAP. They are trying to slowly get themselves off of SAP and hired me due to my experience in more modern ETL/Data platforms.\nI was just wondering if anyone here had experience in modernizing an SAP platform and the best tools for the job. Thanks", "author_fullname": "t2_6ra3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with SAP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11swnax", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all. I recently started a position at a manufacturer where the entire IT and data process is done through SAP. They are trying to slowly get themselves off of SAP and hired me due to my experience in more modern ETL/Data platforms.\nI was just wondering if anyone here had experience in modernizing an SAP platform and the best tools for the job. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11swnax", "is_robot_indexable": true, "report_reasons": null, "author": "deemerritt", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11swnax/working_with_sap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11swnax/working_with_sap/", "subreddit_subscribers": 93289, "created_utc": 1678979041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My day to day job mostly uses GBQ and dbt. So, I was doing a little bit of research on orchestration tools. I was looking at AirFlow initially. It looked complicated (at least the installation part)\n\nSo, I installed Prefect to my local machine and connected it to Prefect cloud. And I was experimenting.\n\nSo, this is what I did using Prefect.\n- Extract data from our production DB. I used pandas to load the tables. The table has about 2000 rows with about 100 columns.\n- There were some sensitive information. I hashed it as a transformation stage. And converted all the fields into string (if not it throws an error when I load it into GBQ)\n- Loaded it to GBQ\n\nI have a few questions here. Is Prefect overkill for my use case? What could have done better? \n\nLet\u2019s say if I have to update the tables in GBQ every 7 days. Is it possible to schedule it on Prefect? For scheduling part, do we need to have a VM running 24/7? \n\nWould appreciate your feedbacks. Not gonna lie, when the job was successful. It felt like an achievement.\n\nThank you.", "author_fullname": "t2_emzh9atv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created my first Prefect workflow.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sve4i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678976150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My day to day job mostly uses GBQ and dbt. So, I was doing a little bit of research on orchestration tools. I was looking at AirFlow initially. It looked complicated (at least the installation part)&lt;/p&gt;\n\n&lt;p&gt;So, I installed Prefect to my local machine and connected it to Prefect cloud. And I was experimenting.&lt;/p&gt;\n\n&lt;p&gt;So, this is what I did using Prefect.\n- Extract data from our production DB. I used pandas to load the tables. The table has about 2000 rows with about 100 columns.\n- There were some sensitive information. I hashed it as a transformation stage. And converted all the fields into string (if not it throws an error when I load it into GBQ)\n- Loaded it to GBQ&lt;/p&gt;\n\n&lt;p&gt;I have a few questions here. Is Prefect overkill for my use case? What could have done better? &lt;/p&gt;\n\n&lt;p&gt;Let\u2019s say if I have to update the tables in GBQ every 7 days. Is it possible to schedule it on Prefect? For scheduling part, do we need to have a VM running 24/7? &lt;/p&gt;\n\n&lt;p&gt;Would appreciate your feedbacks. Not gonna lie, when the job was successful. It felt like an achievement.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sve4i", "is_robot_indexable": true, "report_reasons": null, "author": "MaintenanceSad6825", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sve4i/i_created_my_first_prefect_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sve4i/i_created_my_first_prefect_workflow/", "subreddit_subscribers": 93289, "created_utc": 1678976150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a tool like Airbyte but for unstructured files? \n\nSomething that transfers between different file systems or object storage systems like S3 but not imposing a structured schema is what I'm looking for. The replication, extract/load and auth functiona are really what I want to achieve - no transform functionality is necessary.\n\nLike S3 to sftp, https to filesystem, etc.", "author_fullname": "t2_4i8pl31g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tool like Airbyte but for unstructured data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sucai", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678974579.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678973665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a tool like Airbyte but for unstructured files? &lt;/p&gt;\n\n&lt;p&gt;Something that transfers between different file systems or object storage systems like S3 but not imposing a structured schema is what I&amp;#39;m looking for. The replication, extract/load and auth functiona are really what I want to achieve - no transform functionality is necessary.&lt;/p&gt;\n\n&lt;p&gt;Like S3 to sftp, https to filesystem, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sucai", "is_robot_indexable": true, "report_reasons": null, "author": "knowledgebass", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sucai/tool_like_airbyte_but_for_unstructured_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sucai/tool_like_airbyte_but_for_unstructured_data/", "subreddit_subscribers": 93289, "created_utc": 1678973665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my job I've inherited a Synapse Workspace with a dedicated SQL Pool serverless.\nOn this serverless we do have a SQL Server 2012 and I was wondering if there was a way to run some health checks (like you run scandisk on your memory).\n\nDoes someone know how to achieve it? Or which guidelines should be used?", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you check if your DWH is not corrupted?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11su26h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678973007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my job I&amp;#39;ve inherited a Synapse Workspace with a dedicated SQL Pool serverless.\nOn this serverless we do have a SQL Server 2012 and I was wondering if there was a way to run some health checks (like you run scandisk on your memory).&lt;/p&gt;\n\n&lt;p&gt;Does someone know how to achieve it? Or which guidelines should be used?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11su26h", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11su26h/how_do_you_check_if_your_dwh_is_not_corrupted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11su26h/how_do_you_check_if_your_dwh_is_not_corrupted/", "subreddit_subscribers": 93289, "created_utc": 1678973007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to come up with a modern blueprint for an on-premises data lake and data warehouse.\n\nI was thinking to use pyspark and use spark as compute engine.\n\nFor storage, I am looking at Apache Iceberg (as an alternative to postgresql, mongodb etc), but I am not sure I understand how clustering works with Iceberg.\n\nHow can Iceberg best be setup in an on-premises solution as a data cluster?\n\nCan I use Hive? Are there any limitations or alternatives to Hive with Iceberg?", "author_fullname": "t2_k7zel", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Iceberg as storage for on-premise data store (cluster)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ssjny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678969219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to come up with a modern blueprint for an on-premises data lake and data warehouse.&lt;/p&gt;\n\n&lt;p&gt;I was thinking to use pyspark and use spark as compute engine.&lt;/p&gt;\n\n&lt;p&gt;For storage, I am looking at Apache Iceberg (as an alternative to postgresql, mongodb etc), but I am not sure I understand how clustering works with Iceberg.&lt;/p&gt;\n\n&lt;p&gt;How can Iceberg best be setup in an on-premises solution as a data cluster?&lt;/p&gt;\n\n&lt;p&gt;Can I use Hive? Are there any limitations or alternatives to Hive with Iceberg?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11ssjny", "is_robot_indexable": true, "report_reasons": null, "author": "hgaronfolo", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ssjny/apache_iceberg_as_storage_for_onpremise_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ssjny/apache_iceberg_as_storage_for_onpremise_data/", "subreddit_subscribers": 93289, "created_utc": 1678969219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are on-premises RTU'S which are currently using the medina protocol and Palo Alto gateway. I would like to build a data pipeline to transfer the data from on-premises to AWS cloud through VPN. \n\nAdditional info, there is a postgres database running in an ec2 instance in which the data is being stored currently. \n\nWhat can I do to achieve this?", "author_fullname": "t2_6wgyvyl5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DATA pipeline from on premise RTU's to AWS Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11smn5v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678953699.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678949594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are on-premises RTU&amp;#39;S which are currently using the medina protocol and Palo Alto gateway. I would like to build a data pipeline to transfer the data from on-premises to AWS cloud through VPN. &lt;/p&gt;\n\n&lt;p&gt;Additional info, there is a postgres database running in an ec2 instance in which the data is being stored currently. &lt;/p&gt;\n\n&lt;p&gt;What can I do to achieve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11smn5v", "is_robot_indexable": true, "report_reasons": null, "author": "aimonster7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11smn5v/data_pipeline_from_on_premise_rtus_to_aws_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11smn5v/data_pipeline_from_on_premise_rtus_to_aws_cloud/", "subreddit_subscribers": 93289, "created_utc": 1678949594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we are looking to hire an experienced data engineer / mlops , I know it's not the same, but the company is not big enough to separate the positions.   \nWhat are some good practices in designing an interview process?  \nAre home assignments a thing in those fields?  \nI'm in the DS team, and don't have much experience with DE, and I feel the interview processes are different for DS than a DE", "author_fullname": "t2_1ioyllpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the common practices for a data engineer, mlops interview process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11s6vi6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678909172.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we are looking to hire an experienced data engineer / mlops , I know it&amp;#39;s not the same, but the company is not big enough to separate the positions.&lt;br/&gt;\nWhat are some good practices in designing an interview process?&lt;br/&gt;\nAre home assignments a thing in those fields?&lt;br/&gt;\nI&amp;#39;m in the DS team, and don&amp;#39;t have much experience with DE, and I feel the interview processes are different for DS than a DE&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "11s6vi6", "is_robot_indexable": true, "report_reasons": null, "author": "zoshka", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11s6vi6/what_are_the_common_practices_for_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11s6vi6/what_are_the_common_practices_for_a_data_engineer/", "subreddit_subscribers": 93289, "created_utc": 1678909172.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title", "author_fullname": "t2_c6w52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "do you think Zhamak has any idea how much time has now been wasted on orgs discussing if we should 'do a data mesh'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11t1e0u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678989600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11t1e0u", "is_robot_indexable": true, "report_reasons": null, "author": "EmergenL", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11t1e0u/do_you_think_zhamak_has_any_idea_how_much_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11t1e0u/do_you_think_zhamak_has_any_idea_how_much_time/", "subreddit_subscribers": 93289, "created_utc": 1678989600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why you might not even need a data platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_11sz5li", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cXGLlbuS0Rs6HLENqVxmbwrMfpsEW-85nBoDLafyKEg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678984722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "getdozer.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://getdozer.io/blog/why-you-might-not-need-a-data-platform", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?auto=webp&amp;v=enabled&amp;s=79536e18af301dba74e5c29e6718a2ae65d51da9", "width": 6848, "height": 3438}, "resolutions": [{"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3447de0d00b869f45c48aefd87f4d37eb81257c9", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e22d6500d7e64c02415b75cf0f8b427bb853f5ac", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ee1b3b80081f099248e2252ac19ab373261e8fd", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26f2e2eadcdff278a987872ae6fea6876541f61d", "width": 640, "height": 321}, {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=261163137064f5471f818767eed97f3cc7f33951", "width": 960, "height": 481}, {"url": "https://external-preview.redd.it/f3xpVevQfEmEke9E6rp8i-j0u4t72pDnXhGkAMA8QH0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acd84e6f8183473fd1ab6b2a9373908b49df9deb", "width": 1080, "height": 542}], "variants": {}, "id": "2XxngX9EhUhZzOGE9yT7YlH9c52y5S7afU_8wIpBxMU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11sz5li", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sz5li/why_you_might_not_even_need_a_data_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://getdozer.io/blog/why-you-might-not-need-a-data-platform", "subreddit_subscribers": 93289, "created_utc": 1678984722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all I'm just in the middle of architecting a system where an important requirement is going to be storing information on \"where\" it went. Key info we'll need to store is\n\n\\- Where it went\n\n\\- Schema + Evolution\n\n\\- Arbitrary information about the size and shape of the dataset.\n\n\\- Accessible programatically/can have an API put in front of it.\n\n\\- Works with Datasets stored In AWS that might be in the Multi-TB/PB range.\n\nI'm obviously looking at Hive but at the moment we aren't a JVM company and that requires us to start worrying about Java world  as well (although granted some of our workloads are in Databricks and I've worked in Scala before so it's probably manageable). Are there any less cumbersome options for storing information about datasets around nowadays?\n\nThus far most of the options are things like [https://datahubproject.io/](https://datahubproject.io/) where I'm a little dubious about what it does and it's a little vague on if it's just storing metadata or doing transformations.\n\nI am working with a proprietary data processing system so ideally if it can be extended to cover that as one of our metadata sources too this would also be great if not a show-stopper. ", "author_fullname": "t2_9u69ulzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Metastores/Data Catalogs that aren't Hive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sws79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678979543.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all I&amp;#39;m just in the middle of architecting a system where an important requirement is going to be storing information on &amp;quot;where&amp;quot; it went. Key info we&amp;#39;ll need to store is&lt;/p&gt;\n\n&lt;p&gt;- Where it went&lt;/p&gt;\n\n&lt;p&gt;- Schema + Evolution&lt;/p&gt;\n\n&lt;p&gt;- Arbitrary information about the size and shape of the dataset.&lt;/p&gt;\n\n&lt;p&gt;- Accessible programatically/can have an API put in front of it.&lt;/p&gt;\n\n&lt;p&gt;- Works with Datasets stored In AWS that might be in the Multi-TB/PB range.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m obviously looking at Hive but at the moment we aren&amp;#39;t a JVM company and that requires us to start worrying about Java world  as well (although granted some of our workloads are in Databricks and I&amp;#39;ve worked in Scala before so it&amp;#39;s probably manageable). Are there any less cumbersome options for storing information about datasets around nowadays?&lt;/p&gt;\n\n&lt;p&gt;Thus far most of the options are things like &lt;a href=\"https://datahubproject.io/\"&gt;https://datahubproject.io/&lt;/a&gt; where I&amp;#39;m a little dubious about what it does and it&amp;#39;s a little vague on if it&amp;#39;s just storing metadata or doing transformations.&lt;/p&gt;\n\n&lt;p&gt;I am working with a proprietary data processing system so ideally if it can be extended to cover that as one of our metadata sources too this would also be great if not a show-stopper. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11sws79", "is_robot_indexable": true, "report_reasons": null, "author": "tdatas", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sws79/metastoresdata_catalogs_that_arent_hive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sws79/metastoresdata_catalogs_that_arent_hive/", "subreddit_subscribers": 93289, "created_utc": 1678979359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All,\n\nWhat are the best replacement tools for SSIS, which we used to execute snowflake procedures by creating SSIS packages; one that comes to mind is Airflow. Are there other tools available to perform the same task the SSIS does?\n\nThank you", "author_fullname": "t2_7qs0ir3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSIS Replacement for production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11swovi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678979140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;What are the best replacement tools for SSIS, which we used to execute snowflake procedures by creating SSIS packages; one that comes to mind is Airflow. Are there other tools available to perform the same task the SSIS does?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11swovi", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Archer3356", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11swovi/ssis_replacement_for_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11swovi/ssis_replacement_for_production/", "subreddit_subscribers": 93289, "created_utc": 1678979140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Calling all BI developers! I want to learn from your experiences in creating and maintaining dashboards. If you're involved in the development of business intelligence dashboards, we invite you to take our survey on best practices for ensuring the continuity of your dashboards. Your valuable insights can help inform our understanding of how to create effective and sustainable BI solutions. Take the survey now and share your expertise with the community! I'll share the results in a week.\n\n[https://forms.office.com/e/h3ENF9LNzQ](https://forms.office.com/e/h3ENF9LNzQ)", "author_fullname": "t2_sjc8bva0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business Intelligence Dashboard Continuity/Maintenance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11sszvk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678970354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Calling all BI developers! I want to learn from your experiences in creating and maintaining dashboards. If you&amp;#39;re involved in the development of business intelligence dashboards, we invite you to take our survey on best practices for ensuring the continuity of your dashboards. Your valuable insights can help inform our understanding of how to create effective and sustainable BI solutions. Take the survey now and share your expertise with the community! I&amp;#39;ll share the results in a week.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forms.office.com/e/h3ENF9LNzQ\"&gt;https://forms.office.com/e/h3ENF9LNzQ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11sszvk", "is_robot_indexable": true, "report_reasons": null, "author": "Total_Telephone_1260", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11sszvk/business_intelligence_dashboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11sszvk/business_intelligence_dashboard/", "subreddit_subscribers": 93289, "created_utc": 1678970354.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}