{"kind": "Listing", "data": {"after": "t3_11pzqqa", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "It's really sad and frustrating to lose so much hard work and a possible opportunity due to a stupid mistake.\n\nFor the past six days, I have been working day and night on developing an entire data streaming system using Docker (from streaming APIs and Kafka consumers/producers to the MLspark model). This was a task for a company I am interviewing with for a big data engineering position.\n\nEvery day, I told myself I needed to start a repository or back up the code somewhere, but for an unknown reason, I didn't. I kept procrastinating.\n\nI was so excited to finish the project and share it with the interviewer on GitHub as soon as possible. But I told myself, \"One last test,\" and that's when I accidentally deleted all my code.\n\nThe main project directory was mounted by the Docker Spark container, which would write the output to the folder provided. However, it needed to remove or empty everything in the provided directory before writing. And that's how I lost everything.\n\nI was so pissed off and spent three hours trying different methods to retrieve my work, but I couldn't. Now, I don't even feel like coding anymore.\n\n(Note: I know I should have versioned the code with Git, which I usually do. But this time, I thought, \"What could go wrong?\")", "author_fullname": "t2_a7urc8tl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lost it all.... in a sec!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q1hz2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 108, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 108, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678686643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s really sad and frustrating to lose so much hard work and a possible opportunity due to a stupid mistake.&lt;/p&gt;\n\n&lt;p&gt;For the past six days, I have been working day and night on developing an entire data streaming system using Docker (from streaming APIs and Kafka consumers/producers to the MLspark model). This was a task for a company I am interviewing with for a big data engineering position.&lt;/p&gt;\n\n&lt;p&gt;Every day, I told myself I needed to start a repository or back up the code somewhere, but for an unknown reason, I didn&amp;#39;t. I kept procrastinating.&lt;/p&gt;\n\n&lt;p&gt;I was so excited to finish the project and share it with the interviewer on GitHub as soon as possible. But I told myself, &amp;quot;One last test,&amp;quot; and that&amp;#39;s when I accidentally deleted all my code.&lt;/p&gt;\n\n&lt;p&gt;The main project directory was mounted by the Docker Spark container, which would write the output to the folder provided. However, it needed to remove or empty everything in the provided directory before writing. And that&amp;#39;s how I lost everything.&lt;/p&gt;\n\n&lt;p&gt;I was so pissed off and spent three hours trying different methods to retrieve my work, but I couldn&amp;#39;t. Now, I don&amp;#39;t even feel like coding anymore.&lt;/p&gt;\n\n&lt;p&gt;(Note: I know I should have versioned the code with Git, which I usually do. But this time, I thought, &amp;quot;What could go wrong?&amp;quot;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11q1hz2", "is_robot_indexable": true, "report_reasons": null, "author": "Taylankab", "discussion_type": null, "num_comments": 66, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11q1hz2/lost_it_all_in_a_sec/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11q1hz2/lost_it_all_in_a_sec/", "subreddit_subscribers": 856815, "created_utc": 1678686643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, \n\nRecently I increasingly came across unit testing for data science projects. So far I am roughly familiar with the concept, even though we are not using it at all in the whole department. I would like to properly set up a unit testing framework for my department, especially because we have a lot of stats heavy and code light people in our team. \n\nOne thing I haven't properly understood yet, we are working with quite an amount of data which is normally a piece of garbage and we have to do a lot of preprocessing in order to make it clean. How would you set up in such an environment (very long runtimes for data preparation due to large data) a unit testing framework, since I can not run multiple parameter settings to test somethings because it will just take too long.\n\nThanks a lot in advance, this post here should not only be an question but more a discussion of unit testing in general. \n\nPaul", "author_fullname": "t2_60p6w78m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unit Testing for Big Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q3qa5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678694815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, &lt;/p&gt;\n\n&lt;p&gt;Recently I increasingly came across unit testing for data science projects. So far I am roughly familiar with the concept, even though we are not using it at all in the whole department. I would like to properly set up a unit testing framework for my department, especially because we have a lot of stats heavy and code light people in our team. &lt;/p&gt;\n\n&lt;p&gt;One thing I haven&amp;#39;t properly understood yet, we are working with quite an amount of data which is normally a piece of garbage and we have to do a lot of preprocessing in order to make it clean. How would you set up in such an environment (very long runtimes for data preparation due to large data) a unit testing framework, since I can not run multiple parameter settings to test somethings because it will just take too long.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot in advance, this post here should not only be an question but more a discussion of unit testing in general. &lt;/p&gt;\n\n&lt;p&gt;Paul&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11q3qa5", "is_robot_indexable": true, "report_reasons": null, "author": "Habenzu", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11q3qa5/unit_testing_for_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11q3qa5/unit_testing_for_big_data/", "subreddit_subscribers": 856815, "created_utc": 1678694815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing confidenceinterval, the long missing python library for computing confidence intervals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q5bps", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_k8k9b", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "[https://github.com/jacobgil/confidenceinterval](https://github.com/jacobgil/confidenceinterval)\n\npip install confidenceinterval\n\ntldr: You don't have an excuse anymore to not use confidence intervals !\n\n&amp;#x200B;\n\nIn statistics, confidence intervals are commonly reported along accuracy metrics to help interpret them.\n\nFor example, an AUC metric might be 0.9 but if the 95% confidence interval is in the range \\[0.7, 0.96\\], we can't confidently say we didn't just get lucky - we should be really careful making decisions around that result.\n\nMore formally, a confidence interval gives us a range on where the true unknown accuracy metric could be, and a 95% confidence interval means that if we would repeat the experiment many times, 95% of the confidence-intervals we reported would have the actual true metric (which is unknown) inside them - coverage.\n\nConfidence intervals are usually computed analytically, by making some assumptions about the metric distribution and using the central limit theorem,or by using bootstrapping - resampling the results again and again, computing the metric, and checking the resulting distribution.\n\nHowever, in the python data science world, I rarely saw these being used. I guess part of the reason is the culture, where many data science practitioners don't come from the statistics world. But I think the main reason is that there aren't easy to use libraries that do this. While in the R language there is fantastic support for confidence intervals, for python there are mostly scattered pieces of code and blog posts.\n\n&amp;#x200B;\n\nThe confidenceinterval package keeps the clean and popular scikit-learn metric API,\n\ne.g roc\\_auc\\_score(y\\_true, y\\_pred), but also returns confidence intervals.\n\nIt supports analytical computations for many methods (including AUC with the delong method, or F1 with macro, micro averaging, following the recent results from [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8936911/#APP2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8936911/#APP2), or binary proportions like the TPR using binomial CI methods like the wilson interval).\n\nIt can be easily switched to using bootstrapping (with several supported bootstrapping methods),\n\nand also gives you a way to easily compute the confidence interval for any metric with bootstrapping.", "author_fullname": "t2_k8k9b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[P] Introducing confidenceinterval, the long missing python library for computing confidence intervals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "four", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11orezx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 115, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Project", "can_mod_post": false, "score": 115, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678561930.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678559767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/jacobgil/confidenceinterval\"&gt;https://github.com/jacobgil/confidenceinterval&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;pip install confidenceinterval&lt;/p&gt;\n\n&lt;p&gt;tldr: You don&amp;#39;t have an excuse anymore to not use confidence intervals !&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In statistics, confidence intervals are commonly reported along accuracy metrics to help interpret them.&lt;/p&gt;\n\n&lt;p&gt;For example, an AUC metric might be 0.9 but if the 95% confidence interval is in the range [0.7, 0.96], we can&amp;#39;t confidently say we didn&amp;#39;t just get lucky - we should be really careful making decisions around that result.&lt;/p&gt;\n\n&lt;p&gt;More formally, a confidence interval gives us a range on where the true unknown accuracy metric could be, and a 95% confidence interval means that if we would repeat the experiment many times, 95% of the confidence-intervals we reported would have the actual true metric (which is unknown) inside them - coverage.&lt;/p&gt;\n\n&lt;p&gt;Confidence intervals are usually computed analytically, by making some assumptions about the metric distribution and using the central limit theorem,or by using bootstrapping - resampling the results again and again, computing the metric, and checking the resulting distribution.&lt;/p&gt;\n\n&lt;p&gt;However, in the python data science world, I rarely saw these being used. I guess part of the reason is the culture, where many data science practitioners don&amp;#39;t come from the statistics world. But I think the main reason is that there aren&amp;#39;t easy to use libraries that do this. While in the R language there is fantastic support for confidence intervals, for python there are mostly scattered pieces of code and blog posts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The confidenceinterval package keeps the clean and popular scikit-learn metric API,&lt;/p&gt;\n\n&lt;p&gt;e.g roc_auc_score(y_true, y_pred), but also returns confidence intervals.&lt;/p&gt;\n\n&lt;p&gt;It supports analytical computations for many methods (including AUC with the delong method, or F1 with macro, micro averaging, following the recent results from &lt;a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8936911/#APP2\"&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8936911/#APP2&lt;/a&gt;, or binary proportions like the TPR using binomial CI methods like the wilson interval).&lt;/p&gt;\n\n&lt;p&gt;It can be easily switched to using bootstrapping (with several supported bootstrapping methods),&lt;/p&gt;\n\n&lt;p&gt;and also gives you a way to easily compute the confidence interval for any metric with bootstrapping.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?auto=webp&amp;v=enabled&amp;s=a5d2ecdd7772d8bc508c6c31bf66a3ebf6a36f9b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84265ed517381e53abccab809f4457681fe3c639", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7bb99fba7a31874d7c683916fa1cefb7bc3e50b9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17b19e9c1fe0d262fc2327c44dd600b13b7e0db4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f49c39b998f64942a836cf968ffa02593e39f60", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12f1484661166648bac2efcb4bb90c0fd6d50b6b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b33e1ea5932c8029ec48c2f1082eca2001226aac", "width": 1080, "height": 540}], "variants": {}, "id": "VEtVW8Q9v0zCOQrmkPZvdkf-myANUJNotk7vDz0TFtA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11orezx", "is_robot_indexable": true, "report_reasons": null, "author": "jacobgil", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/11orezx/p_introducing_confidenceinterval_the_long_missing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/MachineLearning/comments/11orezx/p_introducing_confidenceinterval_the_long_missing/", "subreddit_subscribers": 2598139, "created_utc": 1678559767.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1678700803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/MachineLearning/comments/11orezx/p_introducing_confidenceinterval_the_long_missing/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?auto=webp&amp;v=enabled&amp;s=a5d2ecdd7772d8bc508c6c31bf66a3ebf6a36f9b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84265ed517381e53abccab809f4457681fe3c639", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7bb99fba7a31874d7c683916fa1cefb7bc3e50b9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17b19e9c1fe0d262fc2327c44dd600b13b7e0db4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f49c39b998f64942a836cf968ffa02593e39f60", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12f1484661166648bac2efcb4bb90c0fd6d50b6b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/CROI3aTcH3zmr0jZ1tMPkj12KS9h-bmcBzIW_MdSL6Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b33e1ea5932c8029ec48c2f1082eca2001226aac", "width": 1080, "height": 540}], "variants": {}, "id": "VEtVW8Q9v0zCOQrmkPZvdkf-myANUJNotk7vDz0TFtA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11q5bps", "is_robot_indexable": true, "report_reasons": null, "author": "jacobgil", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_11orezx", "author_flair_text_color": null, "permalink": "/r/datascience/comments/11q5bps/introducing_confidenceinterval_the_long_missing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/MachineLearning/comments/11orezx/p_introducing_confidenceinterval_the_long_missing/", "subreddit_subscribers": 856815, "created_utc": 1678700803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_p54xpfwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Turn text into flowcharts with ChatGPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 135, "top_awarded_type": null, "hide_score": false, "name": "t3_11q8c09", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rP99Yi2JpBBcaiRUtJ_8MYW8VNvC3TlKby9Wiq8018k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678710496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/kogrkirk2ina1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/kogrkirk2ina1.png?auto=webp&amp;v=enabled&amp;s=f4a6b2d85ee45fa3354ef893206d1b1b088b2bf2", "width": 1780, "height": 1728}, "resolutions": [{"url": "https://preview.redd.it/kogrkirk2ina1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6afdfe9b7a667d5bdfff4a63943c7be52d96f02b", "width": 108, "height": 104}, {"url": "https://preview.redd.it/kogrkirk2ina1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ab5b1c6d61fb094ceb9101b285ae36fdb055769", "width": 216, "height": 209}, {"url": "https://preview.redd.it/kogrkirk2ina1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2484d4258286164e7070c6d50546927a4ab6db5e", "width": 320, "height": 310}, {"url": "https://preview.redd.it/kogrkirk2ina1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0504f64ff6c0874f33a8190d475e53e06e4783e5", "width": 640, "height": 621}, {"url": "https://preview.redd.it/kogrkirk2ina1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1991684d5772469a97270abb46b4e8a4ad3700a", "width": 960, "height": 931}, {"url": "https://preview.redd.it/kogrkirk2ina1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04fa203c1b43822c47e00730d2b263c35db26ff6", "width": 1080, "height": 1048}], "variants": {}, "id": "lTZ8q2byhJItaaGNdPbIbRcZCU0ORcVZWbaCfMBuWbo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11q8c09", "is_robot_indexable": true, "report_reasons": null, "author": "colabDog", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11q8c09/turn_text_into_flowcharts_with_chatgpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/kogrkirk2ina1.png", "subreddit_subscribers": 856815, "created_utc": 1678710496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " my project about predicting used car prices. \n\nI have done feature selection, and also got the model from gridsearch.\n\n[this a based feature importances](https://preview.redd.it/8n17bm2vzfna1.png?width=992&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c148dfe05a6e6909b6c26d46d739f9c103f142fb)\n\n&amp;#x200B;\n\n[this is permutation importances](https://preview.redd.it/oq2chcq20gna1.png?width=697&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6fde29ed1393b0bc0360982888ea6229b96ce719)\n\ni've read an article that permutation importance have disadvantage. \" **If features are correlated, the permutation feature importance** **can be biased by unrealistic data instances** \". Age and mileage  have a correlation of 0.7.\n\nThank You\n\n[https://christophm.github.io/interpretable-ml-book/feature-importance.html](https://christophm.github.io/interpretable-ml-book/feature-importance.html)", "author_fullname": "t2_lzfa50u3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I use permutation importance or based feature importance GradientBoostingRegressor sklearn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "media_metadata": {"8n17bm2vzfna1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/8n17bm2vzfna1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc1d54979f78c2d7658bf5d24b2c19a2c94ea23b"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/8n17bm2vzfna1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95848852cf93b9ad4ad51e5a9d39447fca36603b"}, {"y": 225, "x": 320, "u": "https://preview.redd.it/8n17bm2vzfna1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=667d02fc49961e3f71d5a13d8f0d4ce56f80f7df"}, {"y": 451, "x": 640, "u": "https://preview.redd.it/8n17bm2vzfna1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cad10371dfcdb76ced1332711086141c63a7e73"}, {"y": 677, "x": 960, "u": "https://preview.redd.it/8n17bm2vzfna1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c9952cd2b5f8b6ff1493477564e0b3a5beb8b15"}], "s": {"y": 700, "x": 992, "u": "https://preview.redd.it/8n17bm2vzfna1.png?width=992&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c148dfe05a6e6909b6c26d46d739f9c103f142fb"}, "id": "8n17bm2vzfna1"}, "oq2chcq20gna1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 104, "x": 108, "u": "https://preview.redd.it/oq2chcq20gna1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3208d44c0c3d7c9831abc86a91a255196bde0679"}, {"y": 209, "x": 216, "u": "https://preview.redd.it/oq2chcq20gna1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1ebaf0146fc9811b478c4a214ab25aaa4b521f6"}, {"y": 309, "x": 320, "u": "https://preview.redd.it/oq2chcq20gna1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ea520a8367dfdd0b1014e7636423a2237c00eb7"}, {"y": 619, "x": 640, "u": "https://preview.redd.it/oq2chcq20gna1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41077a3503dafc782e566bd81729d1e4bba235b8"}], "s": {"y": 675, "x": 697, "u": "https://preview.redd.it/oq2chcq20gna1.png?width=697&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6fde29ed1393b0bc0360982888ea6229b96ce719"}, "id": "oq2chcq20gna1"}}, "name": "t3_11q1eaw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7gNQ40M-aRjKtosn_vjyurTKB4hLCbQxmRgZigDikpk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678686269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;my project about predicting used car prices. &lt;/p&gt;\n\n&lt;p&gt;I have done feature selection, and also got the model from gridsearch.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8n17bm2vzfna1.png?width=992&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c148dfe05a6e6909b6c26d46d739f9c103f142fb\"&gt;this a based feature importances&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/oq2chcq20gna1.png?width=697&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6fde29ed1393b0bc0360982888ea6229b96ce719\"&gt;this is permutation importances&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;ve read an article that permutation importance have disadvantage. &amp;quot; &lt;strong&gt;If features are correlated, the permutation feature importance&lt;/strong&gt; &lt;strong&gt;can be biased by unrealistic data instances&lt;/strong&gt; &amp;quot;. Age and mileage  have a correlation of 0.7.&lt;/p&gt;\n\n&lt;p&gt;Thank You&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://christophm.github.io/interpretable-ml-book/feature-importance.html\"&gt;https://christophm.github.io/interpretable-ml-book/feature-importance.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11q1eaw", "is_robot_indexable": true, "report_reasons": null, "author": "xochaels", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11q1eaw/should_i_use_permutation_importance_or_based/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11q1eaw/should_i_use_permutation_importance_or_based/", "subreddit_subscribers": 856815, "created_utc": 1678686269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a pandas dataframe that represents a time series. My time series is segmented over the phase type that the robot is performing (i.e. I have a column with the phase type per timestamp and the phases are known). Some easy examples: if the machine is cleaning something the phase is \"cleaning\", if It is moving the phase is \"moving\".\n\nI have no domain knowledge about the phase in which I am and the constraints in the value that each signal must respect. I am searching for rules with a certain confidence.\n\nI would like to say: I am in this phase, then from the data I have seen in the past, I know that for sure signal A will be less than signal B with 90% confidence. Or again, signal C should not be negative according to what I have seen in the past with 70% confidence. I want to extract historical simple rules that I would like to validate at the end of the process with a domain expert.\n\nIs there any library or method that can handle this type of problem? I didn't find much online. It seems like association rule mining, but I am working on time series and I am looking at time periods spanning the whole phase, so a very long time period.\n\nOtherwise, if I should pass from other methods as correlation/cross-correlation analysis between time series, can you point me out the best analysis I should use? And also explain to me how should I exploit my analysis result for transforming them into more simple rules like the one I have described before.", "author_fullname": "t2_i6lyoywf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mining association rules between time series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qcrhv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678722322.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678721342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a pandas dataframe that represents a time series. My time series is segmented over the phase type that the robot is performing (i.e. I have a column with the phase type per timestamp and the phases are known). Some easy examples: if the machine is cleaning something the phase is &amp;quot;cleaning&amp;quot;, if It is moving the phase is &amp;quot;moving&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I have no domain knowledge about the phase in which I am and the constraints in the value that each signal must respect. I am searching for rules with a certain confidence.&lt;/p&gt;\n\n&lt;p&gt;I would like to say: I am in this phase, then from the data I have seen in the past, I know that for sure signal A will be less than signal B with 90% confidence. Or again, signal C should not be negative according to what I have seen in the past with 70% confidence. I want to extract historical simple rules that I would like to validate at the end of the process with a domain expert.&lt;/p&gt;\n\n&lt;p&gt;Is there any library or method that can handle this type of problem? I didn&amp;#39;t find much online. It seems like association rule mining, but I am working on time series and I am looking at time periods spanning the whole phase, so a very long time period.&lt;/p&gt;\n\n&lt;p&gt;Otherwise, if I should pass from other methods as correlation/cross-correlation analysis between time series, can you point me out the best analysis I should use? And also explain to me how should I exploit my analysis result for transforming them into more simple rules like the one I have described before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qcrhv", "is_robot_indexable": true, "report_reasons": null, "author": "ginotherhino1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qcrhv/mining_association_rules_between_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qcrhv/mining_association_rules_between_time_series/", "subreddit_subscribers": 856815, "created_utc": 1678721342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! \n\nI work in a company with a two-sided marketplace. One such as doordash or Uber.\n\nComing from an engineering background with stats, I am looking for sources that can help me be better at my job.\n\nI know this might be too ad hoc, but I really think knowledge from economy can help me and make me have a better background for my position.\n\nAny help is welcome!", "author_fullname": "t2_367xvs9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Econ knowledge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qardt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678716608.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! &lt;/p&gt;\n\n&lt;p&gt;I work in a company with a two-sided marketplace. One such as doordash or Uber.&lt;/p&gt;\n\n&lt;p&gt;Coming from an engineering background with stats, I am looking for sources that can help me be better at my job.&lt;/p&gt;\n\n&lt;p&gt;I know this might be too ad hoc, but I really think knowledge from economy can help me and make me have a better background for my position.&lt;/p&gt;\n\n&lt;p&gt;Any help is welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qardt", "is_robot_indexable": true, "report_reasons": null, "author": "Quentin-Martell", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qardt/looking_for_econ_knowledge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qardt/looking_for_econ_knowledge/", "subreddit_subscribers": 856815, "created_utc": 1678716608.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/11qaizm)", "author_fullname": "t2_5njryk22", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-ended questions, and do feel free to comment your opinion on the matter below. But do you think that the launch of increasingly \"Smart\" AIs will be seen in the future as akin to the beginning of the industrial revolution (AKA a turning point in history) ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qaizm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678716048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/11qaizm\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "11qaizm", "is_robot_indexable": true, "report_reasons": null, "author": "Oldthriftmaan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1679148048207, "options": [{"text": "Yes", "id": "22042341"}, {"text": "No", "id": "22042342"}, {"text": "Nuance", "id": "22042343"}, {"text": "Something else (Comment)", "id": "22042344"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 67, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qaizm/openended_questions_and_do_feel_free_to_comment/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/datascience/comments/11qaizm/openended_questions_and_do_feel_free_to_comment/", "subreddit_subscribers": 856815, "created_utc": 1678716048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weekly Entering &amp; Transitioning - Thread 13 Mar, 2023 - 20 Mar, 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11pzhux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678680088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;\n&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;\n&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;\n&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;\n&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=\"https://www.reddit.com/r/datascience/wiki/frequently-asked-questions\"&gt;FAQ&lt;/a&gt; and Resources pages on our wiki. You can also search for answers in &lt;a href=\"https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new\"&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "new", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11pzhux", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 15, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11pzhux/weekly_entering_transitioning_thread_13_mar_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/datascience/comments/11pzhux/weekly_entering_transitioning_thread_13_mar_2023/", "subreddit_subscribers": 856815, "created_utc": 1678680088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I know cost functions is a way to better fit your model to the training data and performance measure is to see how well the model generalize but for linear regression isn\u2019t rmse used as the cost function and the performance measure, isn\u2019t it the same mathematical formula? . I\u2019m just confused", "author_fullname": "t2_9ple7b7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hello guys been trying to sleep but I can\u2019t cause the I\u2019m confused by cost function and performance measures what the difference", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11pygyk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678677066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know cost functions is a way to better fit your model to the training data and performance measure is to see how well the model generalize but for linear regression isn\u2019t rmse used as the cost function and the performance measure, isn\u2019t it the same mathematical formula? . I\u2019m just confused&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11pygyk", "is_robot_indexable": true, "report_reasons": null, "author": "Longjumping_Ad_7053", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11pygyk/hello_guys_been_trying_to_sleep_but_i_cant_cause/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11pygyk/hello_guys_been_trying_to_sleep_but_i_cant_cause/", "subreddit_subscribers": 856815, "created_utc": 1678677066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone,\n\nI had to leave my job due to some severe medical issues, and am still undergoing treatment for them and cannot currently work. I was working on Data Governance and Data Management before I had to quit, and I loved that line of work. I want to become an attorney in the field of data and information privacy, so I was set to do all sorts of trainings and have logged hours for certifications that would help me stand out as an attorney with data experience in the field.\n\nSince I am currently unemployed, I have a lot of free time and am looking to work on getting certified in things such as Data Governance, Data Management, Data Architecture, and Data Visualization. However, a lot of these certifications require logged hours, which usually would come from a job. \n\nHow can I work on these certifications without a job right now? And for any attorneys who may be in this subreddit, would these certifications even matter in the grand scheme of things?\n\nAny recommendations or advice would be greatly appreciated!\n\nEdit: Removed some personal information", "author_fullname": "t2_2a0jbore", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Becoming Certified While Unemployed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qdl0h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1678723714.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678723270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I had to leave my job due to some severe medical issues, and am still undergoing treatment for them and cannot currently work. I was working on Data Governance and Data Management before I had to quit, and I loved that line of work. I want to become an attorney in the field of data and information privacy, so I was set to do all sorts of trainings and have logged hours for certifications that would help me stand out as an attorney with data experience in the field.&lt;/p&gt;\n\n&lt;p&gt;Since I am currently unemployed, I have a lot of free time and am looking to work on getting certified in things such as Data Governance, Data Management, Data Architecture, and Data Visualization. However, a lot of these certifications require logged hours, which usually would come from a job. &lt;/p&gt;\n\n&lt;p&gt;How can I work on these certifications without a job right now? And for any attorneys who may be in this subreddit, would these certifications even matter in the grand scheme of things?&lt;/p&gt;\n\n&lt;p&gt;Any recommendations or advice would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Edit: Removed some personal information&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qdl0h", "is_robot_indexable": true, "report_reasons": null, "author": "cmondothefoxSWAT", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qdl0h/becoming_certified_while_unemployed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qdl0h/becoming_certified_while_unemployed/", "subreddit_subscribers": 856815, "created_utc": 1678723270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For example, I have a bookshelf with a lot of books. I have magazines, novels, science textbooks, math textbooks, writing textbooks, and books of poems. Suppose I were to take a picture of the front covers of these books and then design a convolutional neural network to predict whether a book is a magazine, novel, etc. Could I design a convolutional neural network such that it doesn't just learn the labels I give it, but broader labels that apply to other items to which I haven't assigned the same labels (superlabels)? How would I do that? For example, it might learn what textbook covers usually look like and then deduce that if a book cover doesn't look like the cover of a textbook, then the book isn't a math textbook, or a science textbook, or a writing textbook. Or it might notice that my science magazines and science textbooks have pictures of a microscope or something else science-y.\n\nAnd would this be the best approach?\n\nIntuitively, I would expect to get a good model that does exactly what I specified, given the nature of CNNs. A CNN layer produces a vector representation of either an image or the output of another CNN layer. So using multiple CNN layers might essentially lead the CNN to identify superlabels.", "author_fullname": "t2_5zg4jvtk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can a convolutional neural network be designed to deduce superclassifications without such classes being explicitly labeled? How would I do that Would it be better or worse than other approaches?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11qoumb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678748269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example, I have a bookshelf with a lot of books. I have magazines, novels, science textbooks, math textbooks, writing textbooks, and books of poems. Suppose I were to take a picture of the front covers of these books and then design a convolutional neural network to predict whether a book is a magazine, novel, etc. Could I design a convolutional neural network such that it doesn&amp;#39;t just learn the labels I give it, but broader labels that apply to other items to which I haven&amp;#39;t assigned the same labels (superlabels)? How would I do that? For example, it might learn what textbook covers usually look like and then deduce that if a book cover doesn&amp;#39;t look like the cover of a textbook, then the book isn&amp;#39;t a math textbook, or a science textbook, or a writing textbook. Or it might notice that my science magazines and science textbooks have pictures of a microscope or something else science-y.&lt;/p&gt;\n\n&lt;p&gt;And would this be the best approach?&lt;/p&gt;\n\n&lt;p&gt;Intuitively, I would expect to get a good model that does exactly what I specified, given the nature of CNNs. A CNN layer produces a vector representation of either an image or the output of another CNN layer. So using multiple CNN layers might essentially lead the CNN to identify superlabels.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qoumb", "is_robot_indexable": true, "report_reasons": null, "author": "Comprehensive-Ad3963", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qoumb/can_a_convolutional_neural_network_be_designed_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qoumb/can_a_convolutional_neural_network_be_designed_to/", "subreddit_subscribers": 856815, "created_utc": 1678748269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi,\nI am an Senior EM. But I want to learn and practise about DataScience. I know some theory and some python. Can you guys give me advice from which material i can start and go through and learn my way up on the Implementation stuffs. Because there are so many materials online and everybody has their biased responses. Looking for advices and guidelines.", "author_fullname": "t2_18m10vei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Materials for Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qhzbi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678733443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI am an Senior EM. But I want to learn and practise about DataScience. I know some theory and some python. Can you guys give me advice from which material i can start and go through and learn my way up on the Implementation stuffs. Because there are so many materials online and everybody has their biased responses. Looking for advices and guidelines.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qhzbi", "is_robot_indexable": true, "report_reasons": null, "author": "mahabubakram", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qhzbi/materials_for_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qhzbi/materials_for_data_science/", "subreddit_subscribers": 856815, "created_utc": 1678733443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Im very new to data science, I needed help regarding a project ive been working on. I want to extract number of years of experience from job description i.e 1-2 years, 2 years etc. The text is free formatted hence the need for NLP. However can anyone give a brief idea on how to go about this", "author_fullname": "t2_8drjhm8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NER to extract data from job description", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qb556", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678717504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im very new to data science, I needed help regarding a project ive been working on. I want to extract number of years of experience from job description i.e 1-2 years, 2 years etc. The text is free formatted hence the need for NLP. However can anyone give a brief idea on how to go about this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qb556", "is_robot_indexable": true, "report_reasons": null, "author": "Helix-x", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qb556/ner_to_extract_data_from_job_description/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qb556/ner_to_extract_data_from_job_description/", "subreddit_subscribers": 856815, "created_utc": 1678717504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone!  \nI want to work on a project to build a website that suggests Hiking Trails based on Weather Forecast.\n\nI have some experience with Data Pipelines (mostly on GCP) and basic Web Design. As a first step I want some advice on what the overall architecture should be to build a MVP.\n\nMy idea is to collect weather data from OpenWeatherMaps and cross that data with trails from most popular Trail websites, to build a score combination based on weather quality for hiking and trail reviews.\n\nLet me know if you have some suggestions as that will definitely help!", "author_fullname": "t2_7940judc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Website that suggests Hiking Trails based on Weather Forecast", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qb4j5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678717462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;br/&gt;\nI want to work on a project to build a website that suggests Hiking Trails based on Weather Forecast.&lt;/p&gt;\n\n&lt;p&gt;I have some experience with Data Pipelines (mostly on GCP) and basic Web Design. As a first step I want some advice on what the overall architecture should be to build a MVP.&lt;/p&gt;\n\n&lt;p&gt;My idea is to collect weather data from OpenWeatherMaps and cross that data with trails from most popular Trail websites, to build a score combination based on weather quality for hiking and trail reviews.&lt;/p&gt;\n\n&lt;p&gt;Let me know if you have some suggestions as that will definitely help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qb4j5", "is_robot_indexable": true, "report_reasons": null, "author": "zecerqueira", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qb4j5/website_that_suggests_hiking_trails_based_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qb4j5/website_that_suggests_hiking_trails_based_on/", "subreddit_subscribers": 856815, "created_utc": 1678717462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am an Applied Math major with a Data Analytics internship in the summer. I am currently planning on having two minors; one in Engineering Science and one in Data Science. I am planning on graduating in the fall but due to some mistakes on both my part and an advisor, I would have to stay another semester for just one class to finish the minor. I am already going into my fifth year as I switched from engineering to math (hence the engineering minor), I would say I am a solid programmer and pretty much don't need the classes. Without the minor my last semester in the Fall would consist of 2 math classes and 2 online gen-eds, which would be an easy way to end my college experience (and save me a lot of money as my scholarship is for 4 years). \n\nShould I finish the minor or is the internship a good substitute?", "author_fullname": "t2_c2ryz920", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Data Science Minor Worth It", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q9s7y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678714231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an Applied Math major with a Data Analytics internship in the summer. I am currently planning on having two minors; one in Engineering Science and one in Data Science. I am planning on graduating in the fall but due to some mistakes on both my part and an advisor, I would have to stay another semester for just one class to finish the minor. I am already going into my fifth year as I switched from engineering to math (hence the engineering minor), I would say I am a solid programmer and pretty much don&amp;#39;t need the classes. Without the minor my last semester in the Fall would consist of 2 math classes and 2 online gen-eds, which would be an easy way to end my college experience (and save me a lot of money as my scholarship is for 4 years). &lt;/p&gt;\n\n&lt;p&gt;Should I finish the minor or is the internship a good substitute?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11q9s7y", "is_robot_indexable": true, "report_reasons": null, "author": "Secure-Atmosphere-36", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11q9s7y/is_data_science_minor_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11q9s7y/is_data_science_minor_worth_it/", "subreddit_subscribers": 856815, "created_utc": 1678714231.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_lrllrd1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you building internal aaps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qdkts", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678723258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qdkts", "is_robot_indexable": true, "report_reasons": null, "author": "vishal-vora", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qdkts/how_are_you_building_internal_aaps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qdkts/how_are_you_building_internal_aaps/", "subreddit_subscribers": 856815, "created_utc": 1678723258.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "**Package here: https://github.com/dwreeves/dbt_linreg**\n\n# Overview\n\nHey everyone! I'm sharing this here because some of you may use dbt at work to manage your data pipelines, and you might find this useful for whipping up quick analyses.\n\nI made a package that lets you run linear regression and ridge regression in any SQL engine using dbt, including Snowflake and DuckDB, which don't natively have multiple regression implementations.\n\nIn theory, this implementation supports any number of variables (in practice, adding more variables exponentially increases the computation time though with the current implementation).\n\nThe code is thoroughly tested against Statsmodels's `OLS` implementation, which you can see in the `integration_tests/` folder.\n\n# Example\n\nExample linear regression here:\n\n    {{\n      config(\n        materialized=\"table\"\n      )\n    }}\n    select * from {{\n      dbt_linreg.ols(\n        table=ref('simple_matrix')\n        endog='y',\n        exog=['xa', 'xb', 'xc'],\n        format='long'\n      )\n    }}\n\nThe above code would run a linear regression on `ref('simple_matrix')` using `y` as the y-variable, and `['xa', 'xb', 'xc']` as the X-variables. The constant term is always included and doesn't need to be specified.\n\n# Install\n\nInstallation instructions are simply to add this to your `packages.yml`:\n\n      - git: \"https://github.com/dwreeves/dbt_linreg.git\"\n        revision: \"v0.1.1\"\n\n# How it works under the hood\n\nSince this is the data science subreddit, there may be some interest in how this actually works, so I'll quote from the README on the matter:\n\nSimple univariate regression coefficients are simply `covar_pop(y, x) / var_pop(x)`.\n\nThe multiple regression implementation uses a technique described in section `3.2.3 Multiple Regression from Simple Univariate Regression` of TEoSL ([source](https://hastie.su.domains/Papers/ESLII.pdf#page=71)). Econometricians know this as the Frisch-Waugh-Lowell theorem, hence the method is referred to as `'fwl'` internally in the code base.\n\nRidge regression is implemented using the augmentation technique described in Exercise 12 of Chapter 3 of TEoSL ([source](https://hastie.su.domains/Papers/ESLII.pdf#page=115)).\n\nAll approaches were validated using Statsmodels `sm.OLS()`. Note that the ridge regression coefficients differ very slightly from Statsmodels's outputs for currently unknown reasons, but the coefficients are very close (I enforce a `&lt;0.01%` deviation from Statsmodels's ridge regression coefficients in my integration tests).", "author_fullname": "t2_h0zsodpe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linear regression and ridge regression in SQL + dbt with dbt_linreg", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qd0hq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678721933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Package here: &lt;a href=\"https://github.com/dwreeves/dbt_linreg\"&gt;https://github.com/dwreeves/dbt_linreg&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;Overview&lt;/h1&gt;\n\n&lt;p&gt;Hey everyone! I&amp;#39;m sharing this here because some of you may use dbt at work to manage your data pipelines, and you might find this useful for whipping up quick analyses.&lt;/p&gt;\n\n&lt;p&gt;I made a package that lets you run linear regression and ridge regression in any SQL engine using dbt, including Snowflake and DuckDB, which don&amp;#39;t natively have multiple regression implementations.&lt;/p&gt;\n\n&lt;p&gt;In theory, this implementation supports any number of variables (in practice, adding more variables exponentially increases the computation time though with the current implementation).&lt;/p&gt;\n\n&lt;p&gt;The code is thoroughly tested against Statsmodels&amp;#39;s &lt;code&gt;OLS&lt;/code&gt; implementation, which you can see in the &lt;code&gt;integration_tests/&lt;/code&gt; folder.&lt;/p&gt;\n\n&lt;h1&gt;Example&lt;/h1&gt;\n\n&lt;p&gt;Example linear regression here:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{{\n  config(\n    materialized=&amp;quot;table&amp;quot;\n  )\n}}\nselect * from {{\n  dbt_linreg.ols(\n    table=ref(&amp;#39;simple_matrix&amp;#39;)\n    endog=&amp;#39;y&amp;#39;,\n    exog=[&amp;#39;xa&amp;#39;, &amp;#39;xb&amp;#39;, &amp;#39;xc&amp;#39;],\n    format=&amp;#39;long&amp;#39;\n  )\n}}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The above code would run a linear regression on &lt;code&gt;ref(&amp;#39;simple_matrix&amp;#39;)&lt;/code&gt; using &lt;code&gt;y&lt;/code&gt; as the y-variable, and &lt;code&gt;[&amp;#39;xa&amp;#39;, &amp;#39;xb&amp;#39;, &amp;#39;xc&amp;#39;]&lt;/code&gt; as the X-variables. The constant term is always included and doesn&amp;#39;t need to be specified.&lt;/p&gt;\n\n&lt;h1&gt;Install&lt;/h1&gt;\n\n&lt;p&gt;Installation instructions are simply to add this to your &lt;code&gt;packages.yml&lt;/code&gt;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;  - git: &amp;quot;https://github.com/dwreeves/dbt_linreg.git&amp;quot;\n    revision: &amp;quot;v0.1.1&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;How it works under the hood&lt;/h1&gt;\n\n&lt;p&gt;Since this is the data science subreddit, there may be some interest in how this actually works, so I&amp;#39;ll quote from the README on the matter:&lt;/p&gt;\n\n&lt;p&gt;Simple univariate regression coefficients are simply &lt;code&gt;covar_pop(y, x) / var_pop(x)&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;The multiple regression implementation uses a technique described in section &lt;code&gt;3.2.3 Multiple Regression from Simple Univariate Regression&lt;/code&gt; of TEoSL (&lt;a href=\"https://hastie.su.domains/Papers/ESLII.pdf#page=71\"&gt;source&lt;/a&gt;). Econometricians know this as the Frisch-Waugh-Lowell theorem, hence the method is referred to as &lt;code&gt;&amp;#39;fwl&amp;#39;&lt;/code&gt; internally in the code base.&lt;/p&gt;\n\n&lt;p&gt;Ridge regression is implemented using the augmentation technique described in Exercise 12 of Chapter 3 of TEoSL (&lt;a href=\"https://hastie.su.domains/Papers/ESLII.pdf#page=115\"&gt;source&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;All approaches were validated using Statsmodels &lt;code&gt;sm.OLS()&lt;/code&gt;. Note that the ridge regression coefficients differ very slightly from Statsmodels&amp;#39;s outputs for currently unknown reasons, but the coefficients are very close (I enforce a &lt;code&gt;&amp;lt;0.01%&lt;/code&gt; deviation from Statsmodels&amp;#39;s ridge regression coefficients in my integration tests).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aafOh_i1nqewUphpG_wAR8390qZF6S4mNEDEIw1soqA.jpg?auto=webp&amp;v=enabled&amp;s=89760627f58497231c343fc84cd33b5cee42f2c5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aafOh_i1nqewUphpG_wAR8390qZF6S4mNEDEIw1soqA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10439ec2f6a4e74b18a5398fb471d0fc0c07f98b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aafOh_i1nqewUphpG_wAR8390qZF6S4mNEDEIw1soqA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fb2389e3fb1cf7b3ad657684bf2d22519b1c2b8", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aafOh_i1nqewUphpG_wAR8390qZF6S4mNEDEIw1soqA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2dad9a4b70596413b52b9e5d031114222df8fdf", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aafOh_i1nqewUphpG_wAR8390qZF6S4mNEDEIw1soqA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5aded51c9a4c51928ca846185d94aac43f8e334b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aafOh_i1nqewUphpG_wAR8390qZF6S4mNEDEIw1soqA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc3f9cd3190a0b90156f6ae7fc018482cda85553", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aafOh_i1nqewUphpG_wAR8390qZF6S4mNEDEIw1soqA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=647bc89ce3528f169182e0e568faef6e54a2c044", "width": 1080, "height": 540}], "variants": {}, "id": "47aZudkG3WJNkYxFgpIQmMmLZxuySLSLUFluVEhwJgo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qd0hq", "is_robot_indexable": true, "report_reasons": null, "author": "danielwreeves", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qd0hq/linear_regression_and_ridge_regression_in_sql_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qd0hq/linear_regression_and_ridge_regression_in_sql_dbt/", "subreddit_subscribers": 856815, "created_utc": 1678721933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey Guys, \n\nI applied to UKG and will soon be having 2 hr interview with the lead and also the team. can someone guide what to expect thanks. The role is Data scientist \n\nthanks", "author_fullname": "t2_3wxh329i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UKG- Data scientist interview prep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qgztd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678731231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys, &lt;/p&gt;\n\n&lt;p&gt;I applied to UKG and will soon be having 2 hr interview with the lead and also the team. can someone guide what to expect thanks. The role is Data scientist &lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qgztd", "is_robot_indexable": true, "report_reasons": null, "author": "ghost00069", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qgztd/ukg_data_scientist_interview_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qgztd/ukg_data_scientist_interview_prep/", "subreddit_subscribers": 856815, "created_utc": 1678731231.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_4ff830od", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does anyone know data science collaboration with aerospace industry?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11qgq9o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678730609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11qgq9o", "is_robot_indexable": true, "report_reasons": null, "author": "Nethma_peiris", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11qgq9o/does_anyone_know_data_science_collaboration_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11qgq9o/does_anyone_know_data_science_collaboration_with/", "subreddit_subscribers": 856815, "created_utc": 1678730609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_64362e59t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Applications and Features of Master Data Management Software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "name": "t3_11q5f8m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WSzIhuAwdZLbUn-gK1MQ9XRUmzhx0V8scDpj281uFdI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1678701171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mdmsoftwaresolutions.blogspot.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://mdmsoftwaresolutions.blogspot.com/2023/03/features-to-look-for-in-master-data-management-software.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uFtOaUgWVwEXlWDAqOzMCkIGmd0bF9jqyc7w8Fa6UjM.jpg?auto=webp&amp;v=enabled&amp;s=74c9bc6f61944a94c673d765cca59cd1014dbde2", "width": 780, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/uFtOaUgWVwEXlWDAqOzMCkIGmd0bF9jqyc7w8Fa6UjM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc3b3c3262f50f6ff4f89fd66d252ca2685d168b", "width": 108, "height": 55}, {"url": "https://external-preview.redd.it/uFtOaUgWVwEXlWDAqOzMCkIGmd0bF9jqyc7w8Fa6UjM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c16972ed1359c93a64c2c51b6cf5dca373f1cb19", "width": 216, "height": 110}, {"url": "https://external-preview.redd.it/uFtOaUgWVwEXlWDAqOzMCkIGmd0bF9jqyc7w8Fa6UjM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03809f7e221a12c6506ccfe3b8876fceb3fb0998", "width": 320, "height": 164}, {"url": "https://external-preview.redd.it/uFtOaUgWVwEXlWDAqOzMCkIGmd0bF9jqyc7w8Fa6UjM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b59ca74489d4b7f3d2f8167309373acb75dbea10", "width": 640, "height": 328}], "variants": {}, "id": "Mx7WfddgprTvfO4rZLEbQwoVabwDeBBqHEr4dyBjAFo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11q5f8m", "is_robot_indexable": true, "report_reasons": null, "author": "dataiocompany", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11q5f8m/applications_and_features_of_master_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://mdmsoftwaresolutions.blogspot.com/2023/03/features-to-look-for-in-master-data-management-software.html", "subreddit_subscribers": 856815, "created_utc": 1678701171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "With the exception of the internet, spreadsheets are the most popular user-facing software in the world. From simple data analysis to advanced regressions and predictions \u2014 some spreadsheet use cases are better suited for Python automation than others. [Here is a guide](https://blog.trymito.io/automating-spreadsheets-with-python-101/) on how to decide what spreadsheet processes to automate. \n\nI am a huge proponent of taking slow spreadsheet processes and using Python to automate them. Many spreadsheet processes actually mirror the steps that data scientists are doing in Python. The main difference is often the size of the dataset. Spreadsheets obviously limit at about 1 million rows, but many people are running processes on hundreds of thousands of lines of data, and put up with hours of wait time a week for their spreadsheets to load. I think this will go away, and Python is going to be the solution. \n\nI'd love to hear your thoughts on if you think spreadsheet can be automated with Python, and if yes, when is best to do so.", "author_fullname": "t2_7vpi3es9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Guide and Discussion on Automating Spreadsheet with Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q340m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1678692509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the exception of the internet, spreadsheets are the most popular user-facing software in the world. From simple data analysis to advanced regressions and predictions \u2014 some spreadsheet use cases are better suited for Python automation than others. &lt;a href=\"https://blog.trymito.io/automating-spreadsheets-with-python-101/\"&gt;Here is a guide&lt;/a&gt; on how to decide what spreadsheet processes to automate. &lt;/p&gt;\n\n&lt;p&gt;I am a huge proponent of taking slow spreadsheet processes and using Python to automate them. Many spreadsheet processes actually mirror the steps that data scientists are doing in Python. The main difference is often the size of the dataset. Spreadsheets obviously limit at about 1 million rows, but many people are running processes on hundreds of thousands of lines of data, and put up with hours of wait time a week for their spreadsheets to load. I think this will go away, and Python is going to be the solution. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear your thoughts on if you think spreadsheet can be automated with Python, and if yes, when is best to do so.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?auto=webp&amp;v=enabled&amp;s=9ff7051f337e335d18563c726d7c244b133f26f4", "width": 5000, "height": 3500}, "resolutions": [{"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=423a950d1ce38a96ec3d95381ba4ba06f8832d06", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88c25a286ea85640bd3cb07259c63ea6cbfe83e0", "width": 216, "height": 151}, {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a7eedcb5bf84c9d992b1f3717f71362f6c7d461", "width": 320, "height": 224}, {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f93fd7b09f37f6edca1890c7772ae3885179953d", "width": 640, "height": 448}, {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3ea872b87df9eceff2d292b887f72a4ffb69c6f", "width": 960, "height": 672}, {"url": "https://external-preview.redd.it/whO-tUhkl6k_Jl0DFM1C0ogOq9520RUjbhSlKdCysv0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7dcf46e9d404e8246243dac4c10b27d234c2806f", "width": 1080, "height": 756}], "variants": {}, "id": "4Y55QpCpbLy1dJdjtwk3WSaENOBK22AizVs8dftlcFI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11q340m", "is_robot_indexable": true, "report_reasons": null, "author": "Jake_Stack808", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11q340m/a_guide_and_discussion_on_automating_spreadsheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11q340m/a_guide_and_discussion_on_automating_spreadsheet/", "subreddit_subscribers": 856815, "created_utc": 1678692509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\n\nI'm loading a big csv / txt file using duckdb python api aka duckdb.read_csv \n\nAfter running a specific query on a column, I'm getting invalidinputexceotion error \n\n- on line #####, quote should be followed by end of valye, end of row, or another quote \n\nAny solutions?", "author_fullname": "t2_rtximd9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "duckdb issue: invalid input error after running a query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11pv9q0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678668412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m loading a big csv / txt file using duckdb python api aka duckdb.read_csv &lt;/p&gt;\n\n&lt;p&gt;After running a specific query on a column, I&amp;#39;m getting invalidinputexceotion error &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;on line #####, quote should be followed by end of valye, end of row, or another quote &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11pv9q0", "is_robot_indexable": true, "report_reasons": null, "author": "macORnvidia", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11pv9q0/duckdb_issue_invalid_input_error_after_running_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11pv9q0/duckdb_issue_invalid_input_error_after_running_a/", "subreddit_subscribers": 856815, "created_utc": 1678668412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_aahodm3w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How AI is Impacting the Future of Employment: Exploring the Pros and Cons", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11q70nu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1678706638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "uk.sganalytics.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://uk.sganalytics.com//blog/ai-is-impacting-the-future-of-employment-exploring-the-pros-and-cons/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11q70nu", "is_robot_indexable": true, "report_reasons": null, "author": "Raj_9898", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11q70nu/how_ai_is_impacting_the_future_of_employment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://uk.sganalytics.com//blog/ai-is-impacting-the-future-of-employment-exploring-the-pros-and-cons/", "subreddit_subscribers": 856815, "created_utc": 1678706638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_lrllrd1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone building DIGITAL TWINS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11pzqqa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1678680822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11pzqqa", "is_robot_indexable": true, "report_reasons": null, "author": "vishal-vora", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11pzqqa/anyone_building_digital_twins/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11pzqqa/anyone_building_digital_twins/", "subreddit_subscribers": 856815, "created_utc": 1678680822.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}