{"kind": "Listing", "data": {"after": "t3_124ira5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m05ys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "State of Data Engineering 2022", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_124d6qi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 321, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 321, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XmlxxjlEhCNUJSpfPjnRzbG1hzK9Z09cc4VEVqVrHyo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1679978995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/3rc8hxffueqa1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?auto=webp&amp;v=enabled&amp;s=5bfc9a5f597301e66c74afd35b5ca66828c9dd16", "width": 2560, "height": 1440}, "resolutions": [{"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a13140d19c377599589380edb419b3a63b68b8f2", "width": 108, "height": 60}, {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43bad1618623404a3bf95da32678a69518d1fe54", "width": 216, "height": 121}, {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d786150cd0e697484cf1dd7b5ae2bdd2888cbe76", "width": 320, "height": 180}, {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=defadaa89bef2c15d0d3a5a57771d3e5a295b248", "width": 640, "height": 360}, {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7231f337000a18a5fdbedadfbb539d3a07b9cdaa", "width": 960, "height": 540}, {"url": "https://preview.redd.it/3rc8hxffueqa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8208f54f0c1783e220f30b7f8cb69071fe240b39", "width": 1080, "height": 607}], "variants": {}, "id": "it_5DM1yYpIndVcCpr5Y7cS912omKACEHGt4msvndo0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "124d6qi", "is_robot_indexable": true, "report_reasons": null, "author": "SyntheticBlood", "discussion_type": null, "num_comments": 81, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124d6qi/state_of_data_engineering_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/3rc8hxffueqa1.jpg", "subreddit_subscribers": 94771, "created_utc": 1679978995.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Join us in the Rockies for an unforgettable conference featuring thought-provoking talks, workshops, craft beer tasting, hiking, and immersive experiences that will change the way you think about software development. Grab your Early Bird Ticket: [https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687](https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687)", "author_fullname": "t2_qhroetn8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big news! LambdaConf returns Sept 16-19th and is better than ever! \ud83d\udd25", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124ozln", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 106, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 106, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680010541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Join us in the Rockies for an unforgettable conference featuring thought-provoking talks, workshops, craft beer tasting, hiking, and immersive experiences that will change the way you think about software development. Grab your Early Bird Ticket: &lt;a href=\"https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687\"&gt;https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?auto=webp&amp;v=enabled&amp;s=6813a1ea4b5d401e335caa02583865546ee29898", "width": 1000, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b30c45d7350f71944e7ed6d27d03555b62a91684", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0379a777f803e71a1b906de4a5bcb32220be1005", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa1fdba3bf291c3b14470d653f8ebd575fd72c25", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4436df3f9c64439cf26ced83657393f9c08df83", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f84ac18aab2e94b4f7e4951bca30d20fc0fb70c8", "width": 960, "height": 480}], "variants": {}, "id": "DaWPZMpGJTzhhl3d5dsciFzXnZ4QqNEDthE7iz5CkU0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "124ozln", "is_robot_indexable": true, "report_reasons": null, "author": "Agataziverge", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124ozln/big_news_lambdaconf_returns_sept_1619th_and_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124ozln/big_news_lambdaconf_returns_sept_1619th_and_is/", "subreddit_subscribers": 94771, "created_utc": 1680010541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_92dedrzo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SMBC-comics.com \"now squeeze your points together to make your results look big\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_124mi0z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 109, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 109, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680004763.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/cz9tbicqygqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "preview": {"images": [{"source": {"url": "https://preview.redd.it/cz9tbicqygqa1.png?auto=webp&amp;v=enabled&amp;s=90836b30830e55386ac9b7c92ee4ab49d5b8c5ef", "width": 684, "height": 710}, "resolutions": [{"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b7b39b47ef18f4c425929d90b90df902a33a95e", "width": 108, "height": 112}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b047fb2fa23d8baa5445531cf278c8b4c1f9f87", "width": 216, "height": 224}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31c15c84067af3ef3911763420b5fb26c12c1733", "width": 320, "height": 332}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6aec00547714de581430e536c536f33ac4a5d2d", "width": 640, "height": 664}], "variants": {"obfuscated": {"source": {"url": "https://preview.redd.it/cz9tbicqygqa1.png?blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6519edc62e55abdf79ca69bce3f2da3a4214ad1c", "width": 684, "height": 710}, "resolutions": [{"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3ff45f8a205382c9be541f2f1ed00b65d08d001d", "width": 108, "height": 112}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=bb493ae483cf64ca6319edf4804d970b4a9fc724", "width": 216, "height": 224}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6730631a29389d92806643438810c7fad87eb076", "width": 320, "height": 332}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=96deb0aba516333cefd018f89758acbd383c1131", "width": 640, "height": 664}]}, "nsfw": {"source": {"url": "https://preview.redd.it/cz9tbicqygqa1.png?blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6519edc62e55abdf79ca69bce3f2da3a4214ad1c", "width": 684, "height": 710}, "resolutions": [{"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3ff45f8a205382c9be541f2f1ed00b65d08d001d", "width": 108, "height": 112}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=bb493ae483cf64ca6319edf4804d970b4a9fc724", "width": 216, "height": 224}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6730631a29389d92806643438810c7fad87eb076", "width": 320, "height": 332}, {"url": "https://preview.redd.it/cz9tbicqygqa1.png?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=96deb0aba516333cefd018f89758acbd383c1131", "width": 640, "height": 664}]}}, "id": "o_ZfZakw9vonrfxEDxzx3f9eg682BMvwf-Xnnf7ApQ0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "124mi0z", "is_robot_indexable": true, "report_reasons": null, "author": "rackhamlerouge9", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124mi0z/smbccomicscom_now_squeeze_your_points_together_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/cz9tbicqygqa1.png", "subreddit_subscribers": 94771, "created_utc": 1680004763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey /r/dataengineering! \n\nI\u2019m Toby and over the last few months, I\u2019ve been working with a team of engineers from Airbnb, Apple, Google, and Netflix, to simplify developing data pipelines with [SQLMesh](https://github.com/TobikoData/sqlmesh). \n\nWe\u2019re tired of fragile pipelines, untested SQL queries, and expensive staging environments for data. Software engineers have reaped the benefits of DevOps through unit tests, continuous integration, and continuous deployment for years. We felt like it was time for data teams to have the same confidence and efficiency in development as their peers. It\u2019s time for DataOps!\n\nSQLMesh can be used through a CLI/notebook or in our open source web based IDE (in preview). SQLMesh builds efficient dev / staging environments through \u201cVirtual Data Marts\u201d using views, which allows you to seamlessly rollback or roll forward your changes! With a simple pointer swap you can promote your \u201cstaging\u201d data into production. This means you get unlimited copy-on-write environments that make data exploration and preview of changes cheap, easy, safe. Some other key features are:\n\n* Automatic DAG generation by semantically parsing and understanding SQL or Python scripts\n* CI-Runnable Unit and Integration tests with optional conversion to DuckDB\n* Change detection and reconciliation through column level lineage \n* Native Airflow Integration\n* Import an existing DBT project and run it on SQLMesh\u2019s runtime (in preview)\n\nWe\u2019re just getting started on our journey to change the way data pipelines are built and deployed. We\u2019re huge proponents of open source and hope that we can grow together with your feedback and contributions. Try out SQLMesh by following the [quick start guide](https://sqlmesh.readthedocs.io/en/stable/quick_start/). We\u2019d love to chat and hear about your experiences and ideas in our [Slack community](https://join.slack.com/t/tobiko-data/shared_invite/zt-1ma66d79v-a4dbf4DUpLAQJ8ptQrJygg).", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQLMesh: The future of DataOps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124tspm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680019793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m Toby and over the last few months, I\u2019ve been working with a team of engineers from Airbnb, Apple, Google, and Netflix, to simplify developing data pipelines with &lt;a href=\"https://github.com/TobikoData/sqlmesh\"&gt;SQLMesh&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;We\u2019re tired of fragile pipelines, untested SQL queries, and expensive staging environments for data. Software engineers have reaped the benefits of DevOps through unit tests, continuous integration, and continuous deployment for years. We felt like it was time for data teams to have the same confidence and efficiency in development as their peers. It\u2019s time for DataOps!&lt;/p&gt;\n\n&lt;p&gt;SQLMesh can be used through a CLI/notebook or in our open source web based IDE (in preview). SQLMesh builds efficient dev / staging environments through \u201cVirtual Data Marts\u201d using views, which allows you to seamlessly rollback or roll forward your changes! With a simple pointer swap you can promote your \u201cstaging\u201d data into production. This means you get unlimited copy-on-write environments that make data exploration and preview of changes cheap, easy, safe. Some other key features are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Automatic DAG generation by semantically parsing and understanding SQL or Python scripts&lt;/li&gt;\n&lt;li&gt;CI-Runnable Unit and Integration tests with optional conversion to DuckDB&lt;/li&gt;\n&lt;li&gt;Change detection and reconciliation through column level lineage &lt;/li&gt;\n&lt;li&gt;Native Airflow Integration&lt;/li&gt;\n&lt;li&gt;Import an existing DBT project and run it on SQLMesh\u2019s runtime (in preview)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We\u2019re just getting started on our journey to change the way data pipelines are built and deployed. We\u2019re huge proponents of open source and hope that we can grow together with your feedback and contributions. Try out SQLMesh by following the &lt;a href=\"https://sqlmesh.readthedocs.io/en/stable/quick_start/\"&gt;quick start guide&lt;/a&gt;. We\u2019d love to chat and hear about your experiences and ideas in our &lt;a href=\"https://join.slack.com/t/tobiko-data/shared_invite/zt-1ma66d79v-a4dbf4DUpLAQJ8ptQrJygg\"&gt;Slack community&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "124tspm", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124tspm/sqlmesh_the_future_of_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124tspm/sqlmesh_the_future_of_dataops/", "subreddit_subscribers": 94771, "created_utc": 1680019793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've just completed my 3rd data project to help me understand how to work with Airflow and running services in Docker.\n\n# Links\n\n* [GitHub Repository](https://github.com/digitalghost-dev/global-data-pipeline)\n* [Looker Studio Visualization](https://lookerstudio.google.com/reporting/3710d6bb-25b2-4d64-b6e8-2889bc57c74b) \\- not a great experience on mobile, Air Quality page doesn't seem to load.\n* [Documentation](https://github.com/digitalghost-dev/global-data-pipeline/wiki/Global-Data-Pipeline-Documentation) \\- tried my best with this, will need to run through it again and proof read.\n* [Discord Server Invite](https://discord.gg/j2HEfpebuH) \\- feel free to join to see the bot in action. There is only one channel and it's locked down so not much do in here but thought I would add it in case someone was curious. The bot will query the database and look for the highest current\\_temp and will send a message with the city name and the temperature in celsius.\n\n# Overview\n\n* A `docker-compose.yml` file runs Airflow, Postgres, and Redis in Docker containers.\n* Python scripts reach out to different data sources to extract, transform and load the data into a Postgres database, orchestrated through Airflow on various schedules.\n* Using Airflow operators, data is moved from Postgres to Google Cloud Storage then to BigQuery where the data is visualized with Looker Studio.\n* A Discord Airflow operator is used to send a daily message to a server with current weather stats.\n\n# Data Sources\n\nThis project uses two APIs and web scrapes some tables from Wikipedia. All the city data derives from choosing the 50 most populated cities in the world according to [MacroTrends](https://www.macrotrends.net/cities/largest-cities-by-population).\n\n* City Weather - (updated hourly) with [Weatherstack](https://weatherstack.com) API - costs $10 a month for 50,000 calls.\n   * Current temperature, humidity, precipitation, wind speed\n* City Air Quality - (updated hourly) with [OpenWeatherMap](https://openweathermap.org) API\n   * CO, NO2, O2, SO2, PM2.5, PM10\n* City population\n* Country statistics\n   * Fertility rates, homicide rates, Human Development Index, unemployments rates\n\n[Flowchart](https://preview.redd.it/zz181kpt6iqa1.png?width=2112&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b1485fbb458838ec703bbaeceea381cd635d2db1)\n\n# Notes\n\nSetting up Airflow was pretty painless with the predefined `docker-compose.yml` file found [here](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html). I did have to modify the original file a bit to allow containers to talk to each other on my host machine.\n\nSpeaking of host machines, all of this is running on my desktop.\n\nLooker Studio is okay... it's free so I guess I can't complain too much but the experience for viewers on mobile is pretty bad.\n\nThe visualizations I made in Looker Studio are elementary at best but my goal wasn't to build the prettiest dashboard. I will continue to update it though in the future.", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My 3rd data project, with Airflow, Docker, Postgres, and Looker Studio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zz181kpt6iqa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 65, "x": 108, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cecb6cf097ae6c7442827e55a85ef64d18ca18e"}, {"y": 131, "x": 216, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=edb440a66cf60727d5afd2ed6492dfb5a622d3f2"}, {"y": 195, "x": 320, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f297b107b70090c6504a542f6185a35a12dd231"}, {"y": 390, "x": 640, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d551441f10fa5dc8df7c67ed23fcf42348b6058a"}, {"y": 585, "x": 960, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cdf0fa788b94f59330fcbf6e3633ed26e63095f"}, {"y": 658, "x": 1080, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f60f656bbef58516246acbe6296c717f4b87d6fa"}], "s": {"y": 1288, "x": 2112, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=2112&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b1485fbb458838ec703bbaeceea381cd635d2db1"}, "id": "zz181kpt6iqa1"}}, "name": "t3_124wcjb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EWJ1r_6hf0lhfYglqOaweEIFjMkf0VQ2GvKa6YqbKS0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1680025179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just completed my 3rd data project to help me understand how to work with Airflow and running services in Docker.&lt;/p&gt;\n\n&lt;h1&gt;Links&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/digitalghost-dev/global-data-pipeline\"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://lookerstudio.google.com/reporting/3710d6bb-25b2-4d64-b6e8-2889bc57c74b\"&gt;Looker Studio Visualization&lt;/a&gt; - not a great experience on mobile, Air Quality page doesn&amp;#39;t seem to load.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/digitalghost-dev/global-data-pipeline/wiki/Global-Data-Pipeline-Documentation\"&gt;Documentation&lt;/a&gt; - tried my best with this, will need to run through it again and proof read.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://discord.gg/j2HEfpebuH\"&gt;Discord Server Invite&lt;/a&gt; - feel free to join to see the bot in action. There is only one channel and it&amp;#39;s locked down so not much do in here but thought I would add it in case someone was curious. The bot will query the database and look for the highest current_temp and will send a message with the city name and the temperature in celsius.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Overview&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A &lt;code&gt;docker-compose.yml&lt;/code&gt; file runs Airflow, Postgres, and Redis in Docker containers.&lt;/li&gt;\n&lt;li&gt;Python scripts reach out to different data sources to extract, transform and load the data into a Postgres database, orchestrated through Airflow on various schedules.&lt;/li&gt;\n&lt;li&gt;Using Airflow operators, data is moved from Postgres to Google Cloud Storage then to BigQuery where the data is visualized with Looker Studio.&lt;/li&gt;\n&lt;li&gt;A Discord Airflow operator is used to send a daily message to a server with current weather stats.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Data Sources&lt;/h1&gt;\n\n&lt;p&gt;This project uses two APIs and web scrapes some tables from Wikipedia. All the city data derives from choosing the 50 most populated cities in the world according to &lt;a href=\"https://www.macrotrends.net/cities/largest-cities-by-population\"&gt;MacroTrends&lt;/a&gt;.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;City Weather - (updated hourly) with &lt;a href=\"https://weatherstack.com\"&gt;Weatherstack&lt;/a&gt; API - costs $10 a month for 50,000 calls.\n\n&lt;ul&gt;\n&lt;li&gt;Current temperature, humidity, precipitation, wind speed&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;City Air Quality - (updated hourly) with &lt;a href=\"https://openweathermap.org\"&gt;OpenWeatherMap&lt;/a&gt; API\n\n&lt;ul&gt;\n&lt;li&gt;CO, NO2, O2, SO2, PM2.5, PM10&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;City population&lt;/li&gt;\n&lt;li&gt;Country statistics\n\n&lt;ul&gt;\n&lt;li&gt;Fertility rates, homicide rates, Human Development Index, unemployments rates&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zz181kpt6iqa1.png?width=2112&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b1485fbb458838ec703bbaeceea381cd635d2db1\"&gt;Flowchart&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Notes&lt;/h1&gt;\n\n&lt;p&gt;Setting up Airflow was pretty painless with the predefined &lt;code&gt;docker-compose.yml&lt;/code&gt; file found &lt;a href=\"https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html\"&gt;here&lt;/a&gt;. I did have to modify the original file a bit to allow containers to talk to each other on my host machine.&lt;/p&gt;\n\n&lt;p&gt;Speaking of host machines, all of this is running on my desktop.&lt;/p&gt;\n\n&lt;p&gt;Looker Studio is okay... it&amp;#39;s free so I guess I can&amp;#39;t complain too much but the experience for viewers on mobile is pretty bad.&lt;/p&gt;\n\n&lt;p&gt;The visualizations I made in Looker Studio are elementary at best but my goal wasn&amp;#39;t to build the prettiest dashboard. I will continue to update it though in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?auto=webp&amp;v=enabled&amp;s=dc7b06ba82e2a238e6a1e9732af3c38e0694e6b0", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bcb73cfd09ed2e135d54bf6ee2c9ce08e61fea7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f22c542e5e61014373bc88e9fb88fbf5f092391", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0bea044b442d5c5cfc86170d6673d169d1d8f14", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68a01789cc3524d21df30163274621bf690ed230", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=995876c9cdc7ea26d77e80bede7d8c18bdebb1db", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60a5df26e9b64687f8eb5b08795cc711fd35424b", "width": 1080, "height": 540}], "variants": {}, "id": "65nZrXRToPy1C10OOcyNpu-vcLkcCd-DzvBSwTc_SbY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "124wcjb", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124wcjb/my_3rd_data_project_with_airflow_docker_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124wcjb/my_3rd_data_project_with_airflow_docker_postgres/", "subreddit_subscribers": 94771, "created_utc": 1680025179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you guys charge for independent contracting rates? I am an experienced Senior Data Engineer. Will be leading key projects for the company.\n\nRight now I am thinking $125/hr or $1000/day given a multi-year engagement. Client has somewhat \"limited\" resources, but a field I am very interested in learning more about. I don't want to hear \"Oh yeah, you could easily get $500/hr!!!\" that's not realistic. Give it to me straight. Am I lowballing myself here?", "author_fullname": "t2_qleqo7a5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consulting Rates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1251kbg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680036019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you guys charge for independent contracting rates? I am an experienced Senior Data Engineer. Will be leading key projects for the company.&lt;/p&gt;\n\n&lt;p&gt;Right now I am thinking $125/hr or $1000/day given a multi-year engagement. Client has somewhat &amp;quot;limited&amp;quot; resources, but a field I am very interested in learning more about. I don&amp;#39;t want to hear &amp;quot;Oh yeah, you could easily get $500/hr!!!&amp;quot; that&amp;#39;s not realistic. Give it to me straight. Am I lowballing myself here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1251kbg", "is_robot_indexable": true, "report_reasons": null, "author": "IndependentElk3997", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1251kbg/consulting_rates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1251kbg/consulting_rates/", "subreddit_subscribers": 94771, "created_utc": 1680036019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently passed my DP-203 certification exam with the lowest score possible. I was a little sad bc I put in about 3 weeks of studying for it but at the end of the day... a pass is a pass.\n\n**Background:** I recently moved into a Cloud Data Eng role within my company a month ago. I had little to no experience with data eng. All I had was just course on the concept of ETL Data Warehousing. Previous experience role: Developer Analyst in my companies logistics domain working with our Global WMS solution.\n\n**Conclusion:** Test was a lot harder than expected. The exam consisted of curveballs and trick questions which made consistently second guess myself.  I read a couple of other ppls post on this sub that passed the exam and they made it sound like the exam was piece of cake. I think I may have struggled more because I had little to no real life hands on experience in regards to Data Eng. Also, exam went into more detail in regards to syntax as well as security and access. \n\n**Prep:** I prepared for about a month. Below is what I did\n\n1)My company payed for me to take the instructor led course offered on the Microsoft website and it was a complete nightmare/waste of time. The course was outsourced to a company named Koenig and they were struggling with the fact that microsoft had updated course content earlier this year in February. The instructor felt scripted and the labs did not match the course content. The instructor also had no real life experience and struggled with scenario based questions. For example, \"in which case would this distribution be best'?'... The only good thing about the course is that they later gave me a workaround lab environment with the updated labs which provided me with some simple generic hands on experience. I think besides the lab environments, I would suggest to take the self paced version and go through it in a manner where you understand the content with detail. The level of detail that they covered did not suffice the level of detail that exam questions covered.\n\n2)Practice test offered on Microsoft test: this were good but again did not match the level of detail that the questions of the exams consisted off. I would \\*\\*suggest\\*\\* getting the questions wrong and studying the corresponding links of the knowledge articles that they provide.\n\n3)Knowledge check questions on the course. Similar to the practice test\n\n4)Labs on the course . \\*\\*Recommend\\*\\*: reading knowledge articles corresponding to each lab\n\n5)[www.examtopics.com/exams/microsoft/dp-203](https://www.examtopics.com/exams/microsoft/dp-203) : I didn't hit this that hard but I would recommend doing so. These are the type of questions that are on the exams.\n\nPlease feel free to reach out if you guys have any questions.", "author_fullname": "t2_hi3uk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recently passed DP-203 Certification Exam", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124t0qi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680018157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently passed my DP-203 certification exam with the lowest score possible. I was a little sad bc I put in about 3 weeks of studying for it but at the end of the day... a pass is a pass.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; I recently moved into a Cloud Data Eng role within my company a month ago. I had little to no experience with data eng. All I had was just course on the concept of ETL Data Warehousing. Previous experience role: Developer Analyst in my companies logistics domain working with our Global WMS solution.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Test was a lot harder than expected. The exam consisted of curveballs and trick questions which made consistently second guess myself.  I read a couple of other ppls post on this sub that passed the exam and they made it sound like the exam was piece of cake. I think I may have struggled more because I had little to no real life hands on experience in regards to Data Eng. Also, exam went into more detail in regards to syntax as well as security and access. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Prep:&lt;/strong&gt; I prepared for about a month. Below is what I did&lt;/p&gt;\n\n&lt;p&gt;1)My company payed for me to take the instructor led course offered on the Microsoft website and it was a complete nightmare/waste of time. The course was outsourced to a company named Koenig and they were struggling with the fact that microsoft had updated course content earlier this year in February. The instructor felt scripted and the labs did not match the course content. The instructor also had no real life experience and struggled with scenario based questions. For example, &amp;quot;in which case would this distribution be best&amp;#39;?&amp;#39;... The only good thing about the course is that they later gave me a workaround lab environment with the updated labs which provided me with some simple generic hands on experience. I think besides the lab environments, I would suggest to take the self paced version and go through it in a manner where you understand the content with detail. The level of detail that they covered did not suffice the level of detail that exam questions covered.&lt;/p&gt;\n\n&lt;p&gt;2)Practice test offered on Microsoft test: this were good but again did not match the level of detail that the questions of the exams consisted off. I would **suggest** getting the questions wrong and studying the corresponding links of the knowledge articles that they provide.&lt;/p&gt;\n\n&lt;p&gt;3)Knowledge check questions on the course. Similar to the practice test&lt;/p&gt;\n\n&lt;p&gt;4)Labs on the course . **Recommend**: reading knowledge articles corresponding to each lab&lt;/p&gt;\n\n&lt;p&gt;5)&lt;a href=\"https://www.examtopics.com/exams/microsoft/dp-203\"&gt;www.examtopics.com/exams/microsoft/dp-203&lt;/a&gt; : I didn&amp;#39;t hit this that hard but I would recommend doing so. These are the type of questions that are on the exams.&lt;/p&gt;\n\n&lt;p&gt;Please feel free to reach out if you guys have any questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "124t0qi", "is_robot_indexable": true, "report_reasons": null, "author": "Jpvilla5454", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124t0qi/recently_passed_dp203_certification_exam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124t0qi/recently_passed_dp203_certification_exam/", "subreddit_subscribers": 94771, "created_utc": 1680018157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel like frontend developers get a better developer experience as they can see impact from their code instantly on the browser as they update the code locally. Have you experienced something similar, where you can see the impact of your code changes on the data, using an iso-prod environment and that within seconds or minutes?", "author_fullname": "t2_975og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best developer experience you had as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124nms9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680007509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like frontend developers get a better developer experience as they can see impact from their code instantly on the browser as they update the code locally. Have you experienced something similar, where you can see the impact of your code changes on the data, using an iso-prod environment and that within seconds or minutes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "124nms9", "is_robot_indexable": true, "report_reasons": null, "author": "arimbr", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124nms9/what_is_the_best_developer_experience_you_had_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124nms9/what_is_the_best_developer_experience_you_had_as/", "subreddit_subscribers": 94771, "created_utc": 1680007509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just got the greenlight to implement airflow. What do you find the most scalable way to use it for ETL?\nDont feel pressured to answer everything. Any advice could help .\n\n Here's some background info:\n\n1. Handling Confidential data, not allowed to be on main ETL with rest of business. \n\n2. We just built a server with a fresh install\n\n3. I am the only data engineer outside ETL team. \n\n4. Using postgres data warehouse for this project.\n\n5. 6 different source systems, with API or SFTP drops.\n\n6. I usually  Use psycopg2 execute values because it's super fast. Pandas to SQL always seems too slow, but I love pandas for transformations before loading. I convert df to tuple value for speed.\n\nNow, here are my questions on extract and load:\n\nIn airflow, using python operator:\n\nHow do you structure the tasks within your DAG? Do you build the ETL all in a single task? Or do you split the extract, transform, load into different  tasks and make them a task group? If you don't need to transform (outside of selecting which fields you want from the API json format) is it cool to just put it all in one task? \n\nIf you need data from 10 different methods from a rest API , and are just doing a raw data pull, I am trying to determine if that should be 10,20 or 30 tasks. \n\nFor the loads in your DAG, are you passing the same connection to each of your functions, or are your tasks opening and closing the connections as they complete? ( Committing along the way)\n\nDo you use bash operator at all? Bash operator would be nice because we have a ton of stuff on cron and all of the scripts are built to run pull and load raw data. We could just swap to a different scheduler. At the same time, I have freedom and time to make this an ideal implementation. \n\n\n\n\nTransformations:\n\nNot allowed to use dbt, so the team before me made a bunch of view on view relationships to do transforms. They are BI analysts with no real tech support until now. I plan on using dbt cli in dev to re create the views as tables (or ephemeral if not used in any downstream queries and tables) and then stealing the create table scripts out of the target folder as a starting place and tweaking, adding some referential integrity, etc. \n\nDbt was really nice with scheduling transformations and building DAGs for them as you go. Airflow seems more manual (but way better than cron or view on view). How do you manage tons of different SQL based transformations? Are you using python operator to werap your SQL in ? or are you executing SQL files via bash operator?\n\nAnd I guess I'll leave it there.", "author_fullname": "t2_7mo0tj9f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most ideal Airflow task structure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124mxjc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680005842.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just got the greenlight to implement airflow. What do you find the most scalable way to use it for ETL?\nDont feel pressured to answer everything. Any advice could help .&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s some background info:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Handling Confidential data, not allowed to be on main ETL with rest of business. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We just built a server with a fresh install&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I am the only data engineer outside ETL team. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using postgres data warehouse for this project.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;6 different source systems, with API or SFTP drops.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I usually  Use psycopg2 execute values because it&amp;#39;s super fast. Pandas to SQL always seems too slow, but I love pandas for transformations before loading. I convert df to tuple value for speed.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, here are my questions on extract and load:&lt;/p&gt;\n\n&lt;p&gt;In airflow, using python operator:&lt;/p&gt;\n\n&lt;p&gt;How do you structure the tasks within your DAG? Do you build the ETL all in a single task? Or do you split the extract, transform, load into different  tasks and make them a task group? If you don&amp;#39;t need to transform (outside of selecting which fields you want from the API json format) is it cool to just put it all in one task? &lt;/p&gt;\n\n&lt;p&gt;If you need data from 10 different methods from a rest API , and are just doing a raw data pull, I am trying to determine if that should be 10,20 or 30 tasks. &lt;/p&gt;\n\n&lt;p&gt;For the loads in your DAG, are you passing the same connection to each of your functions, or are your tasks opening and closing the connections as they complete? ( Committing along the way)&lt;/p&gt;\n\n&lt;p&gt;Do you use bash operator at all? Bash operator would be nice because we have a ton of stuff on cron and all of the scripts are built to run pull and load raw data. We could just swap to a different scheduler. At the same time, I have freedom and time to make this an ideal implementation. &lt;/p&gt;\n\n&lt;p&gt;Transformations:&lt;/p&gt;\n\n&lt;p&gt;Not allowed to use dbt, so the team before me made a bunch of view on view relationships to do transforms. They are BI analysts with no real tech support until now. I plan on using dbt cli in dev to re create the views as tables (or ephemeral if not used in any downstream queries and tables) and then stealing the create table scripts out of the target folder as a starting place and tweaking, adding some referential integrity, etc. &lt;/p&gt;\n\n&lt;p&gt;Dbt was really nice with scheduling transformations and building DAGs for them as you go. Airflow seems more manual (but way better than cron or view on view). How do you manage tons of different SQL based transformations? Are you using python operator to werap your SQL in ? or are you executing SQL files via bash operator?&lt;/p&gt;\n\n&lt;p&gt;And I guess I&amp;#39;ll leave it there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124mxjc", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial_Ranger_5", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124mxjc/most_ideal_airflow_task_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124mxjc/most_ideal_airflow_task_structure/", "subreddit_subscribers": 94771, "created_utc": 1680005842.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to get some tips or guidance regarding tools, pipeline or resources on building a simple datapipeline that runs daily and fetches the data from an sql database.\n\nWhere I have the most questions is after pulling the data from the sql database, and transform it and load it into s3, how can I make sure that the next day the new records added will be pulled and inserted into its respective tables? I don't think well built pipelines fetch all the tables on a daily basis, but the new information. (How to keep track of new records and old ones that were edited) \n\nThank you so much for your time", "author_fullname": "t2_5x9e117l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to insert new updates to dwh from transactional database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124ebk0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679981134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to get some tips or guidance regarding tools, pipeline or resources on building a simple datapipeline that runs daily and fetches the data from an sql database.&lt;/p&gt;\n\n&lt;p&gt;Where I have the most questions is after pulling the data from the sql database, and transform it and load it into s3, how can I make sure that the next day the new records added will be pulled and inserted into its respective tables? I don&amp;#39;t think well built pipelines fetch all the tables on a daily basis, but the new information. (How to keep track of new records and old ones that were edited) &lt;/p&gt;\n\n&lt;p&gt;Thank you so much for your time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124ebk0", "is_robot_indexable": true, "report_reasons": null, "author": "readoyniando", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124ebk0/how_to_insert_new_updates_to_dwh_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124ebk0/how_to_insert_new_updates_to_dwh_from/", "subreddit_subscribers": 94771, "created_utc": 1679981134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS or azure doesn't matter.", "author_fullname": "t2_vtx6qjs2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you be using for one time ingestion of large volume of data today, say 100tb.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1251wbd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680036691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS or azure doesn&amp;#39;t matter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1251wbd", "is_robot_indexable": true, "report_reasons": null, "author": "Budget_Assignment457", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1251wbd/what_would_you_be_using_for_one_time_ingestion_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1251wbd/what_would_you_be_using_for_one_time_ingestion_of/", "subreddit_subscribers": 94771, "created_utc": 1680036691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Issue: The Airbyte MySQL connector does not parallelize read workload\n\nResult: Syncs take forever to finish\n\nSolution: Update Airbyte MySQL connector to use parallel connections for reading with JDBC\nOne can use subconnections to read data in parallel from server. This will increase performance considerably.\n\nI was wondering if anyone has done anything like this?  If not, anyone interested in working with me (I have used JAVA in like forever) to give back to the open source community and update the Airbyte connector to preform parallel reads", "author_fullname": "t2_9o0tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte MySQL connector does not parallelize read", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1253fa1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680039846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Issue: The Airbyte MySQL connector does not parallelize read workload&lt;/p&gt;\n\n&lt;p&gt;Result: Syncs take forever to finish&lt;/p&gt;\n\n&lt;p&gt;Solution: Update Airbyte MySQL connector to use parallel connections for reading with JDBC\nOne can use subconnections to read data in parallel from server. This will increase performance considerably.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone has done anything like this?  If not, anyone interested in working with me (I have used JAVA in like forever) to give back to the open source community and update the Airbyte connector to preform parallel reads&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1253fa1", "is_robot_indexable": true, "report_reasons": null, "author": "Amphagory", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1253fa1/airbyte_mysql_connector_does_not_parallelize_read/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1253fa1/airbyte_mysql_connector_does_not_parallelize_read/", "subreddit_subscribers": 94771, "created_utc": 1680039846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had planned to put together some beginners training material to cover the gambit from data collection and monitoring to analytics and reporting. Having recently joined this community, I have noticed some themes that made me want to ask: what would you like if you were new to data engineering? \n\nAdditionally, what sort of content? How detailed and technical? Does the tech stack matter? Do you prefer apps and code or Jupyter Notebooks? Let me know your thoughts! \n\nThanks and cheers all \ud83d\udcaa\ud83c\udffb", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What makes for good DE training material?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124zowl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680032238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had planned to put together some beginners training material to cover the gambit from data collection and monitoring to analytics and reporting. Having recently joined this community, I have noticed some themes that made me want to ask: what would you like if you were new to data engineering? &lt;/p&gt;\n\n&lt;p&gt;Additionally, what sort of content? How detailed and technical? Does the tech stack matter? Do you prefer apps and code or Jupyter Notebooks? Let me know your thoughts! &lt;/p&gt;\n\n&lt;p&gt;Thanks and cheers all \ud83d\udcaa\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "124zowl", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124zowl/what_makes_for_good_de_training_material/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124zowl/what_makes_for_good_de_training_material/", "subreddit_subscribers": 94771, "created_utc": 1680032238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://github.com/awslabs/mountpoint-s3", "author_fullname": "t2_anaphz9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mountpoint for S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": true, "name": "t3_125cd6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BzCeCYVYoRezIcRAh51k_PJz49QWs8fkDVSTdaNsu2c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680062177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/awslabs/mountpoint-s3\"&gt;https://github.com/awslabs/mountpoint-s3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/tjtdex6f7nqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?auto=webp&amp;v=enabled&amp;s=53e157290d01953c562e2e9aff21f421200970ef", "width": 1080, "height": 636}, "resolutions": [{"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6cc52fcf4e0b3224b4dc0e4e68050d570a3c76d", "width": 108, "height": 63}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8449b091dc519f5194a98d2709fed9e71baeb4a0", "width": 216, "height": 127}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ed365562b9aad4866830e6371ee41f3b143c309", "width": 320, "height": 188}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ff92623702f37a5300c92e08b75a599c156ecfb", "width": 640, "height": 376}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9736621428871edb6d85add8b0a58a09bdbc4d26", "width": 960, "height": 565}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b56b22a5177542b6f31407d868bf7e038d1541de", "width": 1080, "height": 636}], "variants": {}, "id": "lKbMI6JMXWTO1kGTw436EAb1GUZ0XysjvnbN03C2Yf8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125cd6n", "is_robot_indexable": true, "report_reasons": null, "author": "Sweet-Butterscotch11", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125cd6n/mountpoint_for_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/tjtdex6f7nqa1.png", "subreddit_subscribers": 94771, "created_utc": 1680062177.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ui4m14ke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to transition to a Data Architect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1257sut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/J3Yh7qA10YYJTrTkuGOZ9Yw2_LQx4Ss8f6scA_auTXE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680050106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@anupmoncy/how-to-transition-to-a-data-architect-83aa835ea53c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?auto=webp&amp;v=enabled&amp;s=7dfd4ccc789b8180380a2c52c7643250c8cb0bda", "width": 1200, "height": 799}, "resolutions": [{"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=104214fa8c81fdd0d7d07e7012cba83b15add55d", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2928ce334f49dfdb0c12dbde7585fa633b1838d", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ed546a816ab21e6c5e63529f7f43b0bd33dbc82", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db94f0be8640fbebe58040d5ed5f0698ca085a1d", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39868eb7151bad0588663013f17598991947f4c0", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ee531ccf658f0c5aa84e9bfee7034c06d0311f7", "width": 1080, "height": 719}], "variants": {}, "id": "vwHNrRaZgH1udSmqaM1jQ6og5WHaxlO6Yq77OeRN7io"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1257sut", "is_robot_indexable": true, "report_reasons": null, "author": "Alarmed-Sock4915", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1257sut/how_to_transition_to_a_data_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@anupmoncy/how-to-transition-to-a-data-architect-83aa835ea53c", "subreddit_subscribers": 94771, "created_utc": 1680050106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey DE reddit,\n\nJust looking for any guidance or suggestions on how to tackle a problem we have at my workplace.\n\nWe have an old data warehouse where 15 years of old SSIS jobs populate it with a lot of technical debt and knowledge of how they work no longer with the company. \n\nWe have a new data warehouse spun up for an ERP migration project and some of the data that gets populated with SSIS is required on the new data warehouse as a short term stop gap for business reporting continuity. \n\nThis stop gap will be cleaned up post go live of the ERP project but for now we need a quick and efficient way to replicate the SSIS populated table data from old data warehouse to new data warehouse.\n\nThe old data warehouse is MSSQL 2014 Standard and does not have CDC as a result...\n\nI've created python code that replicates the data using pandas and it works reasonably well other than the length it takes to replicate and the amount of RAM the pandas dataframe consumes. So i can only run this once a day meaning some data will be a bit stale in the new data warehouse till it runs again the next day.\n\nI'm trying to come up with a way to be able to run this replication more often given time and ram limitations. \n\nOne idea I had was to hash the table contents from the last run and save it, then check the hash to see if any of the table changed before using valuable replication time/ram resources. Has anyone ever done anything like this ? Any libraries that can help me ?\n\nOr any other completely different ways to handle this without CDC or large amounts of hardware spend ? \n\nAny guidance/help is much appreciated!", "author_fullname": "t2_9sf3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replication of data without CDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1254m9v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680042547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey DE reddit,&lt;/p&gt;\n\n&lt;p&gt;Just looking for any guidance or suggestions on how to tackle a problem we have at my workplace.&lt;/p&gt;\n\n&lt;p&gt;We have an old data warehouse where 15 years of old SSIS jobs populate it with a lot of technical debt and knowledge of how they work no longer with the company. &lt;/p&gt;\n\n&lt;p&gt;We have a new data warehouse spun up for an ERP migration project and some of the data that gets populated with SSIS is required on the new data warehouse as a short term stop gap for business reporting continuity. &lt;/p&gt;\n\n&lt;p&gt;This stop gap will be cleaned up post go live of the ERP project but for now we need a quick and efficient way to replicate the SSIS populated table data from old data warehouse to new data warehouse.&lt;/p&gt;\n\n&lt;p&gt;The old data warehouse is MSSQL 2014 Standard and does not have CDC as a result...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve created python code that replicates the data using pandas and it works reasonably well other than the length it takes to replicate and the amount of RAM the pandas dataframe consumes. So i can only run this once a day meaning some data will be a bit stale in the new data warehouse till it runs again the next day.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to come up with a way to be able to run this replication more often given time and ram limitations. &lt;/p&gt;\n\n&lt;p&gt;One idea I had was to hash the table contents from the last run and save it, then check the hash to see if any of the table changed before using valuable replication time/ram resources. Has anyone ever done anything like this ? Any libraries that can help me ?&lt;/p&gt;\n\n&lt;p&gt;Or any other completely different ways to handle this without CDC or large amounts of hardware spend ? &lt;/p&gt;\n\n&lt;p&gt;Any guidance/help is much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1254m9v", "is_robot_indexable": true, "report_reasons": null, "author": "msthree", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1254m9v/replication_of_data_without_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1254m9v/replication_of_data_without_cdc/", "subreddit_subscribers": 94771, "created_utc": 1680042547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi.  Over the last few months we have been developing an open source CI/CD platform for dbt.\n\nThough it's possible to do CI/CD and orchestration with Gitlab, Jenkins, Airflow etc, I felt there was an opportunity to do something specific for dbt.  This would reduce the need to build a custom pipeline and mean that we could build a GUI that is tailored to Data Engineers.  \n\nThe bigger aim is also to help Data Engineers adopt a strict development workflow, with pull requests, environments, source control, CI/CD etc as sometimes this is lacking.  \n\nI would be really interested in any constructive feedback and thoughts on the idea and the more general problem of CI/CD with dbt and data transformation code.\n\nDemo video here - [https://www.youtube.com/watch?v=-s-bULKecvY&amp;t=64s](https://www.youtube.com/watch?v=-s-bULKecvY&amp;t=64s)\n\nIntroductory blog post here - [https://timeflow.systems/blog/introducing-timeflow](https://timeflow.systems/blog/introducing-timeflow)", "author_fullname": "t2_599jquwy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interested in thoughts on a CI/CD platform for dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124jhu3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1679996823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.  Over the last few months we have been developing an open source CI/CD platform for dbt.&lt;/p&gt;\n\n&lt;p&gt;Though it&amp;#39;s possible to do CI/CD and orchestration with Gitlab, Jenkins, Airflow etc, I felt there was an opportunity to do something specific for dbt.  This would reduce the need to build a custom pipeline and mean that we could build a GUI that is tailored to Data Engineers.  &lt;/p&gt;\n\n&lt;p&gt;The bigger aim is also to help Data Engineers adopt a strict development workflow, with pull requests, environments, source control, CI/CD etc as sometimes this is lacking.  &lt;/p&gt;\n\n&lt;p&gt;I would be really interested in any constructive feedback and thoughts on the idea and the more general problem of CI/CD with dbt and data transformation code.&lt;/p&gt;\n\n&lt;p&gt;Demo video here - &lt;a href=\"https://www.youtube.com/watch?v=-s-bULKecvY&amp;amp;t=64s\"&gt;https://www.youtube.com/watch?v=-s-bULKecvY&amp;amp;t=64s&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Introductory blog post here - &lt;a href=\"https://timeflow.systems/blog/introducing-timeflow\"&gt;https://timeflow.systems/blog/introducing-timeflow&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wUvpuFvCbiBrKmgzAE51FmYH7I8w9Z8Oy7ssZvxNmMw.jpg?auto=webp&amp;v=enabled&amp;s=f3f3b026ff445ae93f720d470dbdbbd81bcd6d89", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/wUvpuFvCbiBrKmgzAE51FmYH7I8w9Z8Oy7ssZvxNmMw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf8f7ed84a73d257380e708f1bca6ab553436dfb", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/wUvpuFvCbiBrKmgzAE51FmYH7I8w9Z8Oy7ssZvxNmMw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8a148459d5a3f1fd8bb082cd8e79c00fa55e844", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/wUvpuFvCbiBrKmgzAE51FmYH7I8w9Z8Oy7ssZvxNmMw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81794c66641227265b59927a713c6b8eed4dcdf4", "width": 320, "height": 240}], "variants": {}, "id": "Bk_76OB7iZZqNzV4J6Xz67nCfxKHvNH4IGGQd2J6RVo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "124jhu3", "is_robot_indexable": true, "report_reasons": null, "author": "benjaminwootton888", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124jhu3/interested_in_thoughts_on_a_cicd_platform_for_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124jhu3/interested_in_thoughts_on_a_cicd_platform_for_dbt/", "subreddit_subscribers": 94771, "created_utc": 1679996823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This config block cannot be parsed by IDEs I use (Datagrip/PyCharm). Why doesn't dbt simply uses a json of same name to do the job? Why invent this format? This really bugs me as I have to write a translation script to generate some dbt model files and json files are obviously a lot easier to dump on disk.", "author_fullname": "t2_ldvtxo0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does DBT model have such a weird format for configs in models?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12523tl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680037120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This config block cannot be parsed by IDEs I use (Datagrip/PyCharm). Why doesn&amp;#39;t dbt simply uses a json of same name to do the job? Why invent this format? This really bugs me as I have to write a translation script to generate some dbt model files and json files are obviously a lot easier to dump on disk.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12523tl", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway20220231", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12523tl/why_does_dbt_model_have_such_a_weird_format_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12523tl/why_does_dbt_model_have_such_a_weird_format_for/", "subreddit_subscribers": 94771, "created_utc": 1680037120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently built a Data Catalog ROI Template to assist data people in evaluating the value and impact of their data catalog investments. Data catalogs are all the rage, but are we really getting the most out of them?  I'd love to get your feedback:\n\n\ud83d\udd17 [**Data Catalog ROI Template**](https://www.castordoc.com/blog/data-catalog-roi-a-primer)\n\nThis template will help you:\n\n1. Gauge your data catalog's current state and spot growth potential\n2. Pinpoint improvement areas to optimize performance\n3. Measure your data catalog's tangible benefits\n\nShare your feedback &amp; ideas to improve it.", "author_fullname": "t2_bu8cw718", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Catalog ROI Template", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124y0d8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680028697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently built a Data Catalog ROI Template to assist data people in evaluating the value and impact of their data catalog investments. Data catalogs are all the rage, but are we really getting the most out of them?  I&amp;#39;d love to get your feedback:&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd17 &lt;a href=\"https://www.castordoc.com/blog/data-catalog-roi-a-primer\"&gt;&lt;strong&gt;Data Catalog ROI Template&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This template will help you:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Gauge your data catalog&amp;#39;s current state and spot growth potential&lt;/li&gt;\n&lt;li&gt;Pinpoint improvement areas to optimize performance&lt;/li&gt;\n&lt;li&gt;Measure your data catalog&amp;#39;s tangible benefits&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Share your feedback &amp;amp; ideas to improve it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?auto=webp&amp;v=enabled&amp;s=2a34d956b4a6d3ac1880cdaeb89cbca64fb60ff2", "width": 1692, "height": 884}, "resolutions": [{"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b856feaa0456c35484f8e54bd714db9492aeece1", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=207bf9a6898283f0d1439dd4f63dbfadec8fa701", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5313d3b5f2d563bcd54420d52ddd2cf1c33e27c8", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2986b7515841c0ce590048052d5af6783d11c36a", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f0a94bb36543c9ce1d5ac6e4ca53bf69eb69fa4", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11d3ccfaeb8aac987f157348ca18b1cd2f39c525", "width": 1080, "height": 564}], "variants": {}, "id": "zs6zgWh0o00ZqNct8b7a2b5xz2XX77i6_ThrG6WNQY8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "124y0d8", "is_robot_indexable": true, "report_reasons": null, "author": "castor-metadata", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124y0d8/data_catalog_roi_template/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124y0d8/data_catalog_roi_template/", "subreddit_subscribers": 94771, "created_utc": 1680028697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI like to do a personal project and welcome any advice, feedback, recommendations. First, an intro so you know my starting position.\n\n\\- I work in Data Analytics, Management, Governance, including the quality control, process improvement, stakeholder management, and creating internal tools.  \n\\- I've always been interested in the tech/IT side and truly excited and fascinated about all the available stuff these days, which is one of two reasons for this project  \n\\- I did research on 100 data-related manager/lead/director job openings (in my region) and one of the things that was mentioned in 78% of them was \"... knowledge and experiences of data pipelines, how they work, what every part does (on high level), and able to see opportunities to make it more effective, better quality checks, better security, and maximize the value\"; I fully agree with this requirement because even though I might not be the one who will actually do all this stuff, I find it important to understand what the department does, and what my IT colleagues are telling me.\n\n**So what am I thinking off (ballpark)**\n\n\\- not a giant spaghetti that will likely become an untamable beast to complete, but not a super small one just to guarantee success, I like a challenge and it needs to be valuable and relevant to today's business challenges  \n\\- something modular, so I can start with one part and at the end, it works, then move to a second part to expand the pipeline, not necessarily linear, but perhaps a side-chain  \n\\- a bit with automation and a bit with automated SQL transformation  \n\\- a bit with manual input, maybe to validate data which triggers different flows  \n\\- creating custom datasets specifically for this project (but I don't need a million datapoints please)  \n\\- a pipeline that is intentionally not 1000% optimized, I want it to reflect the common situation of companies growing their pipelines along the way to the point that it's just barely/acceptable to manage lol (you know what I mean) \n\n**What do I have available?**\n\n\\- MS 365 + Power Platform  \n\\- PostgreSQL and MySQL in cloud, plus dbeaver  \n\\- Retool\n\n**What Have I been looking at?**\n\n\\- Fivetran for both orchestration and transformation\n\n**What I want to exclude**\n\n\\- anything that requires me to be a programmer, I'm definitely not and will likely drown in the project due to the scripting part\n\n**What I am asking you**\n\nI've been trying to come up with the project content for two months and just drawing blank, coming up with something from scratch has never been my strong suit. I like to ask you for ideas.\n\nYou can throw anything to me: words, stories, bullet-points, even (or especially) sketches of pipelines that you ~~recommend~~ challenge me to do.\n\nThanks for reading and taking the effort to help.\n\nCheers.", "author_fullname": "t2_kh3ubtce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Doing a personal project as a non-DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124v675", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680022693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I like to do a personal project and welcome any advice, feedback, recommendations. First, an intro so you know my starting position.&lt;/p&gt;\n\n&lt;p&gt;- I work in Data Analytics, Management, Governance, including the quality control, process improvement, stakeholder management, and creating internal tools.&lt;br/&gt;\n- I&amp;#39;ve always been interested in the tech/IT side and truly excited and fascinated about all the available stuff these days, which is one of two reasons for this project&lt;br/&gt;\n- I did research on 100 data-related manager/lead/director job openings (in my region) and one of the things that was mentioned in 78% of them was &amp;quot;... knowledge and experiences of data pipelines, how they work, what every part does (on high level), and able to see opportunities to make it more effective, better quality checks, better security, and maximize the value&amp;quot;; I fully agree with this requirement because even though I might not be the one who will actually do all this stuff, I find it important to understand what the department does, and what my IT colleagues are telling me.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So what am I thinking off (ballpark)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- not a giant spaghetti that will likely become an untamable beast to complete, but not a super small one just to guarantee success, I like a challenge and it needs to be valuable and relevant to today&amp;#39;s business challenges&lt;br/&gt;\n- something modular, so I can start with one part and at the end, it works, then move to a second part to expand the pipeline, not necessarily linear, but perhaps a side-chain&lt;br/&gt;\n- a bit with automation and a bit with automated SQL transformation&lt;br/&gt;\n- a bit with manual input, maybe to validate data which triggers different flows&lt;br/&gt;\n- creating custom datasets specifically for this project (but I don&amp;#39;t need a million datapoints please)&lt;br/&gt;\n- a pipeline that is intentionally not 1000% optimized, I want it to reflect the common situation of companies growing their pipelines along the way to the point that it&amp;#39;s just barely/acceptable to manage lol (you know what I mean) &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What do I have available?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- MS 365 + Power Platform&lt;br/&gt;\n- PostgreSQL and MySQL in cloud, plus dbeaver&lt;br/&gt;\n- Retool&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What Have I been looking at?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Fivetran for both orchestration and transformation&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I want to exclude&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- anything that requires me to be a programmer, I&amp;#39;m definitely not and will likely drown in the project due to the scripting part&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I am asking you&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to come up with the project content for two months and just drawing blank, coming up with something from scratch has never been my strong suit. I like to ask you for ideas.&lt;/p&gt;\n\n&lt;p&gt;You can throw anything to me: words, stories, bullet-points, even (or especially) sketches of pipelines that you &lt;del&gt;recommend&lt;/del&gt; challenge me to do.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading and taking the effort to help.&lt;/p&gt;\n\n&lt;p&gt;Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "124v675", "is_robot_indexable": true, "report_reasons": null, "author": "SquidsAndMartians", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124v675/doing_a_personal_project_as_a_nonde/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124v675/doing_a_personal_project_as_a_nonde/", "subreddit_subscribers": 94771, "created_utc": 1680022693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nRecently, I just received a date for a technical interview for an entry-level data engineer position at a large retailer Fortune X company. While I am extremely happy to hear that I get a chance to dive into the data engineering field, I am also extremely nervous and unprepared for the interview. The interview will be covering both Python and PySpark; I have some exposure to Python (nothing Leetcode-level at all) and absolutely zero experience with PySpark. The interview will be in about a week.\n\nFor context: I have a bachelors in industrial engineering and making a career jump from a process engineering role (tbh more of a \u201cdata analyst\u201d then process engineer). In my role, I primarily work with Dataiku (very similar to Alteryx) by using built-in recipes, SQL scripts, and minor Python scripts to read, transform, and write data into BigQuery tables for other engineers on my team to use. \n\nMy questions are: \n\n-\twhat types of Python and PySpark questions and topics should I expect for my interview?\n-\twhat are some recommended technical programming resources that I should grind on before my interview?\n-\tif I try to start Leetcoding now, do I need to familiarize with all the data structures and algorithms? What should I primarily focus on? I tried Leetcode a long time ago and sucked at it\n-\tis it feasible to learn PySpark within a week or should I tell them that I have no experience with PySpark? This was never brought up in my prior interview and my resume does not mention PySpark experience at all\n\nI really appreciate any feedback I can get!", "author_fullname": "t2_9dofcyu1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python/PySpark Interview Questions for Entry-Level Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124pw6d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680012442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Recently, I just received a date for a technical interview for an entry-level data engineer position at a large retailer Fortune X company. While I am extremely happy to hear that I get a chance to dive into the data engineering field, I am also extremely nervous and unprepared for the interview. The interview will be covering both Python and PySpark; I have some exposure to Python (nothing Leetcode-level at all) and absolutely zero experience with PySpark. The interview will be in about a week.&lt;/p&gt;\n\n&lt;p&gt;For context: I have a bachelors in industrial engineering and making a career jump from a process engineering role (tbh more of a \u201cdata analyst\u201d then process engineer). In my role, I primarily work with Dataiku (very similar to Alteryx) by using built-in recipes, SQL scripts, and minor Python scripts to read, transform, and write data into BigQuery tables for other engineers on my team to use. &lt;/p&gt;\n\n&lt;p&gt;My questions are: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;  what types of Python and PySpark questions and topics should I expect for my interview?&lt;/li&gt;\n&lt;li&gt;  what are some recommended technical programming resources that I should grind on before my interview?&lt;/li&gt;\n&lt;li&gt;  if I try to start Leetcoding now, do I need to familiarize with all the data structures and algorithms? What should I primarily focus on? I tried Leetcode a long time ago and sucked at it&lt;/li&gt;\n&lt;li&gt;  is it feasible to learn PySpark within a week or should I tell them that I have no experience with PySpark? This was never brought up in my prior interview and my resume does not mention PySpark experience at all&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I really appreciate any feedback I can get!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "124pw6d", "is_robot_indexable": true, "report_reasons": null, "author": "mirai_e", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124pw6d/pythonpyspark_interview_questions_for_entrylevel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124pw6d/pythonpyspark_interview_questions_for_entrylevel/", "subreddit_subscribers": 94771, "created_utc": 1680012442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modeling, Visualizing, and Navigating a Transportation Network with Memgraph", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_124nrnn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PIL_MqQBQnNitwsKOx28oRm-wvsdrXPtNVHxFxIuF3o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680007823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memgraph.com/blog/modeling-visualizing-navigating-a-transportation-network-with-memgraph", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?auto=webp&amp;v=enabled&amp;s=0c9f1b23bba282af67a247becd39519c0edbe17c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14ff056a80c39be180153fce59b2081bdeba8410", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8077d07db4e4c707984c73abab7eaee45f9f7c0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0d59358e6ad575e0a5713660b78fd51b3b02f84", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68774c1f2e0e7d990b9f90069676d51b0d9f9cd1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4e0dc784f7bdb785a87fd524d6b846b89bf524f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/-Hd4GDVZvJ2NWXM8_7PL81uV4r8flKuKXogDlLdQvIs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47361b6a6eb55fc89a8d1c50c67dbc1b5c29eab5", "width": 1080, "height": 540}], "variants": {}, "id": "N-DWmh8WkOxwsiQVxILtoW-lg1gVclggDSKUbODSu2M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "124nrnn", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124nrnn/modeling_visualizing_and_navigating_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memgraph.com/blog/modeling-visualizing-navigating-a-transportation-network-with-memgraph", "subreddit_subscribers": 94771, "created_utc": 1680007823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'll get to the point. I am taking as a premise that Sketch Engine and AntConc, which are commonly used for concordance searches, lie within the realm of data engineering inasmuch as they facilitate data. My understanding is that both accept data in XML, meaning that you can bring in a linguistic corpus with a predefined scheme for the tags and the software knows how to handle this. Am I correct about this?   \nEven if it's not the two examples I've noted, is it true that there is software which allows for data analysis (of varying sophistication) where the input is an XML document with its schema?  \n\n\nMany thanks in advance!", "author_fullname": "t2_graxcz2q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Quick Question] How do software such as Sketch Engine and AntConc work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124ndnm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680006917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll get to the point. I am taking as a premise that Sketch Engine and AntConc, which are commonly used for concordance searches, lie within the realm of data engineering inasmuch as they facilitate data. My understanding is that both accept data in XML, meaning that you can bring in a linguistic corpus with a predefined scheme for the tags and the software knows how to handle this. Am I correct about this?&lt;br/&gt;\nEven if it&amp;#39;s not the two examples I&amp;#39;ve noted, is it true that there is software which allows for data analysis (of varying sophistication) where the input is an XML document with its schema?  &lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124ndnm", "is_robot_indexable": true, "report_reasons": null, "author": "alpolvovolvere", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124ndnm/quick_question_how_do_software_such_as_sketch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124ndnm/quick_question_how_do_software_such_as_sketch/", "subreddit_subscribers": 94771, "created_utc": 1680006917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My expectations:\n\n&amp;#x200B;\n\n1. Open Source\n2. Relatively simple\n3. Support for SparQL", "author_fullname": "t2_8w2gvpfk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please recommend a graph DB like RedisGraph but with SparQL support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124kfrg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679999665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My expectations:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Open Source&lt;/li&gt;\n&lt;li&gt;Relatively simple&lt;/li&gt;\n&lt;li&gt;Support for SparQL&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124kfrg", "is_robot_indexable": true, "report_reasons": null, "author": "Vitaly_v_ch", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124kfrg/please_recommend_a_graph_db_like_redisgraph_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124kfrg/please_recommend_a_graph_db_like_redisgraph_but/", "subreddit_subscribers": 94771, "created_utc": 1679999665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,.\nWhat could be the best way to built Zoho CRM or any other crm streaming data pipeline inside gcp?\nI am thinking to use bigquery as a data warehouse. But confused about which tool to use or which will be better and cost efficient in this case. Any suggestions will be appreciated.", "author_fullname": "t2_vkmvzdm7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline for Zoho/other CRM in gcp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124ira5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679994579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,.\nWhat could be the best way to built Zoho CRM or any other crm streaming data pipeline inside gcp?\nI am thinking to use bigquery as a data warehouse. But confused about which tool to use or which will be better and cost efficient in this case. Any suggestions will be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "124ira5", "is_robot_indexable": true, "report_reasons": null, "author": "shaikh21", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124ira5/data_pipeline_for_zohoother_crm_in_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124ira5/data_pipeline_for_zohoother_crm_in_gcp/", "subreddit_subscribers": 94771, "created_utc": 1679994579.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}