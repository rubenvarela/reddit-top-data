{"kind": "Listing", "data": {"after": "t3_125jkxw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "froid_san, Creator of English Patches for JP Exclusive Games for PS3, PSV, and PS4 Will be Shutting Down His Website.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 114, "top_awarded_type": null, "hide_score": false, "name": "t3_1258t25", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 472, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_6jesv", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 472, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7efBSECf6W9k-a4-yCBKnUsYY56S89EsgbfyiG2sMAo.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "VitaPiracy", "selftext": "", "author_fullname": "t2_3gz7qdgx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "froid_san, Creator of English Patches for JP Exclusive Games for PS3, PSV, and PS4 Will be Shutting Down His Website.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/VitaPiracy", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 114, "top_awarded_type": null, "hide_score": false, "name": "t3_124fl8f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 244, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 244, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7efBSECf6W9k-a4-yCBKnUsYY56S89EsgbfyiG2sMAo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "created": 1679984850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/0i8h50f3bfqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?auto=webp&amp;v=enabled&amp;s=97b9ad866a58999f0ad633a231a9d42ed991540f", "width": 770, "height": 628}, "resolutions": [{"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eaa72324231f7a4945ff32da822ca9a7753cc614", "width": 108, "height": 88}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b60de6d27a5934298b7b7f62b78e3b2c1dbda8ce", "width": 216, "height": 176}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3aec77160cdc97698879f70acc553f5a84a445f", "width": 320, "height": 260}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=022714839e8f28584e638dddf0b847d9c3743a33", "width": 640, "height": 521}], "variants": {}, "id": "FL_ro_C4G_dj-T453tPbQC1sEdGMqzWYfkFCSJLmI0Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3fxlj", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "124fl8f", "is_robot_indexable": true, "report_reasons": null, "author": "ANG-123", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/VitaPiracy/comments/124fl8f/froid_san_creator_of_english_patches_for_jp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/0i8h50f3bfqa1.png", "subreddit_subscribers": 80041, "created_utc": 1679984850.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1680052614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/0i8h50f3bfqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?auto=webp&amp;v=enabled&amp;s=97b9ad866a58999f0ad633a231a9d42ed991540f", "width": 770, "height": 628}, "resolutions": [{"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eaa72324231f7a4945ff32da822ca9a7753cc614", "width": 108, "height": 88}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b60de6d27a5934298b7b7f62b78e3b2c1dbda8ce", "width": 216, "height": 176}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3aec77160cdc97698879f70acc553f5a84a445f", "width": 320, "height": 260}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=022714839e8f28584e638dddf0b847d9c3743a33", "width": 640, "height": 521}], "variants": {}, "id": "FL_ro_C4G_dj-T453tPbQC1sEdGMqzWYfkFCSJLmI0Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "35TB + 8TB NAS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1258t25", "is_robot_indexable": true, "report_reasons": null, "author": "seamonkey420", "discussion_type": null, "num_comments": 30, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_124fl8f", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1258t25/froid_san_creator_of_english_patches_for_jp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/0i8h50f3bfqa1.png", "subreddit_subscribers": 675963, "created_utc": 1680052614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have images from several laptops that I've archived using macrium, and I just found out the free version is discontinued.  \n\nAm I looking at suffering the consequences of using a free, proprietary format, and potentially not being able to access the images? \n\nThanks.", "author_fullname": "t2_3kfjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Macrium Free discontinued. Am I looking at losing all of my disk images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1258d46", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680051511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have images from several laptops that I&amp;#39;ve archived using macrium, and I just found out the free version is discontinued.  &lt;/p&gt;\n\n&lt;p&gt;Am I looking at suffering the consequences of using a free, proprietary format, and potentially not being able to access the images? &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1258d46", "is_robot_indexable": true, "report_reasons": null, "author": "hoyfkd", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1258d46/macrium_free_discontinued_am_i_looking_at_losing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1258d46/macrium_free_discontinued_am_i_looking_at_losing/", "subreddit_subscribers": 675963, "created_utc": 1680051511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Or just the e-books part of the site?\n\nAsking because I have uploaded alot of old videos from the Romanian television archives that can't be found anywhere else. I still have them backed up on my external HDD's, but still....", "author_fullname": "t2_sdmay15h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will Internet Archive be closed forever if the lose in court?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125846a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680050877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or just the e-books part of the site?&lt;/p&gt;\n\n&lt;p&gt;Asking because I have uploaded alot of old videos from the Romanian television archives that can&amp;#39;t be found anywhere else. I still have them backed up on my external HDD&amp;#39;s, but still....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125846a", "is_robot_indexable": true, "report_reasons": null, "author": "Creative-Detail4369", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125846a/will_internet_archive_be_closed_forever_if_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125846a/will_internet_archive_be_closed_forever_if_the/", "subreddit_subscribers": 675963, "created_utc": 1680050877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello\n\nSome sad news for the photography community and also for the resource it offers but Amazon is closing down the whole of DP review on 10th April 2023.\n\nWhat is the best way to keep the website resource (and youtube) online?\n\n&amp;#x200B;\n\nSee: [https://www.dpreview.com/news/5901145460/dpreview-com-to-close](https://www.dpreview.com/news/5901145460/dpreview-com-to-close)", "author_fullname": "t2_l8l3l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP Review is closing down (youtube, website) on 10th April - Best Archive Instructions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1252ay3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680037515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;Some sad news for the photography community and also for the resource it offers but Amazon is closing down the whole of DP review on 10th April 2023.&lt;/p&gt;\n\n&lt;p&gt;What is the best way to keep the website resource (and youtube) online?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;See: &lt;a href=\"https://www.dpreview.com/news/5901145460/dpreview-com-to-close\"&gt;https://www.dpreview.com/news/5901145460/dpreview-com-to-close&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?auto=webp&amp;v=enabled&amp;s=62800be361d0e710877115d585fc54f6bc091987", "width": 745, "height": 745}, "resolutions": [{"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c85ecc4153f61d9a247d4758d9430569622282d", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a74b7694c73ea2345e65d9ea046b7453dce349cd", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3dbcef5e3e8892307008a8899734f8a2c64159c5", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b46d7621039f2b84e5eb82a6de9d0d1874068788", "width": 640, "height": 640}], "variants": {}, "id": "KDAVLRZTXaBom3MDvF95i0FH6bCPweDv6BPK8qmCi6E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1252ay3", "is_robot_indexable": true, "report_reasons": null, "author": "Fkmeitscold", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1252ay3/dp_review_is_closing_down_youtube_website_on_10th/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1252ay3/dp_review_is_closing_down_youtube_website_on_10th/", "subreddit_subscribers": 675963, "created_utc": 1680037515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_6bkhzbkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitHub - StephaneCouturier/Katalog: Katalog is a desktop application to manage catalogs of disks and files.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_124q3io", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HGHHFC4jmOYy3Yo0PP5xRAeqO1qpbIA7Ui-qD0XI_bU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680012884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/StephaneCouturier/Katalog", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?auto=webp&amp;v=enabled&amp;s=f0345fbfa155c42f9ed3a015d8d454057224a656", "width": 1192, "height": 746}, "resolutions": [{"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68ac3d245bbe00f527e6ae98c04d2be7ccaf7779", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b71d17edd47e3be60bf936b51aa0ddccce8b40cc", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fad31efc016a152921bbd0d644dfd8414b22cefc", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4daa8d14d844362135f0b74796849c40fd98c7ea", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1bc48cb4bf8f59134b0ede3495ea2b6788fa407", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12b2d68feed5753904239ca33fddd17e9e39475a", "width": 1080, "height": 675}], "variants": {}, "id": "irDtZCYVAKY-0XAuUWFYOOtO_6uvN6Z80XPqklo8AGE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124q3io", "is_robot_indexable": true, "report_reasons": null, "author": "justanotherquestionq", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124q3io/github_stephanecouturierkatalog_katalog_is_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/StephaneCouturier/Katalog", "subreddit_subscribers": 675963, "created_utc": 1680012884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm collecting old movies (pre-1970) for long term and occasional use storage. These movies can either be OOP, Region B, or have never received a proper Blu-Ray release.I want to watch them semi-reguarly and own them 10-20 years from now. I posted on this sub a few days ago asking about BD-R burning, but my understanding is that even an authored BD-R disc won't function on my PS5 and maybe not on my PS3 (which are my only drives atm). Do I copy my movies onto an HDD and continually buy new ones as backups over the years or is there a better way?", "author_fullname": "t2_8zpyfcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to archive old movies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124y2ge", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680028814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m collecting old movies (pre-1970) for long term and occasional use storage. These movies can either be OOP, Region B, or have never received a proper Blu-Ray release.I want to watch them semi-reguarly and own them 10-20 years from now. I posted on this sub a few days ago asking about BD-R burning, but my understanding is that even an authored BD-R disc won&amp;#39;t function on my PS5 and maybe not on my PS3 (which are my only drives atm). Do I copy my movies onto an HDD and continually buy new ones as backups over the years or is there a better way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124y2ge", "is_robot_indexable": true, "report_reasons": null, "author": "HunteHorseman", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124y2ge/best_way_to_archive_old_movies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124y2ge/best_way_to_archive_old_movies/", "subreddit_subscribers": 675963, "created_utc": 1680028814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for recommendations on BDXL Windows burning software? Thank you in advance", "author_fullname": "t2_pzfb76t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended Windows bdxl burning software?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125142c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680035123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for recommendations on BDXL Windows burning software? Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125142c", "is_robot_indexable": true, "report_reasons": null, "author": "DisturbedBeaker", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125142c/recommended_windows_bdxl_burning_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125142c/recommended_windows_bdxl_burning_software/", "subreddit_subscribers": 675963, "created_utc": 1680035123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "EDIT: This was originally a \"Question/Advice\" flaired post asking for best practices. I've since developed my own method which works for me. Please read below if you'd like to know what I ended up doing or feel welcome to share your own solution!   \n\n\nORIGINAL POST:\n\nI'm hoping that the community can help me.\n\nI am using blurays for archiving files that I have long held on HDDs. I've read that it's common to run a file check on the data before burning to disc and to then also add those logs to the blurays along with your data.\n\nIs there a best practices guide on what tools to use to achieve this? I've been searching for a best practices guide but I'm coming up short in my search.\n\nBest tools/methodology for arching files to bluray would be really appreciated.  \n\n\nUPDATE: So on to my solution.   \n\n\nFrustrated by my inability to find a best practices, I ended up resorting to my own solution.  \nI set up a project folder in Win10 where prepare each bluray by compressing folders of files into .7zip format. This was simple for me and kept everything organized the way that I liked it.  \n\n\nOnce I had various folders with various .7zip compressed archives, I go to the root of my project folder (this would also be the root once these files are copied over the bluray), I highlight all directories on the root and any root level files, right click on the highlighted files go to 7zip in your context menu, then CRC SHA, then SHA-256 -&gt; \\[name\\].sha256  \n\n\nThis will output a sha-256 hash for every file selected into a new file called \\[name\\].sha256. I then verified that these hashes were accurate by opening up my text editor (Sublime). You'll see all of the hashes, you can then check random files by going back into your folder and selecting a file, go back through the context menu but this time select: SHA-256 to get only the hash for that file.  \n\n\nOnce I was confident that the hashes were accurate, I opened up ImgBurn, went to \"Write files/folders to disc\" and added my files including my SHA-256 file specific to only the files added to my bluray disc. I then went to the options tab and changed the File System to UDF revision 2.60. Selected Verify and then burnt my files to bluray (build).   \n\n\nOnce the burn was successful, I open the bluray in Windows and compare the hashes of the files again against the SHA-256 file I made to make sure that they show the right values. They did so it's all good! Copying files to the PC from disc to do another check (super redundant) proved this to be a good method for myself.   \n\n\nI make a couple disc copies and then make a third copy on an external HDD. all cold storage.  \n\n\nIt's really late here but I wanted to leave this for anyone struggling to find a good path forward. I hope this helps someone! Let me know if it helped you! And if you know of a better way, also please let me know! ", "author_fullname": "t2_yu2xz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optical Archive Logging/File Archiving Best Practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124zhcn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680070563.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680031814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;EDIT: This was originally a &amp;quot;Question/Advice&amp;quot; flaired post asking for best practices. I&amp;#39;ve since developed my own method which works for me. Please read below if you&amp;#39;d like to know what I ended up doing or feel welcome to share your own solution!   &lt;/p&gt;\n\n&lt;p&gt;ORIGINAL POST:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping that the community can help me.&lt;/p&gt;\n\n&lt;p&gt;I am using blurays for archiving files that I have long held on HDDs. I&amp;#39;ve read that it&amp;#39;s common to run a file check on the data before burning to disc and to then also add those logs to the blurays along with your data.&lt;/p&gt;\n\n&lt;p&gt;Is there a best practices guide on what tools to use to achieve this? I&amp;#39;ve been searching for a best practices guide but I&amp;#39;m coming up short in my search.&lt;/p&gt;\n\n&lt;p&gt;Best tools/methodology for arching files to bluray would be really appreciated.  &lt;/p&gt;\n\n&lt;p&gt;UPDATE: So on to my solution.   &lt;/p&gt;\n\n&lt;p&gt;Frustrated by my inability to find a best practices, I ended up resorting to my own solution.&lt;br/&gt;\nI set up a project folder in Win10 where prepare each bluray by compressing folders of files into .7zip format. This was simple for me and kept everything organized the way that I liked it.  &lt;/p&gt;\n\n&lt;p&gt;Once I had various folders with various .7zip compressed archives, I go to the root of my project folder (this would also be the root once these files are copied over the bluray), I highlight all directories on the root and any root level files, right click on the highlighted files go to 7zip in your context menu, then CRC SHA, then SHA-256 -&amp;gt; [name].sha256  &lt;/p&gt;\n\n&lt;p&gt;This will output a sha-256 hash for every file selected into a new file called [name].sha256. I then verified that these hashes were accurate by opening up my text editor (Sublime). You&amp;#39;ll see all of the hashes, you can then check random files by going back into your folder and selecting a file, go back through the context menu but this time select: SHA-256 to get only the hash for that file.  &lt;/p&gt;\n\n&lt;p&gt;Once I was confident that the hashes were accurate, I opened up ImgBurn, went to &amp;quot;Write files/folders to disc&amp;quot; and added my files including my SHA-256 file specific to only the files added to my bluray disc. I then went to the options tab and changed the File System to UDF revision 2.60. Selected Verify and then burnt my files to bluray (build).   &lt;/p&gt;\n\n&lt;p&gt;Once the burn was successful, I open the bluray in Windows and compare the hashes of the files again against the SHA-256 file I made to make sure that they show the right values. They did so it&amp;#39;s all good! Copying files to the PC from disc to do another check (super redundant) proved this to be a good method for myself.   &lt;/p&gt;\n\n&lt;p&gt;I make a couple disc copies and then make a third copy on an external HDD. all cold storage.  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s really late here but I wanted to leave this for anyone struggling to find a good path forward. I hope this helps someone! Let me know if it helped you! And if you know of a better way, also please let me know! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124zhcn", "is_robot_indexable": true, "report_reasons": null, "author": "0101-ERROR-1001", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124zhcn/optical_archive_loggingfile_archiving_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124zhcn/optical_archive_loggingfile_archiving_best/", "subreddit_subscribers": 675963, "created_utc": 1680031814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have two dedicated leaseweb servers one with 4TB storage and another bigger one with 16TB storage. I mainly use them as seedboxes as well as for emby/plex streaming. Recently, I learned about nextcloud from this sub and its amazing. \n\nSo I want to migrate all files from my first server to second server, mainly TV Series and movies so that I can use the first server as a dedicated nextCloud machine while I still use the second one for plex and other stuff because my usage for the first server is drastically reduced.\n\nI want to know what's the best way to transfer files between these two machines? Initially I was thinking  SFTPing the files to my local and then SFTPing back to second server. But I live in a different continent from my servers while both my servers share the same country. So it's probably easy to just transfer between two 1Gbps machines. But I want to if there's a better way to do this.", "author_fullname": "t2_5tdnrb9j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to migrate files from one dedicated server to another", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125d6hu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680064573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have two dedicated leaseweb servers one with 4TB storage and another bigger one with 16TB storage. I mainly use them as seedboxes as well as for emby/plex streaming. Recently, I learned about nextcloud from this sub and its amazing. &lt;/p&gt;\n\n&lt;p&gt;So I want to migrate all files from my first server to second server, mainly TV Series and movies so that I can use the first server as a dedicated nextCloud machine while I still use the second one for plex and other stuff because my usage for the first server is drastically reduced.&lt;/p&gt;\n\n&lt;p&gt;I want to know what&amp;#39;s the best way to transfer files between these two machines? Initially I was thinking  SFTPing the files to my local and then SFTPing back to second server. But I live in a different continent from my servers while both my servers share the same country. So it&amp;#39;s probably easy to just transfer between two 1Gbps machines. But I want to if there&amp;#39;s a better way to do this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125d6hu", "is_robot_indexable": true, "report_reasons": null, "author": "Terminal_Monk", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125d6hu/best_way_to_migrate_files_from_one_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125d6hu/best_way_to_migrate_files_from_one_dedicated/", "subreddit_subscribers": 675963, "created_utc": 1680064573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There's an internal wiki that is highly valued at my work, however they have lost their funding and it's looking like it will be gone in the next few months. No plans to retain data or anything.\n\nDoes anyone have any ideas or recommendations on how we could save this data from being lost? I'm a hobbyist in this kind of stuff and have no clue where to start.", "author_fullname": "t2_sor17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations on archiving a wiki?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124tpqe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680019615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s an internal wiki that is highly valued at my work, however they have lost their funding and it&amp;#39;s looking like it will be gone in the next few months. No plans to retain data or anything.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any ideas or recommendations on how we could save this data from being lost? I&amp;#39;m a hobbyist in this kind of stuff and have no clue where to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124tpqe", "is_robot_indexable": true, "report_reasons": null, "author": "myS_", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124tpqe/recommendations_on_archiving_a_wiki/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124tpqe/recommendations_on_archiving_a_wiki/", "subreddit_subscribers": 675963, "created_utc": 1680019615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "*Library of H* is a GUI application for downloading to, viewing from, and managing the ero manga/anime in your possession. It can download either individual galleries or whole of artist/group galleries from a \\*bunch of ero content sites. *(currently only works with Hitomi.)* Get/try it: [https://github.com/hikineet0/library-of-h-python](https://github.com/hikineet0/library-of-h-python)\n\nFeel free to contact me either in this thread or (more preferably) in the repository be it through issues and what not.", "author_fullname": "t2_707b95qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Library of H: Library of Alexandria but for \u30a8\u30ed\u30b9* (*Erosu).", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125h24x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680077316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Library of H&lt;/em&gt; is a GUI application for downloading to, viewing from, and managing the ero manga/anime in your possession. It can download either individual galleries or whole of artist/group galleries from a *bunch of ero content sites. &lt;em&gt;(currently only works with Hitomi.)&lt;/em&gt; Get/try it: &lt;a href=\"https://github.com/hikineet0/library-of-h-python\"&gt;https://github.com/hikineet0/library-of-h-python&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feel free to contact me either in this thread or (more preferably) in the repository be it through issues and what not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?auto=webp&amp;v=enabled&amp;s=2b5a71c737c7d2f491f077a893c2594d259cce84", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b38a177653b3b8fee18334d2e8ae8d415e97c52", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db9131a4ed54dec78620bca3c51f9d0fa62c8269", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dca95f557f6687e4ae2b23b11b1125bbfbb3b014", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3d6f7a50bdbfeae56c456dcdfd80f9fc940b962", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef17cf2b7ef2543c2369193461d92d38128c031a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb95a73b9690016940c30d9ce6cda5e7263faa15", "width": 1080, "height": 540}], "variants": {}, "id": "S-wn7zIuHmgKMhRYd_4Ltqv0Kv2z_990YoylGjrouCE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125h24x", "is_robot_indexable": true, "report_reasons": null, "author": "letters-and-dashes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125h24x/library_of_h_library_of_alexandria_but_for_\u30a8\u30ed\u30b9/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125h24x/library_of_h_library_of_alexandria_but_for_\u30a8\u30ed\u30b9/", "subreddit_subscribers": 675963, "created_utc": 1680077316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've never bought used drives before, but I'm in the sweet spot of having sufficient backups that I can risk it more than in the past, and I also need to get my $/tb down.\n\nI'm looking at some \"manufacture recertified\" drives from serverpartdeals.com. I've seen a lot of good things about them, including around this sub. I wanted to ask from those of you who have bought from them:\n\n1. is there any real consistency in the sort of wear on the drives? Like, are you getting drives with easily 10k+hours on the drive and/or drives that have DOM at least 2-3 years out? Conversely, are you receiving drives that are consistently newer in age and/or lower in hours than you'd have expected? Or is it all over the place? \n\n\n2. The exos seem to run a hair cheaper than the Ultrastars when I look around. Honestly, I haven't bought a Seagate drive in six years so I'm out of the loop on the dependability. I'm a bit worried about what I've seen from reviews and the like on Exos drives regarding reliability. I know all drives fail, some very fast and it's foolish to try and predict reliability in any shape or form. So, I'm guiltily confessing that I'm interested in your anecdotal experiences on this one, in buying those recertified Exos drives and how it went. \n\n\n3. I saw a two year limited warranty on these drives from Serverpartdeals. Have any of you ever had to take advantage of this? How did RMA go?\n\nI really appreciate any help, advice or guidance. I really want to try out some used drives, just have these questions and concerns hanging over me. But I also understand these questions might be kind of dumb. Thanks for any help.", "author_fullname": "t2_sn9i9n2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about \"manufacture recertified\" drives from serverpartdeals.com", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1254n0i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680042974.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680042591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never bought used drives before, but I&amp;#39;m in the sweet spot of having sufficient backups that I can risk it more than in the past, and I also need to get my $/tb down.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking at some &amp;quot;manufacture recertified&amp;quot; drives from serverpartdeals.com. I&amp;#39;ve seen a lot of good things about them, including around this sub. I wanted to ask from those of you who have bought from them:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;is there any real consistency in the sort of wear on the drives? Like, are you getting drives with easily 10k+hours on the drive and/or drives that have DOM at least 2-3 years out? Conversely, are you receiving drives that are consistently newer in age and/or lower in hours than you&amp;#39;d have expected? Or is it all over the place? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The exos seem to run a hair cheaper than the Ultrastars when I look around. Honestly, I haven&amp;#39;t bought a Seagate drive in six years so I&amp;#39;m out of the loop on the dependability. I&amp;#39;m a bit worried about what I&amp;#39;ve seen from reviews and the like on Exos drives regarding reliability. I know all drives fail, some very fast and it&amp;#39;s foolish to try and predict reliability in any shape or form. So, I&amp;#39;m guiltily confessing that I&amp;#39;m interested in your anecdotal experiences on this one, in buying those recertified Exos drives and how it went. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I saw a two year limited warranty on these drives from Serverpartdeals. Have any of you ever had to take advantage of this? How did RMA go?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I really appreciate any help, advice or guidance. I really want to try out some used drives, just have these questions and concerns hanging over me. But I also understand these questions might be kind of dumb. Thanks for any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1254n0i", "is_robot_indexable": true, "report_reasons": null, "author": "Peruvian_Poo_Pickler", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1254n0i/questions_about_manufacture_recertified_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1254n0i/questions_about_manufacture_recertified_drives/", "subreddit_subscribers": 675963, "created_utc": 1680042591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As far as I can work out, it\u2019s usually cheaper to create a brand new NAS than it is to host a backup of your data in S3/Glacier for more than a year?\n\nFor example, \u201cS3 Glacier Flexible Retrieval\u201d is $0.00405 per GB, which sounds cheap but multiply that by 1000 to get TB then again by, say, 36TB which is your average RAID 6 with 5 x 12TB disks and you get $145 a month. ($4.05 per month, per TB)\n\nCosts over a year would be $1749.60 for Glacier, and you could build a new fully populated Synology NAS for that, and that\u2019s a one-time purchase which should in theory last 5 years or more.\n\nIs AWS the cheapest solution, when you factor in power costs as well? Worth noting that Backblaze is $5 per TB so Glacier is cheaper.\n\nObviously there\u2019s the offsite considerations too, which the cloud providers definitely have an advantage of.", "author_fullname": "t2_8429z2fz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With a decently large NAS, how do you back up this amount of data cost-effectively?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12526pe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680037272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As far as I can work out, it\u2019s usually cheaper to create a brand new NAS than it is to host a backup of your data in S3/Glacier for more than a year?&lt;/p&gt;\n\n&lt;p&gt;For example, \u201cS3 Glacier Flexible Retrieval\u201d is $0.00405 per GB, which sounds cheap but multiply that by 1000 to get TB then again by, say, 36TB which is your average RAID 6 with 5 x 12TB disks and you get $145 a month. ($4.05 per month, per TB)&lt;/p&gt;\n\n&lt;p&gt;Costs over a year would be $1749.60 for Glacier, and you could build a new fully populated Synology NAS for that, and that\u2019s a one-time purchase which should in theory last 5 years or more.&lt;/p&gt;\n\n&lt;p&gt;Is AWS the cheapest solution, when you factor in power costs as well? Worth noting that Backblaze is $5 per TB so Glacier is cheaper.&lt;/p&gt;\n\n&lt;p&gt;Obviously there\u2019s the offsite considerations too, which the cloud providers definitely have an advantage of.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12526pe", "is_robot_indexable": true, "report_reasons": null, "author": "BowtieChickenAlfredo", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12526pe/with_a_decently_large_nas_how_do_you_back_up_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12526pe/with_a_decently_large_nas_how_do_you_back_up_this/", "subreddit_subscribers": 675963, "created_utc": 1680037272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a lot of backups of backups -- I imagine many of you can relate. I started using \\`git annex\\` a few years back, so now for the most part I'm at least not making the problem worse, but I still have a lot of old redundant folders that I need to manually cleanup and every now and then I make a little more progress. \n\nI know there are plenty of options for finding duplicate and even similar files, but I'm wondering if any of them fold that up into a tree or folder level to identify near duplicate folders based on the files they contain (it would be even better yet if they happened to support zip and tar archives!)\n\nThe obvious (but wrong) solution I once thought I was looking for was a way to merge near duplicates -- with the assumption that Backup A contained files 1-70 and backup B contained 30-100, so I obviously wanted the superset of 1-100.  Over time though, I've discovered that such a naive approach makes deleting files impossible -- because really, the last thing that I want to do is add back all of the junk photos that I've painstakingly deleted out of a curated folder. \n\nIf anybody has a good process or methodology for handling that reality, I'm all ears!", "author_fullname": "t2_d1jtiqct", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does czkawka (or any other linux tool) have a feature for finding duplicate folders?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124o93z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680008984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a lot of backups of backups -- I imagine many of you can relate. I started using `git annex` a few years back, so now for the most part I&amp;#39;m at least not making the problem worse, but I still have a lot of old redundant folders that I need to manually cleanup and every now and then I make a little more progress. &lt;/p&gt;\n\n&lt;p&gt;I know there are plenty of options for finding duplicate and even similar files, but I&amp;#39;m wondering if any of them fold that up into a tree or folder level to identify near duplicate folders based on the files they contain (it would be even better yet if they happened to support zip and tar archives!)&lt;/p&gt;\n\n&lt;p&gt;The obvious (but wrong) solution I once thought I was looking for was a way to merge near duplicates -- with the assumption that Backup A contained files 1-70 and backup B contained 30-100, so I obviously wanted the superset of 1-100.  Over time though, I&amp;#39;ve discovered that such a naive approach makes deleting files impossible -- because really, the last thing that I want to do is add back all of the junk photos that I&amp;#39;ve painstakingly deleted out of a curated folder. &lt;/p&gt;\n\n&lt;p&gt;If anybody has a good process or methodology for handling that reality, I&amp;#39;m all ears!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124o93z", "is_robot_indexable": true, "report_reasons": null, "author": "dscheffy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124o93z/does_czkawka_or_any_other_linux_tool_have_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124o93z/does_czkawka_or_any_other_linux_tool_have_a/", "subreddit_subscribers": 675963, "created_utc": 1680008984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am attempting to extract a file from the latest dump download from Wikipedia, but it comes out corrupted every time. I've used WinZip and 7zip, but when I use my python code to clean up the file for the learning model I want to make, it says that the file is corrupted. I am very sure that the code is correct, as I have checked it with many sources.", "author_fullname": "t2_8v06mu07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wikipedia Dump Extraction", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125eqjc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680069513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am attempting to extract a file from the latest dump download from Wikipedia, but it comes out corrupted every time. I&amp;#39;ve used WinZip and 7zip, but when I use my python code to clean up the file for the learning model I want to make, it says that the file is corrupted. I am very sure that the code is correct, as I have checked it with many sources.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125eqjc", "is_robot_indexable": true, "report_reasons": null, "author": "Swirly403", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125eqjc/wikipedia_dump_extraction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125eqjc/wikipedia_dump_extraction/", "subreddit_subscribers": 675963, "created_utc": 1680069513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have 8 smaller SSDs ranging from 120 GBs to 480 GBs from my previous employer who no longer needed them.\n\nMy question to you guys is; What would be the best way to hook these up to my computer? I am using all my sata connections already. I looked into something like this https://www.amazon.com/Sabrent-Tool-free-Enclosure-Optimized-EC-UASP/dp/B00OJ3UJ2S - but I figure there has got to be a better way and you guys would be the ones to know.\n\nThanks!", "author_fullname": "t2_5v1ec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smaller Sized SSDs Laying Around The House", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12593av", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680053347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have 8 smaller SSDs ranging from 120 GBs to 480 GBs from my previous employer who no longer needed them.&lt;/p&gt;\n\n&lt;p&gt;My question to you guys is; What would be the best way to hook these up to my computer? I am using all my sata connections already. I looked into something like this &lt;a href=\"https://www.amazon.com/Sabrent-Tool-free-Enclosure-Optimized-EC-UASP/dp/B00OJ3UJ2S\"&gt;https://www.amazon.com/Sabrent-Tool-free-Enclosure-Optimized-EC-UASP/dp/B00OJ3UJ2S&lt;/a&gt; - but I figure there has got to be a better way and you guys would be the ones to know.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12593av", "is_robot_indexable": true, "report_reasons": null, "author": "FlameHaze", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12593av/smaller_sized_ssds_laying_around_the_house/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12593av/smaller_sized_ssds_laying_around_the_house/", "subreddit_subscribers": 675963, "created_utc": 1680053347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I was reading some fantasy fiction about having to evacuate earth and set up on another planet, and it got me thinking: were I to be in those shoes and wanted to preserve some IT infrastructure, knowing I wouldn't have the internet later, I would need an offline database of every version of every device driver for every OS it was developed for (among other stuff).\n\nThis isn't something I've seen covered before and couldn't find an example of such a database on initial searches (at least nothing meant to be easily downloaded / mirrored).\n\nHas this wheel already been invented?  If so, where would I go to download a copy of it?", "author_fullname": "t2_3c8ce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comprehensive offline driver database: all devices, all OSs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1257p0p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Seeking data set", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680050022.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680049841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I was reading some fantasy fiction about having to evacuate earth and set up on another planet, and it got me thinking: were I to be in those shoes and wanted to preserve some IT infrastructure, knowing I wouldn&amp;#39;t have the internet later, I would need an offline database of every version of every device driver for every OS it was developed for (among other stuff).&lt;/p&gt;\n\n&lt;p&gt;This isn&amp;#39;t something I&amp;#39;ve seen covered before and couldn&amp;#39;t find an example of such a database on initial searches (at least nothing meant to be easily downloaded / mirrored).&lt;/p&gt;\n\n&lt;p&gt;Has this wheel already been invented?  If so, where would I go to download a copy of it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Only 18TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1257p0p", "is_robot_indexable": true, "report_reasons": null, "author": "l_one", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1257p0p/comprehensive_offline_driver_database_all_devices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1257p0p/comprehensive_offline_driver_database_all_devices/", "subreddit_subscribers": 675963, "created_utc": 1680049841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "...I've got many files listed in the \"file list\" and I've right-clicked on one particular folder that I want excluded from the transfer and I've selected \"Ignore\". Does that mean it will skip copying that folder?\n\nIt's probably obvious, right? I only ask because I tried \"removing\" the folder from the list and it said that that option was only available in the paid version of the software. But...Ignore, if my understanding of the definition of the word is accurate, is...sorta...the same thing? Or, more specifically, \"ignore\" and \"remove\" would both achieve the same goal of not copying that folder...?", "author_fullname": "t2_96z9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dumb Teracopy question: I'm currently transferring some data from one drive to another...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1257lak", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680049801.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680049592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;...I&amp;#39;ve got many files listed in the &amp;quot;file list&amp;quot; and I&amp;#39;ve right-clicked on one particular folder that I want excluded from the transfer and I&amp;#39;ve selected &amp;quot;Ignore&amp;quot;. Does that mean it will skip copying that folder?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s probably obvious, right? I only ask because I tried &amp;quot;removing&amp;quot; the folder from the list and it said that that option was only available in the paid version of the software. But...Ignore, if my understanding of the definition of the word is accurate, is...sorta...the same thing? Or, more specifically, &amp;quot;ignore&amp;quot; and &amp;quot;remove&amp;quot; would both achieve the same goal of not copying that folder...?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1257lak", "is_robot_indexable": true, "report_reasons": null, "author": "ultranothing", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1257lak/dumb_teracopy_question_im_currently_transferring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1257lak/dumb_teracopy_question_im_currently_transferring/", "subreddit_subscribers": 675963, "created_utc": 1680049592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Which cases are good for having 10+ HDDs The most common one I see here is the Fractal Design Define 7 XL. I have also seen the Meshify 2 XL mentioned. Any other good ones I should be looking at?\n\nI am planning to start transitioning to either 18TB or 20TB HDDs. I'll start off with only 2 in this new case, but I know I'm going to get more and fill it up the case in the years to come. I want to buy something now that I can keep using and building in.\n\nThis is my current build and I'm trying to make a second server-only PC: https://pcpartpicker.com/list/TGDPBj\n\nAny advice appreciated. Thanks!", "author_fullname": "t2_qulcas15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended cases for DIY NAS? Hoping to eventually build to 10+ HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1257f3j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680049167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which cases are good for having 10+ HDDs The most common one I see here is the Fractal Design Define 7 XL. I have also seen the Meshify 2 XL mentioned. Any other good ones I should be looking at?&lt;/p&gt;\n\n&lt;p&gt;I am planning to start transitioning to either 18TB or 20TB HDDs. I&amp;#39;ll start off with only 2 in this new case, but I know I&amp;#39;m going to get more and fill it up the case in the years to come. I want to buy something now that I can keep using and building in.&lt;/p&gt;\n\n&lt;p&gt;This is my current build and I&amp;#39;m trying to make a second server-only PC: &lt;a href=\"https://pcpartpicker.com/list/TGDPBj\"&gt;https://pcpartpicker.com/list/TGDPBj&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any advice appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1257f3j", "is_robot_indexable": true, "report_reasons": null, "author": "IHateReddit_1153151", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1257f3j/recommended_cases_for_diy_nas_hoping_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1257f3j/recommended_cases_for_diy_nas_hoping_to/", "subreddit_subscribers": 675963, "created_utc": 1680049167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I stumbled across a couple of pretty cheap SC280 but i havent used dell storage systems yet.  \nOn the spec sheet they offer 84bay of 6g (6x 4x 6g so total of 144G/18gb/s uplink) in a 5u chassis all for under 300\u20ac\n\nI couldnt really find any reviews or experiences from other homelabbers, feel free to share any thoughts about them.\n\nMy main questions are:\n\n**What type of licensing do they have?** (Management/IDrac or per disk etc)\n\n**Do they use any specialized stuff that would stop them from working with normal lsi hbas/truenas?**\n\n**Is there a limit on disk size?** (planing on using a mix of 3,6 and 8tb all sas drives)\n\n**Is there some kind of vendor locking of disks?**\n\nAny suggestions appreciated :)", "author_fullname": "t2_8jnr5wv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dell Compellent SC280 | Thoughts | Licensing? | Max Disk size? | Power draw?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1256jpq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680046998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled across a couple of pretty cheap SC280 but i havent used dell storage systems yet.&lt;br/&gt;\nOn the spec sheet they offer 84bay of 6g (6x 4x 6g so total of 144G/18gb/s uplink) in a 5u chassis all for under 300\u20ac&lt;/p&gt;\n\n&lt;p&gt;I couldnt really find any reviews or experiences from other homelabbers, feel free to share any thoughts about them.&lt;/p&gt;\n\n&lt;p&gt;My main questions are:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What type of licensing do they have?&lt;/strong&gt; (Management/IDrac or per disk etc)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Do they use any specialized stuff that would stop them from working with normal lsi hbas/truenas?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there a limit on disk size?&lt;/strong&gt; (planing on using a mix of 3,6 and 8tb all sas drives)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there some kind of vendor locking of disks?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1256jpq", "is_robot_indexable": true, "report_reasons": null, "author": "Pommes254", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1256jpq/dell_compellent_sc280_thoughts_licensing_max_disk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1256jpq/dell_compellent_sc280_thoughts_licensing_max_disk/", "subreddit_subscribers": 675963, "created_utc": 1680046998.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I Have 6 2.5\" hdd's laying around.\nI would like to use it for some purpose like a low powered temporary Plex server where I can connect all the drives and use it as one single drive via Snapraid or something.\nReliability is not an issue so even if the drives fail later on, not bothered.\nI need some compact solution which can work via USB.", "author_fullname": "t2_6zlh9fwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connect multiple 2.5\" hdd via USB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124xez5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680027437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I Have 6 2.5&amp;quot; hdd&amp;#39;s laying around.\nI would like to use it for some purpose like a low powered temporary Plex server where I can connect all the drives and use it as one single drive via Snapraid or something.\nReliability is not an issue so even if the drives fail later on, not bothered.\nI need some compact solution which can work via USB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124xez5", "is_robot_indexable": true, "report_reasons": null, "author": "Vap0rx47", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124xez5/connect_multiple_25_hdd_via_usb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124xez5/connect_multiple_25_hdd_via_usb/", "subreddit_subscribers": 675963, "created_utc": 1680027437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I need help managing my data chaos. I have a lot of photos and recently experienced a data disaster. While trying to recover some of the lost photos, I ended up with duplicates and files with different names. Some photos are also missing EXIF information or have a lower resolution. How would you go about organizing this mess? My goal is to have only one version of each photo (\"the original\") with the best possible quality, including the highest resolution, minimal compression, and complete EXIF data.", "author_fullname": "t2_a2o3q909", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing a Chaotic 4 TB Photo Collection and Eliminating Duplicates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124vknh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680023548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I need help managing my data chaos. I have a lot of photos and recently experienced a data disaster. While trying to recover some of the lost photos, I ended up with duplicates and files with different names. Some photos are also missing EXIF information or have a lower resolution. How would you go about organizing this mess? My goal is to have only one version of each photo (&amp;quot;the original&amp;quot;) with the best possible quality, including the highest resolution, minimal compression, and complete EXIF data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124vknh", "is_robot_indexable": true, "report_reasons": null, "author": "silvermir", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124vknh/organizing_a_chaotic_4_tb_photo_collection_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124vknh/organizing_a_chaotic_4_tb_photo_collection_and/", "subreddit_subscribers": 675963, "created_utc": 1680023548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone!\n\nI was going through some old hard drives and found a folder with about 200k+ emails back from my college days. They are all .eml single messages. I would love to get them loaded into Outlook or some other app/web service that will let me easily search/find a specific thread when needed. Right now Windows Explorer and Mac Finder both severely lag/crash when I try to do anything with 200k files. \n\nMy original thought process was to get them uploaded to my gmail account using Thunderbird + IMAP connection. I figured Google can help me with the search aspect and it would be nice to have all my emails in one place. But this is a super slow process and Thunderbird crashes after a day of uploading. I have also tried importing the emails to Outlook desktop but the app doesn't support eml files without conversion.\n\nI figured I can't be the only one facing this and would love to get advice as to how to get these eml files into a searchable form. \n\nThanks in advance for all your help!", "author_fullname": "t2_nvhbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommends on Email Archiving with Searchability in Mind?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124va37", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680022912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I was going through some old hard drives and found a folder with about 200k+ emails back from my college days. They are all .eml single messages. I would love to get them loaded into Outlook or some other app/web service that will let me easily search/find a specific thread when needed. Right now Windows Explorer and Mac Finder both severely lag/crash when I try to do anything with 200k files. &lt;/p&gt;\n\n&lt;p&gt;My original thought process was to get them uploaded to my gmail account using Thunderbird + IMAP connection. I figured Google can help me with the search aspect and it would be nice to have all my emails in one place. But this is a super slow process and Thunderbird crashes after a day of uploading. I have also tried importing the emails to Outlook desktop but the app doesn&amp;#39;t support eml files without conversion.&lt;/p&gt;\n\n&lt;p&gt;I figured I can&amp;#39;t be the only one facing this and would love to get advice as to how to get these eml files into a searchable form. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for all your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124va37", "is_robot_indexable": true, "report_reasons": null, "author": "sgtawesomesauce", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124va37/recommends_on_email_archiving_with_searchability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124va37/recommends_on_email_archiving_with_searchability/", "subreddit_subscribers": 675963, "created_utc": 1680022912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure when this went on sale but for a Pro, this seems like an impeccable price for coming direct from Newegg. I know there are those that won\u2019t buy from a third party even if it\u2019s new, and if that\u2019s you, I\u2019d pounce.", "author_fullname": "t2_a65nr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sold direct from Newegg, $229/$249 for 14TB/16TB Iron Wolf Pro", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_125mwm8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1680093512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "newegg.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure when this went on sale but for a Pro, this seems like an impeccable price for coming direct from Newegg. I know there are those that won\u2019t buy from a third party even if it\u2019s new, and if that\u2019s you, I\u2019d pounce.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.newegg.com/seagate-ironwolf-pro-st16000ne000-16tb/p/N82E16822184804?Item=", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "125mwm8", "is_robot_indexable": true, "report_reasons": null, "author": "Gymnastboatman", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125mwm8/sold_direct_from_newegg_229249_for_14tb16tb_iron/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.newegg.com/seagate-ironwolf-pro-st16000ne000-16tb/p/N82E16822184804?Item=", "subreddit_subscribers": 675963, "created_utc": 1680093512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nI've  been looking to download some 1080p versions of some old shows, but  I've been hoping to find 1080p versions of them. I can manage some,  where it looks as though the uploader has remasted it.\n\nHow do I find these uploads, or do I have to try and find a way to up the quality myself?  \n\n\nDo you think it's better to say Fuck It and just have old shows in their older resolution?", "author_fullname": "t2_w6sa6uq6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you guys do to get better quality versions of old shows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125jkxw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680085684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve  been looking to download some 1080p versions of some old shows, but  I&amp;#39;ve been hoping to find 1080p versions of them. I can manage some,  where it looks as though the uploader has remasted it.&lt;/p&gt;\n\n&lt;p&gt;How do I find these uploads, or do I have to try and find a way to up the quality myself?  &lt;/p&gt;\n\n&lt;p&gt;Do you think it&amp;#39;s better to say Fuck It and just have old shows in their older resolution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125jkxw", "is_robot_indexable": true, "report_reasons": null, "author": "Loglogloglog11221", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125jkxw/what_do_you_guys_do_to_get_better_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125jkxw/what_do_you_guys_do_to_get_better_quality/", "subreddit_subscribers": 675963, "created_utc": 1680085684.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}