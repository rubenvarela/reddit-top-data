{"kind": "Listing", "data": {"after": "t3_12523tl", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've just completed my 3rd data project to help me understand how to work with Airflow and running services in Docker.\n\n# Links\n\n* [GitHub Repository](https://github.com/digitalghost-dev/global-data-pipeline)\n* [Looker Studio Visualization](https://lookerstudio.google.com/reporting/3710d6bb-25b2-4d64-b6e8-2889bc57c74b) \\- not a great experience on mobile, Air Quality page doesn't seem to load.\n* [Documentation](https://github.com/digitalghost-dev/global-data-pipeline/wiki/Global-Data-Pipeline-Documentation) \\- tried my best with this, will need to run through it again and proof read.\n* [Discord Server Invite](https://discord.gg/j2HEfpebuH) \\- feel free to join to see the bot in action. There is only one channel and it's locked down so not much do in here but thought I would add it in case someone was curious. The bot will query the database and look for the highest current\\_temp and will send a message with the city name and the temperature in celsius.\n\n# Overview\n\n* A `docker-compose.yml` file runs Airflow, Postgres, and Redis in Docker containers.\n* Python scripts reach out to different data sources to extract, transform and load the data into a Postgres database, orchestrated through Airflow on various schedules.\n* Using Airflow operators, data is moved from Postgres to Google Cloud Storage then to BigQuery where the data is visualized with Looker Studio.\n* A Discord Airflow operator is used to send a daily message to a server with current weather stats.\n\n# Data Sources\n\nThis project uses two APIs and web scrapes some tables from Wikipedia. All the city data derives from choosing the 50 most populated cities in the world according to [MacroTrends](https://www.macrotrends.net/cities/largest-cities-by-population).\n\n* City Weather - (updated hourly) with [Weatherstack](https://weatherstack.com) API - costs $10 a month for 50,000 calls.\n   * Current temperature, humidity, precipitation, wind speed\n* City Air Quality - (updated hourly) with [OpenWeatherMap](https://openweathermap.org) API\n   * CO, NO2, O2, SO2, PM2.5, PM10\n* City population\n* Country statistics\n   * Fertility rates, homicide rates, Human Development Index, unemployments rates\n\n[Flowchart](https://preview.redd.it/zz181kpt6iqa1.png?width=2112&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b1485fbb458838ec703bbaeceea381cd635d2db1)\n\n# Notes\n\nSetting up Airflow was pretty painless with the predefined `docker-compose.yml` file found [here](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html). I did have to modify the original file a bit to allow containers to talk to each other on my host machine.\n\nSpeaking of host machines, all of this is running on my desktop.\n\nLooker Studio is okay... it's free so I guess I can't complain too much but the experience for viewers on mobile is pretty bad.\n\nThe visualizations I made in Looker Studio are elementary at best but my goal wasn't to build the prettiest dashboard. I will continue to update it though in the future.", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My 3rd data project, with Airflow, Docker, Postgres, and Looker Studio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zz181kpt6iqa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 65, "x": 108, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cecb6cf097ae6c7442827e55a85ef64d18ca18e"}, {"y": 131, "x": 216, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=edb440a66cf60727d5afd2ed6492dfb5a622d3f2"}, {"y": 195, "x": 320, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f297b107b70090c6504a542f6185a35a12dd231"}, {"y": 390, "x": 640, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d551441f10fa5dc8df7c67ed23fcf42348b6058a"}, {"y": 585, "x": 960, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cdf0fa788b94f59330fcbf6e3633ed26e63095f"}, {"y": 658, "x": 1080, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f60f656bbef58516246acbe6296c717f4b87d6fa"}], "s": {"y": 1288, "x": 2112, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=2112&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b1485fbb458838ec703bbaeceea381cd635d2db1"}, "id": "zz181kpt6iqa1"}}, "name": "t3_124wcjb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EWJ1r_6hf0lhfYglqOaweEIFjMkf0VQ2GvKa6YqbKS0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1680025179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just completed my 3rd data project to help me understand how to work with Airflow and running services in Docker.&lt;/p&gt;\n\n&lt;h1&gt;Links&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/digitalghost-dev/global-data-pipeline\"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://lookerstudio.google.com/reporting/3710d6bb-25b2-4d64-b6e8-2889bc57c74b\"&gt;Looker Studio Visualization&lt;/a&gt; - not a great experience on mobile, Air Quality page doesn&amp;#39;t seem to load.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/digitalghost-dev/global-data-pipeline/wiki/Global-Data-Pipeline-Documentation\"&gt;Documentation&lt;/a&gt; - tried my best with this, will need to run through it again and proof read.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://discord.gg/j2HEfpebuH\"&gt;Discord Server Invite&lt;/a&gt; - feel free to join to see the bot in action. There is only one channel and it&amp;#39;s locked down so not much do in here but thought I would add it in case someone was curious. The bot will query the database and look for the highest current_temp and will send a message with the city name and the temperature in celsius.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Overview&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A &lt;code&gt;docker-compose.yml&lt;/code&gt; file runs Airflow, Postgres, and Redis in Docker containers.&lt;/li&gt;\n&lt;li&gt;Python scripts reach out to different data sources to extract, transform and load the data into a Postgres database, orchestrated through Airflow on various schedules.&lt;/li&gt;\n&lt;li&gt;Using Airflow operators, data is moved from Postgres to Google Cloud Storage then to BigQuery where the data is visualized with Looker Studio.&lt;/li&gt;\n&lt;li&gt;A Discord Airflow operator is used to send a daily message to a server with current weather stats.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Data Sources&lt;/h1&gt;\n\n&lt;p&gt;This project uses two APIs and web scrapes some tables from Wikipedia. All the city data derives from choosing the 50 most populated cities in the world according to &lt;a href=\"https://www.macrotrends.net/cities/largest-cities-by-population\"&gt;MacroTrends&lt;/a&gt;.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;City Weather - (updated hourly) with &lt;a href=\"https://weatherstack.com\"&gt;Weatherstack&lt;/a&gt; API - costs $10 a month for 50,000 calls.\n\n&lt;ul&gt;\n&lt;li&gt;Current temperature, humidity, precipitation, wind speed&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;City Air Quality - (updated hourly) with &lt;a href=\"https://openweathermap.org\"&gt;OpenWeatherMap&lt;/a&gt; API\n\n&lt;ul&gt;\n&lt;li&gt;CO, NO2, O2, SO2, PM2.5, PM10&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;City population&lt;/li&gt;\n&lt;li&gt;Country statistics\n\n&lt;ul&gt;\n&lt;li&gt;Fertility rates, homicide rates, Human Development Index, unemployments rates&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zz181kpt6iqa1.png?width=2112&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b1485fbb458838ec703bbaeceea381cd635d2db1\"&gt;Flowchart&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Notes&lt;/h1&gt;\n\n&lt;p&gt;Setting up Airflow was pretty painless with the predefined &lt;code&gt;docker-compose.yml&lt;/code&gt; file found &lt;a href=\"https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html\"&gt;here&lt;/a&gt;. I did have to modify the original file a bit to allow containers to talk to each other on my host machine.&lt;/p&gt;\n\n&lt;p&gt;Speaking of host machines, all of this is running on my desktop.&lt;/p&gt;\n\n&lt;p&gt;Looker Studio is okay... it&amp;#39;s free so I guess I can&amp;#39;t complain too much but the experience for viewers on mobile is pretty bad.&lt;/p&gt;\n\n&lt;p&gt;The visualizations I made in Looker Studio are elementary at best but my goal wasn&amp;#39;t to build the prettiest dashboard. I will continue to update it though in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?auto=webp&amp;v=enabled&amp;s=dc7b06ba82e2a238e6a1e9732af3c38e0694e6b0", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bcb73cfd09ed2e135d54bf6ee2c9ce08e61fea7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f22c542e5e61014373bc88e9fb88fbf5f092391", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0bea044b442d5c5cfc86170d6673d169d1d8f14", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68a01789cc3524d21df30163274621bf690ed230", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=995876c9cdc7ea26d77e80bede7d8c18bdebb1db", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60a5df26e9b64687f8eb5b08795cc711fd35424b", "width": 1080, "height": 540}], "variants": {}, "id": "65nZrXRToPy1C10OOcyNpu-vcLkcCd-DzvBSwTc_SbY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "124wcjb", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124wcjb/my_3rd_data_project_with_airflow_docker_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124wcjb/my_3rd_data_project_with_airflow_docker_postgres/", "subreddit_subscribers": 94851, "created_utc": 1680025179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://github.com/awslabs/mountpoint-s3", "author_fullname": "t2_anaphz9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mountpoint for S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_125cd6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BzCeCYVYoRezIcRAh51k_PJz49QWs8fkDVSTdaNsu2c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680062177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/awslabs/mountpoint-s3\"&gt;https://github.com/awslabs/mountpoint-s3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/tjtdex6f7nqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?auto=webp&amp;v=enabled&amp;s=53e157290d01953c562e2e9aff21f421200970ef", "width": 1080, "height": 636}, "resolutions": [{"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6cc52fcf4e0b3224b4dc0e4e68050d570a3c76d", "width": 108, "height": 63}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8449b091dc519f5194a98d2709fed9e71baeb4a0", "width": 216, "height": 127}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ed365562b9aad4866830e6371ee41f3b143c309", "width": 320, "height": 188}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ff92623702f37a5300c92e08b75a599c156ecfb", "width": 640, "height": 376}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9736621428871edb6d85add8b0a58a09bdbc4d26", "width": 960, "height": 565}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b56b22a5177542b6f31407d868bf7e038d1541de", "width": 1080, "height": 636}], "variants": {}, "id": "lKbMI6JMXWTO1kGTw436EAb1GUZ0XysjvnbN03C2Yf8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125cd6n", "is_robot_indexable": true, "report_reasons": null, "author": "Sweet-Butterscotch11", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125cd6n/mountpoint_for_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/tjtdex6f7nqa1.png", "subreddit_subscribers": 94851, "created_utc": 1680062177.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you guys charge for independent contracting rates? I am an experienced Senior Data Engineer. Will be leading key projects for the company.\n\nRight now I am thinking $125/hr or $1000/day given a multi-year engagement. Client has somewhat \"limited\" resources, but a field I am very interested in learning more about. I don't want to hear \"Oh yeah, you could easily get $500/hr!!!\" that's not realistic. Give it to me straight. Am I lowballing myself here?", "author_fullname": "t2_qleqo7a5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consulting Rates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1251kbg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680036019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you guys charge for independent contracting rates? I am an experienced Senior Data Engineer. Will be leading key projects for the company.&lt;/p&gt;\n\n&lt;p&gt;Right now I am thinking $125/hr or $1000/day given a multi-year engagement. Client has somewhat &amp;quot;limited&amp;quot; resources, but a field I am very interested in learning more about. I don&amp;#39;t want to hear &amp;quot;Oh yeah, you could easily get $500/hr!!!&amp;quot; that&amp;#39;s not realistic. Give it to me straight. Am I lowballing myself here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1251kbg", "is_robot_indexable": true, "report_reasons": null, "author": "IndependentElk3997", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1251kbg/consulting_rates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1251kbg/consulting_rates/", "subreddit_subscribers": 94851, "created_utc": 1680036019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am getting a new job in a company that they mainly use Azure + Databricks. I don't have experience with those 2. Where do you suggest I should start to learn them?\n\nIt can be a course on udemy , a course on microsoft for a certification I don't mind.", "author_fullname": "t2_154xx5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure + Databricks course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125knww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680088188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am getting a new job in a company that they mainly use Azure + Databricks. I don&amp;#39;t have experience with those 2. Where do you suggest I should start to learn them?&lt;/p&gt;\n\n&lt;p&gt;It can be a course on udemy , a course on microsoft for a certification I don&amp;#39;t mind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "125knww", "is_robot_indexable": true, "report_reasons": null, "author": "darkvoidman", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125knww/azure_databricks_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125knww/azure_databricks_course/", "subreddit_subscribers": 94851, "created_utc": 1680088188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks!\n\nNew to Azure. I'm wondering what's the preferred method for sending email error alerts in ADF. Is it using: \n1. ADF's metric and alerts ; or\n2. Using a LogicApp\n\nThanks!", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the preferred method for email notification in Azure Data Factory?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125lueq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680090752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks!&lt;/p&gt;\n\n&lt;p&gt;New to Azure. I&amp;#39;m wondering what&amp;#39;s the preferred method for sending email error alerts in ADF. Is it using: \n1. ADF&amp;#39;s metric and alerts ; or\n2. Using a LogicApp&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125lueq", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125lueq/whats_the_preferred_method_for_email_notification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125lueq/whats_the_preferred_method_for_email_notification/", "subreddit_subscribers": 94851, "created_utc": 1680090752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Interview tomorrow. I was told the focus of the session will be technical questions regarding the ongoing technical management of BAU systems. We are talking Azure, both batch and stream ingestion to a lakehouse architecture. The use of databricks and cloudera, ARM templates and CI/CD is scattered throughout the job desc. \n\nKeen to hear what you guys consider the technical management of BAU systems!", "author_fullname": "t2_r7bmwiee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you consider \"ongoing technical management of BAU systems\" in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125pwxn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680100454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interview tomorrow. I was told the focus of the session will be technical questions regarding the ongoing technical management of BAU systems. We are talking Azure, both batch and stream ingestion to a lakehouse architecture. The use of databricks and cloudera, ARM templates and CI/CD is scattered throughout the job desc. &lt;/p&gt;\n\n&lt;p&gt;Keen to hear what you guys consider the technical management of BAU systems!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "125pwxn", "is_robot_indexable": true, "report_reasons": null, "author": "va1kyrja-kara", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125pwxn/what_do_you_consider_ongoing_technical_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125pwxn/what_do_you_consider_ongoing_technical_management/", "subreddit_subscribers": 94851, "created_utc": 1680100454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS or azure doesn't matter.", "author_fullname": "t2_vtx6qjs2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you be using for one time ingestion of large volume of data today, say 100tb.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1251wbd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680036691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS or azure doesn&amp;#39;t matter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1251wbd", "is_robot_indexable": true, "report_reasons": null, "author": "Budget_Assignment457", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1251wbd/what_would_you_be_using_for_one_time_ingestion_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1251wbd/what_would_you_be_using_for_one_time_ingestion_of/", "subreddit_subscribers": 94851, "created_utc": 1680036691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is doing some restructuring as we move into a more mature dev shop. I'm just looking for ideas to better integrate data and product teams.\n\nDo you like how it's structured? Anything you wish you could change?", "author_fullname": "t2_gtjfu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you and your team interact with the Product team at your organization?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125q1b9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680100646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is doing some restructuring as we move into a more mature dev shop. I&amp;#39;m just looking for ideas to better integrate data and product teams.&lt;/p&gt;\n\n&lt;p&gt;Do you like how it&amp;#39;s structured? Anything you wish you could change?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125q1b9", "is_robot_indexable": true, "report_reasons": null, "author": "LemurPrime", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125q1b9/how_do_you_and_your_team_interact_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125q1b9/how_do_you_and_your_team_interact_with_the/", "subreddit_subscribers": 94851, "created_utc": 1680100646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we are using prefect and generally I'm quite happy so far but I still did not figure out how to do proper GitOps with it (eg to kubernetes with argocd/flux). Main issue is mostly the deployments. The actual workflows are already dockerimages. Currently we have deployment code or CLI calls checked into our deployment repository and they will be automatically executed in a pod within the cluster in case of changes. Thats not real gitops though as we never really mirror the state of the server to the state of the repo and server devistions are not fixed automatically as well.\n\nWhats your experience? Are there better solutions with other frameworks? Ideal would be probably a kuberetes operator and deployments via CRD\n\nEdit: added gitops to k8s flux/argocd", "author_fullname": "t2_24837qxb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitOps with prefect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125h3nq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680102811.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680077458.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we are using prefect and generally I&amp;#39;m quite happy so far but I still did not figure out how to do proper GitOps with it (eg to kubernetes with argocd/flux). Main issue is mostly the deployments. The actual workflows are already dockerimages. Currently we have deployment code or CLI calls checked into our deployment repository and they will be automatically executed in a pod within the cluster in case of changes. Thats not real gitops though as we never really mirror the state of the server to the state of the repo and server devistions are not fixed automatically as well.&lt;/p&gt;\n\n&lt;p&gt;Whats your experience? Are there better solutions with other frameworks? Ideal would be probably a kuberetes operator and deployments via CRD&lt;/p&gt;\n\n&lt;p&gt;Edit: added gitops to k8s flux/argocd&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125h3nq", "is_robot_indexable": true, "report_reasons": null, "author": "jeremyZen2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125h3nq/gitops_with_prefect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125h3nq/gitops_with_prefect/", "subreddit_subscribers": 94851, "created_utc": 1680077458.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "By Bill Inmon, Any one read it, its DB sponsored (lakehouse tm). Worthy read? I assuming very specific to DB workflows?", "author_fullname": "t2_6lfdg0it", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building the Data Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125dxf8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680066916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;By Bill Inmon, Any one read it, its DB sponsored (lakehouse tm). Worthy read? I assuming very specific to DB workflows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125dxf8", "is_robot_indexable": true, "report_reasons": null, "author": "soundboyselecta", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125dxf8/building_the_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125dxf8/building_the_data_lakehouse/", "subreddit_subscribers": 94851, "created_utc": 1680066916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen this done so many ways at different companies that it confuses me that it's not more standardized. Let's say you have a bunch of data, do a bunch of transformation to it (say, up through dimensional models in your DWH) and it is the trusted location for that data. The company may then want to have data like that available to customers via an API. I'm guessing most companies don't have an external API sitting on top of snowflake or similar due to compute costs of people querying it often. \n\n&amp;#x200B;\n\nDoes your company expose transformed DWH data out through an external API? And if so, what's the pattern of data flow &amp; platforms involved?", "author_fullname": "t2_c6w52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does your company expose data from your DWH out to external customers with an API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125dnju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680066048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen this done so many ways at different companies that it confuses me that it&amp;#39;s not more standardized. Let&amp;#39;s say you have a bunch of data, do a bunch of transformation to it (say, up through dimensional models in your DWH) and it is the trusted location for that data. The company may then want to have data like that available to customers via an API. I&amp;#39;m guessing most companies don&amp;#39;t have an external API sitting on top of snowflake or similar due to compute costs of people querying it often. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does your company expose transformed DWH data out through an external API? And if so, what&amp;#39;s the pattern of data flow &amp;amp; platforms involved?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125dnju", "is_robot_indexable": true, "report_reasons": null, "author": "EmergenL", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125dnju/does_your_company_expose_data_from_your_dwh_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125dnju/does_your_company_expose_data_from_your_dwh_out/", "subreddit_subscribers": 94851, "created_utc": 1680066048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ui4m14ke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to transition to a Data Architect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1257sut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/J3Yh7qA10YYJTrTkuGOZ9Yw2_LQx4Ss8f6scA_auTXE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680050106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@anupmoncy/how-to-transition-to-a-data-architect-83aa835ea53c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?auto=webp&amp;v=enabled&amp;s=7dfd4ccc789b8180380a2c52c7643250c8cb0bda", "width": 1200, "height": 799}, "resolutions": [{"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=104214fa8c81fdd0d7d07e7012cba83b15add55d", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2928ce334f49dfdb0c12dbde7585fa633b1838d", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ed546a816ab21e6c5e63529f7f43b0bd33dbc82", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db94f0be8640fbebe58040d5ed5f0698ca085a1d", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39868eb7151bad0588663013f17598991947f4c0", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ee531ccf658f0c5aa84e9bfee7034c06d0311f7", "width": 1080, "height": 719}], "variants": {}, "id": "vwHNrRaZgH1udSmqaM1jQ6og5WHaxlO6Yq77OeRN7io"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1257sut", "is_robot_indexable": true, "report_reasons": null, "author": "Alarmed-Sock4915", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1257sut/how_to_transition_to_a_data_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@anupmoncy/how-to-transition-to-a-data-architect-83aa835ea53c", "subreddit_subscribers": 94851, "created_utc": 1680050106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Let's take the following use case as an example: data from several physical servers (really high volume) need to be ingested into Snowflake.**\n\n**Points I am considering:** \n\n1. Ingesting directly from the servers to Snowflake is too much of a coupling. The servers need to be able to send data independently. So we need at least one middleware before Snowflake which will behave as an interface for any future changes.\n2. Will be more than one consumer in the future, ie., will Snowflake ingestion be the only target?\n3. Only aggregations/rollups/etc will be saved for long periods. Raw data is too much and will be kept for a pre-defined TTL and then deleted for good.\n\nI can think about several possible architectures and their pros and cons for the above considerations:\n\n# 1) Servers -&gt; S3 -&gt; Snowflake\n\nhttps://preview.redd.it/98d97wghjoqa1.png?width=783&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=797744e56a586c2f3f516779c4f0e0abc2a18e85\n\n# 2) Servers -&gt; Kafka -&gt; Snowflake\n\n&amp;#x200B;\n\nhttps://preview.redd.it/yu3c7j4ojoqa1.png?width=780&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89fc2c211fa78043d267becfa93b5f447ca718b2\n\n# 3) Servers -&gt; Kafka -&gt; S3 -&gt; Snowflake \n\n&amp;#x200B;\n\nhttps://preview.redd.it/23lflynjjoqa1.png?width=783&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=729660524004e75c88b754e6403d24e19be16d1b\n\n\\---\n\nMy first tendency would be to add Kafka as the first stage. Why? \n\n* Easy to implement TTL\n* Easy to allow multiple consumers in the future\n* Built-in for streaming high volume.\n\nOn the other hand, thinking about having only S3 as the middle stage:\n\n* TTL is harder to implement, but maybe AWS has it as a simple configuration that I'm not aware of.\n* Also can support multiple consumers, since it's just storage, how consumers can keep track of what they ingested and what not? Maybe there are also built-in solutions for this problem I'm not aware of.\n* Debugging data ingested to S3 is way easier than Kafka. \n\n&amp;#x200B;\n\n**Would love to hear your opinions and considerations. :)**", "author_fullname": "t2_vd6ewwka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the advantages of putting Kafka vs S3 as a middleware before ingesting data for analytics in Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "media_metadata": {"98d97wghjoqa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 58, "x": 108, "u": "https://preview.redd.it/98d97wghjoqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1288f2cd54e997280ca7b86b782c1b1497c76f6a"}, {"y": 117, "x": 216, "u": "https://preview.redd.it/98d97wghjoqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0be3cd145e638b281d9cf0ef037b0aadb7067fa9"}, {"y": 173, "x": 320, "u": "https://preview.redd.it/98d97wghjoqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=232ae66951f61e8e075426b87ed3fae01beb6c39"}, {"y": 347, "x": 640, "u": "https://preview.redd.it/98d97wghjoqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4164f1addc3037dc75c4cdd17397c56d42013e54"}], "s": {"y": 425, "x": 783, "u": "https://preview.redd.it/98d97wghjoqa1.png?width=783&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=797744e56a586c2f3f516779c4f0e0abc2a18e85"}, "id": "98d97wghjoqa1"}, "yu3c7j4ojoqa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 58, "x": 108, "u": "https://preview.redd.it/yu3c7j4ojoqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73ed74f2c758fa99b72e5f272af551457c45af0c"}, {"y": 117, "x": 216, "u": "https://preview.redd.it/yu3c7j4ojoqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c667f2af7d0cf024f2f6a16d4847e452ac4b7d18"}, {"y": 174, "x": 320, "u": "https://preview.redd.it/yu3c7j4ojoqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54066b256f8f2eabcd5302ea477e75ab4006fffb"}, {"y": 348, "x": 640, "u": "https://preview.redd.it/yu3c7j4ojoqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc3ca8fde34d6d2ea5699260296b908d27ee88df"}], "s": {"y": 425, "x": 780, "u": "https://preview.redd.it/yu3c7j4ojoqa1.png?width=780&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89fc2c211fa78043d267becfa93b5f447ca718b2"}, "id": "yu3c7j4ojoqa1"}, "23lflynjjoqa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 58, "x": 108, "u": "https://preview.redd.it/23lflynjjoqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68a693f9f5826c61a156e3f9c88f15632a42d248"}, {"y": 117, "x": 216, "u": "https://preview.redd.it/23lflynjjoqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38c44d559cc97c041129a1a7b01e46d6c3c75f99"}, {"y": 173, "x": 320, "u": "https://preview.redd.it/23lflynjjoqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec41053d1a1b2e3e7fedd11349b8ea096a7518b3"}, {"y": 347, "x": 640, "u": "https://preview.redd.it/23lflynjjoqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc7457d3feecc1dff47077ab9f49ce48fba2182f"}], "s": {"y": 425, "x": 783, "u": "https://preview.redd.it/23lflynjjoqa1.png?width=783&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=729660524004e75c88b754e6403d24e19be16d1b"}, "id": "23lflynjjoqa1"}}, "name": "t3_125ofvz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/k8eEkWYCVVDvxQE41ZnxFL0oXpcHznVuUpf3Uj_sQls.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680097230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Let&amp;#39;s take the following use case as an example: data from several physical servers (really high volume) need to be ingested into Snowflake.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Points I am considering:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ingesting directly from the servers to Snowflake is too much of a coupling. The servers need to be able to send data independently. So we need at least one middleware before Snowflake which will behave as an interface for any future changes.&lt;/li&gt;\n&lt;li&gt;Will be more than one consumer in the future, ie., will Snowflake ingestion be the only target?&lt;/li&gt;\n&lt;li&gt;Only aggregations/rollups/etc will be saved for long periods. Raw data is too much and will be kept for a pre-defined TTL and then deleted for good.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I can think about several possible architectures and their pros and cons for the above considerations:&lt;/p&gt;\n\n&lt;h1&gt;1) Servers -&amp;gt; S3 -&amp;gt; Snowflake&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/98d97wghjoqa1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=797744e56a586c2f3f516779c4f0e0abc2a18e85\"&gt;https://preview.redd.it/98d97wghjoqa1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=797744e56a586c2f3f516779c4f0e0abc2a18e85&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;2) Servers -&amp;gt; Kafka -&amp;gt; Snowflake&lt;/h1&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yu3c7j4ojoqa1.png?width=780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89fc2c211fa78043d267becfa93b5f447ca718b2\"&gt;https://preview.redd.it/yu3c7j4ojoqa1.png?width=780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89fc2c211fa78043d267becfa93b5f447ca718b2&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;3) Servers -&amp;gt; Kafka -&amp;gt; S3 -&amp;gt; Snowflake&lt;/h1&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/23lflynjjoqa1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=729660524004e75c88b754e6403d24e19be16d1b\"&gt;https://preview.redd.it/23lflynjjoqa1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=729660524004e75c88b754e6403d24e19be16d1b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;My first tendency would be to add Kafka as the first stage. Why? &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Easy to implement TTL&lt;/li&gt;\n&lt;li&gt;Easy to allow multiple consumers in the future&lt;/li&gt;\n&lt;li&gt;Built-in for streaming high volume.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;On the other hand, thinking about having only S3 as the middle stage:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;TTL is harder to implement, but maybe AWS has it as a simple configuration that I&amp;#39;m not aware of.&lt;/li&gt;\n&lt;li&gt;Also can support multiple consumers, since it&amp;#39;s just storage, how consumers can keep track of what they ingested and what not? Maybe there are also built-in solutions for this problem I&amp;#39;m not aware of.&lt;/li&gt;\n&lt;li&gt;Debugging data ingested to S3 is way easier than Kafka. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Would love to hear your opinions and considerations. :)&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125ofvz", "is_robot_indexable": true, "report_reasons": null, "author": "yfeltz", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125ofvz/what_are_the_advantages_of_putting_kafka_vs_s3_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125ofvz/what_are_the_advantages_of_putting_kafka_vs_s3_as/", "subreddit_subscribers": 94851, "created_utc": 1680097230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey DE reddit,\n\nJust looking for any guidance or suggestions on how to tackle a problem we have at my workplace.\n\nWe have an old data warehouse where 15 years of old SSIS jobs populate it with a lot of technical debt and knowledge of how they work no longer with the company. \n\nWe have a new data warehouse spun up for an ERP migration project and some of the data that gets populated with SSIS is required on the new data warehouse as a short term stop gap for business reporting continuity. \n\nThis stop gap will be cleaned up post go live of the ERP project but for now we need a quick and efficient way to replicate the SSIS populated table data from old data warehouse to new data warehouse.\n\nThe old data warehouse is MSSQL 2014 Standard and does not have CDC as a result...\n\nI've created python code that replicates the data using pandas and it works reasonably well other than the length it takes to replicate and the amount of RAM the pandas dataframe consumes. So i can only run this once a day meaning some data will be a bit stale in the new data warehouse till it runs again the next day.\n\nI'm trying to come up with a way to be able to run this replication more often given time and ram limitations. \n\nOne idea I had was to hash the table contents from the last run and save it, then check the hash to see if any of the table changed before using valuable replication time/ram resources. Has anyone ever done anything like this ? Any libraries that can help me ?\n\nOr any other completely different ways to handle this without CDC or large amounts of hardware spend ? \n\nAny guidance/help is much appreciated!", "author_fullname": "t2_9sf3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replication of data without CDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1254m9v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680042547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey DE reddit,&lt;/p&gt;\n\n&lt;p&gt;Just looking for any guidance or suggestions on how to tackle a problem we have at my workplace.&lt;/p&gt;\n\n&lt;p&gt;We have an old data warehouse where 15 years of old SSIS jobs populate it with a lot of technical debt and knowledge of how they work no longer with the company. &lt;/p&gt;\n\n&lt;p&gt;We have a new data warehouse spun up for an ERP migration project and some of the data that gets populated with SSIS is required on the new data warehouse as a short term stop gap for business reporting continuity. &lt;/p&gt;\n\n&lt;p&gt;This stop gap will be cleaned up post go live of the ERP project but for now we need a quick and efficient way to replicate the SSIS populated table data from old data warehouse to new data warehouse.&lt;/p&gt;\n\n&lt;p&gt;The old data warehouse is MSSQL 2014 Standard and does not have CDC as a result...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve created python code that replicates the data using pandas and it works reasonably well other than the length it takes to replicate and the amount of RAM the pandas dataframe consumes. So i can only run this once a day meaning some data will be a bit stale in the new data warehouse till it runs again the next day.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to come up with a way to be able to run this replication more often given time and ram limitations. &lt;/p&gt;\n\n&lt;p&gt;One idea I had was to hash the table contents from the last run and save it, then check the hash to see if any of the table changed before using valuable replication time/ram resources. Has anyone ever done anything like this ? Any libraries that can help me ?&lt;/p&gt;\n\n&lt;p&gt;Or any other completely different ways to handle this without CDC or large amounts of hardware spend ? &lt;/p&gt;\n\n&lt;p&gt;Any guidance/help is much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1254m9v", "is_robot_indexable": true, "report_reasons": null, "author": "msthree", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1254m9v/replication_of_data_without_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1254m9v/replication_of_data_without_cdc/", "subreddit_subscribers": 94851, "created_utc": 1680042547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Issue: The Airbyte MySQL connector does not parallelize read workload\n\nResult: Syncs take forever to finish\n\nSolution: Update Airbyte MySQL connector to use parallel connections for reading with JDBC\nOne can use subconnections to read data in parallel from server. This will increase performance considerably.\n\nI was wondering if anyone has done anything like this?  If not, anyone interested in working with me (I have used JAVA in like forever) to give back to the open source community and update the Airbyte connector to preform parallel reads", "author_fullname": "t2_9o0tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte MySQL connector does not parallelize read", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1253fa1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680039846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Issue: The Airbyte MySQL connector does not parallelize read workload&lt;/p&gt;\n\n&lt;p&gt;Result: Syncs take forever to finish&lt;/p&gt;\n\n&lt;p&gt;Solution: Update Airbyte MySQL connector to use parallel connections for reading with JDBC\nOne can use subconnections to read data in parallel from server. This will increase performance considerably.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone has done anything like this?  If not, anyone interested in working with me (I have used JAVA in like forever) to give back to the open source community and update the Airbyte connector to preform parallel reads&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1253fa1", "is_robot_indexable": true, "report_reasons": null, "author": "Amphagory", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1253fa1/airbyte_mysql_connector_does_not_parallelize_read/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1253fa1/airbyte_mysql_connector_does_not_parallelize_read/", "subreddit_subscribers": 94851, "created_utc": 1680039846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had planned to put together some beginners training material to cover the gambit from data collection and monitoring to analytics and reporting. Having recently joined this community, I have noticed some themes that made me want to ask: what would you like if you were new to data engineering? \n\nAdditionally, what sort of content? How detailed and technical? Does the tech stack matter? Do you prefer apps and code or Jupyter Notebooks? Let me know your thoughts! \n\nThanks and cheers all \ud83d\udcaa\ud83c\udffb", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What makes for good DE training material?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124zowl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680032238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had planned to put together some beginners training material to cover the gambit from data collection and monitoring to analytics and reporting. Having recently joined this community, I have noticed some themes that made me want to ask: what would you like if you were new to data engineering? &lt;/p&gt;\n\n&lt;p&gt;Additionally, what sort of content? How detailed and technical? Does the tech stack matter? Do you prefer apps and code or Jupyter Notebooks? Let me know your thoughts! &lt;/p&gt;\n\n&lt;p&gt;Thanks and cheers all \ud83d\udcaa\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "124zowl", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124zowl/what_makes_for_good_de_training_material/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124zowl/what_makes_for_good_de_training_material/", "subreddit_subscribers": 94851, "created_utc": 1680032238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,  when naming a datetime column, is there a good rule on using \"created\\_on\" vs \"start\\_date\"?", "author_fullname": "t2_77mz0n8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "start_date vs created_on as column names", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_125ryws", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680104827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,  when naming a datetime column, is there a good rule on using &amp;quot;created_on&amp;quot; vs &amp;quot;start_date&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125ryws", "is_robot_indexable": true, "report_reasons": null, "author": "Programmer_Virtual", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125ryws/start_date_vs_created_on_as_column_names/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125ryws/start_date_vs_created_on_as_column_names/", "subreddit_subscribers": 94851, "created_utc": 1680104827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello, What might be some good article or books that cover best practices of table designs? For example, naming conventions of table and column, creating primary keys etc. Thanks", "author_fullname": "t2_77mz0n8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices of designing tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125q9k3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680101022.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, What might be some good article or books that cover best practices of table designs? For example, naming conventions of table and column, creating primary keys etc. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "125q9k3", "is_robot_indexable": true, "report_reasons": null, "author": "Programmer_Virtual", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125q9k3/best_practices_of_designing_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125q9k3/best_practices_of_designing_tables/", "subreddit_subscribers": 94851, "created_utc": 1680101022.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a team lead in Utah. I've been thinking of looking around for something new. My company has made some drastic changes in the last few months. Going in a direction I don't necessarily agree with. With the job market the way it is, should I look or wait until things are in a better spot? \nPros: \nLove my team, great coworkers and direct leadership\nI feel I make a impact\nPotentially great equity when go public\n\nCons:\nBack in office full time (loved hybrid it was perfect for me)\nTook away some paid holidays\nTook away other perks ie conferences\nNo merit raises for most, performance review didn't matter\n\nI understand we are in weird times, just wondering if it's worth taking the effort to job search right now. \n\nThanks for all your input.", "author_fullname": "t2_4y6zmea2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Worth Looking Around?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125f07x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680070369.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a team lead in Utah. I&amp;#39;ve been thinking of looking around for something new. My company has made some drastic changes in the last few months. Going in a direction I don&amp;#39;t necessarily agree with. With the job market the way it is, should I look or wait until things are in a better spot? \nPros: \nLove my team, great coworkers and direct leadership\nI feel I make a impact\nPotentially great equity when go public&lt;/p&gt;\n\n&lt;p&gt;Cons:\nBack in office full time (loved hybrid it was perfect for me)\nTook away some paid holidays\nTook away other perks ie conferences\nNo merit raises for most, performance review didn&amp;#39;t matter&lt;/p&gt;\n\n&lt;p&gt;I understand we are in weird times, just wondering if it&amp;#39;s worth taking the effort to job search right now. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all your input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "125f07x", "is_robot_indexable": true, "report_reasons": null, "author": "angleofthedangle90", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125f07x/worth_looking_around/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125f07x/worth_looking_around/", "subreddit_subscribers": 94851, "created_utc": 1680070369.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently my manager approached me about switching my job title from data engineer II to software engineer II, and I\u2019m not sure what to do.\n\nFor some background I\u2019m three years into my career: two years in a post-grad rotational program, and one year as a data engineer. During those two years I was always working to become a data engineer and hope to grow in the data space to a data architect (my company only has architects at the leadership level). I work on an agile squad that is 1 of 6 squads working on an overarching master data management solution. On those squads there are two managers that developers report to- all data engineers report to one manager and all software engineers report to the other. Previously all the squads were a mix of data and software developers, but now all squads are made up of one or the other. I am the only person on the 6 teams misaligned, a data engineer on a team of software engineers.\n\nThis is why they want me to change titles. The managers want to be able to \u201cown\u201d squads, so they can be better aligned to the products they support. I totally understand the reasoning, but it really seems like I\u2019m just an inconvenience to them for HR purposes. My day to day won\u2019t be changing but I\u2019m hesitant to be called a software engineer when I don\u2019t have the technical skills to match that role. I\u2019m also worried about career prospects if I want to leave my current company because I\u2019m not inclined to lie about my job title and all my interests are in data roles. \n\nSorry for the long post I\u2019m early in my career and just don\u2019t know how much things like this matter. Does anyone have any advice? I\u2019m usually a team player, but when it comes to things like this I feel like I need to think about myself first.", "author_fullname": "t2_dkplimcv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineer vs Data engineer, does job title matter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_125tqsk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680108933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently my manager approached me about switching my job title from data engineer II to software engineer II, and I\u2019m not sure what to do.&lt;/p&gt;\n\n&lt;p&gt;For some background I\u2019m three years into my career: two years in a post-grad rotational program, and one year as a data engineer. During those two years I was always working to become a data engineer and hope to grow in the data space to a data architect (my company only has architects at the leadership level). I work on an agile squad that is 1 of 6 squads working on an overarching master data management solution. On those squads there are two managers that developers report to- all data engineers report to one manager and all software engineers report to the other. Previously all the squads were a mix of data and software developers, but now all squads are made up of one or the other. I am the only person on the 6 teams misaligned, a data engineer on a team of software engineers.&lt;/p&gt;\n\n&lt;p&gt;This is why they want me to change titles. The managers want to be able to \u201cown\u201d squads, so they can be better aligned to the products they support. I totally understand the reasoning, but it really seems like I\u2019m just an inconvenience to them for HR purposes. My day to day won\u2019t be changing but I\u2019m hesitant to be called a software engineer when I don\u2019t have the technical skills to match that role. I\u2019m also worried about career prospects if I want to leave my current company because I\u2019m not inclined to lie about my job title and all my interests are in data roles. &lt;/p&gt;\n\n&lt;p&gt;Sorry for the long post I\u2019m early in my career and just don\u2019t know how much things like this matter. Does anyone have any advice? I\u2019m usually a team player, but when it comes to things like this I feel like I need to think about myself first.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "125tqsk", "is_robot_indexable": true, "report_reasons": null, "author": "Individual-Course294", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125tqsk/software_engineer_vs_data_engineer_does_job_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125tqsk/software_engineer_vs_data_engineer_does_job_title/", "subreddit_subscribers": 94851, "created_utc": 1680108933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm confused on the use of ADF/managed vnet with private endpoints and would appreciate any pointers. \n\nTrying to connect to a MongoDB instance in Atlas (managed). I think we can consider this completely external to our vnet/subnets. \n\nADF doesn't allow itself to be placed within a specific vnet anyway. It has a managed vnet set up wherein you create private endpoints to other resources and then linked services use those private endpoints to transfer data. \n\nWe have set up a private endpoint between Mongo Atlas and Azure, within the same resource group as our ADF.\n\nWhen I try to connect ADF to that instance of Mongo the connection timeouts. The error is a simple timeout error but it does show the correct private endpoint IP address. \n\nSo it is routing the request to the endpoint which should forward to mongo atlas. \n\nDoes anyone have a high level overview of how this sort of thing is meant to be set up? \n\nShould all resources be in the same vnet/subnet? Connecting to a storage account in the same subnet works. But we can't do this since we have different environments for a reason. \n\nIs there a piece I'm missing when connecting to an external source via private endpoint? \n\nAny tips or pointers would be appreciated.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to connect data factory to external data source?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_125thp4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680108340.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m confused on the use of ADF/managed vnet with private endpoints and would appreciate any pointers. &lt;/p&gt;\n\n&lt;p&gt;Trying to connect to a MongoDB instance in Atlas (managed). I think we can consider this completely external to our vnet/subnets. &lt;/p&gt;\n\n&lt;p&gt;ADF doesn&amp;#39;t allow itself to be placed within a specific vnet anyway. It has a managed vnet set up wherein you create private endpoints to other resources and then linked services use those private endpoints to transfer data. &lt;/p&gt;\n\n&lt;p&gt;We have set up a private endpoint between Mongo Atlas and Azure, within the same resource group as our ADF.&lt;/p&gt;\n\n&lt;p&gt;When I try to connect ADF to that instance of Mongo the connection timeouts. The error is a simple timeout error but it does show the correct private endpoint IP address. &lt;/p&gt;\n\n&lt;p&gt;So it is routing the request to the endpoint which should forward to mongo atlas. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a high level overview of how this sort of thing is meant to be set up? &lt;/p&gt;\n\n&lt;p&gt;Should all resources be in the same vnet/subnet? Connecting to a storage account in the same subnet works. But we can&amp;#39;t do this since we have different environments for a reason. &lt;/p&gt;\n\n&lt;p&gt;Is there a piece I&amp;#39;m missing when connecting to an external source via private endpoint? &lt;/p&gt;\n\n&lt;p&gt;Any tips or pointers would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "125thp4", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125thp4/how_to_connect_data_factory_to_external_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125thp4/how_to_connect_data_factory_to_external_data/", "subreddit_subscribers": 94851, "created_utc": 1680108340.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I am currently tasked with creating an in-house IT-database that would allow end users to both submit and manage tickets, and also provide reporting/dashboard functionality. This would eventually expand off to fit other needs if we find success. \n\nWe currently use O365 and Azure AD, so I figured that we would be able to build the data warehouse on SQL for Azure, use our pre-existing Active Directory roles to control the levels of access, and use PowerAutomate/PowerBI for visualization and workflows. \n\nDoes anyone have any experience on this kind of project? Is this a feasible solution, or is there something similar that we could build that may be similar/less expensive? We originally though about using SharePoint lists as our data warehouse, but that would become a headache quickly. Thanks!!", "author_fullname": "t2_913h0mrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for a SQL Server database that connects to a SharePoint UI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125lzc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680091136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I am currently tasked with creating an in-house IT-database that would allow end users to both submit and manage tickets, and also provide reporting/dashboard functionality. This would eventually expand off to fit other needs if we find success. &lt;/p&gt;\n\n&lt;p&gt;We currently use O365 and Azure AD, so I figured that we would be able to build the data warehouse on SQL for Azure, use our pre-existing Active Directory roles to control the levels of access, and use PowerAutomate/PowerBI for visualization and workflows. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience on this kind of project? Is this a feasible solution, or is there something similar that we could build that may be similar/less expensive? We originally though about using SharePoint lists as our data warehouse, but that would become a headache quickly. Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "125lzc5", "is_robot_indexable": true, "report_reasons": null, "author": "CadenJPov", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125lzc5/advice_for_a_sql_server_database_that_connects_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125lzc5/advice_for_a_sql_server_database_that_connects_to/", "subreddit_subscribers": 94851, "created_utc": 1680091136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi everyone,\n\nCurrently, I am teaching(online) a large group of students with low technical skills. Well, it wasn't my decision to teach DWH/BI to second-year students. Around 40% of them believe that \"WHERE\" is a mandatory component in every SQL query.\n\nMy biggest challenge now is to encourage them to get involved in building simple data pipelines. However, I anticipate that several student teams may fail to accomplish even the most basic task, such as loading a CSV file to a staging area through Java or Python. As a result, I'm exploring the use of ETL tools as a workaround to enable even the slowest and least competent students to complete this task with a few clicks.\n\nCan anyone recommend an ETL tool that is exceptionally simple to use? I'm looking for a tool that's so user-friendly that even a monkey can drag-and-drop a CSV file into a data mart. I have used Datastage, Powercenter, and SSIS before, but none of them fall into this category.\n\nThank you in advance!", "author_fullname": "t2_3qdnz3g9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on ETL tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125iueu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680083421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Currently, I am teaching(online) a large group of students with low technical skills. Well, it wasn&amp;#39;t my decision to teach DWH/BI to second-year students. Around 40% of them believe that &amp;quot;WHERE&amp;quot; is a mandatory component in every SQL query.&lt;/p&gt;\n\n&lt;p&gt;My biggest challenge now is to encourage them to get involved in building simple data pipelines. However, I anticipate that several student teams may fail to accomplish even the most basic task, such as loading a CSV file to a staging area through Java or Python. As a result, I&amp;#39;m exploring the use of ETL tools as a workaround to enable even the slowest and least competent students to complete this task with a few clicks.&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend an ETL tool that is exceptionally simple to use? I&amp;#39;m looking for a tool that&amp;#39;s so user-friendly that even a monkey can drag-and-drop a CSV file into a data mart. I have used Datastage, Powercenter, and SSIS before, but none of them fall into this category.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "125iueu", "is_robot_indexable": true, "report_reasons": null, "author": "IndependentSpend7434", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125iueu/need_advice_on_etl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125iueu/need_advice_on_etl_tool/", "subreddit_subscribers": 94851, "created_utc": 1680083421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have 10 pipelines that gets data from different places and I use copy data activity to transform the data from json to parquet and rename some columns. Everything is parameterized. But there is also stuff like switching \u201d,\u201d to \u201d.\u201d  that I want to do. And to my understanding, it is not possible to do such transformations using copy activity. Thus I must do this in the last step where I use databricks for scd transformation. But the current notebook is supposed to be generic and the scd method is written in a way so that all pipelines can use it. But now the notebook is also cluttered with all the source specific transformations since I don\u2019t know where else to put them.\nDoes anyone have recommendations on how to design this?\n(I do not want to use data flow since it is very costly and it means that I must start another cluster)\n\nKind regards and thank you in advance!", "author_fullname": "t2_qiw67sj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adf: where to do source specific transformations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125ifv1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680082105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have 10 pipelines that gets data from different places and I use copy data activity to transform the data from json to parquet and rename some columns. Everything is parameterized. But there is also stuff like switching \u201d,\u201d to \u201d.\u201d  that I want to do. And to my understanding, it is not possible to do such transformations using copy activity. Thus I must do this in the last step where I use databricks for scd transformation. But the current notebook is supposed to be generic and the scd method is written in a way so that all pipelines can use it. But now the notebook is also cluttered with all the source specific transformations since I don\u2019t know where else to put them.\nDoes anyone have recommendations on how to design this?\n(I do not want to use data flow since it is very costly and it means that I must start another cluster)&lt;/p&gt;\n\n&lt;p&gt;Kind regards and thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "125ifv1", "is_robot_indexable": true, "report_reasons": null, "author": "aLyapunov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125ifv1/adf_where_to_do_source_specific_transformations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125ifv1/adf_where_to_do_source_specific_transformations/", "subreddit_subscribers": 94851, "created_utc": 1680082105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This config block cannot be parsed by IDEs I use (Datagrip/PyCharm). Why doesn't dbt simply uses a json of same name to do the job? Why invent this format? This really bugs me as I have to write a translation script to generate some dbt model files and json files are obviously a lot easier to dump on disk.", "author_fullname": "t2_ldvtxo0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does DBT model have such a weird format for configs in models?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12523tl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680037120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This config block cannot be parsed by IDEs I use (Datagrip/PyCharm). Why doesn&amp;#39;t dbt simply uses a json of same name to do the job? Why invent this format? This really bugs me as I have to write a translation script to generate some dbt model files and json files are obviously a lot easier to dump on disk.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12523tl", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway20220231", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12523tl/why_does_dbt_model_have_such_a_weird_format_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12523tl/why_does_dbt_model_have_such_a_weird_format_for/", "subreddit_subscribers": 94851, "created_utc": 1680037120.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}