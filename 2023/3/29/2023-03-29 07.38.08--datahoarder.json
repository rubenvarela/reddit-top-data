{"kind": "Listing", "data": {"after": "t3_124wdwg", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "froid_san, Creator of English Patches for JP Exclusive Games for PS3, PSV, and PS4 Will be Shutting Down His Website.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 114, "top_awarded_type": null, "hide_score": false, "name": "t3_1258t25", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 251, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_6jesv", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 251, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7efBSECf6W9k-a4-yCBKnUsYY56S89EsgbfyiG2sMAo.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "VitaPiracy", "selftext": "", "author_fullname": "t2_3gz7qdgx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "froid_san, Creator of English Patches for JP Exclusive Games for PS3, PSV, and PS4 Will be Shutting Down His Website.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/VitaPiracy", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 114, "top_awarded_type": null, "hide_score": false, "name": "t3_124fl8f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 212, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 212, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7efBSECf6W9k-a4-yCBKnUsYY56S89EsgbfyiG2sMAo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "created": 1679984850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/0i8h50f3bfqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?auto=webp&amp;v=enabled&amp;s=97b9ad866a58999f0ad633a231a9d42ed991540f", "width": 770, "height": 628}, "resolutions": [{"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eaa72324231f7a4945ff32da822ca9a7753cc614", "width": 108, "height": 88}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b60de6d27a5934298b7b7f62b78e3b2c1dbda8ce", "width": 216, "height": 176}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3aec77160cdc97698879f70acc553f5a84a445f", "width": 320, "height": 260}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=022714839e8f28584e638dddf0b847d9c3743a33", "width": 640, "height": 521}], "variants": {}, "id": "FL_ro_C4G_dj-T453tPbQC1sEdGMqzWYfkFCSJLmI0Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3fxlj", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "124fl8f", "is_robot_indexable": true, "report_reasons": null, "author": "ANG-123", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/VitaPiracy/comments/124fl8f/froid_san_creator_of_english_patches_for_jp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/0i8h50f3bfqa1.png", "subreddit_subscribers": 80032, "created_utc": 1679984850.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1680052614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/0i8h50f3bfqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?auto=webp&amp;v=enabled&amp;s=97b9ad866a58999f0ad633a231a9d42ed991540f", "width": 770, "height": 628}, "resolutions": [{"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eaa72324231f7a4945ff32da822ca9a7753cc614", "width": 108, "height": 88}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b60de6d27a5934298b7b7f62b78e3b2c1dbda8ce", "width": 216, "height": 176}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3aec77160cdc97698879f70acc553f5a84a445f", "width": 320, "height": 260}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=022714839e8f28584e638dddf0b847d9c3743a33", "width": 640, "height": 521}], "variants": {}, "id": "FL_ro_C4G_dj-T453tPbQC1sEdGMqzWYfkFCSJLmI0Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "35TB + 8TB NAS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1258t25", "is_robot_indexable": true, "report_reasons": null, "author": "seamonkey420", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_124fl8f", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1258t25/froid_san_creator_of_english_patches_for_jp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/0i8h50f3bfqa1.png", "subreddit_subscribers": 675950, "created_utc": 1680052614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I usually buy used drives off of ebay, which is usually from a reputable place, just through ebay. \n\nToday I received a message, over a month after I received them, asking if I can return a few 10 TB HGST drives I purchased, as they weren't supposed to be sold in the first place. \n\nWhat would be the reasoning and why would they want it back now? I'm sitting with them full after a wipe, extended test, and reformat, so if it was data they were worried about, its a bit late for that. \n\nAll they said was (paraphrasing) \"they weren't supposed to be sold, please send them back for a full refund\". They won't say anything more than that. \n\nShould I even send them back after a few wipe cycles? I never sell my old drives because of concerns of not getting 100% of all the data off of them.", "author_fullname": "t2_16u0wi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone buy used drives a month ago and receive a message from the seller that they want them back?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124l7pa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 138, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 138, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680001621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually buy used drives off of ebay, which is usually from a reputable place, just through ebay. &lt;/p&gt;\n\n&lt;p&gt;Today I received a message, over a month after I received them, asking if I can return a few 10 TB HGST drives I purchased, as they weren&amp;#39;t supposed to be sold in the first place. &lt;/p&gt;\n\n&lt;p&gt;What would be the reasoning and why would they want it back now? I&amp;#39;m sitting with them full after a wipe, extended test, and reformat, so if it was data they were worried about, its a bit late for that. &lt;/p&gt;\n\n&lt;p&gt;All they said was (paraphrasing) &amp;quot;they weren&amp;#39;t supposed to be sold, please send them back for a full refund&amp;quot;. They won&amp;#39;t say anything more than that. &lt;/p&gt;\n\n&lt;p&gt;Should I even send them back after a few wipe cycles? I never sell my old drives because of concerns of not getting 100% of all the data off of them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124l7pa", "is_robot_indexable": true, "report_reasons": null, "author": "TheGleanerBaldwin", "discussion_type": null, "num_comments": 155, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124l7pa/anyone_buy_used_drives_a_month_ago_and_receive_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124l7pa/anyone_buy_used_drives_a_month_ago_and_receive_a/", "subreddit_subscribers": 675950, "created_utc": 1680001621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have images from several laptops that I've archived using macrium, and I just found out the free version is discontinued.  \n\nAm I looking at suffering the consequences of using a free, proprietary format, and potentially not being able to access the images? \n\nThanks.", "author_fullname": "t2_3kfjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Macrium Free discontinued. Am I looking at losing all of my disk images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1258d46", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680051511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have images from several laptops that I&amp;#39;ve archived using macrium, and I just found out the free version is discontinued.  &lt;/p&gt;\n\n&lt;p&gt;Am I looking at suffering the consequences of using a free, proprietary format, and potentially not being able to access the images? &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1258d46", "is_robot_indexable": true, "report_reasons": null, "author": "hoyfkd", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1258d46/macrium_free_discontinued_am_i_looking_at_losing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1258d46/macrium_free_discontinued_am_i_looking_at_losing/", "subreddit_subscribers": 675950, "created_utc": 1680051511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Or just the e-books part of the site?\n\nAsking because I have uploaded alot of old videos from the Romanian television archives that can't be found anywhere else. I still have them backed up on my external HDD's, but still....", "author_fullname": "t2_sdmay15h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will Internet Archive be closed forever if the lose in court?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125846a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680050877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or just the e-books part of the site?&lt;/p&gt;\n\n&lt;p&gt;Asking because I have uploaded alot of old videos from the Romanian television archives that can&amp;#39;t be found anywhere else. I still have them backed up on my external HDD&amp;#39;s, but still....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125846a", "is_robot_indexable": true, "report_reasons": null, "author": "Creative-Detail4369", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125846a/will_internet_archive_be_closed_forever_if_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125846a/will_internet_archive_be_closed_forever_if_the/", "subreddit_subscribers": 675950, "created_utc": 1680050877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello\n\nSome sad news for the photography community and also for the resource it offers but Amazon is closing down the whole of DP review on 10th April 2023.\n\nWhat is the best way to keep the website resource (and youtube) online?\n\n&amp;#x200B;\n\nSee: [https://www.dpreview.com/news/5901145460/dpreview-com-to-close](https://www.dpreview.com/news/5901145460/dpreview-com-to-close)", "author_fullname": "t2_l8l3l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP Review is closing down (youtube, website) on 10th April - Best Archive Instructions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1252ay3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680037515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;Some sad news for the photography community and also for the resource it offers but Amazon is closing down the whole of DP review on 10th April 2023.&lt;/p&gt;\n\n&lt;p&gt;What is the best way to keep the website resource (and youtube) online?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;See: &lt;a href=\"https://www.dpreview.com/news/5901145460/dpreview-com-to-close\"&gt;https://www.dpreview.com/news/5901145460/dpreview-com-to-close&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?auto=webp&amp;v=enabled&amp;s=62800be361d0e710877115d585fc54f6bc091987", "width": 745, "height": 745}, "resolutions": [{"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c85ecc4153f61d9a247d4758d9430569622282d", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a74b7694c73ea2345e65d9ea046b7453dce349cd", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3dbcef5e3e8892307008a8899734f8a2c64159c5", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b46d7621039f2b84e5eb82a6de9d0d1874068788", "width": 640, "height": 640}], "variants": {}, "id": "KDAVLRZTXaBom3MDvF95i0FH6bCPweDv6BPK8qmCi6E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1252ay3", "is_robot_indexable": true, "report_reasons": null, "author": "Fkmeitscold", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1252ay3/dp_review_is_closing_down_youtube_website_on_10th/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1252ay3/dp_review_is_closing_down_youtube_website_on_10th/", "subreddit_subscribers": 675950, "created_utc": 1680037515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm collecting old movies (pre-1970) for long term and occasional use storage. These movies can either be OOP, Region B, or have never received a proper Blu-Ray release.I want to watch them semi-reguarly and own them 10-20 years from now. I posted on this sub a few days ago asking about BD-R burning, but my understanding is that even an authored BD-R disc won't function on my PS5 and maybe not on my PS3 (which are my only drives atm). Do I copy my movies onto an HDD and continually buy new ones as backups over the years or is there a better way?", "author_fullname": "t2_8zpyfcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to archive old movies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124y2ge", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680028814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m collecting old movies (pre-1970) for long term and occasional use storage. These movies can either be OOP, Region B, or have never received a proper Blu-Ray release.I want to watch them semi-reguarly and own them 10-20 years from now. I posted on this sub a few days ago asking about BD-R burning, but my understanding is that even an authored BD-R disc won&amp;#39;t function on my PS5 and maybe not on my PS3 (which are my only drives atm). Do I copy my movies onto an HDD and continually buy new ones as backups over the years or is there a better way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124y2ge", "is_robot_indexable": true, "report_reasons": null, "author": "HunteHorseman", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124y2ge/best_way_to_archive_old_movies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124y2ge/best_way_to_archive_old_movies/", "subreddit_subscribers": 675950, "created_utc": 1680028814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_6bkhzbkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitHub - StephaneCouturier/Katalog: Katalog is a desktop application to manage catalogs of disks and files.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_124q3io", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HGHHFC4jmOYy3Yo0PP5xRAeqO1qpbIA7Ui-qD0XI_bU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680012884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/StephaneCouturier/Katalog", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?auto=webp&amp;v=enabled&amp;s=f0345fbfa155c42f9ed3a015d8d454057224a656", "width": 1192, "height": 746}, "resolutions": [{"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68ac3d245bbe00f527e6ae98c04d2be7ccaf7779", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b71d17edd47e3be60bf936b51aa0ddccce8b40cc", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fad31efc016a152921bbd0d644dfd8414b22cefc", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4daa8d14d844362135f0b74796849c40fd98c7ea", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1bc48cb4bf8f59134b0ede3495ea2b6788fa407", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/Rf0GfjMjyxPjTBAGWsrIbUNOq7B_ugg57Ppay3dSgm4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12b2d68feed5753904239ca33fddd17e9e39475a", "width": 1080, "height": 675}], "variants": {}, "id": "irDtZCYVAKY-0XAuUWFYOOtO_6uvN6Z80XPqklo8AGE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124q3io", "is_robot_indexable": true, "report_reasons": null, "author": "justanotherquestionq", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124q3io/github_stephanecouturierkatalog_katalog_is_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/StephaneCouturier/Katalog", "subreddit_subscribers": 675950, "created_utc": 1680012884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for recommendations on BDXL Windows burning software? Thank you in advance", "author_fullname": "t2_pzfb76t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended Windows bdxl burning software?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125142c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680035123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for recommendations on BDXL Windows burning software? Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125142c", "is_robot_indexable": true, "report_reasons": null, "author": "DisturbedBeaker", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125142c/recommended_windows_bdxl_burning_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125142c/recommended_windows_bdxl_burning_software/", "subreddit_subscribers": 675950, "created_utc": 1680035123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "EDIT: This was originally a \"Question/Advice\" flaired post asking for best practices. I've since developed my own method which works for me. Please read below if you'd like to know what I ended up doing or feel welcome to share your own solution!   \n\n\nORIGINAL POST:\n\nI'm hoping that the community can help me.\n\nI am using blurays for archiving files that I have long held on HDDs. I've read that it's common to run a file check on the data before burning to disc and to then also add those logs to the blurays along with your data.\n\nIs there a best practices guide on what tools to use to achieve this? I've been searching for a best practices guide but I'm coming up short in my search.\n\nBest tools/methodology for arching files to bluray would be really appreciated.  \n\n\nUPDATE: So on to my solution.   \n\n\nFrustrated by my inability to find a best practices, I ended up resorting to my own solution.  \nI set up a project folder in Win10 where prepare each bluray by compressing folders of files into .7zip format. This was simple for me and kept everything organized the way that I liked it.  \n\n\nOnce I had various folders with various .7zip compressed archives, I go to the root of my project folder (this would also be the root once these files are copied over the bluray), I highlight all directories on the root and any root level files, right click on the highlighted files go to 7zip in your context menu, then CRC SHA, then SHA-256 -&gt; \\[name\\].sha256  \n\n\nThis will output a sha-256 hash for every file selected into a new file called \\[name\\].sha256. I then verified that these hashes were accurate by opening up my text editor (Sublime). You'll see all of the hashes, you can then check random files by going back into your folder and selecting a file, go back through the context menu but this time select: SHA-256 to get only the hash for that file.  \n\n\nOnce I was confident that the hashes were accurate, I opened up ImgBurn, went to \"Write files/folders to disc\" and added my files including my SHA-256 file specific to only the files added to my bluray disc. I then went to the options tab and changed the File System to UDF revision 2.60. Selected Verify and then burnt my files to bluray (build).   \n\n\nOnce the burn was successful, I open the bluray in Windows and compare the hashes of the files again against the SHA-256 file I made to make sure that they show the right values. They did so it's all good! Copying files to the PC from disc to do another check (super redundant) proved this to be a good method for myself.   \n\n\nI make a couple disc copies and then make a third copy on an external HDD. all cold storage.  \n\n\nIt's really late here but I wanted to leave this for anyone struggling to find a good path forward. I hope this helps someone! Let me know if it helped you! And if you know of a better way, also please let me know! ", "author_fullname": "t2_yu2xz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optical Archive Logging/File Archiving Best Practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124zhcn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680070563.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680031814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;EDIT: This was originally a &amp;quot;Question/Advice&amp;quot; flaired post asking for best practices. I&amp;#39;ve since developed my own method which works for me. Please read below if you&amp;#39;d like to know what I ended up doing or feel welcome to share your own solution!   &lt;/p&gt;\n\n&lt;p&gt;ORIGINAL POST:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping that the community can help me.&lt;/p&gt;\n\n&lt;p&gt;I am using blurays for archiving files that I have long held on HDDs. I&amp;#39;ve read that it&amp;#39;s common to run a file check on the data before burning to disc and to then also add those logs to the blurays along with your data.&lt;/p&gt;\n\n&lt;p&gt;Is there a best practices guide on what tools to use to achieve this? I&amp;#39;ve been searching for a best practices guide but I&amp;#39;m coming up short in my search.&lt;/p&gt;\n\n&lt;p&gt;Best tools/methodology for arching files to bluray would be really appreciated.  &lt;/p&gt;\n\n&lt;p&gt;UPDATE: So on to my solution.   &lt;/p&gt;\n\n&lt;p&gt;Frustrated by my inability to find a best practices, I ended up resorting to my own solution.&lt;br/&gt;\nI set up a project folder in Win10 where prepare each bluray by compressing folders of files into .7zip format. This was simple for me and kept everything organized the way that I liked it.  &lt;/p&gt;\n\n&lt;p&gt;Once I had various folders with various .7zip compressed archives, I go to the root of my project folder (this would also be the root once these files are copied over the bluray), I highlight all directories on the root and any root level files, right click on the highlighted files go to 7zip in your context menu, then CRC SHA, then SHA-256 -&amp;gt; [name].sha256  &lt;/p&gt;\n\n&lt;p&gt;This will output a sha-256 hash for every file selected into a new file called [name].sha256. I then verified that these hashes were accurate by opening up my text editor (Sublime). You&amp;#39;ll see all of the hashes, you can then check random files by going back into your folder and selecting a file, go back through the context menu but this time select: SHA-256 to get only the hash for that file.  &lt;/p&gt;\n\n&lt;p&gt;Once I was confident that the hashes were accurate, I opened up ImgBurn, went to &amp;quot;Write files/folders to disc&amp;quot; and added my files including my SHA-256 file specific to only the files added to my bluray disc. I then went to the options tab and changed the File System to UDF revision 2.60. Selected Verify and then burnt my files to bluray (build).   &lt;/p&gt;\n\n&lt;p&gt;Once the burn was successful, I open the bluray in Windows and compare the hashes of the files again against the SHA-256 file I made to make sure that they show the right values. They did so it&amp;#39;s all good! Copying files to the PC from disc to do another check (super redundant) proved this to be a good method for myself.   &lt;/p&gt;\n\n&lt;p&gt;I make a couple disc copies and then make a third copy on an external HDD. all cold storage.  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s really late here but I wanted to leave this for anyone struggling to find a good path forward. I hope this helps someone! Let me know if it helped you! And if you know of a better way, also please let me know! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124zhcn", "is_robot_indexable": true, "report_reasons": null, "author": "0101-ERROR-1001", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124zhcn/optical_archive_loggingfile_archiving_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124zhcn/optical_archive_loggingfile_archiving_best/", "subreddit_subscribers": 675950, "created_utc": 1680031814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There's an internal wiki that is highly valued at my work, however they have lost their funding and it's looking like it will be gone in the next few months. No plans to retain data or anything.\n\nDoes anyone have any ideas or recommendations on how we could save this data from being lost? I'm a hobbyist in this kind of stuff and have no clue where to start.", "author_fullname": "t2_sor17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations on archiving a wiki?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124tpqe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680019615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s an internal wiki that is highly valued at my work, however they have lost their funding and it&amp;#39;s looking like it will be gone in the next few months. No plans to retain data or anything.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any ideas or recommendations on how we could save this data from being lost? I&amp;#39;m a hobbyist in this kind of stuff and have no clue where to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124tpqe", "is_robot_indexable": true, "report_reasons": null, "author": "myS_", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124tpqe/recommendations_on_archiving_a_wiki/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124tpqe/recommendations_on_archiving_a_wiki/", "subreddit_subscribers": 675950, "created_utc": 1680019615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have two dedicated leaseweb servers one with 4TB storage and another bigger one with 16TB storage. I mainly use them as seedboxes as well as for emby/plex streaming. Recently, I learned about nextcloud from this sub and its amazing. \n\nSo I want to migrate all files from my first server to second server, mainly TV Series and movies so that I can use the first server as a dedicated nextCloud machine while I still use the second one for plex and other stuff because my usage for the first server is drastically reduced.\n\nI want to know what's the best way to transfer files between these two machines? Initially I was thinking  SFTPing the files to my local and then SFTPing back to second server. But I live in a different continent from my servers while both my servers share the same country. So it's probably easy to just transfer between two 1Gbps machines. But I want to if there's a better way to do this.", "author_fullname": "t2_5tdnrb9j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to migrate files from one dedicated server to another", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125d6hu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680064573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have two dedicated leaseweb servers one with 4TB storage and another bigger one with 16TB storage. I mainly use them as seedboxes as well as for emby/plex streaming. Recently, I learned about nextcloud from this sub and its amazing. &lt;/p&gt;\n\n&lt;p&gt;So I want to migrate all files from my first server to second server, mainly TV Series and movies so that I can use the first server as a dedicated nextCloud machine while I still use the second one for plex and other stuff because my usage for the first server is drastically reduced.&lt;/p&gt;\n\n&lt;p&gt;I want to know what&amp;#39;s the best way to transfer files between these two machines? Initially I was thinking  SFTPing the files to my local and then SFTPing back to second server. But I live in a different continent from my servers while both my servers share the same country. So it&amp;#39;s probably easy to just transfer between two 1Gbps machines. But I want to if there&amp;#39;s a better way to do this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125d6hu", "is_robot_indexable": true, "report_reasons": null, "author": "Terminal_Monk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125d6hu/best_way_to_migrate_files_from_one_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125d6hu/best_way_to_migrate_files_from_one_dedicated/", "subreddit_subscribers": 675950, "created_utc": 1680064573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've never bought used drives before, but I'm in the sweet spot of having sufficient backups that I can risk it more than in the past, and I also need to get my $/tb down.\n\nI'm looking at some \"manufacture recertified\" drives from serverpartdeals.com. I've seen a lot of good things about them, including around this sub. I wanted to ask from those of you who have bought from them:\n\n1. is there any real consistency in the sort of wear on the drives? Like, are you getting drives with easily 10k+hours on the drive and/or drives that have DOM at least 2-3 years out? Conversely, are you receiving drives that are consistently newer in age and/or lower in hours than you'd have expected? Or is it all over the place? \n\n\n2. The exos seem to run a hair cheaper than the Ultrastars when I look around. Honestly, I haven't bought a Seagate drive in six years so I'm out of the loop on the dependability. I'm a bit worried about what I've seen from reviews and the like on Exos drives regarding reliability. I know all drives fail, some very fast and it's foolish to try and predict reliability in any shape or form. So, I'm guiltily confessing that I'm interested in your anecdotal experiences on this one, in buying those recertified Exos drives and how it went. \n\n\n3. I saw a two year limited warranty on these drives from Serverpartdeals. Have any of you ever had to take advantage of this? How did RMA go?\n\nI really appreciate any help, advice or guidance. I really want to try out some used drives, just have these questions and concerns hanging over me. But I also understand these questions might be kind of dumb. Thanks for any help.", "author_fullname": "t2_sn9i9n2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about \"manufacture recertified\" drives from serverpartdeals.com", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1254n0i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680042974.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680042591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never bought used drives before, but I&amp;#39;m in the sweet spot of having sufficient backups that I can risk it more than in the past, and I also need to get my $/tb down.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking at some &amp;quot;manufacture recertified&amp;quot; drives from serverpartdeals.com. I&amp;#39;ve seen a lot of good things about them, including around this sub. I wanted to ask from those of you who have bought from them:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;is there any real consistency in the sort of wear on the drives? Like, are you getting drives with easily 10k+hours on the drive and/or drives that have DOM at least 2-3 years out? Conversely, are you receiving drives that are consistently newer in age and/or lower in hours than you&amp;#39;d have expected? Or is it all over the place? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The exos seem to run a hair cheaper than the Ultrastars when I look around. Honestly, I haven&amp;#39;t bought a Seagate drive in six years so I&amp;#39;m out of the loop on the dependability. I&amp;#39;m a bit worried about what I&amp;#39;ve seen from reviews and the like on Exos drives regarding reliability. I know all drives fail, some very fast and it&amp;#39;s foolish to try and predict reliability in any shape or form. So, I&amp;#39;m guiltily confessing that I&amp;#39;m interested in your anecdotal experiences on this one, in buying those recertified Exos drives and how it went. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I saw a two year limited warranty on these drives from Serverpartdeals. Have any of you ever had to take advantage of this? How did RMA go?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I really appreciate any help, advice or guidance. I really want to try out some used drives, just have these questions and concerns hanging over me. But I also understand these questions might be kind of dumb. Thanks for any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1254n0i", "is_robot_indexable": true, "report_reasons": null, "author": "Peruvian_Poo_Pickler", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1254n0i/questions_about_manufacture_recertified_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1254n0i/questions_about_manufacture_recertified_drives/", "subreddit_subscribers": 675950, "created_utc": 1680042591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have 8 smaller SSDs ranging from 120 GBs to 480 GBs from my previous employer who no longer needed them.\n\nMy question to you guys is; What would be the best way to hook these up to my computer? I am using all my sata connections already. I looked into something like this https://www.amazon.com/Sabrent-Tool-free-Enclosure-Optimized-EC-UASP/dp/B00OJ3UJ2S - but I figure there has got to be a better way and you guys would be the ones to know.\n\nThanks!", "author_fullname": "t2_5v1ec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smaller Sized SSDs Laying Around The House", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12593av", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680053347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have 8 smaller SSDs ranging from 120 GBs to 480 GBs from my previous employer who no longer needed them.&lt;/p&gt;\n\n&lt;p&gt;My question to you guys is; What would be the best way to hook these up to my computer? I am using all my sata connections already. I looked into something like this &lt;a href=\"https://www.amazon.com/Sabrent-Tool-free-Enclosure-Optimized-EC-UASP/dp/B00OJ3UJ2S\"&gt;https://www.amazon.com/Sabrent-Tool-free-Enclosure-Optimized-EC-UASP/dp/B00OJ3UJ2S&lt;/a&gt; - but I figure there has got to be a better way and you guys would be the ones to know.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12593av", "is_robot_indexable": true, "report_reasons": null, "author": "FlameHaze", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12593av/smaller_sized_ssds_laying_around_the_house/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12593av/smaller_sized_ssds_laying_around_the_house/", "subreddit_subscribers": 675950, "created_utc": 1680053347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I was reading some fantasy fiction about having to evacuate earth and set up on another planet, and it got me thinking: were I to be in those shoes and wanted to preserve some IT infrastructure, knowing I wouldn't have the internet later, I would need an offline database of every version of every device driver for every OS it was developed for (among other stuff).\n\nThis isn't something I've seen covered before and couldn't find an example of such a database on initial searches (at least nothing meant to be easily downloaded / mirrored).\n\nHas this wheel already been invented?  If so, where would I go to download a copy of it?", "author_fullname": "t2_3c8ce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comprehensive offline driver database: all devices, all OSs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1257p0p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Seeking data set", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680050022.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680049841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I was reading some fantasy fiction about having to evacuate earth and set up on another planet, and it got me thinking: were I to be in those shoes and wanted to preserve some IT infrastructure, knowing I wouldn&amp;#39;t have the internet later, I would need an offline database of every version of every device driver for every OS it was developed for (among other stuff).&lt;/p&gt;\n\n&lt;p&gt;This isn&amp;#39;t something I&amp;#39;ve seen covered before and couldn&amp;#39;t find an example of such a database on initial searches (at least nothing meant to be easily downloaded / mirrored).&lt;/p&gt;\n\n&lt;p&gt;Has this wheel already been invented?  If so, where would I go to download a copy of it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Only 18TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1257p0p", "is_robot_indexable": true, "report_reasons": null, "author": "l_one", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1257p0p/comprehensive_offline_driver_database_all_devices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1257p0p/comprehensive_offline_driver_database_all_devices/", "subreddit_subscribers": 675950, "created_utc": 1680049841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As far as I can work out, it\u2019s usually cheaper to create a brand new NAS than it is to host a backup of your data in S3/Glacier for more than a year?\n\nFor example, \u201cS3 Glacier Flexible Retrieval\u201d is $0.00405 per GB, which sounds cheap but multiply that by 1000 to get TB then again by, say, 36TB which is your average RAID 6 with 5 x 12TB disks and you get $145 a month. ($4.05 per month, per TB)\n\nCosts over a year would be $1749.60 for Glacier, and you could build a new fully populated Synology NAS for that, and that\u2019s a one-time purchase which should in theory last 5 years or more.\n\nIs AWS the cheapest solution, when you factor in power costs as well? Worth noting that Backblaze is $5 per TB so Glacier is cheaper.\n\nObviously there\u2019s the offsite considerations too, which the cloud providers definitely have an advantage of.", "author_fullname": "t2_8429z2fz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With a decently large NAS, how do you back up this amount of data cost-effectively?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12526pe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680037272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As far as I can work out, it\u2019s usually cheaper to create a brand new NAS than it is to host a backup of your data in S3/Glacier for more than a year?&lt;/p&gt;\n\n&lt;p&gt;For example, \u201cS3 Glacier Flexible Retrieval\u201d is $0.00405 per GB, which sounds cheap but multiply that by 1000 to get TB then again by, say, 36TB which is your average RAID 6 with 5 x 12TB disks and you get $145 a month. ($4.05 per month, per TB)&lt;/p&gt;\n\n&lt;p&gt;Costs over a year would be $1749.60 for Glacier, and you could build a new fully populated Synology NAS for that, and that\u2019s a one-time purchase which should in theory last 5 years or more.&lt;/p&gt;\n\n&lt;p&gt;Is AWS the cheapest solution, when you factor in power costs as well? Worth noting that Backblaze is $5 per TB so Glacier is cheaper.&lt;/p&gt;\n\n&lt;p&gt;Obviously there\u2019s the offsite considerations too, which the cloud providers definitely have an advantage of.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12526pe", "is_robot_indexable": true, "report_reasons": null, "author": "BowtieChickenAlfredo", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12526pe/with_a_decently_large_nas_how_do_you_back_up_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12526pe/with_a_decently_large_nas_how_do_you_back_up_this/", "subreddit_subscribers": 675950, "created_utc": 1680037272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a lot of backups of backups -- I imagine many of you can relate. I started using \\`git annex\\` a few years back, so now for the most part I'm at least not making the problem worse, but I still have a lot of old redundant folders that I need to manually cleanup and every now and then I make a little more progress. \n\nI know there are plenty of options for finding duplicate and even similar files, but I'm wondering if any of them fold that up into a tree or folder level to identify near duplicate folders based on the files they contain (it would be even better yet if they happened to support zip and tar archives!)\n\nThe obvious (but wrong) solution I once thought I was looking for was a way to merge near duplicates -- with the assumption that Backup A contained files 1-70 and backup B contained 30-100, so I obviously wanted the superset of 1-100.  Over time though, I've discovered that such a naive approach makes deleting files impossible -- because really, the last thing that I want to do is add back all of the junk photos that I've painstakingly deleted out of a curated folder. \n\nIf anybody has a good process or methodology for handling that reality, I'm all ears!", "author_fullname": "t2_d1jtiqct", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does czkawka (or any other linux tool) have a feature for finding duplicate folders?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124o93z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680008984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a lot of backups of backups -- I imagine many of you can relate. I started using `git annex` a few years back, so now for the most part I&amp;#39;m at least not making the problem worse, but I still have a lot of old redundant folders that I need to manually cleanup and every now and then I make a little more progress. &lt;/p&gt;\n\n&lt;p&gt;I know there are plenty of options for finding duplicate and even similar files, but I&amp;#39;m wondering if any of them fold that up into a tree or folder level to identify near duplicate folders based on the files they contain (it would be even better yet if they happened to support zip and tar archives!)&lt;/p&gt;\n\n&lt;p&gt;The obvious (but wrong) solution I once thought I was looking for was a way to merge near duplicates -- with the assumption that Backup A contained files 1-70 and backup B contained 30-100, so I obviously wanted the superset of 1-100.  Over time though, I&amp;#39;ve discovered that such a naive approach makes deleting files impossible -- because really, the last thing that I want to do is add back all of the junk photos that I&amp;#39;ve painstakingly deleted out of a curated folder. &lt;/p&gt;\n\n&lt;p&gt;If anybody has a good process or methodology for handling that reality, I&amp;#39;m all ears!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124o93z", "is_robot_indexable": true, "report_reasons": null, "author": "dscheffy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124o93z/does_czkawka_or_any_other_linux_tool_have_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124o93z/does_czkawka_or_any_other_linux_tool_have_a/", "subreddit_subscribers": 675950, "created_utc": 1680008984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am attempting to extract a file from the latest dump download from Wikipedia, but it comes out corrupted every time. I've used WinZip and 7zip, but when I use my python code to clean up the file for the learning model I want to make, it says that the file is corrupted. I am very sure that the code is correct, as I have checked it with many sources.", "author_fullname": "t2_8v06mu07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wikipedia Dump Extraction", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_125eqjc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680069513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am attempting to extract a file from the latest dump download from Wikipedia, but it comes out corrupted every time. I&amp;#39;ve used WinZip and 7zip, but when I use my python code to clean up the file for the learning model I want to make, it says that the file is corrupted. I am very sure that the code is correct, as I have checked it with many sources.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125eqjc", "is_robot_indexable": true, "report_reasons": null, "author": "Swirly403", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125eqjc/wikipedia_dump_extraction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125eqjc/wikipedia_dump_extraction/", "subreddit_subscribers": 675950, "created_utc": 1680069513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "...I've got many files listed in the \"file list\" and I've right-clicked on one particular folder that I want excluded from the transfer and I've selected \"Ignore\". Does that mean it will skip copying that folder?\n\nIt's probably obvious, right? I only ask because I tried \"removing\" the folder from the list and it said that that option was only available in the paid version of the software. But...Ignore, if my understanding of the definition of the word is accurate, is...sorta...the same thing? Or, more specifically, \"ignore\" and \"remove\" would both achieve the same goal of not copying that folder...?", "author_fullname": "t2_96z9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dumb Teracopy question: I'm currently transferring some data from one drive to another...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1257lak", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680049801.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680049592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;...I&amp;#39;ve got many files listed in the &amp;quot;file list&amp;quot; and I&amp;#39;ve right-clicked on one particular folder that I want excluded from the transfer and I&amp;#39;ve selected &amp;quot;Ignore&amp;quot;. Does that mean it will skip copying that folder?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s probably obvious, right? I only ask because I tried &amp;quot;removing&amp;quot; the folder from the list and it said that that option was only available in the paid version of the software. But...Ignore, if my understanding of the definition of the word is accurate, is...sorta...the same thing? Or, more specifically, &amp;quot;ignore&amp;quot; and &amp;quot;remove&amp;quot; would both achieve the same goal of not copying that folder...?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1257lak", "is_robot_indexable": true, "report_reasons": null, "author": "ultranothing", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1257lak/dumb_teracopy_question_im_currently_transferring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1257lak/dumb_teracopy_question_im_currently_transferring/", "subreddit_subscribers": 675950, "created_utc": 1680049592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Which cases are good for having 10+ HDDs The most common one I see here is the Fractal Design Define 7 XL. I have also seen the Meshify 2 XL mentioned. Any other good ones I should be looking at?\n\nI am planning to start transitioning to either 18TB or 20TB HDDs. I'll start off with only 2 in this new case, but I know I'm going to get more and fill it up the case in the years to come. I want to buy something now that I can keep using and building in.\n\nThis is my current build and I'm trying to make a second server-only PC: https://pcpartpicker.com/list/TGDPBj\n\nAny advice appreciated. Thanks!", "author_fullname": "t2_qulcas15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended cases for DIY NAS? Hoping to eventually build to 10+ HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1257f3j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680049167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which cases are good for having 10+ HDDs The most common one I see here is the Fractal Design Define 7 XL. I have also seen the Meshify 2 XL mentioned. Any other good ones I should be looking at?&lt;/p&gt;\n\n&lt;p&gt;I am planning to start transitioning to either 18TB or 20TB HDDs. I&amp;#39;ll start off with only 2 in this new case, but I know I&amp;#39;m going to get more and fill it up the case in the years to come. I want to buy something now that I can keep using and building in.&lt;/p&gt;\n\n&lt;p&gt;This is my current build and I&amp;#39;m trying to make a second server-only PC: &lt;a href=\"https://pcpartpicker.com/list/TGDPBj\"&gt;https://pcpartpicker.com/list/TGDPBj&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any advice appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1257f3j", "is_robot_indexable": true, "report_reasons": null, "author": "IHateReddit_1153151", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1257f3j/recommended_cases_for_diy_nas_hoping_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1257f3j/recommended_cases_for_diy_nas_hoping_to/", "subreddit_subscribers": 675950, "created_utc": 1680049167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I Have 6 2.5\" hdd's laying around.\nI would like to use it for some purpose like a low powered temporary Plex server where I can connect all the drives and use it as one single drive via Snapraid or something.\nReliability is not an issue so even if the drives fail later on, not bothered.\nI need some compact solution which can work via USB.", "author_fullname": "t2_6zlh9fwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connect multiple 2.5\" hdd via USB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124xez5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680027437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I Have 6 2.5&amp;quot; hdd&amp;#39;s laying around.\nI would like to use it for some purpose like a low powered temporary Plex server where I can connect all the drives and use it as one single drive via Snapraid or something.\nReliability is not an issue so even if the drives fail later on, not bothered.\nI need some compact solution which can work via USB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124xez5", "is_robot_indexable": true, "report_reasons": null, "author": "Vap0rx47", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124xez5/connect_multiple_25_hdd_via_usb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124xez5/connect_multiple_25_hdd_via_usb/", "subreddit_subscribers": 675950, "created_utc": 1680027437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I need help managing my data chaos. I have a lot of photos and recently experienced a data disaster. While trying to recover some of the lost photos, I ended up with duplicates and files with different names. Some photos are also missing EXIF information or have a lower resolution. How would you go about organizing this mess? My goal is to have only one version of each photo (\"the original\") with the best possible quality, including the highest resolution, minimal compression, and complete EXIF data.", "author_fullname": "t2_a2o3q909", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing a Chaotic 4 TB Photo Collection and Eliminating Duplicates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124vknh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680023548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I need help managing my data chaos. I have a lot of photos and recently experienced a data disaster. While trying to recover some of the lost photos, I ended up with duplicates and files with different names. Some photos are also missing EXIF information or have a lower resolution. How would you go about organizing this mess? My goal is to have only one version of each photo (&amp;quot;the original&amp;quot;) with the best possible quality, including the highest resolution, minimal compression, and complete EXIF data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124vknh", "is_robot_indexable": true, "report_reasons": null, "author": "silvermir", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124vknh/organizing_a_chaotic_4_tb_photo_collection_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124vknh/organizing_a_chaotic_4_tb_photo_collection_and/", "subreddit_subscribers": 675950, "created_utc": 1680023548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone!\n\nI was going through some old hard drives and found a folder with about 200k+ emails back from my college days. They are all .eml single messages. I would love to get them loaded into Outlook or some other app/web service that will let me easily search/find a specific thread when needed. Right now Windows Explorer and Mac Finder both severely lag/crash when I try to do anything with 200k files. \n\nMy original thought process was to get them uploaded to my gmail account using Thunderbird + IMAP connection. I figured Google can help me with the search aspect and it would be nice to have all my emails in one place. But this is a super slow process and Thunderbird crashes after a day of uploading. I have also tried importing the emails to Outlook desktop but the app doesn't support eml files without conversion.\n\nI figured I can't be the only one facing this and would love to get advice as to how to get these eml files into a searchable form. \n\nThanks in advance for all your help!", "author_fullname": "t2_nvhbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommends on Email Archiving with Searchability in Mind?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124va37", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680022912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I was going through some old hard drives and found a folder with about 200k+ emails back from my college days. They are all .eml single messages. I would love to get them loaded into Outlook or some other app/web service that will let me easily search/find a specific thread when needed. Right now Windows Explorer and Mac Finder both severely lag/crash when I try to do anything with 200k files. &lt;/p&gt;\n\n&lt;p&gt;My original thought process was to get them uploaded to my gmail account using Thunderbird + IMAP connection. I figured Google can help me with the search aspect and it would be nice to have all my emails in one place. But this is a super slow process and Thunderbird crashes after a day of uploading. I have also tried importing the emails to Outlook desktop but the app doesn&amp;#39;t support eml files without conversion.&lt;/p&gt;\n\n&lt;p&gt;I figured I can&amp;#39;t be the only one facing this and would love to get advice as to how to get these eml files into a searchable form. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for all your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124va37", "is_robot_indexable": true, "report_reasons": null, "author": "sgtawesomesauce", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124va37/recommends_on_email_archiving_with_searchability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124va37/recommends_on_email_archiving_with_searchability/", "subreddit_subscribers": 675950, "created_utc": 1680022912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey   \nive read through several posts as well as the 3-2-1 backup method. At the moment i want to buy a second off-site medium to back up my data (ive got a WD mypassword for quite some years now, that lives in a shelf and gets some new data every now and then).   \n\n\nWhat would be the best solution for the second off-site medium? I doesnt need to be fast or portable. I just want it to be reliable. Should i aim for another ext. HDD or should i rather grab a ext. SDD this time?   \n\n\nIts a vast jungle of options out there, but going through so many threads and articles what keeps coming up is a WD HDD. But i dont want to get another one  \n\n\nI am happy for every intel\n\n&amp;#x200B;\n\nbtw: i live in germany, thus im bound to the european market ;)   \n\n\nCheers", "author_fullname": "t2_bk8sr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Second off-site medium for Backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124kato", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1679999273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey&lt;br/&gt;\nive read through several posts as well as the 3-2-1 backup method. At the moment i want to buy a second off-site medium to back up my data (ive got a WD mypassword for quite some years now, that lives in a shelf and gets some new data every now and then).   &lt;/p&gt;\n\n&lt;p&gt;What would be the best solution for the second off-site medium? I doesnt need to be fast or portable. I just want it to be reliable. Should i aim for another ext. HDD or should i rather grab a ext. SDD this time?   &lt;/p&gt;\n\n&lt;p&gt;Its a vast jungle of options out there, but going through so many threads and articles what keeps coming up is a WD HDD. But i dont want to get another one  &lt;/p&gt;\n\n&lt;p&gt;I am happy for every intel&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;btw: i live in germany, thus im bound to the european market ;)   &lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124kato", "is_robot_indexable": true, "report_reasons": null, "author": "risikorolf", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124kato/second_offsite_medium_for_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124kato/second_offsite_medium_for_backup/", "subreddit_subscribers": 675950, "created_utc": 1679999273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I stumbled across a couple of pretty cheap SC280 but i havent used dell storage systems yet.  \nOn the spec sheet they offer 84bay of 6g (6x 4x 6g so total of 144G/18gb/s uplink) in a 5u chassis all for under 300\u20ac\n\nI couldnt really find any reviews or experiences from other homelabbers, feel free to share any thoughts about them.\n\nMy main questions are:\n\n**What type of licensing do they have?** (Management/IDrac or per disk etc)\n\n**Do they use any specialized stuff that would stop them from working with normal lsi hbas/truenas?**\n\n**Is there a limit on disk size?** (planing on using a mix of 3,6 and 8tb all sas drives)\n\n**Is there some kind of vendor locking of disks?**\n\nAny suggestions appreciated :)", "author_fullname": "t2_8jnr5wv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dell Compellent SC280 | Thoughts | Licensing? | Max Disk size? | Power draw?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1256jpq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680046998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled across a couple of pretty cheap SC280 but i havent used dell storage systems yet.&lt;br/&gt;\nOn the spec sheet they offer 84bay of 6g (6x 4x 6g so total of 144G/18gb/s uplink) in a 5u chassis all for under 300\u20ac&lt;/p&gt;\n\n&lt;p&gt;I couldnt really find any reviews or experiences from other homelabbers, feel free to share any thoughts about them.&lt;/p&gt;\n\n&lt;p&gt;My main questions are:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What type of licensing do they have?&lt;/strong&gt; (Management/IDrac or per disk etc)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Do they use any specialized stuff that would stop them from working with normal lsi hbas/truenas?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there a limit on disk size?&lt;/strong&gt; (planing on using a mix of 3,6 and 8tb all sas drives)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there some kind of vendor locking of disks?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1256jpq", "is_robot_indexable": true, "report_reasons": null, "author": "Pommes254", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1256jpq/dell_compellent_sc280_thoughts_licensing_max_disk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1256jpq/dell_compellent_sc280_thoughts_licensing_max_disk/", "subreddit_subscribers": 675950, "created_utc": 1680046998.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So currently, 2 of my 6 TB are inside an OptiPlex 7010. Its running an Ubuntu web server, however I cant utilize the other 1.8 TB that's not the server because I cant figure how to auto mount the disc and set up my server (apache, HTML) to read it. However, that's not as important. A local college is selling 2 TB discs for about 35 bucks. I plan to get another cheap office machine and put maybe 2 or 3 2 TB discs in it. Total cost would come to around 160$. My OptiPlex, however, still has 1.8 TB of unused space and one more hard drive slot. I cant figure out what to do because if I wipe and rebuild the 7010 it would no longer run a web server, but would run a NAS, but if I build a separate NAS I don't have to retire the 7010 from its web jobs, but I pay more. Which should I do, or is there a way to click the second 1.8 TB partition to the LAN? Is 35$ for 2 TB even good? I paid 45$ for my 2 TB disc. If I build a new NAS it would also be a 7010, as that's the cheapest they have. I also want to know if you can host that web server on the internet but keep the rest of the disc off the wide web and only on the LAN. Or is that more a question for r/selfhosted? Is there even a point in upgrading my 7010? It's missing one of the plastic brackets, so a hard drive wouldn't mount in properly unless I buy one, and I'm 99% sure a hard disk inside a pc with no support brackets is a bad situation. Is it too slow to do what I want it to? Here's the specs. 7010 MT, 3.8 GB ram, 2 TB hdd, 82 GB for Ubuntu, has apache installed, Intel Core i5-3570 CPU at 3.40 Ghz x 4. Also, can I run apache and a NAS software together peacefully? I'm a huge beginner, sorry if I seem annoying.", "author_fullname": "t2_718mpex4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on upgrading/building a file server thingy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124wdwg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680025255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So currently, 2 of my 6 TB are inside an OptiPlex 7010. Its running an Ubuntu web server, however I cant utilize the other 1.8 TB that&amp;#39;s not the server because I cant figure how to auto mount the disc and set up my server (apache, HTML) to read it. However, that&amp;#39;s not as important. A local college is selling 2 TB discs for about 35 bucks. I plan to get another cheap office machine and put maybe 2 or 3 2 TB discs in it. Total cost would come to around 160$. My OptiPlex, however, still has 1.8 TB of unused space and one more hard drive slot. I cant figure out what to do because if I wipe and rebuild the 7010 it would no longer run a web server, but would run a NAS, but if I build a separate NAS I don&amp;#39;t have to retire the 7010 from its web jobs, but I pay more. Which should I do, or is there a way to click the second 1.8 TB partition to the LAN? Is 35$ for 2 TB even good? I paid 45$ for my 2 TB disc. If I build a new NAS it would also be a 7010, as that&amp;#39;s the cheapest they have. I also want to know if you can host that web server on the internet but keep the rest of the disc off the wide web and only on the LAN. Or is that more a question for &lt;a href=\"/r/selfhosted\"&gt;r/selfhosted&lt;/a&gt;? Is there even a point in upgrading my 7010? It&amp;#39;s missing one of the plastic brackets, so a hard drive wouldn&amp;#39;t mount in properly unless I buy one, and I&amp;#39;m 99% sure a hard disk inside a pc with no support brackets is a bad situation. Is it too slow to do what I want it to? Here&amp;#39;s the specs. 7010 MT, 3.8 GB ram, 2 TB hdd, 82 GB for Ubuntu, has apache installed, Intel Core i5-3570 CPU at 3.40 Ghz x 4. Also, can I run apache and a NAS software together peacefully? I&amp;#39;m a huge beginner, sorry if I seem annoying.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "6TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124wdwg", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_McGuggins", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/124wdwg/advice_on_upgradingbuilding_a_file_server_thingy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124wdwg/advice_on_upgradingbuilding_a_file_server_thingy/", "subreddit_subscribers": 675950, "created_utc": 1680025255.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}