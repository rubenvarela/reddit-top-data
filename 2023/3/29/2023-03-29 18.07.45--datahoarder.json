{"kind": "Listing", "data": {"after": "t3_125qt6b", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "froid_san, Creator of English Patches for JP Exclusive Games for PS3, PSV, and PS4 Will be Shutting Down His Website.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 114, "top_awarded_type": null, "hide_score": false, "name": "t3_1258t25", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 625, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_6jesv", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 625, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7efBSECf6W9k-a4-yCBKnUsYY56S89EsgbfyiG2sMAo.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "VitaPiracy", "selftext": "", "author_fullname": "t2_3gz7qdgx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "froid_san, Creator of English Patches for JP Exclusive Games for PS3, PSV, and PS4 Will be Shutting Down His Website.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/VitaPiracy", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 114, "top_awarded_type": null, "hide_score": false, "name": "t3_124fl8f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 265, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 265, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7efBSECf6W9k-a4-yCBKnUsYY56S89EsgbfyiG2sMAo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "created": 1679984850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/0i8h50f3bfqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?auto=webp&amp;v=enabled&amp;s=97b9ad866a58999f0ad633a231a9d42ed991540f", "width": 770, "height": 628}, "resolutions": [{"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eaa72324231f7a4945ff32da822ca9a7753cc614", "width": 108, "height": 88}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b60de6d27a5934298b7b7f62b78e3b2c1dbda8ce", "width": 216, "height": 176}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3aec77160cdc97698879f70acc553f5a84a445f", "width": 320, "height": 260}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=022714839e8f28584e638dddf0b847d9c3743a33", "width": 640, "height": 521}], "variants": {}, "id": "FL_ro_C4G_dj-T453tPbQC1sEdGMqzWYfkFCSJLmI0Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3fxlj", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "124fl8f", "is_robot_indexable": true, "report_reasons": null, "author": "ANG-123", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/VitaPiracy/comments/124fl8f/froid_san_creator_of_english_patches_for_jp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/0i8h50f3bfqa1.png", "subreddit_subscribers": 80050, "created_utc": 1679984850.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1680052614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/0i8h50f3bfqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?auto=webp&amp;v=enabled&amp;s=97b9ad866a58999f0ad633a231a9d42ed991540f", "width": 770, "height": 628}, "resolutions": [{"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eaa72324231f7a4945ff32da822ca9a7753cc614", "width": 108, "height": 88}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b60de6d27a5934298b7b7f62b78e3b2c1dbda8ce", "width": 216, "height": 176}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3aec77160cdc97698879f70acc553f5a84a445f", "width": 320, "height": 260}, {"url": "https://preview.redd.it/0i8h50f3bfqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=022714839e8f28584e638dddf0b847d9c3743a33", "width": 640, "height": 521}], "variants": {}, "id": "FL_ro_C4G_dj-T453tPbQC1sEdGMqzWYfkFCSJLmI0Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "35TB + 8TB NAS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1258t25", "is_robot_indexable": true, "report_reasons": null, "author": "seamonkey420", "discussion_type": null, "num_comments": 44, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_124fl8f", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1258t25/froid_san_creator_of_english_patches_for_jp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/0i8h50f3bfqa1.png", "subreddit_subscribers": 675980, "created_utc": 1680052614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have images from several laptops that I've archived using macrium, and I just found out the free version is discontinued.  \n\nAm I looking at suffering the consequences of using a free, proprietary format, and potentially not being able to access the images? \n\nThanks.", "author_fullname": "t2_3kfjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Macrium Free discontinued. Am I looking at losing all of my disk images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1258d46", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680051511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have images from several laptops that I&amp;#39;ve archived using macrium, and I just found out the free version is discontinued.  &lt;/p&gt;\n\n&lt;p&gt;Am I looking at suffering the consequences of using a free, proprietary format, and potentially not being able to access the images? &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1258d46", "is_robot_indexable": true, "report_reasons": null, "author": "hoyfkd", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1258d46/macrium_free_discontinued_am_i_looking_at_losing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1258d46/macrium_free_discontinued_am_i_looking_at_losing/", "subreddit_subscribers": 675980, "created_utc": 1680051511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Or just the e-books part of the site?\n\nAsking because I have uploaded alot of old videos from the Romanian television archives that can't be found anywhere else. I still have them backed up on my external HDD's, but still....", "author_fullname": "t2_sdmay15h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will Internet Archive be closed forever if the lose in court?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125846a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680050877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or just the e-books part of the site?&lt;/p&gt;\n\n&lt;p&gt;Asking because I have uploaded alot of old videos from the Romanian television archives that can&amp;#39;t be found anywhere else. I still have them backed up on my external HDD&amp;#39;s, but still....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125846a", "is_robot_indexable": true, "report_reasons": null, "author": "Creative-Detail4369", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125846a/will_internet_archive_be_closed_forever_if_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125846a/will_internet_archive_be_closed_forever_if_the/", "subreddit_subscribers": 675980, "created_utc": 1680050877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello\n\nSome sad news for the photography community and also for the resource it offers but Amazon is closing down the whole of DP review on 10th April 2023.\n\nWhat is the best way to keep the website resource (and youtube) online?\n\n&amp;#x200B;\n\nSee: [https://www.dpreview.com/news/5901145460/dpreview-com-to-close](https://www.dpreview.com/news/5901145460/dpreview-com-to-close)", "author_fullname": "t2_l8l3l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP Review is closing down (youtube, website) on 10th April - Best Archive Instructions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1252ay3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680037515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;Some sad news for the photography community and also for the resource it offers but Amazon is closing down the whole of DP review on 10th April 2023.&lt;/p&gt;\n\n&lt;p&gt;What is the best way to keep the website resource (and youtube) online?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;See: &lt;a href=\"https://www.dpreview.com/news/5901145460/dpreview-com-to-close\"&gt;https://www.dpreview.com/news/5901145460/dpreview-com-to-close&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?auto=webp&amp;v=enabled&amp;s=62800be361d0e710877115d585fc54f6bc091987", "width": 745, "height": 745}, "resolutions": [{"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c85ecc4153f61d9a247d4758d9430569622282d", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a74b7694c73ea2345e65d9ea046b7453dce349cd", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3dbcef5e3e8892307008a8899734f8a2c64159c5", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/ZAqZhSYKYUMc8wiSF_ZZK9w4kocyx4S8ml9u-bjPOy4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b46d7621039f2b84e5eb82a6de9d0d1874068788", "width": 640, "height": 640}], "variants": {}, "id": "KDAVLRZTXaBom3MDvF95i0FH6bCPweDv6BPK8qmCi6E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1252ay3", "is_robot_indexable": true, "report_reasons": null, "author": "Fkmeitscold", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1252ay3/dp_review_is_closing_down_youtube_website_on_10th/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1252ay3/dp_review_is_closing_down_youtube_website_on_10th/", "subreddit_subscribers": 675980, "created_utc": 1680037515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm collecting old movies (pre-1970) for long term and occasional use storage. These movies can either be OOP, Region B, or have never received a proper Blu-Ray release.I want to watch them semi-reguarly and own them 10-20 years from now. I posted on this sub a few days ago asking about BD-R burning, but my understanding is that even an authored BD-R disc won't function on my PS5 and maybe not on my PS3 (which are my only drives atm). Do I copy my movies onto an HDD and continually buy new ones as backups over the years or is there a better way?", "author_fullname": "t2_8zpyfcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to archive old movies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124y2ge", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680028814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m collecting old movies (pre-1970) for long term and occasional use storage. These movies can either be OOP, Region B, or have never received a proper Blu-Ray release.I want to watch them semi-reguarly and own them 10-20 years from now. I posted on this sub a few days ago asking about BD-R burning, but my understanding is that even an authored BD-R disc won&amp;#39;t function on my PS5 and maybe not on my PS3 (which are my only drives atm). Do I copy my movies onto an HDD and continually buy new ones as backups over the years or is there a better way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124y2ge", "is_robot_indexable": true, "report_reasons": null, "author": "HunteHorseman", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124y2ge/best_way_to_archive_old_movies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124y2ge/best_way_to_archive_old_movies/", "subreddit_subscribers": 675980, "created_utc": 1680028814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "EDIT: This was originally a \"Question/Advice\" flaired post asking for best practices. I've since developed my own method which works for me. Please read below if you'd like to know what I ended up doing or feel welcome to share your own solution!   \n\n\nORIGINAL POST:\n\nI'm hoping that the community can help me.\n\nI am using blurays for archiving files that I have long held on HDDs. I've read that it's common to run a file check on the data before burning to disc and to then also add those logs to the blurays along with your data.\n\nIs there a best practices guide on what tools to use to achieve this? I've been searching for a best practices guide but I'm coming up short in my search.\n\nBest tools/methodology for arching files to bluray would be really appreciated.  \n\n\nUPDATE: So on to my solution.   \n\n\nFrustrated by my inability to find a best practices, I ended up resorting to my own solution.  \nI set up a project folder in Win10 where prepare each bluray by compressing folders of files into .7zip format. This was simple for me and kept everything organized the way that I liked it.  \n\n\nOnce I had various folders with various .7zip compressed archives, I go to the root of my project folder (this would also be the root once these files are copied over the bluray), I highlight all directories on the root and any root level files, right click on the highlighted files go to 7zip in your context menu, then CRC SHA, then SHA-256 -&gt; \\[name\\].sha256  \n\n\nThis will output a sha-256 hash for every file selected into a new file called \\[name\\].sha256. I then verified that these hashes were accurate by opening up my text editor (Sublime). You'll see all of the hashes, you can then check random files by going back into your folder and selecting a file, go back through the context menu but this time select: SHA-256 to get only the hash for that file.  \n\n\nOnce I was confident that the hashes were accurate, I opened up ImgBurn, went to \"Write files/folders to disc\" and added my files including my SHA-256 file specific to only the files added to my bluray disc. I then went to the options tab and changed the File System to UDF revision 2.60. Selected Verify and then burnt my files to bluray (build).   \n\n\nOnce the burn was successful, I open the bluray in Windows and compare the hashes of the files again against the SHA-256 file I made to make sure that they show the right values. They did so it's all good! Copying files to the PC from disc to do another check (super redundant) proved this to be a good method for myself.   \n\n\nI make a couple disc copies and then make a third copy on an external HDD. all cold storage.  \n\n\nIt's really late here but I wanted to leave this for anyone struggling to find a good path forward. I hope this helps someone! Let me know if it helped you! And if you know of a better way, also please let me know! ", "author_fullname": "t2_yu2xz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optical Archive Logging/File Archiving Best Practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124zhcn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680070563.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680031814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;EDIT: This was originally a &amp;quot;Question/Advice&amp;quot; flaired post asking for best practices. I&amp;#39;ve since developed my own method which works for me. Please read below if you&amp;#39;d like to know what I ended up doing or feel welcome to share your own solution!   &lt;/p&gt;\n\n&lt;p&gt;ORIGINAL POST:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping that the community can help me.&lt;/p&gt;\n\n&lt;p&gt;I am using blurays for archiving files that I have long held on HDDs. I&amp;#39;ve read that it&amp;#39;s common to run a file check on the data before burning to disc and to then also add those logs to the blurays along with your data.&lt;/p&gt;\n\n&lt;p&gt;Is there a best practices guide on what tools to use to achieve this? I&amp;#39;ve been searching for a best practices guide but I&amp;#39;m coming up short in my search.&lt;/p&gt;\n\n&lt;p&gt;Best tools/methodology for arching files to bluray would be really appreciated.  &lt;/p&gt;\n\n&lt;p&gt;UPDATE: So on to my solution.   &lt;/p&gt;\n\n&lt;p&gt;Frustrated by my inability to find a best practices, I ended up resorting to my own solution.&lt;br/&gt;\nI set up a project folder in Win10 where prepare each bluray by compressing folders of files into .7zip format. This was simple for me and kept everything organized the way that I liked it.  &lt;/p&gt;\n\n&lt;p&gt;Once I had various folders with various .7zip compressed archives, I go to the root of my project folder (this would also be the root once these files are copied over the bluray), I highlight all directories on the root and any root level files, right click on the highlighted files go to 7zip in your context menu, then CRC SHA, then SHA-256 -&amp;gt; [name].sha256  &lt;/p&gt;\n\n&lt;p&gt;This will output a sha-256 hash for every file selected into a new file called [name].sha256. I then verified that these hashes were accurate by opening up my text editor (Sublime). You&amp;#39;ll see all of the hashes, you can then check random files by going back into your folder and selecting a file, go back through the context menu but this time select: SHA-256 to get only the hash for that file.  &lt;/p&gt;\n\n&lt;p&gt;Once I was confident that the hashes were accurate, I opened up ImgBurn, went to &amp;quot;Write files/folders to disc&amp;quot; and added my files including my SHA-256 file specific to only the files added to my bluray disc. I then went to the options tab and changed the File System to UDF revision 2.60. Selected Verify and then burnt my files to bluray (build).   &lt;/p&gt;\n\n&lt;p&gt;Once the burn was successful, I open the bluray in Windows and compare the hashes of the files again against the SHA-256 file I made to make sure that they show the right values. They did so it&amp;#39;s all good! Copying files to the PC from disc to do another check (super redundant) proved this to be a good method for myself.   &lt;/p&gt;\n\n&lt;p&gt;I make a couple disc copies and then make a third copy on an external HDD. all cold storage.  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s really late here but I wanted to leave this for anyone struggling to find a good path forward. I hope this helps someone! Let me know if it helped you! And if you know of a better way, also please let me know! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124zhcn", "is_robot_indexable": true, "report_reasons": null, "author": "0101-ERROR-1001", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124zhcn/optical_archive_loggingfile_archiving_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124zhcn/optical_archive_loggingfile_archiving_best/", "subreddit_subscribers": 675980, "created_utc": 1680031814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "*Library of H* is a GUI application for downloading to, viewing from, and managing the ero manga/anime in your possession. It can download either individual galleries or whole of artist/group galleries from a \\*bunch of ero content sites. *(currently only works with Hitomi.)* Get/try it: [https://github.com/hikineet0/library-of-h-python](https://github.com/hikineet0/library-of-h-python)\n\nFeel free to contact me either in this thread or (more preferably) in the repository be it through issues and what not.", "author_fullname": "t2_707b95qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Library of H: Library of Alexandria but for \u30a8\u30ed\u30b9* (*Erosu).", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125h24x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680077316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Library of H&lt;/em&gt; is a GUI application for downloading to, viewing from, and managing the ero manga/anime in your possession. It can download either individual galleries or whole of artist/group galleries from a *bunch of ero content sites. &lt;em&gt;(currently only works with Hitomi.)&lt;/em&gt; Get/try it: &lt;a href=\"https://github.com/hikineet0/library-of-h-python\"&gt;https://github.com/hikineet0/library-of-h-python&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feel free to contact me either in this thread or (more preferably) in the repository be it through issues and what not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?auto=webp&amp;v=enabled&amp;s=2b5a71c737c7d2f491f077a893c2594d259cce84", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b38a177653b3b8fee18334d2e8ae8d415e97c52", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db9131a4ed54dec78620bca3c51f9d0fa62c8269", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dca95f557f6687e4ae2b23b11b1125bbfbb3b014", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3d6f7a50bdbfeae56c456dcdfd80f9fc940b962", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef17cf2b7ef2543c2369193461d92d38128c031a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ojbWpz_4HUSUMhCa9ZL8vbxMMd-WryppWJj274B49f8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb95a73b9690016940c30d9ce6cda5e7263faa15", "width": 1080, "height": 540}], "variants": {}, "id": "S-wn7zIuHmgKMhRYd_4Ltqv0Kv2z_990YoylGjrouCE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125h24x", "is_robot_indexable": true, "report_reasons": null, "author": "letters-and-dashes", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125h24x/library_of_h_library_of_alexandria_but_for_\u30a8\u30ed\u30b9/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125h24x/library_of_h_library_of_alexandria_but_for_\u30a8\u30ed\u30b9/", "subreddit_subscribers": 675980, "created_utc": 1680077316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for recommendations on BDXL Windows burning software? Thank you in advance", "author_fullname": "t2_pzfb76t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended Windows bdxl burning software?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125142c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680035123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for recommendations on BDXL Windows burning software? Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125142c", "is_robot_indexable": true, "report_reasons": null, "author": "DisturbedBeaker", "discussion_type": null, "num_comments": 8, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125142c/recommended_windows_bdxl_burning_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125142c/recommended_windows_bdxl_burning_software/", "subreddit_subscribers": 675980, "created_utc": 1680035123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have two dedicated leaseweb servers one with 4TB storage and another bigger one with 16TB storage. I mainly use them as seedboxes as well as for emby/plex streaming. Recently, I learned about nextcloud from this sub and its amazing. \n\nSo I want to migrate all files from my first server to second server, mainly TV Series and movies so that I can use the first server as a dedicated nextCloud machine while I still use the second one for plex and other stuff because my usage for the first server is drastically reduced.\n\nI want to know what's the best way to transfer files between these two machines? Initially I was thinking  SFTPing the files to my local and then SFTPing back to second server. But I live in a different continent from my servers while both my servers share the same country. So it's probably easy to just transfer between two 1Gbps machines. But I want to if there's a better way to do this.", "author_fullname": "t2_5tdnrb9j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to migrate files from one dedicated server to another", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125d6hu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680064573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have two dedicated leaseweb servers one with 4TB storage and another bigger one with 16TB storage. I mainly use them as seedboxes as well as for emby/plex streaming. Recently, I learned about nextcloud from this sub and its amazing. &lt;/p&gt;\n\n&lt;p&gt;So I want to migrate all files from my first server to second server, mainly TV Series and movies so that I can use the first server as a dedicated nextCloud machine while I still use the second one for plex and other stuff because my usage for the first server is drastically reduced.&lt;/p&gt;\n\n&lt;p&gt;I want to know what&amp;#39;s the best way to transfer files between these two machines? Initially I was thinking  SFTPing the files to my local and then SFTPing back to second server. But I live in a different continent from my servers while both my servers share the same country. So it&amp;#39;s probably easy to just transfer between two 1Gbps machines. But I want to if there&amp;#39;s a better way to do this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125d6hu", "is_robot_indexable": true, "report_reasons": null, "author": "Terminal_Monk", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125d6hu/best_way_to_migrate_files_from_one_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125d6hu/best_way_to_migrate_files_from_one_dedicated/", "subreddit_subscribers": 675980, "created_utc": 1680064573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've never bought used drives before, but I'm in the sweet spot of having sufficient backups that I can risk it more than in the past, and I also need to get my $/tb down.\n\nI'm looking at some \"manufacture recertified\" drives from serverpartdeals.com. I've seen a lot of good things about them, including around this sub. I wanted to ask from those of you who have bought from them:\n\n1. is there any real consistency in the sort of wear on the drives? Like, are you getting drives with easily 10k+hours on the drive and/or drives that have DOM at least 2-3 years out? Conversely, are you receiving drives that are consistently newer in age and/or lower in hours than you'd have expected? Or is it all over the place? \n\n\n2. The exos seem to run a hair cheaper than the Ultrastars when I look around. Honestly, I haven't bought a Seagate drive in six years so I'm out of the loop on the dependability. I'm a bit worried about what I've seen from reviews and the like on Exos drives regarding reliability. I know all drives fail, some very fast and it's foolish to try and predict reliability in any shape or form. So, I'm guiltily confessing that I'm interested in your anecdotal experiences on this one, in buying those recertified Exos drives and how it went. \n\n\n3. I saw a two year limited warranty on these drives from Serverpartdeals. Have any of you ever had to take advantage of this? How did RMA go?\n\nI really appreciate any help, advice or guidance. I really want to try out some used drives, just have these questions and concerns hanging over me. But I also understand these questions might be kind of dumb. Thanks for any help.", "author_fullname": "t2_sn9i9n2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about \"manufacture recertified\" drives from serverpartdeals.com", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1254n0i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680042974.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680042591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never bought used drives before, but I&amp;#39;m in the sweet spot of having sufficient backups that I can risk it more than in the past, and I also need to get my $/tb down.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking at some &amp;quot;manufacture recertified&amp;quot; drives from serverpartdeals.com. I&amp;#39;ve seen a lot of good things about them, including around this sub. I wanted to ask from those of you who have bought from them:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;is there any real consistency in the sort of wear on the drives? Like, are you getting drives with easily 10k+hours on the drive and/or drives that have DOM at least 2-3 years out? Conversely, are you receiving drives that are consistently newer in age and/or lower in hours than you&amp;#39;d have expected? Or is it all over the place? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The exos seem to run a hair cheaper than the Ultrastars when I look around. Honestly, I haven&amp;#39;t bought a Seagate drive in six years so I&amp;#39;m out of the loop on the dependability. I&amp;#39;m a bit worried about what I&amp;#39;ve seen from reviews and the like on Exos drives regarding reliability. I know all drives fail, some very fast and it&amp;#39;s foolish to try and predict reliability in any shape or form. So, I&amp;#39;m guiltily confessing that I&amp;#39;m interested in your anecdotal experiences on this one, in buying those recertified Exos drives and how it went. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I saw a two year limited warranty on these drives from Serverpartdeals. Have any of you ever had to take advantage of this? How did RMA go?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I really appreciate any help, advice or guidance. I really want to try out some used drives, just have these questions and concerns hanging over me. But I also understand these questions might be kind of dumb. Thanks for any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1254n0i", "is_robot_indexable": true, "report_reasons": null, "author": "Peruvian_Poo_Pickler", "discussion_type": null, "num_comments": 12, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1254n0i/questions_about_manufacture_recertified_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1254n0i/questions_about_manufacture_recertified_drives/", "subreddit_subscribers": 675980, "created_utc": 1680042591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As far as I can work out, it\u2019s usually cheaper to create a brand new NAS than it is to host a backup of your data in S3/Glacier for more than a year?\n\nFor example, \u201cS3 Glacier Flexible Retrieval\u201d is $0.00405 per GB, which sounds cheap but multiply that by 1000 to get TB then again by, say, 36TB which is your average RAID 6 with 5 x 12TB disks and you get $145 a month. ($4.05 per month, per TB)\n\nCosts over a year would be $1749.60 for Glacier, and you could build a new fully populated Synology NAS for that, and that\u2019s a one-time purchase which should in theory last 5 years or more.\n\nIs AWS the cheapest solution, when you factor in power costs as well? Worth noting that Backblaze is $5 per TB so Glacier is cheaper.\n\nObviously there\u2019s the offsite considerations too, which the cloud providers definitely have an advantage of.", "author_fullname": "t2_8429z2fz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With a decently large NAS, how do you back up this amount of data cost-effectively?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12526pe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680037272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As far as I can work out, it\u2019s usually cheaper to create a brand new NAS than it is to host a backup of your data in S3/Glacier for more than a year?&lt;/p&gt;\n\n&lt;p&gt;For example, \u201cS3 Glacier Flexible Retrieval\u201d is $0.00405 per GB, which sounds cheap but multiply that by 1000 to get TB then again by, say, 36TB which is your average RAID 6 with 5 x 12TB disks and you get $145 a month. ($4.05 per month, per TB)&lt;/p&gt;\n\n&lt;p&gt;Costs over a year would be $1749.60 for Glacier, and you could build a new fully populated Synology NAS for that, and that\u2019s a one-time purchase which should in theory last 5 years or more.&lt;/p&gt;\n\n&lt;p&gt;Is AWS the cheapest solution, when you factor in power costs as well? Worth noting that Backblaze is $5 per TB so Glacier is cheaper.&lt;/p&gt;\n\n&lt;p&gt;Obviously there\u2019s the offsite considerations too, which the cloud providers definitely have an advantage of.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12526pe", "is_robot_indexable": true, "report_reasons": null, "author": "BowtieChickenAlfredo", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12526pe/with_a_decently_large_nas_how_do_you_back_up_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12526pe/with_a_decently_large_nas_how_do_you_back_up_this/", "subreddit_subscribers": 675980, "created_utc": 1680037272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I hope this is the right sub for my question.\n\nMy dad passed away this month and the crematorium provided a livestream for the people who attend the service in person. The stream was saved and you can access it through their website for 30 days after which it will be deleted, or you can buy it and receive it on USB/DVD for \u20ac160. \ud83d\ude43\n\nI've tried to screen record it but it it doesn't record any sound. Is there any way to download the video?", "author_fullname": "t2_lgop7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to download a video you have to access with a personal code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_125vcif", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680112177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope this is the right sub for my question.&lt;/p&gt;\n\n&lt;p&gt;My dad passed away this month and the crematorium provided a livestream for the people who attend the service in person. The stream was saved and you can access it through their website for 30 days after which it will be deleted, or you can buy it and receive it on USB/DVD for \u20ac160. \ud83d\ude43&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to screen record it but it it doesn&amp;#39;t record any sound. Is there any way to download the video?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125vcif", "is_robot_indexable": true, "report_reasons": null, "author": "TMeganV", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125vcif/is_there_a_way_to_download_a_video_you_have_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125vcif/is_there_a_way_to_download_a_video_you_have_to/", "subreddit_subscribers": 675980, "created_utc": 1680112177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I would like to know any of your recommendations for any cloud sync software. It will be used for a large youtube channel, where a handful of video editors and 3d animators would need it so our project files are always available to anyone. Price is not much of an issue  \nHere are some requirements\n\n* Be able to sync different specific folders on different drives to the cloud\n* Be very robust, with little sketch/chance of going bankrupt\n* Have good upload speeds with fast sync\n* Little to no bandwidth caps\n\nAnd some things that aren't required but would be nice\n\n* The ability for clients to automatically keep folder up to date, aka auto download any uploaded files. \n* Free up space option, sometimes projects can have 150GB of footage\n* Download files to device with their respective folder structure, so we dont have to download one giant zip file \n* Team collaboration tools, like edits or comments\n* File history (not that important but would be a plus) \n\nI have looked at a couple services, but I was unclear about the specifics. If anyone would come with some recommendations I would greatly appreciate it.", "author_fullname": "t2_m3f1s68", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best cloud sync software for media production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125saq4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680105619.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to know any of your recommendations for any cloud sync software. It will be used for a large youtube channel, where a handful of video editors and 3d animators would need it so our project files are always available to anyone. Price is not much of an issue&lt;br/&gt;\nHere are some requirements&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Be able to sync different specific folders on different drives to the cloud&lt;/li&gt;\n&lt;li&gt;Be very robust, with little sketch/chance of going bankrupt&lt;/li&gt;\n&lt;li&gt;Have good upload speeds with fast sync&lt;/li&gt;\n&lt;li&gt;Little to no bandwidth caps&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;And some things that aren&amp;#39;t required but would be nice&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The ability for clients to automatically keep folder up to date, aka auto download any uploaded files. &lt;/li&gt;\n&lt;li&gt;Free up space option, sometimes projects can have 150GB of footage&lt;/li&gt;\n&lt;li&gt;Download files to device with their respective folder structure, so we dont have to download one giant zip file &lt;/li&gt;\n&lt;li&gt;Team collaboration tools, like edits or comments&lt;/li&gt;\n&lt;li&gt;File history (not that important but would be a plus) &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have looked at a couple services, but I was unclear about the specifics. If anyone would come with some recommendations I would greatly appreciate it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125saq4", "is_robot_indexable": true, "report_reasons": null, "author": "Askejm", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125saq4/best_cloud_sync_software_for_media_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125saq4/best_cloud_sync_software_for_media_production/", "subreddit_subscribers": 675980, "created_utc": 1680105619.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nI've  been looking to download some 1080p versions of some old shows, but  I've been hoping to find 1080p versions of them. I can manage some,  where it looks as though the uploader has remasted it.\n\nHow do I find these uploads, or do I have to try and find a way to up the quality myself?  \n\n\nDo you think it's better to say Fuck It and just have old shows in their older resolution?", "author_fullname": "t2_w6sa6uq6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you guys do to get better quality versions of old shows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125jkxw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680085684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve  been looking to download some 1080p versions of some old shows, but  I&amp;#39;ve been hoping to find 1080p versions of them. I can manage some,  where it looks as though the uploader has remasted it.&lt;/p&gt;\n\n&lt;p&gt;How do I find these uploads, or do I have to try and find a way to up the quality myself?  &lt;/p&gt;\n\n&lt;p&gt;Do you think it&amp;#39;s better to say Fuck It and just have old shows in their older resolution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125jkxw", "is_robot_indexable": true, "report_reasons": null, "author": "Loglogloglog11221", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125jkxw/what_do_you_guys_do_to_get_better_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125jkxw/what_do_you_guys_do_to_get_better_quality/", "subreddit_subscribers": 675980, "created_utc": 1680085684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a tool available to download the entire royalty free YouTube Studio music library automatically? I've tried a number of link copy chrome extensions without success as YouTube have all their download links well hidden (likely through json files). Any other ideas? \n\nI'd like to save myself from carpal tunnel clicking all 1600+ song download links if at all possible.", "author_fullname": "t2_3xu6kqgm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YouTube Studio Music Library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125gevo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680075083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a tool available to download the entire royalty free YouTube Studio music library automatically? I&amp;#39;ve tried a number of link copy chrome extensions without success as YouTube have all their download links well hidden (likely through json files). Any other ideas? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to save myself from carpal tunnel clicking all 1600+ song download links if at all possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125gevo", "is_robot_indexable": true, "report_reasons": null, "author": "Yantarlok", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125gevo/youtube_studio_music_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125gevo/youtube_studio_music_library/", "subreddit_subscribers": 675980, "created_utc": 1680075083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am attempting to extract a file from the latest dump download from Wikipedia, but it comes out corrupted every time. I've used WinZip and 7zip, but when I use my python code to clean up the file for the learning model I want to make, it says that the file is corrupted. I am very sure that the code is correct, as I have checked it with many sources.", "author_fullname": "t2_8v06mu07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wikipedia Dump Extraction", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125eqjc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680069513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am attempting to extract a file from the latest dump download from Wikipedia, but it comes out corrupted every time. I&amp;#39;ve used WinZip and 7zip, but when I use my python code to clean up the file for the learning model I want to make, it says that the file is corrupted. I am very sure that the code is correct, as I have checked it with many sources.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125eqjc", "is_robot_indexable": true, "report_reasons": null, "author": "Swirly403", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125eqjc/wikipedia_dump_extraction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125eqjc/wikipedia_dump_extraction/", "subreddit_subscribers": 675980, "created_utc": 1680069513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have 8 smaller SSDs ranging from 120 GBs to 480 GBs from my previous employer who no longer needed them.\n\nMy question to you guys is; What would be the best way to hook these up to my computer? I am using all my sata connections already. I looked into something like this https://www.amazon.com/Sabrent-Tool-free-Enclosure-Optimized-EC-UASP/dp/B00OJ3UJ2S - but I figure there has got to be a better way and you guys would be the ones to know.\n\nThanks!", "author_fullname": "t2_5v1ec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smaller Sized SSDs Laying Around The House", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12593av", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680053347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have 8 smaller SSDs ranging from 120 GBs to 480 GBs from my previous employer who no longer needed them.&lt;/p&gt;\n\n&lt;p&gt;My question to you guys is; What would be the best way to hook these up to my computer? I am using all my sata connections already. I looked into something like this &lt;a href=\"https://www.amazon.com/Sabrent-Tool-free-Enclosure-Optimized-EC-UASP/dp/B00OJ3UJ2S\"&gt;https://www.amazon.com/Sabrent-Tool-free-Enclosure-Optimized-EC-UASP/dp/B00OJ3UJ2S&lt;/a&gt; - but I figure there has got to be a better way and you guys would be the ones to know.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12593av", "is_robot_indexable": true, "report_reasons": null, "author": "FlameHaze", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12593av/smaller_sized_ssds_laying_around_the_house/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12593av/smaller_sized_ssds_laying_around_the_house/", "subreddit_subscribers": 675980, "created_utc": 1680053347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I was reading some fantasy fiction about having to evacuate earth and set up on another planet, and it got me thinking: were I to be in those shoes and wanted to preserve some IT infrastructure, knowing I wouldn't have the internet later, I would need an offline database of every version of every device driver for every OS it was developed for (among other stuff).\n\nThis isn't something I've seen covered before and couldn't find an example of such a database on initial searches (at least nothing meant to be easily downloaded / mirrored).\n\nHas this wheel already been invented?  If so, where would I go to download a copy of it?", "author_fullname": "t2_3c8ce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comprehensive offline driver database: all devices, all OSs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1257p0p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Seeking data set", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680050022.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680049841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I was reading some fantasy fiction about having to evacuate earth and set up on another planet, and it got me thinking: were I to be in those shoes and wanted to preserve some IT infrastructure, knowing I wouldn&amp;#39;t have the internet later, I would need an offline database of every version of every device driver for every OS it was developed for (among other stuff).&lt;/p&gt;\n\n&lt;p&gt;This isn&amp;#39;t something I&amp;#39;ve seen covered before and couldn&amp;#39;t find an example of such a database on initial searches (at least nothing meant to be easily downloaded / mirrored).&lt;/p&gt;\n\n&lt;p&gt;Has this wheel already been invented?  If so, where would I go to download a copy of it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Only 18TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1257p0p", "is_robot_indexable": true, "report_reasons": null, "author": "l_one", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1257p0p/comprehensive_offline_driver_database_all_devices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1257p0p/comprehensive_offline_driver_database_all_devices/", "subreddit_subscribers": 675980, "created_utc": 1680049841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "...I've got many files listed in the \"file list\" and I've right-clicked on one particular folder that I want excluded from the transfer and I've selected \"Ignore\". Does that mean it will skip copying that folder?\n\nIt's probably obvious, right? I only ask because I tried \"removing\" the folder from the list and it said that that option was only available in the paid version of the software. But...Ignore, if my understanding of the definition of the word is accurate, is...sorta...the same thing? Or, more specifically, \"ignore\" and \"remove\" would both achieve the same goal of not copying that folder...?", "author_fullname": "t2_96z9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dumb Teracopy question: I'm currently transferring some data from one drive to another...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1257lak", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680049801.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680049592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;...I&amp;#39;ve got many files listed in the &amp;quot;file list&amp;quot; and I&amp;#39;ve right-clicked on one particular folder that I want excluded from the transfer and I&amp;#39;ve selected &amp;quot;Ignore&amp;quot;. Does that mean it will skip copying that folder?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s probably obvious, right? I only ask because I tried &amp;quot;removing&amp;quot; the folder from the list and it said that that option was only available in the paid version of the software. But...Ignore, if my understanding of the definition of the word is accurate, is...sorta...the same thing? Or, more specifically, &amp;quot;ignore&amp;quot; and &amp;quot;remove&amp;quot; would both achieve the same goal of not copying that folder...?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1257lak", "is_robot_indexable": true, "report_reasons": null, "author": "ultranothing", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1257lak/dumb_teracopy_question_im_currently_transferring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1257lak/dumb_teracopy_question_im_currently_transferring/", "subreddit_subscribers": 675980, "created_utc": 1680049592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Which cases are good for having 10+ HDDs The most common one I see here is the Fractal Design Define 7 XL. I have also seen the Meshify 2 XL mentioned. Any other good ones I should be looking at?\n\nI am planning to start transitioning to either 18TB or 20TB HDDs. I'll start off with only 2 in this new case, but I know I'm going to get more and fill it up the case in the years to come. I want to buy something now that I can keep using and building in.\n\nThis is my current build and I'm trying to make a second server-only PC: https://pcpartpicker.com/list/TGDPBj\n\nAny advice appreciated. Thanks!", "author_fullname": "t2_qulcas15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended cases for DIY NAS? Hoping to eventually build to 10+ HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1257f3j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680049167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which cases are good for having 10+ HDDs The most common one I see here is the Fractal Design Define 7 XL. I have also seen the Meshify 2 XL mentioned. Any other good ones I should be looking at?&lt;/p&gt;\n\n&lt;p&gt;I am planning to start transitioning to either 18TB or 20TB HDDs. I&amp;#39;ll start off with only 2 in this new case, but I know I&amp;#39;m going to get more and fill it up the case in the years to come. I want to buy something now that I can keep using and building in.&lt;/p&gt;\n\n&lt;p&gt;This is my current build and I&amp;#39;m trying to make a second server-only PC: &lt;a href=\"https://pcpartpicker.com/list/TGDPBj\"&gt;https://pcpartpicker.com/list/TGDPBj&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any advice appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1257f3j", "is_robot_indexable": true, "report_reasons": null, "author": "IHateReddit_1153151", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1257f3j/recommended_cases_for_diy_nas_hoping_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1257f3j/recommended_cases_for_diy_nas_hoping_to/", "subreddit_subscribers": 675980, "created_utc": 1680049167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I stumbled across a couple of pretty cheap SC280 but i havent used dell storage systems yet.  \nOn the spec sheet they offer 84bay of 6g (6x 4x 6g so total of 144G/18gb/s uplink) in a 5u chassis all for under 300\u20ac\n\nI couldnt really find any reviews or experiences from other homelabbers, feel free to share any thoughts about them.\n\nMy main questions are:\n\n**What type of licensing do they have?** (Management/IDrac or per disk etc)\n\n**Do they use any specialized stuff that would stop them from working with normal lsi hbas/truenas?**\n\n**Is there a limit on disk size?** (planing on using a mix of 3,6 and 8tb all sas drives)\n\n**Is there some kind of vendor locking of disks?**\n\nAny suggestions appreciated :)", "author_fullname": "t2_8jnr5wv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dell Compellent SC280 | Thoughts | Licensing? | Max Disk size? | Power draw?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1256jpq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680046998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled across a couple of pretty cheap SC280 but i havent used dell storage systems yet.&lt;br/&gt;\nOn the spec sheet they offer 84bay of 6g (6x 4x 6g so total of 144G/18gb/s uplink) in a 5u chassis all for under 300\u20ac&lt;/p&gt;\n\n&lt;p&gt;I couldnt really find any reviews or experiences from other homelabbers, feel free to share any thoughts about them.&lt;/p&gt;\n\n&lt;p&gt;My main questions are:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What type of licensing do they have?&lt;/strong&gt; (Management/IDrac or per disk etc)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Do they use any specialized stuff that would stop them from working with normal lsi hbas/truenas?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there a limit on disk size?&lt;/strong&gt; (planing on using a mix of 3,6 and 8tb all sas drives)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there some kind of vendor locking of disks?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1256jpq", "is_robot_indexable": true, "report_reasons": null, "author": "Pommes254", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1256jpq/dell_compellent_sc280_thoughts_licensing_max_disk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1256jpq/dell_compellent_sc280_thoughts_licensing_max_disk/", "subreddit_subscribers": 675980, "created_utc": 1680046998.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I Have 6 2.5\" hdd's laying around.\nI would like to use it for some purpose like a low powered temporary Plex server where I can connect all the drives and use it as one single drive via Snapraid or something.\nReliability is not an issue so even if the drives fail later on, not bothered.\nI need some compact solution which can work via USB.", "author_fullname": "t2_6zlh9fwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connect multiple 2.5\" hdd via USB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124xez5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680027437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I Have 6 2.5&amp;quot; hdd&amp;#39;s laying around.\nI would like to use it for some purpose like a low powered temporary Plex server where I can connect all the drives and use it as one single drive via Snapraid or something.\nReliability is not an issue so even if the drives fail later on, not bothered.\nI need some compact solution which can work via USB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "124xez5", "is_robot_indexable": true, "report_reasons": null, "author": "Vap0rx47", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/124xez5/connect_multiple_25_hdd_via_usb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/124xez5/connect_multiple_25_hdd_via_usb/", "subreddit_subscribers": 675980, "created_utc": 1680027437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a current price comparison of the unlimited cloud guys and how to go about getting the best prices.  \n\n\nI'm currently on a gsuite and paying too much it's like triple in the 2-3 years I've had it.. Plz Hlp  \n\n\n(I have searched this on google first and the last post about this was a year old in this subreddit.)", "author_fullname": "t2_svufv0ns", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best current price unlimited cloud storage? For March 29th 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_125vr2z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680112877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a current price comparison of the unlimited cloud guys and how to go about getting the best prices.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently on a gsuite and paying too much it&amp;#39;s like triple in the 2-3 years I&amp;#39;ve had it.. Plz Hlp  &lt;/p&gt;\n\n&lt;p&gt;(I have searched this on google first and the last post about this was a year old in this subreddit.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125vr2z", "is_robot_indexable": true, "report_reasons": null, "author": "SuccessfulHawk503", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125vr2z/best_current_price_unlimited_cloud_storage_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125vr2z/best_current_price_unlimited_cloud_storage_for/", "subreddit_subscribers": 675980, "created_utc": 1680112877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I paid for google drive this week, because it is a cheap service and allows pirated files (and porn btw) as long as I don't share them. I also have some important files that I modify daily, so I need to leave them on the computer. So I synced them to my computer folders on google drive, so when I change the file here, it goes straight to the cloud. \n\nI can't just put my files in the cloud, as there is a small chance that google will delete them, and I can't work with them directly from there either.\n\nI want to know if in case some ransomware attack happens on my pc, modifying all my files, is there any practical way to revert them to a state before the ransomware, without having to change the version of each file one by one? Or is there another cloud backup/storage service that better meets my needs?\n\nThanks in advance", "author_fullname": "t2_9jqq57my", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Google drive protect my files in case of a ransomware attack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_125ujq1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680110795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I paid for google drive this week, because it is a cheap service and allows pirated files (and porn btw) as long as I don&amp;#39;t share them. I also have some important files that I modify daily, so I need to leave them on the computer. So I synced them to my computer folders on google drive, so when I change the file here, it goes straight to the cloud. &lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t just put my files in the cloud, as there is a small chance that google will delete them, and I can&amp;#39;t work with them directly from there either.&lt;/p&gt;\n\n&lt;p&gt;I want to know if in case some ransomware attack happens on my pc, modifying all my files, is there any practical way to revert them to a state before the ransomware, without having to change the version of each file one by one? Or is there another cloud backup/storage service that better meets my needs?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125ujq1", "is_robot_indexable": true, "report_reasons": null, "author": "Melodic-Ad9865", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125ujq1/can_google_drive_protect_my_files_in_case_of_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125ujq1/can_google_drive_protect_my_files_in_case_of_a/", "subreddit_subscribers": 675980, "created_utc": 1680110795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm pretty new to mass downloading pictures. It looks like XOWA is the closest thing to what I need. I'm mostly interested in collecting CC0/PD images, and more specifically the work of https://commons.wikimedia.org/wiki/User:Evan-Amos \n\nI've found this: https://colab.research.google.com/drive/12jGo_tm2bAD7NRiqxvF-XfKfEWgKIx4X#scrollTo=Uss5vkuUv047&amp;uniqifier=1 that downloads by category but I am not a programmer and have no idea how to modify it to suit my needs. (Also it doesn't seem to work after it collects about 50-100 images)\n\nI'd like to be able to download full quality images by User, ideally with metadata to make it searchable for future use.\n\nAny advice or pointers would be greatly appreciated!", "author_fullname": "t2_gezst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dowloading Wikimedia Images by User?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125qt6b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680102101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m pretty new to mass downloading pictures. It looks like XOWA is the closest thing to what I need. I&amp;#39;m mostly interested in collecting CC0/PD images, and more specifically the work of &lt;a href=\"https://commons.wikimedia.org/wiki/User:Evan-Amos\"&gt;https://commons.wikimedia.org/wiki/User:Evan-Amos&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found this: &lt;a href=\"https://colab.research.google.com/drive/12jGo_tm2bAD7NRiqxvF-XfKfEWgKIx4X#scrollTo=Uss5vkuUv047&amp;amp;uniqifier=1\"&gt;https://colab.research.google.com/drive/12jGo_tm2bAD7NRiqxvF-XfKfEWgKIx4X#scrollTo=Uss5vkuUv047&amp;amp;uniqifier=1&lt;/a&gt; that downloads by category but I am not a programmer and have no idea how to modify it to suit my needs. (Also it doesn&amp;#39;t seem to work after it collects about 50-100 images)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to be able to download full quality images by User, ideally with metadata to make it searchable for future use.&lt;/p&gt;\n\n&lt;p&gt;Any advice or pointers would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "125qt6b", "is_robot_indexable": true, "report_reasons": null, "author": "Pengu_333", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/125qt6b/dowloading_wikimedia_images_by_user/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/125qt6b/dowloading_wikimedia_images_by_user/", "subreddit_subscribers": 675980, "created_utc": 1680102101.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}