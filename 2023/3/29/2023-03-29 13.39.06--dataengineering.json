{"kind": "Listing", "data": {"after": "t3_125ezxs", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Join us in the Rockies for an unforgettable conference featuring thought-provoking talks, workshops, craft beer tasting, hiking, and immersive experiences that will change the way you think about software development. Grab your Early Bird Ticket: [https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687](https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687)", "author_fullname": "t2_qhroetn8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big news! LambdaConf returns Sept 16-19th and is better than ever! \ud83d\udd25", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124ozln", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 112, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 112, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680010541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Join us in the Rockies for an unforgettable conference featuring thought-provoking talks, workshops, craft beer tasting, hiking, and immersive experiences that will change the way you think about software development. Grab your Early Bird Ticket: &lt;a href=\"https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687\"&gt;https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?auto=webp&amp;v=enabled&amp;s=6813a1ea4b5d401e335caa02583865546ee29898", "width": 1000, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b30c45d7350f71944e7ed6d27d03555b62a91684", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0379a777f803e71a1b906de4a5bcb32220be1005", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa1fdba3bf291c3b14470d653f8ebd575fd72c25", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4436df3f9c64439cf26ced83657393f9c08df83", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/hxaI5-DAbMoirpEPh7pmnmb5D3Nu_ItAo1OKuGSxoos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f84ac18aab2e94b4f7e4951bca30d20fc0fb70c8", "width": 960, "height": 480}], "variants": {}, "id": "DaWPZMpGJTzhhl3d5dsciFzXnZ4QqNEDthE7iz5CkU0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "124ozln", "is_robot_indexable": true, "report_reasons": null, "author": "Agataziverge", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124ozln/big_news_lambdaconf_returns_sept_1619th_and_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124ozln/big_news_lambdaconf_returns_sept_1619th_and_is/", "subreddit_subscribers": 94826, "created_utc": 1680010541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey /r/dataengineering! \n\nI\u2019m Toby and over the last few months, I\u2019ve been working with a team of engineers from Airbnb, Apple, Google, and Netflix, to simplify developing data pipelines with [SQLMesh](https://github.com/TobikoData/sqlmesh). \n\nWe\u2019re tired of fragile pipelines, untested SQL queries, and expensive staging environments for data. Software engineers have reaped the benefits of DevOps through unit tests, continuous integration, and continuous deployment for years. We felt like it was time for data teams to have the same confidence and efficiency in development as their peers. It\u2019s time for DataOps!\n\nSQLMesh can be used through a CLI/notebook or in our open source web based IDE (in preview). SQLMesh builds efficient dev / staging environments through \u201cVirtual Data Marts\u201d using views, which allows you to seamlessly rollback or roll forward your changes! With a simple pointer swap you can promote your \u201cstaging\u201d data into production. This means you get unlimited copy-on-write environments that make data exploration and preview of changes cheap, easy, safe. Some other key features are:\n\n* Automatic DAG generation by semantically parsing and understanding SQL or Python scripts\n* CI-Runnable Unit and Integration tests with optional conversion to DuckDB\n* Change detection and reconciliation through column level lineage \n* Native Airflow Integration\n* Import an existing DBT project and run it on SQLMesh\u2019s runtime (in preview)\n\nWe\u2019re just getting started on our journey to change the way data pipelines are built and deployed. We\u2019re huge proponents of open source and hope that we can grow together with your feedback and contributions. Try out SQLMesh by following the [quick start guide](https://sqlmesh.readthedocs.io/en/stable/quick_start/). We\u2019d love to chat and hear about your experiences and ideas in our [Slack community](https://join.slack.com/t/tobiko-data/shared_invite/zt-1ma66d79v-a4dbf4DUpLAQJ8ptQrJygg).", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQLMesh: The future of DataOps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124tspm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680019793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m Toby and over the last few months, I\u2019ve been working with a team of engineers from Airbnb, Apple, Google, and Netflix, to simplify developing data pipelines with &lt;a href=\"https://github.com/TobikoData/sqlmesh\"&gt;SQLMesh&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;We\u2019re tired of fragile pipelines, untested SQL queries, and expensive staging environments for data. Software engineers have reaped the benefits of DevOps through unit tests, continuous integration, and continuous deployment for years. We felt like it was time for data teams to have the same confidence and efficiency in development as their peers. It\u2019s time for DataOps!&lt;/p&gt;\n\n&lt;p&gt;SQLMesh can be used through a CLI/notebook or in our open source web based IDE (in preview). SQLMesh builds efficient dev / staging environments through \u201cVirtual Data Marts\u201d using views, which allows you to seamlessly rollback or roll forward your changes! With a simple pointer swap you can promote your \u201cstaging\u201d data into production. This means you get unlimited copy-on-write environments that make data exploration and preview of changes cheap, easy, safe. Some other key features are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Automatic DAG generation by semantically parsing and understanding SQL or Python scripts&lt;/li&gt;\n&lt;li&gt;CI-Runnable Unit and Integration tests with optional conversion to DuckDB&lt;/li&gt;\n&lt;li&gt;Change detection and reconciliation through column level lineage &lt;/li&gt;\n&lt;li&gt;Native Airflow Integration&lt;/li&gt;\n&lt;li&gt;Import an existing DBT project and run it on SQLMesh\u2019s runtime (in preview)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We\u2019re just getting started on our journey to change the way data pipelines are built and deployed. We\u2019re huge proponents of open source and hope that we can grow together with your feedback and contributions. Try out SQLMesh by following the &lt;a href=\"https://sqlmesh.readthedocs.io/en/stable/quick_start/\"&gt;quick start guide&lt;/a&gt;. We\u2019d love to chat and hear about your experiences and ideas in our &lt;a href=\"https://join.slack.com/t/tobiko-data/shared_invite/zt-1ma66d79v-a4dbf4DUpLAQJ8ptQrJygg\"&gt;Slack community&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "124tspm", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124tspm/sqlmesh_the_future_of_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124tspm/sqlmesh_the_future_of_dataops/", "subreddit_subscribers": 94826, "created_utc": 1680019793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've just completed my 3rd data project to help me understand how to work with Airflow and running services in Docker.\n\n# Links\n\n* [GitHub Repository](https://github.com/digitalghost-dev/global-data-pipeline)\n* [Looker Studio Visualization](https://lookerstudio.google.com/reporting/3710d6bb-25b2-4d64-b6e8-2889bc57c74b) \\- not a great experience on mobile, Air Quality page doesn't seem to load.\n* [Documentation](https://github.com/digitalghost-dev/global-data-pipeline/wiki/Global-Data-Pipeline-Documentation) \\- tried my best with this, will need to run through it again and proof read.\n* [Discord Server Invite](https://discord.gg/j2HEfpebuH) \\- feel free to join to see the bot in action. There is only one channel and it's locked down so not much do in here but thought I would add it in case someone was curious. The bot will query the database and look for the highest current\\_temp and will send a message with the city name and the temperature in celsius.\n\n# Overview\n\n* A `docker-compose.yml` file runs Airflow, Postgres, and Redis in Docker containers.\n* Python scripts reach out to different data sources to extract, transform and load the data into a Postgres database, orchestrated through Airflow on various schedules.\n* Using Airflow operators, data is moved from Postgres to Google Cloud Storage then to BigQuery where the data is visualized with Looker Studio.\n* A Discord Airflow operator is used to send a daily message to a server with current weather stats.\n\n# Data Sources\n\nThis project uses two APIs and web scrapes some tables from Wikipedia. All the city data derives from choosing the 50 most populated cities in the world according to [MacroTrends](https://www.macrotrends.net/cities/largest-cities-by-population).\n\n* City Weather - (updated hourly) with [Weatherstack](https://weatherstack.com) API - costs $10 a month for 50,000 calls.\n   * Current temperature, humidity, precipitation, wind speed\n* City Air Quality - (updated hourly) with [OpenWeatherMap](https://openweathermap.org) API\n   * CO, NO2, O2, SO2, PM2.5, PM10\n* City population\n* Country statistics\n   * Fertility rates, homicide rates, Human Development Index, unemployments rates\n\n[Flowchart](https://preview.redd.it/zz181kpt6iqa1.png?width=2112&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b1485fbb458838ec703bbaeceea381cd635d2db1)\n\n# Notes\n\nSetting up Airflow was pretty painless with the predefined `docker-compose.yml` file found [here](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html). I did have to modify the original file a bit to allow containers to talk to each other on my host machine.\n\nSpeaking of host machines, all of this is running on my desktop.\n\nLooker Studio is okay... it's free so I guess I can't complain too much but the experience for viewers on mobile is pretty bad.\n\nThe visualizations I made in Looker Studio are elementary at best but my goal wasn't to build the prettiest dashboard. I will continue to update it though in the future.", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My 3rd data project, with Airflow, Docker, Postgres, and Looker Studio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zz181kpt6iqa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 65, "x": 108, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cecb6cf097ae6c7442827e55a85ef64d18ca18e"}, {"y": 131, "x": 216, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=edb440a66cf60727d5afd2ed6492dfb5a622d3f2"}, {"y": 195, "x": 320, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f297b107b70090c6504a542f6185a35a12dd231"}, {"y": 390, "x": 640, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d551441f10fa5dc8df7c67ed23fcf42348b6058a"}, {"y": 585, "x": 960, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cdf0fa788b94f59330fcbf6e3633ed26e63095f"}, {"y": 658, "x": 1080, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f60f656bbef58516246acbe6296c717f4b87d6fa"}], "s": {"y": 1288, "x": 2112, "u": "https://preview.redd.it/zz181kpt6iqa1.png?width=2112&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b1485fbb458838ec703bbaeceea381cd635d2db1"}, "id": "zz181kpt6iqa1"}}, "name": "t3_124wcjb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EWJ1r_6hf0lhfYglqOaweEIFjMkf0VQ2GvKa6YqbKS0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1680025179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just completed my 3rd data project to help me understand how to work with Airflow and running services in Docker.&lt;/p&gt;\n\n&lt;h1&gt;Links&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/digitalghost-dev/global-data-pipeline\"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://lookerstudio.google.com/reporting/3710d6bb-25b2-4d64-b6e8-2889bc57c74b\"&gt;Looker Studio Visualization&lt;/a&gt; - not a great experience on mobile, Air Quality page doesn&amp;#39;t seem to load.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/digitalghost-dev/global-data-pipeline/wiki/Global-Data-Pipeline-Documentation\"&gt;Documentation&lt;/a&gt; - tried my best with this, will need to run through it again and proof read.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://discord.gg/j2HEfpebuH\"&gt;Discord Server Invite&lt;/a&gt; - feel free to join to see the bot in action. There is only one channel and it&amp;#39;s locked down so not much do in here but thought I would add it in case someone was curious. The bot will query the database and look for the highest current_temp and will send a message with the city name and the temperature in celsius.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Overview&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A &lt;code&gt;docker-compose.yml&lt;/code&gt; file runs Airflow, Postgres, and Redis in Docker containers.&lt;/li&gt;\n&lt;li&gt;Python scripts reach out to different data sources to extract, transform and load the data into a Postgres database, orchestrated through Airflow on various schedules.&lt;/li&gt;\n&lt;li&gt;Using Airflow operators, data is moved from Postgres to Google Cloud Storage then to BigQuery where the data is visualized with Looker Studio.&lt;/li&gt;\n&lt;li&gt;A Discord Airflow operator is used to send a daily message to a server with current weather stats.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Data Sources&lt;/h1&gt;\n\n&lt;p&gt;This project uses two APIs and web scrapes some tables from Wikipedia. All the city data derives from choosing the 50 most populated cities in the world according to &lt;a href=\"https://www.macrotrends.net/cities/largest-cities-by-population\"&gt;MacroTrends&lt;/a&gt;.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;City Weather - (updated hourly) with &lt;a href=\"https://weatherstack.com\"&gt;Weatherstack&lt;/a&gt; API - costs $10 a month for 50,000 calls.\n\n&lt;ul&gt;\n&lt;li&gt;Current temperature, humidity, precipitation, wind speed&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;City Air Quality - (updated hourly) with &lt;a href=\"https://openweathermap.org\"&gt;OpenWeatherMap&lt;/a&gt; API\n\n&lt;ul&gt;\n&lt;li&gt;CO, NO2, O2, SO2, PM2.5, PM10&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;City population&lt;/li&gt;\n&lt;li&gt;Country statistics\n\n&lt;ul&gt;\n&lt;li&gt;Fertility rates, homicide rates, Human Development Index, unemployments rates&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zz181kpt6iqa1.png?width=2112&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b1485fbb458838ec703bbaeceea381cd635d2db1\"&gt;Flowchart&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Notes&lt;/h1&gt;\n\n&lt;p&gt;Setting up Airflow was pretty painless with the predefined &lt;code&gt;docker-compose.yml&lt;/code&gt; file found &lt;a href=\"https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html\"&gt;here&lt;/a&gt;. I did have to modify the original file a bit to allow containers to talk to each other on my host machine.&lt;/p&gt;\n\n&lt;p&gt;Speaking of host machines, all of this is running on my desktop.&lt;/p&gt;\n\n&lt;p&gt;Looker Studio is okay... it&amp;#39;s free so I guess I can&amp;#39;t complain too much but the experience for viewers on mobile is pretty bad.&lt;/p&gt;\n\n&lt;p&gt;The visualizations I made in Looker Studio are elementary at best but my goal wasn&amp;#39;t to build the prettiest dashboard. I will continue to update it though in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?auto=webp&amp;v=enabled&amp;s=dc7b06ba82e2a238e6a1e9732af3c38e0694e6b0", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bcb73cfd09ed2e135d54bf6ee2c9ce08e61fea7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f22c542e5e61014373bc88e9fb88fbf5f092391", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0bea044b442d5c5cfc86170d6673d169d1d8f14", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68a01789cc3524d21df30163274621bf690ed230", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=995876c9cdc7ea26d77e80bede7d8c18bdebb1db", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/OY3Ko27YXRMrmtg6ra2zTCt3gzZXBS9ijnsbDea84L8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60a5df26e9b64687f8eb5b08795cc711fd35424b", "width": 1080, "height": 540}], "variants": {}, "id": "65nZrXRToPy1C10OOcyNpu-vcLkcCd-DzvBSwTc_SbY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "124wcjb", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124wcjb/my_3rd_data_project_with_airflow_docker_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124wcjb/my_3rd_data_project_with_airflow_docker_postgres/", "subreddit_subscribers": 94826, "created_utc": 1680025179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://github.com/awslabs/mountpoint-s3", "author_fullname": "t2_anaphz9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mountpoint for S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_125cd6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BzCeCYVYoRezIcRAh51k_PJz49QWs8fkDVSTdaNsu2c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680062177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/awslabs/mountpoint-s3\"&gt;https://github.com/awslabs/mountpoint-s3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/tjtdex6f7nqa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?auto=webp&amp;v=enabled&amp;s=53e157290d01953c562e2e9aff21f421200970ef", "width": 1080, "height": 636}, "resolutions": [{"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6cc52fcf4e0b3224b4dc0e4e68050d570a3c76d", "width": 108, "height": 63}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8449b091dc519f5194a98d2709fed9e71baeb4a0", "width": 216, "height": 127}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ed365562b9aad4866830e6371ee41f3b143c309", "width": 320, "height": 188}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ff92623702f37a5300c92e08b75a599c156ecfb", "width": 640, "height": 376}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9736621428871edb6d85add8b0a58a09bdbc4d26", "width": 960, "height": 565}, {"url": "https://preview.redd.it/tjtdex6f7nqa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b56b22a5177542b6f31407d868bf7e038d1541de", "width": 1080, "height": 636}], "variants": {}, "id": "lKbMI6JMXWTO1kGTw436EAb1GUZ0XysjvnbN03C2Yf8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125cd6n", "is_robot_indexable": true, "report_reasons": null, "author": "Sweet-Butterscotch11", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125cd6n/mountpoint_for_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/tjtdex6f7nqa1.png", "subreddit_subscribers": 94826, "created_utc": 1680062177.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you guys charge for independent contracting rates? I am an experienced Senior Data Engineer. Will be leading key projects for the company.\n\nRight now I am thinking $125/hr or $1000/day given a multi-year engagement. Client has somewhat \"limited\" resources, but a field I am very interested in learning more about. I don't want to hear \"Oh yeah, you could easily get $500/hr!!!\" that's not realistic. Give it to me straight. Am I lowballing myself here?", "author_fullname": "t2_qleqo7a5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consulting Rates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1251kbg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680036019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you guys charge for independent contracting rates? I am an experienced Senior Data Engineer. Will be leading key projects for the company.&lt;/p&gt;\n\n&lt;p&gt;Right now I am thinking $125/hr or $1000/day given a multi-year engagement. Client has somewhat &amp;quot;limited&amp;quot; resources, but a field I am very interested in learning more about. I don&amp;#39;t want to hear &amp;quot;Oh yeah, you could easily get $500/hr!!!&amp;quot; that&amp;#39;s not realistic. Give it to me straight. Am I lowballing myself here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1251kbg", "is_robot_indexable": true, "report_reasons": null, "author": "IndependentElk3997", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1251kbg/consulting_rates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1251kbg/consulting_rates/", "subreddit_subscribers": 94826, "created_utc": 1680036019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently passed my DP-203 certification exam with the lowest score possible. I was a little sad bc I put in about 3 weeks of studying for it but at the end of the day... a pass is a pass.\n\n**Background:** I recently moved into a Cloud Data Eng role within my company a month ago. I had little to no experience with data eng. All I had was just course on the concept of ETL Data Warehousing. Previous experience role: Developer Analyst in my companies logistics domain working with our Global WMS solution.\n\n**Conclusion:** Test was a lot harder than expected. The exam consisted of curveballs and trick questions which made consistently second guess myself.  I read a couple of other ppls post on this sub that passed the exam and they made it sound like the exam was piece of cake. I think I may have struggled more because I had little to no real life hands on experience in regards to Data Eng. Also, exam went into more detail in regards to syntax as well as security and access. \n\n**Prep:** I prepared for about a month. Below is what I did\n\n1)My company payed for me to take the instructor led course offered on the Microsoft website and it was a complete nightmare/waste of time. The course was outsourced to a company named Koenig and they were struggling with the fact that microsoft had updated course content earlier this year in February. The instructor felt scripted and the labs did not match the course content. The instructor also had no real life experience and struggled with scenario based questions. For example, \"in which case would this distribution be best'?'... The only good thing about the course is that they later gave me a workaround lab environment with the updated labs which provided me with some simple generic hands on experience. I think besides the lab environments, I would suggest to take the self paced version and go through it in a manner where you understand the content with detail. The level of detail that they covered did not suffice the level of detail that exam questions covered.\n\n2)Practice test offered on Microsoft test: this were good but again did not match the level of detail that the questions of the exams consisted off. I would \\*\\*suggest\\*\\* getting the questions wrong and studying the corresponding links of the knowledge articles that they provide.\n\n3)Knowledge check questions on the course. Similar to the practice test\n\n4)Labs on the course . \\*\\*Recommend\\*\\*: reading knowledge articles corresponding to each lab\n\n5)[www.examtopics.com/exams/microsoft/dp-203](https://www.examtopics.com/exams/microsoft/dp-203) : I didn't hit this that hard but I would recommend doing so. These are the type of questions that are on the exams.\n\nPlease feel free to reach out if you guys have any questions.", "author_fullname": "t2_hi3uk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recently passed DP-203 Certification Exam", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124t0qi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680018157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently passed my DP-203 certification exam with the lowest score possible. I was a little sad bc I put in about 3 weeks of studying for it but at the end of the day... a pass is a pass.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; I recently moved into a Cloud Data Eng role within my company a month ago. I had little to no experience with data eng. All I had was just course on the concept of ETL Data Warehousing. Previous experience role: Developer Analyst in my companies logistics domain working with our Global WMS solution.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Test was a lot harder than expected. The exam consisted of curveballs and trick questions which made consistently second guess myself.  I read a couple of other ppls post on this sub that passed the exam and they made it sound like the exam was piece of cake. I think I may have struggled more because I had little to no real life hands on experience in regards to Data Eng. Also, exam went into more detail in regards to syntax as well as security and access. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Prep:&lt;/strong&gt; I prepared for about a month. Below is what I did&lt;/p&gt;\n\n&lt;p&gt;1)My company payed for me to take the instructor led course offered on the Microsoft website and it was a complete nightmare/waste of time. The course was outsourced to a company named Koenig and they were struggling with the fact that microsoft had updated course content earlier this year in February. The instructor felt scripted and the labs did not match the course content. The instructor also had no real life experience and struggled with scenario based questions. For example, &amp;quot;in which case would this distribution be best&amp;#39;?&amp;#39;... The only good thing about the course is that they later gave me a workaround lab environment with the updated labs which provided me with some simple generic hands on experience. I think besides the lab environments, I would suggest to take the self paced version and go through it in a manner where you understand the content with detail. The level of detail that they covered did not suffice the level of detail that exam questions covered.&lt;/p&gt;\n\n&lt;p&gt;2)Practice test offered on Microsoft test: this were good but again did not match the level of detail that the questions of the exams consisted off. I would **suggest** getting the questions wrong and studying the corresponding links of the knowledge articles that they provide.&lt;/p&gt;\n\n&lt;p&gt;3)Knowledge check questions on the course. Similar to the practice test&lt;/p&gt;\n\n&lt;p&gt;4)Labs on the course . **Recommend**: reading knowledge articles corresponding to each lab&lt;/p&gt;\n\n&lt;p&gt;5)&lt;a href=\"https://www.examtopics.com/exams/microsoft/dp-203\"&gt;www.examtopics.com/exams/microsoft/dp-203&lt;/a&gt; : I didn&amp;#39;t hit this that hard but I would recommend doing so. These are the type of questions that are on the exams.&lt;/p&gt;\n\n&lt;p&gt;Please feel free to reach out if you guys have any questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "124t0qi", "is_robot_indexable": true, "report_reasons": null, "author": "Jpvilla5454", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124t0qi/recently_passed_dp203_certification_exam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124t0qi/recently_passed_dp203_certification_exam/", "subreddit_subscribers": 94826, "created_utc": 1680018157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS or azure doesn't matter.", "author_fullname": "t2_vtx6qjs2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you be using for one time ingestion of large volume of data today, say 100tb.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1251wbd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680036691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS or azure doesn&amp;#39;t matter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1251wbd", "is_robot_indexable": true, "report_reasons": null, "author": "Budget_Assignment457", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1251wbd/what_would_you_be_using_for_one_time_ingestion_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1251wbd/what_would_you_be_using_for_one_time_ingestion_of/", "subreddit_subscribers": 94826, "created_utc": 1680036691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "By Bill Inmon, Any one read it, its DB sponsored (lakehouse tm). Worthy read? I assuming very specific to DB workflows?", "author_fullname": "t2_6lfdg0it", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building the Data Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125dxf8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680066916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;By Bill Inmon, Any one read it, its DB sponsored (lakehouse tm). Worthy read? I assuming very specific to DB workflows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125dxf8", "is_robot_indexable": true, "report_reasons": null, "author": "soundboyselecta", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125dxf8/building_the_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125dxf8/building_the_data_lakehouse/", "subreddit_subscribers": 94826, "created_utc": 1680066916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen this done so many ways at different companies that it confuses me that it's not more standardized. Let's say you have a bunch of data, do a bunch of transformation to it (say, up through dimensional models in your DWH) and it is the trusted location for that data. The company may then want to have data like that available to customers via an API. I'm guessing most companies don't have an external API sitting on top of snowflake or similar due to compute costs of people querying it often. \n\n&amp;#x200B;\n\nDoes your company expose transformed DWH data out through an external API? And if so, what's the pattern of data flow &amp; platforms involved?", "author_fullname": "t2_c6w52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does your company expose data from your DWH out to external customers with an API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125dnju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680066048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen this done so many ways at different companies that it confuses me that it&amp;#39;s not more standardized. Let&amp;#39;s say you have a bunch of data, do a bunch of transformation to it (say, up through dimensional models in your DWH) and it is the trusted location for that data. The company may then want to have data like that available to customers via an API. I&amp;#39;m guessing most companies don&amp;#39;t have an external API sitting on top of snowflake or similar due to compute costs of people querying it often. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does your company expose transformed DWH data out through an external API? And if so, what&amp;#39;s the pattern of data flow &amp;amp; platforms involved?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125dnju", "is_robot_indexable": true, "report_reasons": null, "author": "EmergenL", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125dnju/does_your_company_expose_data_from_your_dwh_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125dnju/does_your_company_expose_data_from_your_dwh_out/", "subreddit_subscribers": 94826, "created_utc": 1680066048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ui4m14ke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to transition to a Data Architect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1257sut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/J3Yh7qA10YYJTrTkuGOZ9Yw2_LQx4Ss8f6scA_auTXE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680050106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@anupmoncy/how-to-transition-to-a-data-architect-83aa835ea53c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?auto=webp&amp;v=enabled&amp;s=7dfd4ccc789b8180380a2c52c7643250c8cb0bda", "width": 1200, "height": 799}, "resolutions": [{"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=104214fa8c81fdd0d7d07e7012cba83b15add55d", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2928ce334f49dfdb0c12dbde7585fa633b1838d", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ed546a816ab21e6c5e63529f7f43b0bd33dbc82", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db94f0be8640fbebe58040d5ed5f0698ca085a1d", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39868eb7151bad0588663013f17598991947f4c0", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/stsS0uwhPR_-4fazlZ-V2UFcenwVTn-vJdItvdpd7jg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ee531ccf658f0c5aa84e9bfee7034c06d0311f7", "width": 1080, "height": 719}], "variants": {}, "id": "vwHNrRaZgH1udSmqaM1jQ6og5WHaxlO6Yq77OeRN7io"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1257sut", "is_robot_indexable": true, "report_reasons": null, "author": "Alarmed-Sock4915", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1257sut/how_to_transition_to_a_data_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@anupmoncy/how-to-transition-to-a-data-architect-83aa835ea53c", "subreddit_subscribers": 94826, "created_utc": 1680050106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we are using prefect and generally I'm quite happy so far but I still did not figure out how to do proper GitOps with it (eg Kubernetes). Main issue is mostly the deployments. The actual workflows are already dockerimages. Currently we have deployment code or CLI calls checked into our deployment repository and they will be automatically executed in a pod within the cluster in case of changes. Thats not real gitops though as we never really mirror the state of the server to the state of the repo and server devistions are not fixed automatically as well.\n\nWhats your experience? Are there better solutions with other frameworks? Ideal would be probably a kuberetes operator and deployments via CRD", "author_fullname": "t2_24837qxb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitOps with prefect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125h3nq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680077458.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we are using prefect and generally I&amp;#39;m quite happy so far but I still did not figure out how to do proper GitOps with it (eg Kubernetes). Main issue is mostly the deployments. The actual workflows are already dockerimages. Currently we have deployment code or CLI calls checked into our deployment repository and they will be automatically executed in a pod within the cluster in case of changes. Thats not real gitops though as we never really mirror the state of the server to the state of the repo and server devistions are not fixed automatically as well.&lt;/p&gt;\n\n&lt;p&gt;Whats your experience? Are there better solutions with other frameworks? Ideal would be probably a kuberetes operator and deployments via CRD&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125h3nq", "is_robot_indexable": true, "report_reasons": null, "author": "jeremyZen2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125h3nq/gitops_with_prefect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125h3nq/gitops_with_prefect/", "subreddit_subscribers": 94826, "created_utc": 1680077458.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Issue: The Airbyte MySQL connector does not parallelize read workload\n\nResult: Syncs take forever to finish\n\nSolution: Update Airbyte MySQL connector to use parallel connections for reading with JDBC\nOne can use subconnections to read data in parallel from server. This will increase performance considerably.\n\nI was wondering if anyone has done anything like this?  If not, anyone interested in working with me (I have used JAVA in like forever) to give back to the open source community and update the Airbyte connector to preform parallel reads", "author_fullname": "t2_9o0tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte MySQL connector does not parallelize read", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1253fa1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680039846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Issue: The Airbyte MySQL connector does not parallelize read workload&lt;/p&gt;\n\n&lt;p&gt;Result: Syncs take forever to finish&lt;/p&gt;\n\n&lt;p&gt;Solution: Update Airbyte MySQL connector to use parallel connections for reading with JDBC\nOne can use subconnections to read data in parallel from server. This will increase performance considerably.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone has done anything like this?  If not, anyone interested in working with me (I have used JAVA in like forever) to give back to the open source community and update the Airbyte connector to preform parallel reads&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1253fa1", "is_robot_indexable": true, "report_reasons": null, "author": "Amphagory", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1253fa1/airbyte_mysql_connector_does_not_parallelize_read/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1253fa1/airbyte_mysql_connector_does_not_parallelize_read/", "subreddit_subscribers": 94826, "created_utc": 1680039846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had planned to put together some beginners training material to cover the gambit from data collection and monitoring to analytics and reporting. Having recently joined this community, I have noticed some themes that made me want to ask: what would you like if you were new to data engineering? \n\nAdditionally, what sort of content? How detailed and technical? Does the tech stack matter? Do you prefer apps and code or Jupyter Notebooks? Let me know your thoughts! \n\nThanks and cheers all \ud83d\udcaa\ud83c\udffb", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What makes for good DE training material?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124zowl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680032238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had planned to put together some beginners training material to cover the gambit from data collection and monitoring to analytics and reporting. Having recently joined this community, I have noticed some themes that made me want to ask: what would you like if you were new to data engineering? &lt;/p&gt;\n\n&lt;p&gt;Additionally, what sort of content? How detailed and technical? Does the tech stack matter? Do you prefer apps and code or Jupyter Notebooks? Let me know your thoughts! &lt;/p&gt;\n\n&lt;p&gt;Thanks and cheers all \ud83d\udcaa\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "124zowl", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124zowl/what_makes_for_good_de_training_material/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124zowl/what_makes_for_good_de_training_material/", "subreddit_subscribers": 94826, "created_utc": 1680032238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks!\n\nNew to Azure. I'm wondering what's the preferred method for sending email error alerts in ADF. Is it using: \n1. ADF's metric and alerts ; or\n2. Using a LogicApp\n\nThanks!", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the preferred method for email notification in Azure Data Factory?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_125lueq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680090752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks!&lt;/p&gt;\n\n&lt;p&gt;New to Azure. I&amp;#39;m wondering what&amp;#39;s the preferred method for sending email error alerts in ADF. Is it using: \n1. ADF&amp;#39;s metric and alerts ; or\n2. Using a LogicApp&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125lueq", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125lueq/whats_the_preferred_method_for_email_notification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125lueq/whats_the_preferred_method_for_email_notification/", "subreddit_subscribers": 94826, "created_utc": 1680090752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a team lead in Utah. I've been thinking of looking around for something new. My company has made some drastic changes in the last few months. Going in a direction I don't necessarily agree with. With the job market the way it is, should I look or wait until things are in a better spot? \nPros: \nLove my team, great coworkers and direct leadership\nI feel I make a impact\nPotentially great equity when go public\n\nCons:\nBack in office full time (loved hybrid it was perfect for me)\nTook away some paid holidays\nTook away other perks ie conferences\nNo merit raises for most, performance review didn't matter\n\nI understand we are in weird times, just wondering if it's worth taking the effort to job search right now. \n\nThanks for all your input.", "author_fullname": "t2_4y6zmea2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Worth Looking Around?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125f07x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680070369.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a team lead in Utah. I&amp;#39;ve been thinking of looking around for something new. My company has made some drastic changes in the last few months. Going in a direction I don&amp;#39;t necessarily agree with. With the job market the way it is, should I look or wait until things are in a better spot? \nPros: \nLove my team, great coworkers and direct leadership\nI feel I make a impact\nPotentially great equity when go public&lt;/p&gt;\n\n&lt;p&gt;Cons:\nBack in office full time (loved hybrid it was perfect for me)\nTook away some paid holidays\nTook away other perks ie conferences\nNo merit raises for most, performance review didn&amp;#39;t matter&lt;/p&gt;\n\n&lt;p&gt;I understand we are in weird times, just wondering if it&amp;#39;s worth taking the effort to job search right now. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all your input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "125f07x", "is_robot_indexable": true, "report_reasons": null, "author": "angleofthedangle90", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125f07x/worth_looking_around/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125f07x/worth_looking_around/", "subreddit_subscribers": 94826, "created_utc": 1680070369.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey DE reddit,\n\nJust looking for any guidance or suggestions on how to tackle a problem we have at my workplace.\n\nWe have an old data warehouse where 15 years of old SSIS jobs populate it with a lot of technical debt and knowledge of how they work no longer with the company. \n\nWe have a new data warehouse spun up for an ERP migration project and some of the data that gets populated with SSIS is required on the new data warehouse as a short term stop gap for business reporting continuity. \n\nThis stop gap will be cleaned up post go live of the ERP project but for now we need a quick and efficient way to replicate the SSIS populated table data from old data warehouse to new data warehouse.\n\nThe old data warehouse is MSSQL 2014 Standard and does not have CDC as a result...\n\nI've created python code that replicates the data using pandas and it works reasonably well other than the length it takes to replicate and the amount of RAM the pandas dataframe consumes. So i can only run this once a day meaning some data will be a bit stale in the new data warehouse till it runs again the next day.\n\nI'm trying to come up with a way to be able to run this replication more often given time and ram limitations. \n\nOne idea I had was to hash the table contents from the last run and save it, then check the hash to see if any of the table changed before using valuable replication time/ram resources. Has anyone ever done anything like this ? Any libraries that can help me ?\n\nOr any other completely different ways to handle this without CDC or large amounts of hardware spend ? \n\nAny guidance/help is much appreciated!", "author_fullname": "t2_9sf3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replication of data without CDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1254m9v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680042547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey DE reddit,&lt;/p&gt;\n\n&lt;p&gt;Just looking for any guidance or suggestions on how to tackle a problem we have at my workplace.&lt;/p&gt;\n\n&lt;p&gt;We have an old data warehouse where 15 years of old SSIS jobs populate it with a lot of technical debt and knowledge of how they work no longer with the company. &lt;/p&gt;\n\n&lt;p&gt;We have a new data warehouse spun up for an ERP migration project and some of the data that gets populated with SSIS is required on the new data warehouse as a short term stop gap for business reporting continuity. &lt;/p&gt;\n\n&lt;p&gt;This stop gap will be cleaned up post go live of the ERP project but for now we need a quick and efficient way to replicate the SSIS populated table data from old data warehouse to new data warehouse.&lt;/p&gt;\n\n&lt;p&gt;The old data warehouse is MSSQL 2014 Standard and does not have CDC as a result...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve created python code that replicates the data using pandas and it works reasonably well other than the length it takes to replicate and the amount of RAM the pandas dataframe consumes. So i can only run this once a day meaning some data will be a bit stale in the new data warehouse till it runs again the next day.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to come up with a way to be able to run this replication more often given time and ram limitations. &lt;/p&gt;\n\n&lt;p&gt;One idea I had was to hash the table contents from the last run and save it, then check the hash to see if any of the table changed before using valuable replication time/ram resources. Has anyone ever done anything like this ? Any libraries that can help me ?&lt;/p&gt;\n\n&lt;p&gt;Or any other completely different ways to handle this without CDC or large amounts of hardware spend ? &lt;/p&gt;\n\n&lt;p&gt;Any guidance/help is much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1254m9v", "is_robot_indexable": true, "report_reasons": null, "author": "msthree", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1254m9v/replication_of_data_without_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1254m9v/replication_of_data_without_cdc/", "subreddit_subscribers": 94826, "created_utc": 1680042547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am getting a new job in a company that they mainly use Azure + Databricks. I don't have experience with those 2. Where do you suggest I should start to learn them?\n\nIt can be a course on udemy , a course on microsoft for a certification I don't mind.", "author_fullname": "t2_154xx5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure + Databricks course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125knww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680088188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am getting a new job in a company that they mainly use Azure + Databricks. I don&amp;#39;t have experience with those 2. Where do you suggest I should start to learn them?&lt;/p&gt;\n\n&lt;p&gt;It can be a course on udemy , a course on microsoft for a certification I don&amp;#39;t mind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "125knww", "is_robot_indexable": true, "report_reasons": null, "author": "darkvoidman", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125knww/azure_databricks_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125knww/azure_databricks_course/", "subreddit_subscribers": 94826, "created_utc": 1680088188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi everyone,\n\nCurrently, I am teaching(online) a large group of students with low technical skills. Well, it wasn't my decision to teach DWH/BI to second-year students. Around 40% of them believe that \"WHERE\" is a mandatory component in every SQL query.\n\nMy biggest challenge now is to encourage them to get involved in building simple data pipelines. However, I anticipate that several student teams may fail to accomplish even the most basic task, such as loading a CSV file to a staging area through Java or Python. As a result, I'm exploring the use of ETL tools as a workaround to enable even the slowest and least competent students to complete this task with a few clicks.\n\nCan anyone recommend an ETL tool that is exceptionally simple to use? I'm looking for a tool that's so user-friendly that even a monkey can drag-and-drop a CSV file into a data mart. I have used Datastage, Powercenter, and SSIS before, but none of them fall into this category.\n\nThank you in advance!", "author_fullname": "t2_3qdnz3g9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on ETL tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125iueu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680083421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Currently, I am teaching(online) a large group of students with low technical skills. Well, it wasn&amp;#39;t my decision to teach DWH/BI to second-year students. Around 40% of them believe that &amp;quot;WHERE&amp;quot; is a mandatory component in every SQL query.&lt;/p&gt;\n\n&lt;p&gt;My biggest challenge now is to encourage them to get involved in building simple data pipelines. However, I anticipate that several student teams may fail to accomplish even the most basic task, such as loading a CSV file to a staging area through Java or Python. As a result, I&amp;#39;m exploring the use of ETL tools as a workaround to enable even the slowest and least competent students to complete this task with a few clicks.&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend an ETL tool that is exceptionally simple to use? I&amp;#39;m looking for a tool that&amp;#39;s so user-friendly that even a monkey can drag-and-drop a CSV file into a data mart. I have used Datastage, Powercenter, and SSIS before, but none of them fall into this category.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "125iueu", "is_robot_indexable": true, "report_reasons": null, "author": "IndependentSpend7434", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125iueu/need_advice_on_etl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125iueu/need_advice_on_etl_tool/", "subreddit_subscribers": 94826, "created_utc": 1680083421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have 10 pipelines that gets data from different places and I use copy data activity to transform the data from json to parquet and rename some columns. Everything is parameterized. But there is also stuff like switching \u201d,\u201d to \u201d.\u201d  that I want to do. And to my understanding, it is not possible to do such transformations using copy activity. Thus I must do this in the last step where I use databricks for scd transformation. But the current notebook is supposed to be generic and the scd method is written in a way so that all pipelines can use it. But now the notebook is also cluttered with all the source specific transformations since I don\u2019t know where else to put them.\nDoes anyone have recommendations on how to design this?\n(I do not want to use data flow since it is very costly and it means that I must start another cluster)\n\nKind regards and thank you in advance!", "author_fullname": "t2_qiw67sj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adf: where to do source specific transformations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125ifv1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680082105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have 10 pipelines that gets data from different places and I use copy data activity to transform the data from json to parquet and rename some columns. Everything is parameterized. But there is also stuff like switching \u201d,\u201d to \u201d.\u201d  that I want to do. And to my understanding, it is not possible to do such transformations using copy activity. Thus I must do this in the last step where I use databricks for scd transformation. But the current notebook is supposed to be generic and the scd method is written in a way so that all pipelines can use it. But now the notebook is also cluttered with all the source specific transformations since I don\u2019t know where else to put them.\nDoes anyone have recommendations on how to design this?\n(I do not want to use data flow since it is very costly and it means that I must start another cluster)&lt;/p&gt;\n\n&lt;p&gt;Kind regards and thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "125ifv1", "is_robot_indexable": true, "report_reasons": null, "author": "aLyapunov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125ifv1/adf_where_to_do_source_specific_transformations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125ifv1/adf_where_to_do_source_specific_transformations/", "subreddit_subscribers": 94826, "created_utc": 1680082105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This config block cannot be parsed by IDEs I use (Datagrip/PyCharm). Why doesn't dbt simply uses a json of same name to do the job? Why invent this format? This really bugs me as I have to write a translation script to generate some dbt model files and json files are obviously a lot easier to dump on disk.", "author_fullname": "t2_ldvtxo0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does DBT model have such a weird format for configs in models?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12523tl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680037120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This config block cannot be parsed by IDEs I use (Datagrip/PyCharm). Why doesn&amp;#39;t dbt simply uses a json of same name to do the job? Why invent this format? This really bugs me as I have to write a translation script to generate some dbt model files and json files are obviously a lot easier to dump on disk.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12523tl", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway20220231", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12523tl/why_does_dbt_model_have_such_a_weird_format_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12523tl/why_does_dbt_model_have_such_a_weird_format_for/", "subreddit_subscribers": 94826, "created_utc": 1680037120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently built a Data Catalog ROI Template to assist data people in evaluating the value and impact of their data catalog investments. Data catalogs are all the rage, but are we really getting the most out of them?  I'd love to get your feedback:\n\n\ud83d\udd17 [**Data Catalog ROI Template**](https://www.castordoc.com/blog/data-catalog-roi-a-primer)\n\nThis template will help you:\n\n1. Gauge your data catalog's current state and spot growth potential\n2. Pinpoint improvement areas to optimize performance\n3. Measure your data catalog's tangible benefits\n\nShare your feedback &amp; ideas to improve it.", "author_fullname": "t2_bu8cw718", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Catalog ROI Template", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124y0d8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680028697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently built a Data Catalog ROI Template to assist data people in evaluating the value and impact of their data catalog investments. Data catalogs are all the rage, but are we really getting the most out of them?  I&amp;#39;d love to get your feedback:&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd17 &lt;a href=\"https://www.castordoc.com/blog/data-catalog-roi-a-primer\"&gt;&lt;strong&gt;Data Catalog ROI Template&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This template will help you:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Gauge your data catalog&amp;#39;s current state and spot growth potential&lt;/li&gt;\n&lt;li&gt;Pinpoint improvement areas to optimize performance&lt;/li&gt;\n&lt;li&gt;Measure your data catalog&amp;#39;s tangible benefits&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Share your feedback &amp;amp; ideas to improve it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?auto=webp&amp;v=enabled&amp;s=2a34d956b4a6d3ac1880cdaeb89cbca64fb60ff2", "width": 1692, "height": 884}, "resolutions": [{"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b856feaa0456c35484f8e54bd714db9492aeece1", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=207bf9a6898283f0d1439dd4f63dbfadec8fa701", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5313d3b5f2d563bcd54420d52ddd2cf1c33e27c8", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2986b7515841c0ce590048052d5af6783d11c36a", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f0a94bb36543c9ce1d5ac6e4ca53bf69eb69fa4", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/222mY5BSOGC-OD3ZSnmJxrRGa-KCal9aubA41u66CMQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11d3ccfaeb8aac987f157348ca18b1cd2f39c525", "width": 1080, "height": 564}], "variants": {}, "id": "zs6zgWh0o00ZqNct8b7a2b5xz2XX77i6_ThrG6WNQY8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "124y0d8", "is_robot_indexable": true, "report_reasons": null, "author": "castor-metadata", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124y0d8/data_catalog_roi_template/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124y0d8/data_catalog_roi_template/", "subreddit_subscribers": 94826, "created_utc": 1680028697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI like to do a personal project and welcome any advice, feedback, recommendations. First, an intro so you know my starting position.\n\n\\- I work in Data Analytics, Management, Governance, including the quality control, process improvement, stakeholder management, and creating internal tools.  \n\\- I've always been interested in the tech/IT side and truly excited and fascinated about all the available stuff these days, which is one of two reasons for this project  \n\\- I did research on 100 data-related manager/lead/director job openings (in my region) and one of the things that was mentioned in 78% of them was \"... knowledge and experiences of data pipelines, how they work, what every part does (on high level), and able to see opportunities to make it more effective, better quality checks, better security, and maximize the value\"; I fully agree with this requirement because even though I might not be the one who will actually do all this stuff, I find it important to understand what the department does, and what my IT colleagues are telling me.\n\n**So what am I thinking off (ballpark)**\n\n\\- not a giant spaghetti that will likely become an untamable beast to complete, but not a super small one just to guarantee success, I like a challenge and it needs to be valuable and relevant to today's business challenges  \n\\- something modular, so I can start with one part and at the end, it works, then move to a second part to expand the pipeline, not necessarily linear, but perhaps a side-chain  \n\\- a bit with automation and a bit with automated SQL transformation  \n\\- a bit with manual input, maybe to validate data which triggers different flows  \n\\- creating custom datasets specifically for this project (but I don't need a million datapoints please)  \n\\- a pipeline that is intentionally not 1000% optimized, I want it to reflect the common situation of companies growing their pipelines along the way to the point that it's just barely/acceptable to manage lol (you know what I mean) \n\n**What do I have available?**\n\n\\- MS 365 + Power Platform  \n\\- PostgreSQL and MySQL in cloud, plus dbeaver  \n\\- Retool\n\n**What Have I been looking at?**\n\n\\- Fivetran for both orchestration and transformation\n\n**What I want to exclude**\n\n\\- anything that requires me to be a programmer, I'm definitely not and will likely drown in the project due to the scripting part\n\n**What I am asking you**\n\nI've been trying to come up with the project content for two months and just drawing blank, coming up with something from scratch has never been my strong suit. I like to ask you for ideas.\n\nYou can throw anything to me: words, stories, bullet-points, even (or especially) sketches of pipelines that you ~~recommend~~ challenge me to do.\n\nThanks for reading and taking the effort to help.\n\nCheers.", "author_fullname": "t2_kh3ubtce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Doing a personal project as a non-DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_124v675", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680022693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I like to do a personal project and welcome any advice, feedback, recommendations. First, an intro so you know my starting position.&lt;/p&gt;\n\n&lt;p&gt;- I work in Data Analytics, Management, Governance, including the quality control, process improvement, stakeholder management, and creating internal tools.&lt;br/&gt;\n- I&amp;#39;ve always been interested in the tech/IT side and truly excited and fascinated about all the available stuff these days, which is one of two reasons for this project&lt;br/&gt;\n- I did research on 100 data-related manager/lead/director job openings (in my region) and one of the things that was mentioned in 78% of them was &amp;quot;... knowledge and experiences of data pipelines, how they work, what every part does (on high level), and able to see opportunities to make it more effective, better quality checks, better security, and maximize the value&amp;quot;; I fully agree with this requirement because even though I might not be the one who will actually do all this stuff, I find it important to understand what the department does, and what my IT colleagues are telling me.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So what am I thinking off (ballpark)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- not a giant spaghetti that will likely become an untamable beast to complete, but not a super small one just to guarantee success, I like a challenge and it needs to be valuable and relevant to today&amp;#39;s business challenges&lt;br/&gt;\n- something modular, so I can start with one part and at the end, it works, then move to a second part to expand the pipeline, not necessarily linear, but perhaps a side-chain&lt;br/&gt;\n- a bit with automation and a bit with automated SQL transformation&lt;br/&gt;\n- a bit with manual input, maybe to validate data which triggers different flows&lt;br/&gt;\n- creating custom datasets specifically for this project (but I don&amp;#39;t need a million datapoints please)&lt;br/&gt;\n- a pipeline that is intentionally not 1000% optimized, I want it to reflect the common situation of companies growing their pipelines along the way to the point that it&amp;#39;s just barely/acceptable to manage lol (you know what I mean) &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What do I have available?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- MS 365 + Power Platform&lt;br/&gt;\n- PostgreSQL and MySQL in cloud, plus dbeaver&lt;br/&gt;\n- Retool&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What Have I been looking at?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Fivetran for both orchestration and transformation&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I want to exclude&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- anything that requires me to be a programmer, I&amp;#39;m definitely not and will likely drown in the project due to the scripting part&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I am asking you&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to come up with the project content for two months and just drawing blank, coming up with something from scratch has never been my strong suit. I like to ask you for ideas.&lt;/p&gt;\n\n&lt;p&gt;You can throw anything to me: words, stories, bullet-points, even (or especially) sketches of pipelines that you &lt;del&gt;recommend&lt;/del&gt; challenge me to do.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading and taking the effort to help.&lt;/p&gt;\n\n&lt;p&gt;Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "124v675", "is_robot_indexable": true, "report_reasons": null, "author": "SquidsAndMartians", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/124v675/doing_a_personal_project_as_a_nonde/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/124v675/doing_a_personal_project_as_a_nonde/", "subreddit_subscribers": 94826, "created_utc": 1680022693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I am currently tasked with creating an in-house IT-database that would allow end users to both submit and manage tickets, and also provide reporting/dashboard functionality. This would eventually expand off to fit other needs if we find success. \n\nWe currently use O365 and Azure AD, so I figured that we would be able to build the data warehouse on SQL for Azure, use our pre-existing Active Directory roles to control the levels of access, and use PowerAutomate/PowerBI for visualization and workflows. \n\nDoes anyone have any experience on this kind of project? Is this a feasible solution, or is there something similar that we could build that may be similar/less expensive? We originally though about using SharePoint lists as our data warehouse, but that would become a headache quickly. Thanks!!", "author_fullname": "t2_913h0mrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for a SQL Server database that connects to a SharePoint UI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_125lzc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680091136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I am currently tasked with creating an in-house IT-database that would allow end users to both submit and manage tickets, and also provide reporting/dashboard functionality. This would eventually expand off to fit other needs if we find success. &lt;/p&gt;\n\n&lt;p&gt;We currently use O365 and Azure AD, so I figured that we would be able to build the data warehouse on SQL for Azure, use our pre-existing Active Directory roles to control the levels of access, and use PowerAutomate/PowerBI for visualization and workflows. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience on this kind of project? Is this a feasible solution, or is there something similar that we could build that may be similar/less expensive? We originally though about using SharePoint lists as our data warehouse, but that would become a headache quickly. Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "125lzc5", "is_robot_indexable": true, "report_reasons": null, "author": "CadenJPov", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125lzc5/advice_for_a_sql_server_database_that_connects_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125lzc5/advice_for_a_sql_server_database_that_connects_to/", "subreddit_subscribers": 94826, "created_utc": 1680091136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nWe are a team of data analyst. We are doing a survey on the problems data scientists face while doing their work. Please comment if you have anything to share from your experience.\n\nIn our personal work, we have found that sharing our work across our team and saving a history of our work progress is challenging.\n\nThanks", "author_fullname": "t2_srt8lsjb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the problems you face in your data science workflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125j9ac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680084746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;We are a team of data analyst. We are doing a survey on the problems data scientists face while doing their work. Please comment if you have anything to share from your experience.&lt;/p&gt;\n\n&lt;p&gt;In our personal work, we have found that sharing our work across our team and saving a history of our work progress is challenging.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125j9ac", "is_robot_indexable": true, "report_reasons": null, "author": "lightversetech", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125j9ac/what_are_the_problems_you_face_in_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125j9ac/what_are_the_problems_you_face_in_your_data/", "subreddit_subscribers": 94826, "created_utc": 1680084746.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\n\n I am from India. I have 3.5 years of experience in ETL/ BI development/Data Analytics.\n\n* ***Skills:*** *Alteryx(It is a low code tool), SQL, PowerBI, Python(Average)*\n* ***CTC:*** *18.5 L  ( including bonus)*\n* ***Background:*** *Mechanical Engineering from T2 college.*\n* ***Strength:*** *Good problem solving skills, Self learning*\n\nNow I am thinking about switching in to Big data engineering side. I have joined a course to learn DataBricks, Pyspark, Kafka, Azure etc. I have listed my thoughts below. Let me know your suggestions.\n\n* *Salary won't go beyond(30L) in my field. But the upper band in big data is 30-50L.*\n* *If I can find good remote job(US/UK) , I can make up to 30-70L.*\n* *I can find freelance works or start a small company in the future.*\n* *I am good at ETL &amp; Visualization part. If I learn Data engineering and eventually Data science(after some years), I can get good opportunities*", "author_fullname": "t2_g8i7cjd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition to Big Data Engineering......Let me know your thoughts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_125ezxs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680070346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;\n\n&lt;p&gt;I am from India. I have 3.5 years of experience in ETL/ BI development/Data Analytics.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Skills:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Alteryx(It is a low code tool), SQL, PowerBI, Python(Average)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;CTC:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;18.5 L  ( including bonus)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Background:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Mechanical Engineering from T2 college.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Strength:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Good problem solving skills, Self learning&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now I am thinking about switching in to Big data engineering side. I have joined a course to learn DataBricks, Pyspark, Kafka, Azure etc. I have listed my thoughts below. Let me know your suggestions.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Salary won&amp;#39;t go beyond(30L) in my field. But the upper band in big data is 30-50L.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;If I can find good remote job(US/UK) , I can make up to 30-70L.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;I can find freelance works or start a small company in the future.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;I am good at ETL &amp;amp; Visualization part. If I learn Data engineering and eventually Data science(after some years), I can get good opportunities&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "125ezxs", "is_robot_indexable": true, "report_reasons": null, "author": "Jkk_geek", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/125ezxs/transition_to_big_data_engineeringlet_me_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/125ezxs/transition_to_big_data_engineeringlet_me_know/", "subreddit_subscribers": 94826, "created_utc": 1680070346.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}