{"kind": "Listing", "data": {"after": "t3_11bicc2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_y15lw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "4 ways to build dbt Python models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11awcne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ynjDHdVsg0X3Oo_lCwH45EGss1uZvmBFeWuW9r1rE-0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677258417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datafold.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datafold.com/blog/dbt-python", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?auto=webp&amp;v=enabled&amp;s=03aee5abbd9db5e52d344c939e5442500746c6e3", "width": 1920, "height": 1081}, "resolutions": [{"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f2c21159b63ab2c364a83dfb7ac6c113f2d48e6", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e04bdcb313a79198461f83c2cb4f80d99477dccd", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7883781904e302a1cee8ed081e1f091eab59a805", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ff0953ccd82b2db9a4cc9d22aaac670b576e790", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38ec2abb4df9311394968c3c4ea93cad5f3b7886", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1179db66a2f8768ca94ac079dc8d053a21a1b9aa", "width": 1080, "height": 608}], "variants": {}, "id": "nPlCs849bn22ZnA2XvOziYJ0AZmmtYWsIi_Zsj6c2xA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11awcne", "is_robot_indexable": true, "report_reasons": null, "author": "gorkemyurt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11awcne/4_ways_to_build_dbt_python_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datafold.com/blog/dbt-python", "subreddit_subscribers": 91065, "created_utc": 1677258417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got an interview for a solution architect (SA) role at Databricks. There is also a possibility of being a specialist solution architect (SSA) as well. \n\nWondering if anyone from Databricks is here and willing to give their perspective on how working at Databricks has been? \n\nInterested in WLB, Culture, Project structure, and the difference between SA &amp; SSA?", "author_fullname": "t2_ay99iuoj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solution Architect @ Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bcow5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677301753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got an interview for a solution architect (SA) role at Databricks. There is also a possibility of being a specialist solution architect (SSA) as well. &lt;/p&gt;\n\n&lt;p&gt;Wondering if anyone from Databricks is here and willing to give their perspective on how working at Databricks has been? &lt;/p&gt;\n\n&lt;p&gt;Interested in WLB, Culture, Project structure, and the difference between SA &amp;amp; SSA?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11bcow5", "is_robot_indexable": true, "report_reasons": null, "author": "Rich_Repair", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bcow5/solution_architect_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bcow5/solution_architect_databricks/", "subreddit_subscribers": 91065, "created_utc": 1677301753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to be a data engineer in a coding-heavy role. Like, I don\u2019t want to be a DE that uses tools and SQL only. I have worked as a database developer for ~4 years writing SQL and PLSQL only for ETL reasons. I wanted to do more than that. I am currently working as a software engineer for the past 7 months. Should I stay a software engineer for the next few years so I understand software engineering practices before I jump into a data engineering role that is code heavy? Or, should I just jump into a data engineering role that is code-heavy right now? Assume I have both options.", "author_fullname": "t2_jpjpdkcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I wait to get into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11b7g0l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677286327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to be a data engineer in a coding-heavy role. Like, I don\u2019t want to be a DE that uses tools and SQL only. I have worked as a database developer for ~4 years writing SQL and PLSQL only for ETL reasons. I wanted to do more than that. I am currently working as a software engineer for the past 7 months. Should I stay a software engineer for the next few years so I understand software engineering practices before I jump into a data engineering role that is code heavy? Or, should I just jump into a data engineering role that is code-heavy right now? Assume I have both options.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11b7g0l", "is_robot_indexable": true, "report_reasons": null, "author": "iemback", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11b7g0l/should_i_wait_to_get_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11b7g0l/should_i_wait_to_get_into_data_engineering/", "subreddit_subscribers": 91065, "created_utc": 1677286327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video: Demonstration of Trying out Apache Iceberg in Spark Locally using a single Docker Image", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11avnyf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/6dtfvo7nz5ka1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/6dtfvo7nz5ka1/DASH_96.mp4", "dash_url": "https://v.redd.it/6dtfvo7nz5ka1/DASHPlaylist.mpd?a=1679927891%2CY2FkMTAzNDI4MjhiZmY3ZDdhOTVhMDkzYjVhODYzMzE4NmE3MWZlOGU4NzJiY2JlMzYxYmI5NjI2NGNkMjExOQ%3D%3D&amp;v=1&amp;f=sd", "duration": 899, "hls_url": "https://v.redd.it/6dtfvo7nz5ka1/HLSPlaylist.m3u8?a=1679927891%2CNjMxNTRlN2MwZjE5NmY5YWE0YTk1Yjg3MTBkM2RhN2VhMjk4YjM5MzkxOGQwZjcyYzQ1Mzk2YWNhNGM5NzQzMA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AEGCdZLAciaiQCae5diXVPdR-B8dEeupVzeyhWXFP9A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677256697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/6dtfvo7nz5ka1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=00479251a89a675a148d7df12bd18c9267a64c46", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e2cabe9766586ee7d5461e7bb23ea03c8ce949ff", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=933cbc9ead92bf8d80dda203569ef3b3c92478a0", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=625bffe483eda9f054adcbd81b53e347561c88dc", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b1f951764f8edfc44e5ec4f2010f4bfb71a6f564", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=08b67eca962347dcd2546a1597c5d18173cb6aca", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7db6b4ff69fbd5d29532cf817cbe3c8b7cd74ac3", "width": 1080, "height": 607}], "variants": {}, "id": "d8muqGO0greRfC0uhiqc91xBpkQLi7AkX5uw8j29uRE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11avnyf", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11avnyf/video_demonstration_of_trying_out_apache_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/6dtfvo7nz5ka1", "subreddit_subscribers": 91065, "created_utc": 1677256697.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/6dtfvo7nz5ka1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/6dtfvo7nz5ka1/DASH_96.mp4", "dash_url": "https://v.redd.it/6dtfvo7nz5ka1/DASHPlaylist.mpd?a=1679927891%2CY2FkMTAzNDI4MjhiZmY3ZDdhOTVhMDkzYjVhODYzMzE4NmE3MWZlOGU4NzJiY2JlMzYxYmI5NjI2NGNkMjExOQ%3D%3D&amp;v=1&amp;f=sd", "duration": 899, "hls_url": "https://v.redd.it/6dtfvo7nz5ka1/HLSPlaylist.m3u8?a=1679927891%2CNjMxNTRlN2MwZjE5NmY5YWE0YTk1Yjg3MTBkM2RhN2VhMjk4YjM5MzkxOGQwZjcyYzQ1Mzk2YWNhNGM5NzQzMA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am wondering what exact steps do you take in the \"Load\" stage when doing ELT data architecture?\n\nBy Extract I am thinking of simply downloading data from sources and dumping them as is to some object storage. Then the load stage? You take this raw data and without any transformations/validations/cleaning just load it into some RDBMS? Well, I guess at least e.g. schema flattening and data type casting has to be done in this step - but nothing else? No aggregations, derived columns, joins etc? Then aggregations, joins, derived columns in the Transform phase? Also, what if this table loaded in the \"Load\" stage does not require any more transformations? You just copy it 1:1 to different database schema?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What steps do you apply in the \"L\" when doing ELT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11awvyp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677259741.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering what exact steps do you take in the &amp;quot;Load&amp;quot; stage when doing ELT data architecture?&lt;/p&gt;\n\n&lt;p&gt;By Extract I am thinking of simply downloading data from sources and dumping them as is to some object storage. Then the load stage? You take this raw data and without any transformations/validations/cleaning just load it into some RDBMS? Well, I guess at least e.g. schema flattening and data type casting has to be done in this step - but nothing else? No aggregations, derived columns, joins etc? Then aggregations, joins, derived columns in the Transform phase? Also, what if this table loaded in the &amp;quot;Load&amp;quot; stage does not require any more transformations? You just copy it 1:1 to different database schema?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11awvyp", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11awvyp/what_steps_do_you_apply_in_the_l_when_doing_elt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11awvyp/what_steps_do_you_apply_in_the_l_when_doing_elt/", "subreddit_subscribers": 91065, "created_utc": 1677259741.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is one of those out-of-the-blue thoughts that you get randomly. I am an expert in sql with many years of experience, but have yet to use Right Joins lol. Is there any specific reason or use-case for this type of join?", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do Right Joins even matter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bhvux", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677320954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is one of those out-of-the-blue thoughts that you get randomly. I am an expert in sql with many years of experience, but have yet to use Right Joins lol. Is there any specific reason or use-case for this type of join?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11bhvux", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bhvux/do_right_joins_even_matter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bhvux/do_right_joins_even_matter/", "subreddit_subscribers": 91065, "created_utc": 1677320954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is better to prevent table having duplicate rows - composite primary key or unique index and why? I would guess that PK would be more preferable as unique index would take more disk space? Also is there any other added benefit of having unique index?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Composite primary key or Unique index", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bfpmm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677312531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is better to prevent table having duplicate rows - composite primary key or unique index and why? I would guess that PK would be more preferable as unique index would take more disk space? Also is there any other added benefit of having unique index?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11bfpmm", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bfpmm/composite_primary_key_or_unique_index/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bfpmm/composite_primary_key_or_unique_index/", "subreddit_subscribers": 91065, "created_utc": 1677312531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have an ELT architecture in our project where data is pulled from multiple sources into cloud storage (GCS)using APIs invoked by cloud functions. \n\nThe files in GCS are then pushed to BigQuery tables (datalakes) using a data transfer service, configured using the BigQuery console UI.\n\nThe raw data in the datalakes are transformed using scheduled BigQuery SQL queries and stores as dimension and fact tables, which are connected to Tableau dashboards as extracts.\n\nWe did not use any orchestration tool till now as the requirement was to batch load the data once a day only. But now, the dependencies and processes have become quite complicated, that we feel the need for an orchestration tool.\n\nThe preferences are:\n1. The system takes advantage of serverless architecture\n2. Ability to use the transfer services and scheduled queries that we have already built\n3. Minimise costs\n4. Manage dependencies\n5. Easy troubleshooting of failures during pipeline execution \n6. Preferable if Tableau extracts can be invoked by an API call at the last step of the pipeline\n\nWe have narrowed down to:\n1. Cloud Composer which is GCP's flavour of Airflow which is a fully managed service. The pipeline is written in Python.\n2. Cloud workflow which is relatively new, that helps to glue together GCP services and retains the serverless aspect. The pipeline is written in YAML.\n\nPlease suggest whether Cloud Composer or Cloud workflow would be better in my case.", "author_fullname": "t2_ojhggjm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP: Cloud composer vs Cloud Workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bclg9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677313819.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677301435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have an ELT architecture in our project where data is pulled from multiple sources into cloud storage (GCS)using APIs invoked by cloud functions. &lt;/p&gt;\n\n&lt;p&gt;The files in GCS are then pushed to BigQuery tables (datalakes) using a data transfer service, configured using the BigQuery console UI.&lt;/p&gt;\n\n&lt;p&gt;The raw data in the datalakes are transformed using scheduled BigQuery SQL queries and stores as dimension and fact tables, which are connected to Tableau dashboards as extracts.&lt;/p&gt;\n\n&lt;p&gt;We did not use any orchestration tool till now as the requirement was to batch load the data once a day only. But now, the dependencies and processes have become quite complicated, that we feel the need for an orchestration tool.&lt;/p&gt;\n\n&lt;p&gt;The preferences are:\n1. The system takes advantage of serverless architecture\n2. Ability to use the transfer services and scheduled queries that we have already built\n3. Minimise costs\n4. Manage dependencies\n5. Easy troubleshooting of failures during pipeline execution \n6. Preferable if Tableau extracts can be invoked by an API call at the last step of the pipeline&lt;/p&gt;\n\n&lt;p&gt;We have narrowed down to:\n1. Cloud Composer which is GCP&amp;#39;s flavour of Airflow which is a fully managed service. The pipeline is written in Python.\n2. Cloud workflow which is relatively new, that helps to glue together GCP services and retains the serverless aspect. The pipeline is written in YAML.&lt;/p&gt;\n\n&lt;p&gt;Please suggest whether Cloud Composer or Cloud workflow would be better in my case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11bclg9", "is_robot_indexable": true, "report_reasons": null, "author": "raghucc24", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bclg9/gcp_cloud_composer_vs_cloud_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bclg9/gcp_cloud_composer_vs_cloud_workflow/", "subreddit_subscribers": 91065, "created_utc": 1677301435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have two offers for full time data engineer role, one at Amazon and other at Barclays. What are your views on which one shall I join?", "author_fullname": "t2_b3tv7fn0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon vs Barclays ( for Data Engineer role )", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11b40wx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677277584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have two offers for full time data engineer role, one at Amazon and other at Barclays. What are your views on which one shall I join?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11b40wx", "is_robot_indexable": true, "report_reasons": null, "author": "Honest_amicable", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11b40wx/amazon_vs_barclays_for_data_engineer_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11b40wx/amazon_vs_barclays_for_data_engineer_role/", "subreddit_subscribers": 91065, "created_utc": 1677277584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "You build pipelines. Presumably some of these end up in an analytics layer, rendered by Tableau or PowerBI or similar. You may take on the duel role of Data Engineer and Analytics Engineer. If so, this question is aimed at you.\n\nData warehouses are, by definition tightly coupled datasets. Micro services on the other hand are loosely coupled. Regardless of sources, does your team have a preference for tightly/loosely coupled data, and if the latter how do you structure your data to allow seamless reporting by your analytics team?\n\nI would love to move towards a more loosely coupled architecture, however downstream of us are the analysts/report developers who want to build consolidated C-suite reports that pull from as much as they possibly can. Maybe the real question is: where do I start?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How tightly/loosely coupled is your data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bebg0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677307361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You build pipelines. Presumably some of these end up in an analytics layer, rendered by Tableau or PowerBI or similar. You may take on the duel role of Data Engineer and Analytics Engineer. If so, this question is aimed at you.&lt;/p&gt;\n\n&lt;p&gt;Data warehouses are, by definition tightly coupled datasets. Micro services on the other hand are loosely coupled. Regardless of sources, does your team have a preference for tightly/loosely coupled data, and if the latter how do you structure your data to allow seamless reporting by your analytics team?&lt;/p&gt;\n\n&lt;p&gt;I would love to move towards a more loosely coupled architecture, however downstream of us are the analysts/report developers who want to build consolidated C-suite reports that pull from as much as they possibly can. Maybe the real question is: where do I start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11bebg0", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bebg0/how_tightlyloosely_coupled_is_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bebg0/how_tightlyloosely_coupled_is_your_data/", "subreddit_subscribers": 91065, "created_utc": 1677307361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most of my DE experience is in batch ETL with Python or AWS Kinesis + Lambda. Now that I am getting exposed to the world of data streaming with Kafka, most processing apps from what I've seen so far are built in either Java or Scala, whether its Kafka Streams or Flink app. My colleagues have different preferences and I'm not sure which route I want to take. I know it will be a steep learning curve but I am itching to get this hands on experience. Which route seems to be more fun?", "author_fullname": "t2_991xsmvf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kafka Stream Processing in Java or Scala", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11aypfk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677264314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of my DE experience is in batch ETL with Python or AWS Kinesis + Lambda. Now that I am getting exposed to the world of data streaming with Kafka, most processing apps from what I&amp;#39;ve seen so far are built in either Java or Scala, whether its Kafka Streams or Flink app. My colleagues have different preferences and I&amp;#39;m not sure which route I want to take. I know it will be a steep learning curve but I am itching to get this hands on experience. Which route seems to be more fun?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11aypfk", "is_robot_indexable": true, "report_reasons": null, "author": "twadftw10", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11aypfk/kafka_stream_processing_in_java_or_scala/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11aypfk/kafka_stream_processing_in_java_or_scala/", "subreddit_subscribers": 91065, "created_utc": 1677264314.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've installed the stack (Hadoop, Hive, Spark) into a Centos VM, built everything from sources to make sure it fits together. Then added Delta Lake ([delta.io](https://delta.io)) from their maven repo.\n\nEverything works fine so I finally have my own little playground (we're using Databricks at work)\n\nNow I noticed this little waring when saving a table in delta format to HDFS:\n\n**WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table \\`vscode\\_vm\\`.\\`hwtable\\_vm\\_vs\\` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.**\n\nIt doesn't seem to be a problem. Everything works right as far I can see. I can read the table in all \"clients\" (jobs) be it a shell, a notebook or jdbc thrift session. Hence, I wonder what it is about...\n\nSession:\n\n    import pyspark\n    from delta import *\n    \n    appName = \"vs_test\"\n    master = \"spark://pc.home:7077\"\n    \n    builder = pyspark.sql.SparkSession.builder \\\n        .appName(appName) \\\n        .master(master) \\\n        .enableHiveSupport() \\\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n        .config(\"spark.cores.max\", \"4\") \\\n        .config(\"spark.executor.memory\", \"2g\")\n    \n    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n\n(load df from a .csv in HDFS)\n\nSave as Table (this is what warns me):\n\n    df.write.format(\"delta\").saveAsTable(\"vscode_vm.hwtable_vm_vs\")\n\nLook into HDFS:\n\n    hdfs dfs -ls /user/hive/warehouse/vscode_vm.db/hwtable_vm_vs/\n\n/user/hive/warehouse/vscode\\_vm.db/hwtable\\_vm\\_vs/part-00000-de750267-3c9a-46c6-8860-714fa45a7a9d-c000.snappy.parquet\n\nPS: the delta-jars are in $SPARK\\_HOME/jars", "author_fullname": "t2_337g1dil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDFS/Spark + Delta: Is this warning dangerous?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11bk5is", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677328982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve installed the stack (Hadoop, Hive, Spark) into a Centos VM, built everything from sources to make sure it fits together. Then added Delta Lake (&lt;a href=\"https://delta.io\"&gt;delta.io&lt;/a&gt;) from their maven repo.&lt;/p&gt;\n\n&lt;p&gt;Everything works fine so I finally have my own little playground (we&amp;#39;re using Databricks at work)&lt;/p&gt;\n\n&lt;p&gt;Now I noticed this little waring when saving a table in delta format to HDFS:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;WARN HiveExternalCatalog: Couldn&amp;#39;t find corresponding Hive SerDe for data source provider delta. Persisting data source table `vscode_vm`.`hwtable_vm_vs` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;It doesn&amp;#39;t seem to be a problem. Everything works right as far I can see. I can read the table in all &amp;quot;clients&amp;quot; (jobs) be it a shell, a notebook or jdbc thrift session. Hence, I wonder what it is about...&lt;/p&gt;\n\n&lt;p&gt;Session:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import pyspark\nfrom delta import *\n\nappName = &amp;quot;vs_test&amp;quot;\nmaster = &amp;quot;spark://pc.home:7077&amp;quot;\n\nbuilder = pyspark.sql.SparkSession.builder \\\n    .appName(appName) \\\n    .master(master) \\\n    .enableHiveSupport() \\\n    .config(&amp;quot;spark.sql.extensions&amp;quot;, &amp;quot;io.delta.sql.DeltaSparkSessionExtension&amp;quot;) \\\n    .config(&amp;quot;spark.sql.catalog.spark_catalog&amp;quot;, &amp;quot;org.apache.spark.sql.delta.catalog.DeltaCatalog&amp;quot;) \\\n    .config(&amp;quot;spark.cores.max&amp;quot;, &amp;quot;4&amp;quot;) \\\n    .config(&amp;quot;spark.executor.memory&amp;quot;, &amp;quot;2g&amp;quot;)\n\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;(load df from a .csv in HDFS)&lt;/p&gt;\n\n&lt;p&gt;Save as Table (this is what warns me):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.write.format(&amp;quot;delta&amp;quot;).saveAsTable(&amp;quot;vscode_vm.hwtable_vm_vs&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Look into HDFS:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;hdfs dfs -ls /user/hive/warehouse/vscode_vm.db/hwtable_vm_vs/\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;/user/hive/warehouse/vscode_vm.db/hwtable_vm_vs/part-00000-de750267-3c9a-46c6-8860-714fa45a7a9d-c000.snappy.parquet&lt;/p&gt;\n\n&lt;p&gt;PS: the delta-jars are in $SPARK_HOME/jars&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11bk5is", "is_robot_indexable": true, "report_reasons": null, "author": "eierwerfer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bk5is/hdfsspark_delta_is_this_warning_dangerous/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bk5is/hdfsspark_delta_is_this_warning_dangerous/", "subreddit_subscribers": 91065, "created_utc": 1677328982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say I have a DAG that runs every night. Every dag run has assigned start date and end date by default (Airflow built-in). \nOne day I need to rerun some of the tasks in this particular dag run. Currently, our applications rely on the start and end date passed from dag run. The apps take these parameters and based on them loads data between these two dates. So, if we want to rerun some tasks we go to the specific dag run and \u201cclear\u201d the tasks. (btw. we are talking about both - data extraction and transformation apps)\n\nHowever, I am thinking of maybe implementing different approach. That is having \u201cstaging\u201d data area where I would always put the data I want to process and after data is processed, clear up this stage. My motivation for this is that it might simplify rerunning particular tasks for longer history as now if I need to rerun tasks for lets say 30 days - I have to rerun 30 runs (times number of tasks needed to rerun) which is pretty complicated. In this new setup I would just copy all data needed to rerun 30days history into the \u201cstaging\u201d data area and manually trigger new dag run a let entire dag run even if some of the tasks would not have to do any processing. So apps would not rely on specific start and end date parameter but rather on the content of the staging data area. \n\nWhat do you think? Does it make sense? Do you see any potential issues? How do you approach this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow reruns best practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bin0l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677323745.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say I have a DAG that runs every night. Every dag run has assigned start date and end date by default (Airflow built-in). \nOne day I need to rerun some of the tasks in this particular dag run. Currently, our applications rely on the start and end date passed from dag run. The apps take these parameters and based on them loads data between these two dates. So, if we want to rerun some tasks we go to the specific dag run and \u201cclear\u201d the tasks. (btw. we are talking about both - data extraction and transformation apps)&lt;/p&gt;\n\n&lt;p&gt;However, I am thinking of maybe implementing different approach. That is having \u201cstaging\u201d data area where I would always put the data I want to process and after data is processed, clear up this stage. My motivation for this is that it might simplify rerunning particular tasks for longer history as now if I need to rerun tasks for lets say 30 days - I have to rerun 30 runs (times number of tasks needed to rerun) which is pretty complicated. In this new setup I would just copy all data needed to rerun 30days history into the \u201cstaging\u201d data area and manually trigger new dag run a let entire dag run even if some of the tasks would not have to do any processing. So apps would not rely on specific start and end date parameter but rather on the content of the staging data area. &lt;/p&gt;\n\n&lt;p&gt;What do you think? Does it make sense? Do you see any potential issues? How do you approach this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11bin0l", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bin0l/airflow_reruns_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bin0l/airflow_reruns_best_practices/", "subreddit_subscribers": 91065, "created_utc": 1677323745.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for a database for storing primarily time-series data while being suitable for the \"Transform\" phase in the ELT process. We need to preserve time-series functions such as time bucketing, getting last values etc.\n\nIs TimescaleDB suitable for ELT architecture? Meaning is it efficient to do the heavy \"Transform\" steps? Would some distributed OLAP DB like Pinot or Druid be better maybe? Or any other suggestions?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TimescaleDB - suitability for heavy Transformations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11axa3r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677260729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a database for storing primarily time-series data while being suitable for the &amp;quot;Transform&amp;quot; phase in the ELT process. We need to preserve time-series functions such as time bucketing, getting last values etc.&lt;/p&gt;\n\n&lt;p&gt;Is TimescaleDB suitable for ELT architecture? Meaning is it efficient to do the heavy &amp;quot;Transform&amp;quot; steps? Would some distributed OLAP DB like Pinot or Druid be better maybe? Or any other suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11axa3r", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11axa3r/timescaledb_suitability_for_heavy_transformations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11axa3r/timescaledb_suitability_for_heavy_transformations/", "subreddit_subscribers": 91065, "created_utc": 1677260729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for some approach how to get parquet data from S3 into Postgres. Obviously one can use python but I would like to simplify and standardize the process as much as possible. I thought of using dbt python models but it apparently supports only Snowflake and BigQuery which we do not have so I thought of using DuckDB + dbt to load the parquets, do the transformations (I know this is possible) but how to output it then efficiently to Postgres?\n\nPS. The intended data path is raw -&gt; parquet -&gt; postgres", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet through dbt+DuckDB into postgres?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11awcol", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677258420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for some approach how to get parquet data from S3 into Postgres. Obviously one can use python but I would like to simplify and standardize the process as much as possible. I thought of using dbt python models but it apparently supports only Snowflake and BigQuery which we do not have so I thought of using DuckDB + dbt to load the parquets, do the transformations (I know this is possible) but how to output it then efficiently to Postgres?&lt;/p&gt;\n\n&lt;p&gt;PS. The intended data path is raw -&amp;gt; parquet -&amp;gt; postgres&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11awcol", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11awcol/parquet_through_dbtduckdb_into_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11awcol/parquet_through_dbtduckdb_into_postgres/", "subreddit_subscribers": 91065, "created_utc": 1677258420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nWe use debezium connector as a source with confluent kafka in my org. We use debezium for both MySQL and SQL Server. In MySQL, debezium seems to run fine for the most part, but in SQL Server, there are some limitations which greatly increase the manual work.\n\n1) In MySQL, if CDC is disabled for some reason &amp; re-enabled, debezium automatically detects it and triggers a full snapshot. The same is not true for SQL Server where it's left for the users to figure out and snapshot manually.\n\n2) Unlike MySQL, DDL operations are not straightforward. There are a series of steps to follow, both offline and online methods are available. My org doesn't prefer taking DB down for DDL operations, that only leaves the online method. [That] (https://debezium.io/documentation/reference/stable/connectors/sqlserver.html#online-schema-updates) has a limitation that values of the new column will be missed after new column is added &amp; before new CDC instance is created.\n\nAll of this adds lot of manual work. For reasons beyond our control, we cannot absolutely guarentee that such steps will always be followed by the DB team nor that CDC is never taken down. (If CDC is enabled, some operations like TRUNCATE on the main table are blocked, not sure if other operations are too.)\n\nThus, we are looking for alternatives to get near real time data (i.e. gap of 15 mins atmost). Does anyone know of an alternative that doesn't have all the above limitations and has automations built in to mostly take care of itself?", "author_fullname": "t2_14bzgy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to debezium for SQL Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11auhii", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677253696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;We use debezium connector as a source with confluent kafka in my org. We use debezium for both MySQL and SQL Server. In MySQL, debezium seems to run fine for the most part, but in SQL Server, there are some limitations which greatly increase the manual work.&lt;/p&gt;\n\n&lt;p&gt;1) In MySQL, if CDC is disabled for some reason &amp;amp; re-enabled, debezium automatically detects it and triggers a full snapshot. The same is not true for SQL Server where it&amp;#39;s left for the users to figure out and snapshot manually.&lt;/p&gt;\n\n&lt;p&gt;2) Unlike MySQL, DDL operations are not straightforward. There are a series of steps to follow, both offline and online methods are available. My org doesn&amp;#39;t prefer taking DB down for DDL operations, that only leaves the online method. &lt;a href=\"https://debezium.io/documentation/reference/stable/connectors/sqlserver.html#online-schema-updates\"&gt;That&lt;/a&gt; has a limitation that values of the new column will be missed after new column is added &amp;amp; before new CDC instance is created.&lt;/p&gt;\n\n&lt;p&gt;All of this adds lot of manual work. For reasons beyond our control, we cannot absolutely guarentee that such steps will always be followed by the DB team nor that CDC is never taken down. (If CDC is enabled, some operations like TRUNCATE on the main table are blocked, not sure if other operations are too.)&lt;/p&gt;\n\n&lt;p&gt;Thus, we are looking for alternatives to get near real time data (i.e. gap of 15 mins atmost). Does anyone know of an alternative that doesn&amp;#39;t have all the above limitations and has automations built in to mostly take care of itself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11auhii", "is_robot_indexable": true, "report_reasons": null, "author": "downloaderfan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11auhii/alternative_to_debezium_for_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11auhii/alternative_to_debezium_for_sql_server/", "subreddit_subscribers": 91065, "created_utc": 1677253696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I am still very new to STAR schema and decided to pick some dataset to try it out. Can someone please have a look at my schema design and give some feedback on it. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/yorodl9wbcka1.png?width=1283&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cedbf7d5c32c67f45db233572126dd44e77cf7ca\n\nI also add the example of the dataset that I am working on\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ozslnsyybcka1.png?width=2880&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9b4ed1a992f7e3ff46248104c4b3225942b78d3b", "author_fullname": "t2_imktgzwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Design STAR schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 100, "top_awarded_type": null, "hide_score": true, "media_metadata": {"yorodl9wbcka1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 77, "x": 108, "u": "https://preview.redd.it/yorodl9wbcka1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c8b992af9da13b54166b0c7ba3a3f5a91cfe85f"}, {"y": 154, "x": 216, "u": "https://preview.redd.it/yorodl9wbcka1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f1159a99169818caf453c5d72673853c88b0a12"}, {"y": 229, "x": 320, "u": "https://preview.redd.it/yorodl9wbcka1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b68c396b379c8a9c768d3b6eb785e42e627933a"}, {"y": 458, "x": 640, "u": "https://preview.redd.it/yorodl9wbcka1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=927433051b577fc00417ba61551690f7e8de856a"}, {"y": 687, "x": 960, "u": "https://preview.redd.it/yorodl9wbcka1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91a85847d569af2f861a57c33b63387a1785fb94"}, {"y": 773, "x": 1080, "u": "https://preview.redd.it/yorodl9wbcka1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee91d9837c929ff0938534e3ff1105feeb5e056d"}], "s": {"y": 919, "x": 1283, "u": "https://preview.redd.it/yorodl9wbcka1.png?width=1283&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cedbf7d5c32c67f45db233572126dd44e77cf7ca"}, "id": "yorodl9wbcka1"}, "ozslnsyybcka1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/ozslnsyybcka1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa32de5b2c8805b377f88f1cf361da767f319df8"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/ozslnsyybcka1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad9ac06801b984e3c77b10f80425813e62b12f4c"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/ozslnsyybcka1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f41626b3283b55e7750d7fedb4475729b21498e"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/ozslnsyybcka1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd77abea57383f4d7d7fafbddd8283d283929500"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/ozslnsyybcka1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2def6fd61806928bd5ca081d52902116ff6e2f1f"}, {"y": 675, "x": 1080, "u": "https://preview.redd.it/ozslnsyybcka1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=817859a28f489cc0e82a7fff236ad40f91873c16"}], "s": {"y": 1800, "x": 2880, "u": "https://preview.redd.it/ozslnsyybcka1.png?width=2880&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9b4ed1a992f7e3ff46248104c4b3225942b78d3b"}, "id": "ozslnsyybcka1"}}, "name": "t3_11blma9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rCeRPdY_UyoCfSBFuEAfu8S1yhFwn1AL1Wzn8OnzeIA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677333470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I am still very new to STAR schema and decided to pick some dataset to try it out. Can someone please have a look at my schema design and give some feedback on it. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yorodl9wbcka1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cedbf7d5c32c67f45db233572126dd44e77cf7ca\"&gt;https://preview.redd.it/yorodl9wbcka1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cedbf7d5c32c67f45db233572126dd44e77cf7ca&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I also add the example of the dataset that I am working on&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ozslnsyybcka1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9b4ed1a992f7e3ff46248104c4b3225942b78d3b\"&gt;https://preview.redd.it/ozslnsyybcka1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9b4ed1a992f7e3ff46248104c4b3225942b78d3b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11blma9", "is_robot_indexable": true, "report_reasons": null, "author": "Acrobatic-Mobile-221", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11blma9/design_star_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11blma9/design_star_schema/", "subreddit_subscribers": 91065, "created_utc": 1677333470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TL;DR: Engineer with: 1y dev, 2y sysadmin, 2y dba, 4y in devops experience\u2026 What\u2019s the best path make a place for myself in DE? currently doing DataTalks De bootcamp.\n\n\nI started my career with a j2e developer profile, but opportunities made me move to ops fields. \n\nAfter one year developing in Java, I moved to sysadmin where I mostly managed physical servers (linux) and systems like dns, proxy, mta, mda, databases and lot of monitoring. \n\nLater I got an offer to move to other company as DBA, where I worked with big postgresql database clusters (several terabytes), amd most of my duties there was to keep DBs maintained and running lean, check slow sql queries to improve in db o code side, and to create a renewed DR plan. \n\nLater I moved to DevOps like 4 years ago, and I\u2019ve been moving in this field between being the devops engineer for dev cells or being part of platform engineering teams. Working intensively with microservices architecture: CI/CD, k8s, APMs, automated testing, IaC, the three big cloud providers (+OCI) and so on.\n\nI do have developing skills (java, python, ruby and bash, how I love you dear bash) and strong understanding of data structures given my studies and professional background.\n\nSo the question: **What\u2019s the best approach to make me a place in DE field?**\n\nI\u2019m interested to become and find a job as data engineer or analytics engineer during this year. \n\nI\u2019ve started (late) the DataTalk DE bootcamp and I\u2019m having a really good pace with it, first weeks about docker, databases and terraform I completed it in no more than two days given my experience. Next week about pipeline orchestration using prefect also I found very entertaining and wasn\u2019t really challenging for me since those terms aren\u2019t new for me at all\u2026 Now with DBT and Spark I\u2019ve been investing more time in study because those are unexplored territory for me by now.\n\nI\u2019m thinking daily on what side projects can I start after finishing the bootcamp to make my portfolio grow, since  regarding what I\u2019ve reading here and other forums, a good approach would be to finish different side projects using different tools and architectures, but still wondering if there are other ideas from people who made this same or similar switch.\n\nSorry for the long explanation hehe.. I\u2019ll be super attentive on what you people have to say about this. A big thanks in advance.", "author_fullname": "t2_83vb73sh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Yet another career question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11blfbz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677332902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: Engineer with: 1y dev, 2y sysadmin, 2y dba, 4y in devops experience\u2026 What\u2019s the best path make a place for myself in DE? currently doing DataTalks De bootcamp.&lt;/p&gt;\n\n&lt;p&gt;I started my career with a j2e developer profile, but opportunities made me move to ops fields. &lt;/p&gt;\n\n&lt;p&gt;After one year developing in Java, I moved to sysadmin where I mostly managed physical servers (linux) and systems like dns, proxy, mta, mda, databases and lot of monitoring. &lt;/p&gt;\n\n&lt;p&gt;Later I got an offer to move to other company as DBA, where I worked with big postgresql database clusters (several terabytes), amd most of my duties there was to keep DBs maintained and running lean, check slow sql queries to improve in db o code side, and to create a renewed DR plan. &lt;/p&gt;\n\n&lt;p&gt;Later I moved to DevOps like 4 years ago, and I\u2019ve been moving in this field between being the devops engineer for dev cells or being part of platform engineering teams. Working intensively with microservices architecture: CI/CD, k8s, APMs, automated testing, IaC, the three big cloud providers (+OCI) and so on.&lt;/p&gt;\n\n&lt;p&gt;I do have developing skills (java, python, ruby and bash, how I love you dear bash) and strong understanding of data structures given my studies and professional background.&lt;/p&gt;\n\n&lt;p&gt;So the question: &lt;strong&gt;What\u2019s the best approach to make me a place in DE field?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I\u2019m interested to become and find a job as data engineer or analytics engineer during this year. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve started (late) the DataTalk DE bootcamp and I\u2019m having a really good pace with it, first weeks about docker, databases and terraform I completed it in no more than two days given my experience. Next week about pipeline orchestration using prefect also I found very entertaining and wasn\u2019t really challenging for me since those terms aren\u2019t new for me at all\u2026 Now with DBT and Spark I\u2019ve been investing more time in study because those are unexplored territory for me by now.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m thinking daily on what side projects can I start after finishing the bootcamp to make my portfolio grow, since  regarding what I\u2019ve reading here and other forums, a good approach would be to finish different side projects using different tools and architectures, but still wondering if there are other ideas from people who made this same or similar switch.&lt;/p&gt;\n\n&lt;p&gt;Sorry for the long explanation hehe.. I\u2019ll be super attentive on what you people have to say about this. A big thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11blfbz", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Satisfaction8141", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11blfbz/yet_another_career_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11blfbz/yet_another_career_question/", "subreddit_subscribers": 91065, "created_utc": 1677332902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My data lake has 3 layers: Bronze, Silver and Gold\n\nLet me explain purpose of each layer with an example\n\nWe have 3 files coming into my data lake named A, B and C\n\nIn the Bronze layer, we load all three files into hive layer as it is.\n\nFile data is moved from Bronze to Silver layer only if they pass data quality checks \n\nFinally, for gold layer, we do joins and transformations between multiple tables in Silver layer and load the results into gold layer tables.\n\nNow, this is where I am stumped. Should the golden layer just have views built on top of the Silver layer? What is the point of creating real tables just for golden layer?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would it be beneficial to have only \"views\" in my final golden layer if downstream users are just going to read the data and never write anything to the golden layer.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11bkjve", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677330251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My data lake has 3 layers: Bronze, Silver and Gold&lt;/p&gt;\n\n&lt;p&gt;Let me explain purpose of each layer with an example&lt;/p&gt;\n\n&lt;p&gt;We have 3 files coming into my data lake named A, B and C&lt;/p&gt;\n\n&lt;p&gt;In the Bronze layer, we load all three files into hive layer as it is.&lt;/p&gt;\n\n&lt;p&gt;File data is moved from Bronze to Silver layer only if they pass data quality checks &lt;/p&gt;\n\n&lt;p&gt;Finally, for gold layer, we do joins and transformations between multiple tables in Silver layer and load the results into gold layer tables.&lt;/p&gt;\n\n&lt;p&gt;Now, this is where I am stumped. Should the golden layer just have views built on top of the Silver layer? What is the point of creating real tables just for golden layer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11bkjve", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bkjve/would_it_be_beneficial_to_have_only_views_in_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bkjve/would_it_be_beneficial_to_have_only_views_in_my/", "subreddit_subscribers": 91065, "created_utc": 1677330251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "First off I\u2019d like to thank this subreddit. Around this time last year I was working in the energy sector in the Project Management space but because of the extensive resources and discussions here I was able to self teach the Python and SQL skills needed for my first DE job!\n\nThis new role is in the finance sector and pays really well for my age (I\u2019m not long out of uni), with lots of additional benefits/bonuses and I\u2019m really enjoying it.\n\nHowever, I\u2019m slightly concerned about the tech stack used. At the moment, I\u2019d say I\u2019m a bit of a reporting/analytics data engineer. We use a mixture of Teradata, SQLS, SSIS and data vis tools. This has really developed my SQL skills but with Python I\u2019m still just self learning in my spare time through projects and then shoe horning into whatever I can at work.\n\nAdditionally, we lack a lot of the software engineering core principles. Only recently utilising git/GitHub properly, no real adoption of TDD etc. As I\u2019m from a Chemical Engineering education, I\u2019ve had no previous education in these and I\u2019m mainly learning by Googling best practices and incorporating in home projects.\n\nTherefore, it\u2019d be great to get some advice from this subreddit as a lot of you have way more experience than I have. Am I putting too much importance on these principles and a modern tech stack? \n\nApologies for the long post, any advice is much appreciated.", "author_fullname": "t2_2o4st1yo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How important is the tech stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bjv7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677328053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First off I\u2019d like to thank this subreddit. Around this time last year I was working in the energy sector in the Project Management space but because of the extensive resources and discussions here I was able to self teach the Python and SQL skills needed for my first DE job!&lt;/p&gt;\n\n&lt;p&gt;This new role is in the finance sector and pays really well for my age (I\u2019m not long out of uni), with lots of additional benefits/bonuses and I\u2019m really enjoying it.&lt;/p&gt;\n\n&lt;p&gt;However, I\u2019m slightly concerned about the tech stack used. At the moment, I\u2019d say I\u2019m a bit of a reporting/analytics data engineer. We use a mixture of Teradata, SQLS, SSIS and data vis tools. This has really developed my SQL skills but with Python I\u2019m still just self learning in my spare time through projects and then shoe horning into whatever I can at work.&lt;/p&gt;\n\n&lt;p&gt;Additionally, we lack a lot of the software engineering core principles. Only recently utilising git/GitHub properly, no real adoption of TDD etc. As I\u2019m from a Chemical Engineering education, I\u2019ve had no previous education in these and I\u2019m mainly learning by Googling best practices and incorporating in home projects.&lt;/p&gt;\n\n&lt;p&gt;Therefore, it\u2019d be great to get some advice from this subreddit as a lot of you have way more experience than I have. Am I putting too much importance on these principles and a modern tech stack? &lt;/p&gt;\n\n&lt;p&gt;Apologies for the long post, any advice is much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11bjv7c", "is_robot_indexable": true, "report_reasons": null, "author": "el527", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bjv7c/how_important_is_the_tech_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bjv7c/how_important_is_the_tech_stack/", "subreddit_subscribers": 91065, "created_utc": 1677328053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My ingestion script takes a csv file name as input. This csv file has connection details like usernames, password, database connection string, and so on. However, I need some more secure way to store and fetch usernames and passwords in my Python script. How do you handle such scenarios in your system?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle usernames and passwords in my data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bjqgr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677327604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My ingestion script takes a csv file name as input. This csv file has connection details like usernames, password, database connection string, and so on. However, I need some more secure way to store and fetch usernames and passwords in my Python script. How do you handle such scenarios in your system?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11bjqgr", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bjqgr/how_do_you_handle_usernames_and_passwords_in_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bjqgr/how_do_you_handle_usernames_and_passwords_in_my/", "subreddit_subscribers": 91065, "created_utc": 1677327604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How can I update 2 rows in 2 different columns in a table in SQL oracle database?", "author_fullname": "t2_167hj0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bjq32", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677327567.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How can I update 2 rows in 2 different columns in a table in SQL oracle database?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11bjq32", "is_robot_indexable": true, "report_reasons": null, "author": "omkargurme616", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bjq32/sql_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bjq32/sql_query/", "subreddit_subscribers": 91065, "created_utc": 1677327567.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I haven\u2019t found any benchmarks comparing performance of Druid and TimescaleDB. Has anyone tested them?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Druid vs TimescaleDB benchmarks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bjgtp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677326680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I haven\u2019t found any benchmarks comparing performance of Druid and TimescaleDB. Has anyone tested them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11bjgtp", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bjgtp/druid_vs_timescaledb_benchmarks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bjgtp/druid_vs_timescaledb_benchmarks/", "subreddit_subscribers": 91065, "created_utc": 1677326680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As per the title. Curious on how it went, any challenges, and if you started again whether you\u2019d go the same direction?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone implemented Data Vault in a lake house?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11biyc6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677324873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per the title. Curious on how it went, any challenges, and if you started again whether you\u2019d go the same direction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11biyc6", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11biyc6/anyone_implemented_data_vault_in_a_lake_house/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11biyc6/anyone_implemented_data_vault_in_a_lake_house/", "subreddit_subscribers": 91065, "created_utc": 1677324873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing an ETL process from both SQL server and Excel files. The flat source tables need to be structured into foreign key tables the FK added into the final table import. \nIs this something that SQLAlchemy can do and are there some examples I can look at to get me going? Or should I do the normalisation ETL as stored procedures directly on the database?\nIs SQLAlchemy the best tool for this or are there other libraries I should rather use or look at?", "author_fullname": "t2_11t26p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Normalising a source table with FK references during ETL with Python and SQLAlchemy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11bicc2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677322669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing an ETL process from both SQL server and Excel files. The flat source tables need to be structured into foreign key tables the FK added into the final table import. \nIs this something that SQLAlchemy can do and are there some examples I can look at to get me going? Or should I do the normalisation ETL as stored procedures directly on the database?\nIs SQLAlchemy the best tool for this or are there other libraries I should rather use or look at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11bicc2", "is_robot_indexable": true, "report_reasons": null, "author": "byeproduct", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11bicc2/normalising_a_source_table_with_fk_references/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11bicc2/normalising_a_source_table_with_fk_references/", "subreddit_subscribers": 91065, "created_utc": 1677322669.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}