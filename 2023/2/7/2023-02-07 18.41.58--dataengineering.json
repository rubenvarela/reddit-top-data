{"kind": "Listing", "data": {"after": "t3_10w4dsm", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Send in your questions. If you\u2019d like to join the linkedin live, DM me for the link.", "author_fullname": "t2_msviuy1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I\u2019m organizing an AMA with Joe Reis (Co-author of fundamentals of data engineering). Feel free to send in your questions!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vnmzj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 92, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 92, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675730994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Send in your questions. If you\u2019d like to join the linkedin live, DM me for the link.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Applied Data &amp; ML Engineer | Developer Advocate", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10vnmzj", "is_robot_indexable": true, "report_reasons": null, "author": "vino_and_data", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10vnmzj/im_organizing_an_ama_with_joe_reis_coauthor_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vnmzj/im_organizing_an_ama_with_joe_reis_coauthor_of/", "subreddit_subscribers": 88774, "created_utc": 1675730994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have years of experience as a data scientist and now manage a team. I also have the practitioner cert from AWS, but not much else. Looking for big picture principles or anything else you would want your data science partners to understand. Books, videos, trainings, certs, topics, etc whatever you recommend.", "author_fullname": "t2_7qjpofa7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I manage a data science team that works with a data engineering team. I want to learn more about data engineering to be a better colleague. Where should I start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vec4b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675708801.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have years of experience as a data scientist and now manage a team. I also have the practitioner cert from AWS, but not much else. Looking for big picture principles or anything else you would want your data science partners to understand. Books, videos, trainings, certs, topics, etc whatever you recommend.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10vec4b", "is_robot_indexable": true, "report_reasons": null, "author": "DataDrivenPirate", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vec4b/i_manage_a_data_science_team_that_works_with_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vec4b/i_manage_a_data_science_team_that_works_with_a/", "subreddit_subscribers": 88774, "created_utc": 1675708801.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like editing usernames. It\u2019s pretty instantaneous. Interested to know what goes in the backend ?", "author_fullname": "t2_8rc5owyl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does meta/Twitter handle large scale updates.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vl7ex", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675724731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like editing usernames. It\u2019s pretty instantaneous. Interested to know what goes in the backend ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10vl7ex", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive_Wheel_78", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vl7ex/how_does_metatwitter_handle_large_scale_updates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vl7ex/how_does_metatwitter_handle_large_scale_updates/", "subreddit_subscribers": 88774, "created_utc": 1675724731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thinking of my 3rd and 4th projects that I\u2019m planning on building for my resume.\n\nFor my 3rd project, I\u2019m going to include Airflow, PySpark, and Postgres (maybe a cloud data warehouse like BigQuery)\n\nFor my 4th project, I was thinking of building a pipeline that is entirely on AWS and using only AWS services. (I\u2019ve only used Google Cloud so want to learn another vendor)\n\nAre there any other tools that I should be building with/learning for job prospects?", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which technologies should a project include?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vex7q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675710159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thinking of my 3rd and 4th projects that I\u2019m planning on building for my resume.&lt;/p&gt;\n\n&lt;p&gt;For my 3rd project, I\u2019m going to include Airflow, PySpark, and Postgres (maybe a cloud data warehouse like BigQuery)&lt;/p&gt;\n\n&lt;p&gt;For my 4th project, I was thinking of building a pipeline that is entirely on AWS and using only AWS services. (I\u2019ve only used Google Cloud so want to learn another vendor)&lt;/p&gt;\n\n&lt;p&gt;Are there any other tools that I should be building with/learning for job prospects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10vex7q", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vex7q/which_technologies_should_a_project_include/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vex7q/which_technologies_should_a_project_include/", "subreddit_subscribers": 88774, "created_utc": 1675710159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I'm brand new here and to dbt, and already stuck on the Getting Started.\n\nAfter setting up the project and accounts (dbt, postgres, github) and initializing project, the main hub is read-only:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/c9h8ej1zrpga1.png?width=404&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9b4b1857dd1e812675fde6ac49cb7c948da42bc8\n\nAccording to google search and dbt's docs, a new branch needs to be created according to this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/dqnf0qq6spga1.png?width=964&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=734a1b160339c78ec89c256b44843768acd9c491\n\nThe problem, when I click Develop, there is nowhere that says 'Create branch'.\n\nWhat am I missing here?\n\nCheers.", "author_fullname": "t2_kh3ubtce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stuck on dbt Getting Started", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 113, "top_awarded_type": null, "hide_score": false, "media_metadata": {"dqnf0qq6spga1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 45, "x": 108, "u": "https://preview.redd.it/dqnf0qq6spga1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e5dad207e808a168acff28e16704b1cf4a91390"}, {"y": 91, "x": 216, "u": "https://preview.redd.it/dqnf0qq6spga1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61c9d6176fdf0aab8124bf9794d8d1a6df37d2ca"}, {"y": 135, "x": 320, "u": "https://preview.redd.it/dqnf0qq6spga1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5187de9bc4b288519074c46490bf56bf6e3d6e7"}, {"y": 271, "x": 640, "u": "https://preview.redd.it/dqnf0qq6spga1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da4514cb5a5505332435119aeefd55cd3b073289"}, {"y": 407, "x": 960, "u": "https://preview.redd.it/dqnf0qq6spga1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32b1407c9e90886c0aeab45adefa15c27653e7a4"}], "s": {"y": 409, "x": 964, "u": "https://preview.redd.it/dqnf0qq6spga1.png?width=964&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=734a1b160339c78ec89c256b44843768acd9c491"}, "id": "dqnf0qq6spga1"}, "c9h8ej1zrpga1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 87, "x": 108, "u": "https://preview.redd.it/c9h8ej1zrpga1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d7e6994502a3a9ad2ab394815eeeed5363e5b3d"}, {"y": 175, "x": 216, "u": "https://preview.redd.it/c9h8ej1zrpga1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d9cd7351d0ec14a302c36be852d28cc327ce986"}, {"y": 259, "x": 320, "u": "https://preview.redd.it/c9h8ej1zrpga1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fdb456ccb68a49cbbcd84ceae942bc61cf641083"}], "s": {"y": 328, "x": 404, "u": "https://preview.redd.it/c9h8ej1zrpga1.png?width=404&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9b4b1857dd1e812675fde6ac49cb7c948da42bc8"}, "id": "c9h8ej1zrpga1"}}, "name": "t3_10vvbtw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uqvdn_WecQMs0fDDxLyyw_AfemX3-4QPqiwQwuf-bCM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675753021.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m brand new here and to dbt, and already stuck on the Getting Started.&lt;/p&gt;\n\n&lt;p&gt;After setting up the project and accounts (dbt, postgres, github) and initializing project, the main hub is read-only:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/c9h8ej1zrpga1.png?width=404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9b4b1857dd1e812675fde6ac49cb7c948da42bc8\"&gt;https://preview.redd.it/c9h8ej1zrpga1.png?width=404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9b4b1857dd1e812675fde6ac49cb7c948da42bc8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;According to google search and dbt&amp;#39;s docs, a new branch needs to be created according to this:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dqnf0qq6spga1.png?width=964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=734a1b160339c78ec89c256b44843768acd9c491\"&gt;https://preview.redd.it/dqnf0qq6spga1.png?width=964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=734a1b160339c78ec89c256b44843768acd9c491&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The problem, when I click Develop, there is nowhere that says &amp;#39;Create branch&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;What am I missing here?&lt;/p&gt;\n\n&lt;p&gt;Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10vvbtw", "is_robot_indexable": true, "report_reasons": null, "author": "SquidsAndMartians", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vvbtw/stuck_on_dbt_getting_started/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vvbtw/stuck_on_dbt_getting_started/", "subreddit_subscribers": 88774, "created_utc": 1675753021.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I personally don't like that there are so many career related questions in this sub recently. Specifically, because these questions often confuse having a title with having an interesting job as an endgoal. However, wo am I to decide over the content of the sub? So, here I am and have such a question:\n\nWould you consider becoming a pure Azure dev that basically manages the data infrastructure solely by the means of Azure a lateral move onto an inescapable Azure path from a kind of ETL-developer role that was hard coding *everything*?\n\nI don't have any hands-on experience with cloud technology, so you could argue that it could serve me as an accelerator to becoming a good data engineer in the long run (/next job) and also open me up to cloud heavy jobs and devops opportunities. However, I fear to lose contact to scripting (or software engineering to be precise) and hence won't be considered for code heavy roles in the future anymore. Particularly, because I only have little more than one year of work experience.", "author_fullname": "t2_uktyi3l0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is setting up infrastructure completely on Azure a dead end?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10w2fg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675779593.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675778163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I personally don&amp;#39;t like that there are so many career related questions in this sub recently. Specifically, because these questions often confuse having a title with having an interesting job as an endgoal. However, wo am I to decide over the content of the sub? So, here I am and have such a question:&lt;/p&gt;\n\n&lt;p&gt;Would you consider becoming a pure Azure dev that basically manages the data infrastructure solely by the means of Azure a lateral move onto an inescapable Azure path from a kind of ETL-developer role that was hard coding &lt;em&gt;everything&lt;/em&gt;?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have any hands-on experience with cloud technology, so you could argue that it could serve me as an accelerator to becoming a good data engineer in the long run (/next job) and also open me up to cloud heavy jobs and devops opportunities. However, I fear to lose contact to scripting (or software engineering to be precise) and hence won&amp;#39;t be considered for code heavy roles in the future anymore. Particularly, because I only have little more than one year of work experience.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10w2fg3", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive_Doctor_796", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10w2fg3/is_setting_up_infrastructure_completely_on_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10w2fg3/is_setting_up_infrastructure_completely_on_azure/", "subreddit_subscribers": 88774, "created_utc": 1675778163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just got back to using Spark after version 2, seems like things have moved on\n\nIs Schema Inference and AutoLoader also available on Spark Distribution ?\n\nIf not are there any effective solutions available in the the OS community ?", "author_fullname": "t2_kqhkj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks AutoLoader for Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vyt6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675766500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got back to using Spark after version 2, seems like things have moved on&lt;/p&gt;\n\n&lt;p&gt;Is Schema Inference and AutoLoader also available on Spark Distribution ?&lt;/p&gt;\n\n&lt;p&gt;If not are there any effective solutions available in the the OS community ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10vyt6t", "is_robot_indexable": true, "report_reasons": null, "author": "audyoga", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vyt6t/databricks_autoloader_for_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vyt6t/databricks_autoloader_for_spark/", "subreddit_subscribers": 88774, "created_utc": 1675766500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, through repeated experiences of varying success when working with inhouse backend teams and them applying changes to the application, I was wondering how others have solved that problem?\n\nIt always took a frustrating amount of talking and pleading, to make CPOs, CTOs and Backend Leads understand that what they're doing has downstream effects and while a semi-automated alerting was the outcome in several projects, I was never able to find a satisfying technological solution to the issue. \n\nI'd love to read about other experiences to the issue of changing BE data structures.", "author_fullname": "t2_w2y7e99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle source app changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10w0i1e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675772576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, through repeated experiences of varying success when working with inhouse backend teams and them applying changes to the application, I was wondering how others have solved that problem?&lt;/p&gt;\n\n&lt;p&gt;It always took a frustrating amount of talking and pleading, to make CPOs, CTOs and Backend Leads understand that what they&amp;#39;re doing has downstream effects and while a semi-automated alerting was the outcome in several projects, I was never able to find a satisfying technological solution to the issue. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to read about other experiences to the issue of changing BE data structures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10w0i1e", "is_robot_indexable": true, "report_reasons": null, "author": "whichalps", "discussion_type": null, "num_comments": 8, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10w0i1e/how_do_you_handle_source_app_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10w0i1e/how_do_you_handle_source_app_changes/", "subreddit_subscribers": 88774, "created_utc": 1675772576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been a SE (mostly full stack web, not much infra exp) currently transitioning into DE.. purely from a technical perspective is it just me or is it normal to feel a little overwhelmed during this transition?\nThere are so many different tools/aws services used\n\nWhere as what I am more used to code being the center and you\u2019d have to be accustomed to the libraries/packages that are used\u2026 but I feel like in DE there are much more moving parts not just code.. Or will I overcome this as I build more experience in cloud/infra?\nOh and there is all that governance stuff which makes it feel like things move slower.. (my background in SaaS for start ups)", "author_fullname": "t2_2xls9nq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software Engineering transitioning to Data Engineering feelings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vnjnl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675730748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been a SE (mostly full stack web, not much infra exp) currently transitioning into DE.. purely from a technical perspective is it just me or is it normal to feel a little overwhelmed during this transition?\nThere are so many different tools/aws services used&lt;/p&gt;\n\n&lt;p&gt;Where as what I am more used to code being the center and you\u2019d have to be accustomed to the libraries/packages that are used\u2026 but I feel like in DE there are much more moving parts not just code.. Or will I overcome this as I build more experience in cloud/infra?\nOh and there is all that governance stuff which makes it feel like things move slower.. (my background in SaaS for start ups)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10vnjnl", "is_robot_indexable": true, "report_reasons": null, "author": "cremebruleeXO", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vnjnl/software_engineering_transitioning_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vnjnl/software_engineering_transitioning_to_data/", "subreddit_subscribers": 88774, "created_utc": 1675730748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "(**Disclaimer**:  I'm not a data engineer and my impression may  be far away fro truth.  You can laugh at me, and please also give constructive answers) \n\nData migration, also called data moving, data replication or data integration, involves copying pre-existing data and synching incremental data.  It doesn't involve \"data processing\" per se,  like data aggregations such as \\`count()\\` .  It's the basis for system integration and data pipeline building. \n\n**I think it's underestimated because the community don't talk enough about its best practices**. \n\n* If you read a book about data pipeline, it may be focusing on data processing and be talking about map/reduce, spark, stateful processing etc. \n* If you do data migration as a software engineer,  chances are you may not even notice there is a concept called \"data pipeline\" and you just implement everything manually.  For example, my previous company implemented pre-existing one-time data migration using some Java code, and implemented data sync using another piece of code.  We didn't know that there is stream-based solution that can do both. \n\n**In short, data engineers don't pay much attention about its patterns (best practices), and software engineers don't even realize that there are patterns for it.**  If you try to search a book called \"data migration patterns\", you can't even find one. \n\n**But I think it deserves a book**.  I've joined a few data migration projects, and I think there are a few **common problems** to handle:\n\n* As I mentioned before, how to do pre-existing data  migration and afterwards data sync using a single solution ?   How to limit down time caused by write lock when generation a snapshot?  When to do a cut off? \n* If \"update\" event reaches the consumer before the \"insert\" event, how do you handle it?  Is \"timestamp\" trustable? \n* If you need to sync data in both ways between 2 systems, how do you avoid  infinite loop of data change propagation? \n* What if Order creation event arrives before its Item creation event ? \n* How to handle event consumption errors?  What if dead letter queue is also not reachable?\n* What are the reconciliation tools for you to verify that you migration result is correct? \n* ... \n\nI think you may finally find all the answers by searching, asking and so on.   But **it would be nice if there is a book addressing all these problems in a comprehensive way**. \n\n**Is there such a book**?  If not,  why doesn't anybody write one?", "author_fullname": "t2_10j5re", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "May I say \"data migration\" is underestimated and can you recommend me a book about its patterns?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10w76tz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675789963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(&lt;strong&gt;Disclaimer&lt;/strong&gt;:  I&amp;#39;m not a data engineer and my impression may  be far away fro truth.  You can laugh at me, and please also give constructive answers) &lt;/p&gt;\n\n&lt;p&gt;Data migration, also called data moving, data replication or data integration, involves copying pre-existing data and synching incremental data.  It doesn&amp;#39;t involve &amp;quot;data processing&amp;quot; per se,  like data aggregations such as `count()` .  It&amp;#39;s the basis for system integration and data pipeline building. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I think it&amp;#39;s underestimated because the community don&amp;#39;t talk enough about its best practices&lt;/strong&gt;. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If you read a book about data pipeline, it may be focusing on data processing and be talking about map/reduce, spark, stateful processing etc. &lt;/li&gt;\n&lt;li&gt;If you do data migration as a software engineer,  chances are you may not even notice there is a concept called &amp;quot;data pipeline&amp;quot; and you just implement everything manually.  For example, my previous company implemented pre-existing one-time data migration using some Java code, and implemented data sync using another piece of code.  We didn&amp;#39;t know that there is stream-based solution that can do both. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;In short, data engineers don&amp;#39;t pay much attention about its patterns (best practices), and software engineers don&amp;#39;t even realize that there are patterns for it.&lt;/strong&gt;  If you try to search a book called &amp;quot;data migration patterns&amp;quot;, you can&amp;#39;t even find one. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;But I think it deserves a book&lt;/strong&gt;.  I&amp;#39;ve joined a few data migration projects, and I think there are a few &lt;strong&gt;common problems&lt;/strong&gt; to handle:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;As I mentioned before, how to do pre-existing data  migration and afterwards data sync using a single solution ?   How to limit down time caused by write lock when generation a snapshot?  When to do a cut off? &lt;/li&gt;\n&lt;li&gt;If &amp;quot;update&amp;quot; event reaches the consumer before the &amp;quot;insert&amp;quot; event, how do you handle it?  Is &amp;quot;timestamp&amp;quot; trustable? &lt;/li&gt;\n&lt;li&gt;If you need to sync data in both ways between 2 systems, how do you avoid  infinite loop of data change propagation? &lt;/li&gt;\n&lt;li&gt;What if Order creation event arrives before its Item creation event ? &lt;/li&gt;\n&lt;li&gt;How to handle event consumption errors?  What if dead letter queue is also not reachable?&lt;/li&gt;\n&lt;li&gt;What are the reconciliation tools for you to verify that you migration result is correct? &lt;/li&gt;\n&lt;li&gt;... &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I think you may finally find all the answers by searching, asking and so on.   But &lt;strong&gt;it would be nice if there is a book addressing all these problems in a comprehensive way&lt;/strong&gt;. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there such a book&lt;/strong&gt;?  If not,  why doesn&amp;#39;t anybody write one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10w76tz", "is_robot_indexable": true, "report_reasons": null, "author": "shaunyip", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10w76tz/may_i_say_data_migration_is_underestimated_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10w76tz/may_i_say_data_migration_is_underestimated_and/", "subreddit_subscribers": 88774, "created_utc": 1675789963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, i would like to know how you differentiate the different layer of description of a data field in a table\n\n\nExample 1 : \n- a field is a varchar 255\n- the data in the varchar 255 is the code of a product, but it s the short code, not the long code\n- the product encoded is a food product\n\nExample 2:\na char(3) contains a code iso country (FRA, USA,..) and the coutry is a coitry of production of a product, \n\nExample 3 : float -&gt; Amount -&gt; Turnover\n\nI guess that char(3) is called a \"physical data type\" ? \nand the country of origin of a product is called \"business term\"  and how is called the ISO codification ?", "author_fullname": "t2_5r1yj502", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data field : what are the levels ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vm67u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675727192.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i would like to know how you differentiate the different layer of description of a data field in a table&lt;/p&gt;\n\n&lt;p&gt;Example 1 : \n- a field is a varchar 255\n- the data in the varchar 255 is the code of a product, but it s the short code, not the long code\n- the product encoded is a food product&lt;/p&gt;\n\n&lt;p&gt;Example 2:\na char(3) contains a code iso country (FRA, USA,..) and the coutry is a coitry of production of a product, &lt;/p&gt;\n\n&lt;p&gt;Example 3 : float -&amp;gt; Amount -&amp;gt; Turnover&lt;/p&gt;\n\n&lt;p&gt;I guess that char(3) is called a &amp;quot;physical data type&amp;quot; ? \nand the country of origin of a product is called &amp;quot;business term&amp;quot;  and how is called the ISO codification ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10vm67u", "is_robot_indexable": true, "report_reasons": null, "author": "PhilippeCN", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vm67u/data_field_what_are_the_levels/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vm67u/data_field_what_are_the_levels/", "subreddit_subscribers": 88774, "created_utc": 1675727192.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my company we have a data stack built around BigQuery, dbt and Airbyte, and we use a lot of Looker dashboards to visualize data, but we've gotten to the point where we have grown out of it, as we really would need more like a web app to be able to interact with the data and have more options for how we present data. We also want to have some public pages. Want to spend minimum amount of time to build a prototype for this. Have checked out Retool and airplane.dev but they were more aimed at appifying admin flows. The option I'm looking into right now is a React/next.js/Mui app hosted in Firebase, with data mirrored from BigQuery to Firestore using Airbyte, but I'd love to know if anyone have used anything more off the shelf...", "author_fullname": "t2_82s0a64", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web app for data heavy company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vfo9r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675711879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my company we have a data stack built around BigQuery, dbt and Airbyte, and we use a lot of Looker dashboards to visualize data, but we&amp;#39;ve gotten to the point where we have grown out of it, as we really would need more like a web app to be able to interact with the data and have more options for how we present data. We also want to have some public pages. Want to spend minimum amount of time to build a prototype for this. Have checked out Retool and airplane.dev but they were more aimed at appifying admin flows. The option I&amp;#39;m looking into right now is a React/next.js/Mui app hosted in Firebase, with data mirrored from BigQuery to Firestore using Airbyte, but I&amp;#39;d love to know if anyone have used anything more off the shelf...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10vfo9r", "is_robot_indexable": true, "report_reasons": null, "author": "Ootoootooo", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vfo9r/web_app_for_data_heavy_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vfo9r/web_app_for_data_heavy_company/", "subreddit_subscribers": 88774, "created_utc": 1675711879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a Jr. Data Engineer in my firm. Our business team often complains about our product dashboard loading time (say about 50 million records of LIVE DATA taking 15 to 30 seconds to load on a series of dashboards deployed on our product UI). \n\nThey are expecting the loading time to decrease or, in fact, remain the same even when the data size increases in the future. \n\nMy team is brainstorming on finding out the industry standards on how much time does a dashboard take to load w.r.t fluctuation in both data size and the number of records, given that the data is fetched \"Live\". \n\nPlease share your reviews. \n\nAlso, how do the tools (power BI, tableau, etc.) and machine specs affect the loading time of a dashboard online?", "author_fullname": "t2_bjkfmyrt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Industry Standards for Dashboard Loading Time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ve0h5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675708063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a Jr. Data Engineer in my firm. Our business team often complains about our product dashboard loading time (say about 50 million records of LIVE DATA taking 15 to 30 seconds to load on a series of dashboards deployed on our product UI). &lt;/p&gt;\n\n&lt;p&gt;They are expecting the loading time to decrease or, in fact, remain the same even when the data size increases in the future. &lt;/p&gt;\n\n&lt;p&gt;My team is brainstorming on finding out the industry standards on how much time does a dashboard take to load w.r.t fluctuation in both data size and the number of records, given that the data is fetched &amp;quot;Live&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;Please share your reviews. &lt;/p&gt;\n\n&lt;p&gt;Also, how do the tools (power BI, tableau, etc.) and machine specs affect the loading time of a dashboard online?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ve0h5", "is_robot_indexable": true, "report_reasons": null, "author": "shuja097", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ve0h5/industry_standards_for_dashboard_loading_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ve0h5/industry_standards_for_dashboard_loading_time/", "subreddit_subscribers": 88774, "created_utc": 1675708063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nRelatively new to the DE field, and I've been handed a confidential .txt file to troubleshoot. It's missing data needed for the portal we upload it to, so I'm hoping to add it in.   \n\n\nIt looks like the first row has one set of fixed-width length requirements, row 2 has another set of character length requirements, and rows 3 &amp; 4 each have their own length requirements, with every pair after that repeating.  \n\n\nAny advice? I'm trying to open it in Excel, but right now my mindset is:\n\n1. Manually separate the data based on what fixed-width requirements it has\n2. Open each in Excel, painstakingly set the fixed-width requirements to match what the portal is requesting\n3. See what's missing, and try to add the missing field (I know of one, just not where it needs to go)\n4. Then combine all data files back into one .txt file again\n\nSeems tedious as hell...I'm a complete noob; is there a more spreamlined or even programmatic way to go about this? This is one of my first big assignments at work, and I'm hoping to solve the problem for them.\n\nThanks in advance!", "author_fullname": "t2_zhui6d5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with fixed-width txt file I'm struggling with", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10w6trg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675789129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Relatively new to the DE field, and I&amp;#39;ve been handed a confidential .txt file to troubleshoot. It&amp;#39;s missing data needed for the portal we upload it to, so I&amp;#39;m hoping to add it in.   &lt;/p&gt;\n\n&lt;p&gt;It looks like the first row has one set of fixed-width length requirements, row 2 has another set of character length requirements, and rows 3 &amp;amp; 4 each have their own length requirements, with every pair after that repeating.  &lt;/p&gt;\n\n&lt;p&gt;Any advice? I&amp;#39;m trying to open it in Excel, but right now my mindset is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Manually separate the data based on what fixed-width requirements it has&lt;/li&gt;\n&lt;li&gt;Open each in Excel, painstakingly set the fixed-width requirements to match what the portal is requesting&lt;/li&gt;\n&lt;li&gt;See what&amp;#39;s missing, and try to add the missing field (I know of one, just not where it needs to go)&lt;/li&gt;\n&lt;li&gt;Then combine all data files back into one .txt file again&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Seems tedious as hell...I&amp;#39;m a complete noob; is there a more spreamlined or even programmatic way to go about this? This is one of my first big assignments at work, and I&amp;#39;m hoping to solve the problem for them.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10w6trg", "is_robot_indexable": true, "report_reasons": null, "author": "Savagecvnt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10w6trg/help_with_fixedwidth_txt_file_im_struggling_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10w6trg/help_with_fixedwidth_txt_file_im_struggling_with/", "subreddit_subscribers": 88774, "created_utc": 1675789129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I helped my company design/build a backend service that allows employees to fuzzy search for customers stored on a SQL Server. Looking for recommendations for making this blazingly fast rather than just pretty fast\n\nThe search requires atleast a 70% match on both first &amp; last name to produce a search match. This way slight misspellings still return results\n\nSome details of things I already did to improve performance are below.\n\n1) Search is stored in a stored proc for plan caching.\n\n\n2) A distinct/de-duplicated list of all customer first &amp; last names are stored in two seperate dedicated tables to use for searching. This reduces the volume of data the pre-search scans significantly. There are indexed columns for name, first two characters of the name, and last two characters of the name.\n\n\n3) 'Pre-search' queries are ran against the distinct name tables to build a list of name matches in #FirstNames and #LastNames temp tables that will be used later to do an exact match join to the actual customer table to get the final search results.\n\n4) The pre-search queries utilize a Levenshtein distance database function stored in a C# CLR on the SQL Server. To reduce the amount of rows that the Levenshtein functions runs against, the WHERE logic additionally requires that either the first two or last two characters in the name be an exact match.\n\n\n5)Once all of the pre-search queries have run sequentially, the final results are found by joining both the #FirstNames and #LastNames temp tables to the Customers table.\n\nI recently heard of 'In Memory Optimize Tables', and was wondering if the distinct name tables used in the presearch would be a good use case to make the pre-search queries faster.\n\nI also suppose some low hanging fruit would be to make the pre-search queries execute in parallel using a UNION ALL", "author_fullname": "t2_8jq30m4x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing a 'Fuzzy' Customer Search using Levenshtein Distance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vtu4k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675747957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I helped my company design/build a backend service that allows employees to fuzzy search for customers stored on a SQL Server. Looking for recommendations for making this blazingly fast rather than just pretty fast&lt;/p&gt;\n\n&lt;p&gt;The search requires atleast a 70% match on both first &amp;amp; last name to produce a search match. This way slight misspellings still return results&lt;/p&gt;\n\n&lt;p&gt;Some details of things I already did to improve performance are below.&lt;/p&gt;\n\n&lt;p&gt;1) Search is stored in a stored proc for plan caching.&lt;/p&gt;\n\n&lt;p&gt;2) A distinct/de-duplicated list of all customer first &amp;amp; last names are stored in two seperate dedicated tables to use for searching. This reduces the volume of data the pre-search scans significantly. There are indexed columns for name, first two characters of the name, and last two characters of the name.&lt;/p&gt;\n\n&lt;p&gt;3) &amp;#39;Pre-search&amp;#39; queries are ran against the distinct name tables to build a list of name matches in #FirstNames and #LastNames temp tables that will be used later to do an exact match join to the actual customer table to get the final search results.&lt;/p&gt;\n\n&lt;p&gt;4) The pre-search queries utilize a Levenshtein distance database function stored in a C# CLR on the SQL Server. To reduce the amount of rows that the Levenshtein functions runs against, the WHERE logic additionally requires that either the first two or last two characters in the name be an exact match.&lt;/p&gt;\n\n&lt;p&gt;5)Once all of the pre-search queries have run sequentially, the final results are found by joining both the #FirstNames and #LastNames temp tables to the Customers table.&lt;/p&gt;\n\n&lt;p&gt;I recently heard of &amp;#39;In Memory Optimize Tables&amp;#39;, and was wondering if the distinct name tables used in the presearch would be a good use case to make the pre-search queries faster.&lt;/p&gt;\n\n&lt;p&gt;I also suppose some low hanging fruit would be to make the pre-search queries execute in parallel using a UNION ALL&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10vtu4k", "is_robot_indexable": true, "report_reasons": null, "author": "patheticadam", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vtu4k/optimizing_a_fuzzy_customer_search_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vtu4k/optimizing_a_fuzzy_customer_search_using/", "subreddit_subscribers": 88774, "created_utc": 1675747957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys. I consider myself a fairly fresh Data Engineer with only 1+ years of experience, and I recently got hired by a large consultancy company that gives me the freedom to grow in almost any direction I want. I am also not that old, which I see as an advantage. I will be straight forward; I would like to earn as much as possible in the near future and would love to get a few tips about some different paths I could take.\n\nPreviously I have worked mostly with stuff like ETL, Snowflake, Dataiku and Power BI. I will be working with Informatica, Tableau and AWS in the near future on a project. I also know Python, but have still not worked profesionally with Python. \n\nMy current goal is to become a mid-level and then Senior DE, but are there other paths that are more rewarding in form of money? I\u2019m even open to \u00abchange lane\u00bb if there is an IT field that pays much more, but I would obviously like to use the knowledge I have gained so far. I have thought about getting into Data Science in the future for example.\n\nThanks in advance.", "author_fullname": "t2_ivln8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Advice for a Junior DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vmlu5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675728303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys. I consider myself a fairly fresh Data Engineer with only 1+ years of experience, and I recently got hired by a large consultancy company that gives me the freedom to grow in almost any direction I want. I am also not that old, which I see as an advantage. I will be straight forward; I would like to earn as much as possible in the near future and would love to get a few tips about some different paths I could take.&lt;/p&gt;\n\n&lt;p&gt;Previously I have worked mostly with stuff like ETL, Snowflake, Dataiku and Power BI. I will be working with Informatica, Tableau and AWS in the near future on a project. I also know Python, but have still not worked profesionally with Python. &lt;/p&gt;\n\n&lt;p&gt;My current goal is to become a mid-level and then Senior DE, but are there other paths that are more rewarding in form of money? I\u2019m even open to \u00abchange lane\u00bb if there is an IT field that pays much more, but I would obviously like to use the knowledge I have gained so far. I have thought about getting into Data Science in the future for example.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10vmlu5", "is_robot_indexable": true, "report_reasons": null, "author": "qozelqort", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vmlu5/career_advice_for_a_junior_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vmlu5/career_advice_for_a_junior_de/", "subreddit_subscribers": 88774, "created_utc": 1675728303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nEach month we receive a file from a third party which contains information on every postcode in the UK. This file is a full refresh every month, and any number of rows could have been updated. \n\nWe need to load these files into Snowflake, and both merge them into a single SCD2 type table and archive the monthly tables directly. \n\nAre there any tools/packages out there which handle this logic for you?\n\nFailing that I suppose we will look to build our own python code that does something like this:\n- Grab csv and load into a single staging table\n- Implement CDC by comparing new rows to main table\n- Insert new rows into main table with new effective from date\n- Retire old records by setting effective to date\n- Clone staging table to archive schema and append table name with file version and/or date\n\nKeen to hear how others would approach this problem. \n\nThanks!", "author_fullname": "t2_ns4u3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Versioning csv loads into Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vj7vz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675720020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Each month we receive a file from a third party which contains information on every postcode in the UK. This file is a full refresh every month, and any number of rows could have been updated. &lt;/p&gt;\n\n&lt;p&gt;We need to load these files into Snowflake, and both merge them into a single SCD2 type table and archive the monthly tables directly. &lt;/p&gt;\n\n&lt;p&gt;Are there any tools/packages out there which handle this logic for you?&lt;/p&gt;\n\n&lt;p&gt;Failing that I suppose we will look to build our own python code that does something like this:\n- Grab csv and load into a single staging table\n- Implement CDC by comparing new rows to main table\n- Insert new rows into main table with new effective from date\n- Retire old records by setting effective to date\n- Clone staging table to archive schema and append table name with file version and/or date&lt;/p&gt;\n\n&lt;p&gt;Keen to hear how others would approach this problem. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10vj7vz", "is_robot_indexable": true, "report_reasons": null, "author": "agribbon", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vj7vz/versioning_csv_loads_into_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vj7vz/versioning_csv_loads_into_snowflake/", "subreddit_subscribers": 88774, "created_utc": 1675720020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I inherited some serious tech debt on a ETL process of about 100 jobs running in cron. I want to convert it to airflow but am thinking it will be too much work for a few engineers to convert them all into DAGs quickly. With the new product we are building, I need insight into when jobs fail and the ability to rest easy that things are running smoothly or know when they aren't. We have some data providers and scraping jobs that can fail without any code change occurring.\n\nI am thinking of still using airflow but having my DAGs call lambda functions and ECS jobs on AWS so I don't have to refactor much code, but simply write cloud formation templates and get them on AWS. Does this make sense to do? I would still like the visibility of airflow and the ability to quickly rerun jobs etc.\n\nEDIT: I used Airflow in my last job so it came to mind first.", "author_fullname": "t2_138qd6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inherited ETL pipeline running crons -- advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vis57", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675719340.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675718972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I inherited some serious tech debt on a ETL process of about 100 jobs running in cron. I want to convert it to airflow but am thinking it will be too much work for a few engineers to convert them all into DAGs quickly. With the new product we are building, I need insight into when jobs fail and the ability to rest easy that things are running smoothly or know when they aren&amp;#39;t. We have some data providers and scraping jobs that can fail without any code change occurring.&lt;/p&gt;\n\n&lt;p&gt;I am thinking of still using airflow but having my DAGs call lambda functions and ECS jobs on AWS so I don&amp;#39;t have to refactor much code, but simply write cloud formation templates and get them on AWS. Does this make sense to do? I would still like the visibility of airflow and the ability to quickly rerun jobs etc.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I used Airflow in my last job so it came to mind first.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10vis57", "is_robot_indexable": true, "report_reasons": null, "author": "daniel_don_diggles", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vis57/inherited_etl_pipeline_running_crons_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vis57/inherited_etl_pipeline_running_crons_advice/", "subreddit_subscribers": 88774, "created_utc": 1675718972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've not heard of anyone insisting on a \"local only\" development workflow until today. I've been asked to do my development (with Databricks as our target environment) using PySpark locally. I'm open to doing development locally, but I'm struggling to make sense of it given the differences between the Databricks environment and my local environment. I've floated the idea of converting notebooks to a local `dbx` process to better bridge the gap between local and Databricks, but that isn't being considered at this time. Is local development for a Databricks workflow something anyone here has encountered? Any clarity/recommendations here would be appreciated!", "author_fullname": "t2_g6i8swsi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Developing for a cloud based workflow locally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vfuff", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675712277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve not heard of anyone insisting on a &amp;quot;local only&amp;quot; development workflow until today. I&amp;#39;ve been asked to do my development (with Databricks as our target environment) using PySpark locally. I&amp;#39;m open to doing development locally, but I&amp;#39;m struggling to make sense of it given the differences between the Databricks environment and my local environment. I&amp;#39;ve floated the idea of converting notebooks to a local &lt;code&gt;dbx&lt;/code&gt; process to better bridge the gap between local and Databricks, but that isn&amp;#39;t being considered at this time. Is local development for a Databricks workflow something anyone here has encountered? Any clarity/recommendations here would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10vfuff", "is_robot_indexable": true, "report_reasons": null, "author": "Busy-Rule8271", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vfuff/developing_for_a_cloud_based_workflow_locally/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vfuff/developing_for_a_cloud_based_workflow_locally/", "subreddit_subscribers": 88774, "created_utc": 1675712277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tested, versioned and robust ELT - building extract &amp; load processes that don't break", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_10w1o0m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kmcE_p2Z81_Q_7-ES2s95tE-Y9Acb1AvUk8-5MZxG1I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675776065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "meltano.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://meltano.com/blog/why-our-el-processes-never-break-tested-versioned-and-robust-elt/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Qpy5HQ-FwLSFdziEc6rbD3Stc-Wgsp1gEIH3jM6QvZM.jpg?auto=webp&amp;v=enabled&amp;s=c8504bcc6a097d996eb206bb20fc65e2acbcf5b2", "width": 1024, "height": 683}, "resolutions": [{"url": "https://external-preview.redd.it/Qpy5HQ-FwLSFdziEc6rbD3Stc-Wgsp1gEIH3jM6QvZM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0bfd70046e09c6b7d23e9a166df4dd8ddf46dfd8", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/Qpy5HQ-FwLSFdziEc6rbD3Stc-Wgsp1gEIH3jM6QvZM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e9fae7e3fe6332e23de3f96355e5195c70644f0", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/Qpy5HQ-FwLSFdziEc6rbD3Stc-Wgsp1gEIH3jM6QvZM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83267278e7a58ace6e532dc2181e2c91e8a05fe7", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/Qpy5HQ-FwLSFdziEc6rbD3Stc-Wgsp1gEIH3jM6QvZM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9befa40d67ef6011f7f934444724b20030247c8", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/Qpy5HQ-FwLSFdziEc6rbD3Stc-Wgsp1gEIH3jM6QvZM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b8e5ef8b3e78c2fe67f8efddac82a6658853c8d", "width": 960, "height": 640}], "variants": {}, "id": "7Z09yx6aSEVdWq6umh2dGNQlNgUiY-9cP6HTecB66rU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10w1o0m", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10w1o0m/tested_versioned_and_robust_elt_building_extract/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://meltano.com/blog/why-our-el-processes-never-break-tested-versioned-and-robust-elt/", "subreddit_subscribers": 88774, "created_utc": 1675776065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all - I am a new DE and needed some help around this. I've got a new JSON-structured Amazon S3 data source that keeps changing its columns (additional columns are sometimes added). How do I map this on a copy node in an ADF pipeline? One day it may have a few more columns and another day it might have a couple less. I need to copy whatever is in the source data file to a table in my SQL server table.  \nCurrently, I have been explicitly defining them after importing the schemas. From my research, dynamic mapping sounds like the right thing to do but not sure how to do this. Wouldn't the destination dataset have to have dynamic/changeable columns?\n\nThank you very much :)", "author_fullname": "t2_if7fad4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mapping for Amazon S3 tables with changing columns in Azure Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vh0wz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675714925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all - I am a new DE and needed some help around this. I&amp;#39;ve got a new JSON-structured Amazon S3 data source that keeps changing its columns (additional columns are sometimes added). How do I map this on a copy node in an ADF pipeline? One day it may have a few more columns and another day it might have a couple less. I need to copy whatever is in the source data file to a table in my SQL server table.&lt;br/&gt;\nCurrently, I have been explicitly defining them after importing the schemas. From my research, dynamic mapping sounds like the right thing to do but not sure how to do this. Wouldn&amp;#39;t the destination dataset have to have dynamic/changeable columns?&lt;/p&gt;\n\n&lt;p&gt;Thank you very much :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10vh0wz", "is_robot_indexable": true, "report_reasons": null, "author": "Fickle-Diet-3060", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vh0wz/mapping_for_amazon_s3_tables_with_changing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vh0wz/mapping_for_amazon_s3_tables_with_changing/", "subreddit_subscribers": 88774, "created_utc": 1675714925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI'm a data analyst looking to move into data engineering and have been increasingly taking on ETL and data validation tasks involving RDBMs. I have also been studying independently after work (Data Warehouse toolkit, Designing Data Intensive Apps, etc.)\n\nOne thing I am still not clear about since I have not worked on all aspects of a pipeline simultaneously is how are different pieces of the pipeline linked. For example, I have found myself very proficient at manipulating data in python, but would be unable to link a \"final\" pd.Dataframe to databricks or spark. How are those things linked in the pipeline. In other words, how does the data actually move from one tool to another. Does it go into a temporary database or csv?\n\nThanks!", "author_fullname": "t2_od6j7g2w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the physical mechanism to link different parts of a pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10vga2k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675713487.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675713271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a data analyst looking to move into data engineering and have been increasingly taking on ETL and data validation tasks involving RDBMs. I have also been studying independently after work (Data Warehouse toolkit, Designing Data Intensive Apps, etc.)&lt;/p&gt;\n\n&lt;p&gt;One thing I am still not clear about since I have not worked on all aspects of a pipeline simultaneously is how are different pieces of the pipeline linked. For example, I have found myself very proficient at manipulating data in python, but would be unable to link a &amp;quot;final&amp;quot; pd.Dataframe to databricks or spark. How are those things linked in the pipeline. In other words, how does the data actually move from one tool to another. Does it go into a temporary database or csv?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10vga2k", "is_robot_indexable": true, "report_reasons": null, "author": "HoggedHedge", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10vga2k/what_are_the_physical_mechanism_to_link_different/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10vga2k/what_are_the_physical_mechanism_to_link_different/", "subreddit_subscribers": 88774, "created_utc": 1675713271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team uses data vault modeling to arrange source data into hubs/satellites/links from which we build traditional dim/fact tables for our users. We get a number of satellites for different entities that end up representing the same thing. For example, address data shared across three separate entities. Should satellite tables ever be shared? Or should there be three separate satellite tables corresponding to address data?", "author_fullname": "t2_5e0ue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Vault modeling question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10w6wtw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675789308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team uses data vault modeling to arrange source data into hubs/satellites/links from which we build traditional dim/fact tables for our users. We get a number of satellites for different entities that end up representing the same thing. For example, address data shared across three separate entities. Should satellite tables ever be shared? Or should there be three separate satellite tables corresponding to address data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10w6wtw", "is_robot_indexable": true, "report_reasons": null, "author": "harrytrumanprimate", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10w6wtw/data_vault_modeling_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10w6wtw/data_vault_modeling_question/", "subreddit_subscribers": 88774, "created_utc": 1675789308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI've been a lurker for some time on this subreddit. I've been looking for a role that I could eventually transition into a data engineering role. I interviewed for this role because of the ETL tasks listed on the job description (see below). The salary is not much different than what I'm currently making as a business analyst (80k).I want to see the group's reaction if this is something that I should be a lateral jump to. The second piece of question is if I got low balled given I'm in the Bay Area location. \n\n&amp;#x200B;\n\n\\[CompanyX\\] is currently seeking a **Business Operations Analyst** with a data engineering or computer science background to provide support to a Department of Defense research laboratory.\n\n**Roles and responsibilities include, but are not limited to the following:**\n\n* Research and make contact with data providers; negotiate the transfer of data from data providers across the DoD.\n* Draft and edit data use agreements with DoD agencies and other data providers, ensuring that all required language and metadata is present for all datasets.\n* Develop amendments, extensions, or addendums to existing agreements based on needs of projects.\n* Coordinate and follow up with data providers, database administrators, and extract, transform, load specialists to ensure appropriate procurement, scheduling, and transfer of data.\n* Document all contacts and correspondence with data providers, changes to data exchanges and efforts made to obtain data transfers.\n* Provide organized, clear and concise software development feedback (from a user's perspective).\n* Develop software development QA checklists and complete user acceptance testing.\n* Executing and monitoring ETL processes; specifically the transfer and tracking of data exchanges.\n* Communicating with clients, developers, and other team members (with both technical and non-technical expertise) in response to data transfer requirements, issues and problem resolution.\n* Troubleshooting issues with ETL connections, performance, or other errors.", "author_fullname": "t2_5ab2w649", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Last interview for a business operation/data analyst role, is this right path to data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10w53ak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675784932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been a lurker for some time on this subreddit. I&amp;#39;ve been looking for a role that I could eventually transition into a data engineering role. I interviewed for this role because of the ETL tasks listed on the job description (see below). The salary is not much different than what I&amp;#39;m currently making as a business analyst (80k).I want to see the group&amp;#39;s reaction if this is something that I should be a lateral jump to. The second piece of question is if I got low balled given I&amp;#39;m in the Bay Area location. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;[CompanyX] is currently seeking a &lt;strong&gt;Business Operations Analyst&lt;/strong&gt; with a data engineering or computer science background to provide support to a Department of Defense research laboratory.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Roles and responsibilities include, but are not limited to the following:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Research and make contact with data providers; negotiate the transfer of data from data providers across the DoD.&lt;/li&gt;\n&lt;li&gt;Draft and edit data use agreements with DoD agencies and other data providers, ensuring that all required language and metadata is present for all datasets.&lt;/li&gt;\n&lt;li&gt;Develop amendments, extensions, or addendums to existing agreements based on needs of projects.&lt;/li&gt;\n&lt;li&gt;Coordinate and follow up with data providers, database administrators, and extract, transform, load specialists to ensure appropriate procurement, scheduling, and transfer of data.&lt;/li&gt;\n&lt;li&gt;Document all contacts and correspondence with data providers, changes to data exchanges and efforts made to obtain data transfers.&lt;/li&gt;\n&lt;li&gt;Provide organized, clear and concise software development feedback (from a user&amp;#39;s perspective).&lt;/li&gt;\n&lt;li&gt;Develop software development QA checklists and complete user acceptance testing.&lt;/li&gt;\n&lt;li&gt;Executing and monitoring ETL processes; specifically the transfer and tracking of data exchanges.&lt;/li&gt;\n&lt;li&gt;Communicating with clients, developers, and other team members (with both technical and non-technical expertise) in response to data transfer requirements, issues and problem resolution.&lt;/li&gt;\n&lt;li&gt;Troubleshooting issues with ETL connections, performance, or other errors.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10w53ak", "is_robot_indexable": true, "report_reasons": null, "author": "soywaxblend", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10w53ak/last_interview_for_a_business_operationdata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10w53ak/last_interview_for_a_business_operationdata/", "subreddit_subscribers": 88774, "created_utc": 1675784932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A deep dive on the cost/performance impact of driver sizing in Databricks with the TPC-DS 1TB benchmark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_10w4dsm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JDL8QyvkYZ4wWFWR55z84yQr2UO1GlMY5xWjOpLTTTY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675783175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/sync-computing/databricks-driver-sizing-impact-on-cost-and-performance-3b93aa2e3b9a", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dav6aLUhAa77nUEMDUjXwiQXs-Vxuv1n8dcbFu5sc5Q.jpg?auto=webp&amp;v=enabled&amp;s=f3f409125e3c06bdcbd3f33be8b19dee2c02a8ad", "width": 1200, "height": 799}, "resolutions": [{"url": "https://external-preview.redd.it/dav6aLUhAa77nUEMDUjXwiQXs-Vxuv1n8dcbFu5sc5Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd05e3a96b065db5873da1939a5e748b1da4e2a0", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/dav6aLUhAa77nUEMDUjXwiQXs-Vxuv1n8dcbFu5sc5Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=921a7787bee542fa32ac250caf1ec7b978ccc8f5", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/dav6aLUhAa77nUEMDUjXwiQXs-Vxuv1n8dcbFu5sc5Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e9ee1dc95ef59b67e1d762d8d48d7fc23c8e767", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/dav6aLUhAa77nUEMDUjXwiQXs-Vxuv1n8dcbFu5sc5Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e214cf63cd0f1f5adbbc0ec5085a4e05059a5757", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/dav6aLUhAa77nUEMDUjXwiQXs-Vxuv1n8dcbFu5sc5Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83eb138c3a702bfa9240edbd11e80407cff6bda0", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/dav6aLUhAa77nUEMDUjXwiQXs-Vxuv1n8dcbFu5sc5Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58e97a02a85f2ef3e98b37bb7910aac368a659fe", "width": 1080, "height": 719}], "variants": {}, "id": "mvEKM_OGy5Q350p3hPKKhlpJeSBVmpAY1tZocF_s41M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10w4dsm", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10w4dsm/a_deep_dive_on_the_costperformance_impact_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/sync-computing/databricks-driver-sizing-impact-on-cost-and-performance-3b93aa2e3b9a", "subreddit_subscribers": 88774, "created_utc": 1675783175.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}