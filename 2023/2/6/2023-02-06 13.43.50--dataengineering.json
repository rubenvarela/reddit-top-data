{"kind": "Listing", "data": {"after": "t3_10v0byb", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "8 Key Data Structures That Power Modern Databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_10uhjfg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": "#46d160", "ups": 73, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/W_v05d_2RTo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"8 Key Data Structures That Power Modern Databases\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "8 Key Data Structures That Power Modern Databases", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/W_v05d_2RTo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"8 Key Data Structures That Power Modern Databases\"&gt;&lt;/iframe&gt;", "author_name": "ByteByteGo", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/W_v05d_2RTo/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ByteByteGo"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/W_v05d_2RTo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"8 Key Data Structures That Power Modern Databases\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/10uhjfg", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zY0b9zsXDpbRf4m_tPBmFKYlKJ6MSVOij3Tfm06fdVg.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675616973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/W_v05d_2RTo", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dbJ3NYIMGPA-tUGeY-cL9GBgtPbMOSoYKQMC4g828uY.jpg?auto=webp&amp;v=enabled&amp;s=36fd6f07ef58b45a2cd4c607c0259c4f3146866a", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/dbJ3NYIMGPA-tUGeY-cL9GBgtPbMOSoYKQMC4g828uY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8078f2ad7b737b1dad91897dd3e9d27f557a65be", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/dbJ3NYIMGPA-tUGeY-cL9GBgtPbMOSoYKQMC4g828uY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33b49517a2ffaa921a06edeb42cb716b26c90f96", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/dbJ3NYIMGPA-tUGeY-cL9GBgtPbMOSoYKQMC4g828uY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abdfd844cefa928050fbb186669a1f488e1e5895", "width": 320, "height": 240}], "variants": {}, "id": "YlNv470TRLtro5y24MpTmp3uzYVNCkTm5WFEoIAKU44"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10uhjfg", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/10uhjfg/8_key_data_structures_that_power_modern_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/W_v05d_2RTo", "subreddit_subscribers": 88666, "created_utc": 1675616973.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "8 Key Data Structures That Power Modern Databases", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/W_v05d_2RTo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"8 Key Data Structures That Power Modern Databases\"&gt;&lt;/iframe&gt;", "author_name": "ByteByteGo", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/W_v05d_2RTo/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ByteByteGo"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Over the past while I've been battling with some local pyspark and docker-compose setups to have better tests for CI/CD.   \nTo path to solve this problem was to try get the same level of depth of environment that runs in production in the company I work at (databricks, AWS EMR, S3, etc.) as locally  \n\n\nFigured I'd share out some of wrapped up knowledge of battling with specific Jars and hive-metastore (which you don't actually need, just a SQL database set up in certain way!) \n\nThe repo has the following features in it  \n \n\n* Quick and Easy setup for local testing and development environment and abstracting the complexity of configuring up a pyspark environment in this way\n* Full Pyspark Implementation\n* Full S3 like implementation with Minio\n* Read and write data to S3 and access them as tables and databases in Spark through metastore\n* Ability to run pytest on the pyspark container\n* Read and write with delta format\n* Example of testing pyspark code with either unittest or pytest\n* Consistent environment for testing and development with docker-compose and poetry\n* Ability to run tests on push with github actions  \n\n\nIf I've missed anything please feel free to let me know! Hope a few of you find this useful\n\n[https://github.com/emmc15/pyspark-testing-env](https://github.com/emmc15/pyspark-testing-env)", "author_fullname": "t2_1luxgh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End to End Pyspark Testing CI/CD Example Repo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10uw3rn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675653318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past while I&amp;#39;ve been battling with some local pyspark and docker-compose setups to have better tests for CI/CD.&lt;br/&gt;\nTo path to solve this problem was to try get the same level of depth of environment that runs in production in the company I work at (databricks, AWS EMR, S3, etc.) as locally  &lt;/p&gt;\n\n&lt;p&gt;Figured I&amp;#39;d share out some of wrapped up knowledge of battling with specific Jars and hive-metastore (which you don&amp;#39;t actually need, just a SQL database set up in certain way!) &lt;/p&gt;\n\n&lt;p&gt;The repo has the following features in it  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Quick and Easy setup for local testing and development environment and abstracting the complexity of configuring up a pyspark environment in this way&lt;/li&gt;\n&lt;li&gt;Full Pyspark Implementation&lt;/li&gt;\n&lt;li&gt;Full S3 like implementation with Minio&lt;/li&gt;\n&lt;li&gt;Read and write data to S3 and access them as tables and databases in Spark through metastore&lt;/li&gt;\n&lt;li&gt;Ability to run pytest on the pyspark container&lt;/li&gt;\n&lt;li&gt;Read and write with delta format&lt;/li&gt;\n&lt;li&gt;Example of testing pyspark code with either unittest or pytest&lt;/li&gt;\n&lt;li&gt;Consistent environment for testing and development with docker-compose and poetry&lt;/li&gt;\n&lt;li&gt;Ability to run tests on push with github actions&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If I&amp;#39;ve missed anything please feel free to let me know! Hope a few of you find this useful&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/emmc15/pyspark-testing-env\"&gt;https://github.com/emmc15/pyspark-testing-env&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZfAr6jI2jlrWEnHJDFKxQAp9zQbUfDMITmW6A9qaMbg.jpg?auto=webp&amp;v=enabled&amp;s=c2358ececabd505f63105d6d13d81226571e3796", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ZfAr6jI2jlrWEnHJDFKxQAp9zQbUfDMITmW6A9qaMbg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=80ea760474f8bab8cbd7883eef62ec51edbd2b9f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ZfAr6jI2jlrWEnHJDFKxQAp9zQbUfDMITmW6A9qaMbg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa6274870102f08fe20b60ed21e0238487e1966b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ZfAr6jI2jlrWEnHJDFKxQAp9zQbUfDMITmW6A9qaMbg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3362beccb909ec923ced39673009cefd8833238", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ZfAr6jI2jlrWEnHJDFKxQAp9zQbUfDMITmW6A9qaMbg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a17aeb24075702d4cde11f0e06420134c0530494", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ZfAr6jI2jlrWEnHJDFKxQAp9zQbUfDMITmW6A9qaMbg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=810dd44d12ade2c5af041892f8a333a808d30b19", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ZfAr6jI2jlrWEnHJDFKxQAp9zQbUfDMITmW6A9qaMbg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f69ae4ae95c021e6da3a3a1a44dfd61ef1b55fbf", "width": 1080, "height": 540}], "variants": {}, "id": "bBB2SnUsapw54XjP727vA3nYYJUF_fkOqSnGxZn2yiU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "10uw3rn", "is_robot_indexable": true, "report_reasons": null, "author": "Oct8-Danger", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10uw3rn/end_to_end_pyspark_testing_cicd_example_repo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10uw3rn/end_to_end_pyspark_testing_cicd_example_repo/", "subreddit_subscribers": 88666, "created_utc": 1675653318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I\u2019m currently a senior data analyst and I\u2019m looking to make the move to the DE side. My day to day deals with SQL, Power BI and some python for the manipulation of datasets. Started being involved with some DE projects and it seemed very interesting. Can you guys recommend the best books or material for me to learn the basics of DE?", "author_fullname": "t2_202bqogr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best books or material to learn the basics of data engineering.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10uu1j4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675647494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I\u2019m currently a senior data analyst and I\u2019m looking to make the move to the DE side. My day to day deals with SQL, Power BI and some python for the manipulation of datasets. Started being involved with some DE projects and it seemed very interesting. Can you guys recommend the best books or material for me to learn the basics of DE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10uu1j4", "is_robot_indexable": true, "report_reasons": null, "author": "lramirez27", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10uu1j4/best_books_or_material_to_learn_the_basics_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10uu1j4/best_books_or_material_to_learn_the_basics_of/", "subreddit_subscribers": 88666, "created_utc": 1675647494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, what's the situation with the job market right now? Have any of you received callbacks for job interviews? How many job applications have you submitted and how many interviews did you get? I feel like the job market is really dry right now. I have been applying for jobs but haven't heard back.\nDo you think one should utilise this time to prep for the interviews and keep improving skills till the job market gets stable or continue applying?", "author_fullname": "t2_7m65ddby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current Job Market", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10udqar", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675607289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, what&amp;#39;s the situation with the job market right now? Have any of you received callbacks for job interviews? How many job applications have you submitted and how many interviews did you get? I feel like the job market is really dry right now. I have been applying for jobs but haven&amp;#39;t heard back.\nDo you think one should utilise this time to prep for the interviews and keep improving skills till the job market gets stable or continue applying?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10udqar", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive_Ball_965", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10udqar/current_job_market/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10udqar/current_job_market/", "subreddit_subscribers": 88666, "created_utc": 1675607289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Made it through to the second round of interviews for an entry level Data Engineering role. First interview was all SQL, which I\u2019m mostly comfortable with since as current Business Analyst, I use it in my day to day. Within one problem I had to demo Joins, aggregate functions, CASE statements, CTE and Window Functions. \n\nI was notified that for the second interview it will be Python which I have a very general, very basic understanding of. What in your opinion should I expect for the Python interview? I\u2019m looking to determine which areas of Python I should spend my time studying and practicing before the interview. Please note that this is an Entry level role, and the hiring manager did mention that the person hired would spend most of the time working with SQL. I\u2019m not sure what to expect, so not sure where I should spend my time on. What in your opinion are the Python foundations for DE?", "author_fullname": "t2_ry76f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10um7h4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675628098.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Made it through to the second round of interviews for an entry level Data Engineering role. First interview was all SQL, which I\u2019m mostly comfortable with since as current Business Analyst, I use it in my day to day. Within one problem I had to demo Joins, aggregate functions, CASE statements, CTE and Window Functions. &lt;/p&gt;\n\n&lt;p&gt;I was notified that for the second interview it will be Python which I have a very general, very basic understanding of. What in your opinion should I expect for the Python interview? I\u2019m looking to determine which areas of Python I should spend my time studying and practicing before the interview. Please note that this is an Entry level role, and the hiring manager did mention that the person hired would spend most of the time working with SQL. I\u2019m not sure what to expect, so not sure where I should spend my time on. What in your opinion are the Python foundations for DE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10um7h4", "is_robot_indexable": true, "report_reasons": null, "author": "CosmicNightmare", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10um7h4/python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10um7h4/python/", "subreddit_subscribers": 88666, "created_utc": 1675628098.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \nI'm looking for an open-source data lineage app (e.g. [tokern](https://open-metadata.org/), [datahubproject](https://datahubproject.io/), [openmetadata](https://open-metadata.org/)).  \n\nThe key issue is that in addition to automatic ingestion  connectors or deriving schemas from queries, **the tool should also have a friendly way of adding objects and their relationships from something like a csv or json file.**\n\nContext: company has been documenting all its data objects manually and has a large csv explicitly showing each data object and its predescessor/s. These aren't just the standard  database/workflow/dashboard objects; these include things like power automate scripts.  I'm just looking for a good way to show everything in a map, visualize them, and navigate through their connections properly)\n\nAt this point, I'll even be happy with a pure visualization engine, like for instance if I can repurpose [kedro-viz](https://github.com/kedro-org/kedro-viz) or [dbt's](https://docs.getdbt.com/docs/build/python-models) lineage visualizer so that it can take a csv or json of object relationships as an input.   Or even a custom power BI visualization or python graph frontend would be fine, but I can't seem to see one that works.  I'd also be happy if any of the aforementioned lineage tools I mentioned above have this functionality and I just missed it.\n\nThanks everyone!", "author_fullname": "t2_4xg4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an open-source data lineage app, where objects and connections can be manually defined (not just automatically ingested)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10usa5i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675643058.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675642860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, \nI&amp;#39;m looking for an open-source data lineage app (e.g. &lt;a href=\"https://open-metadata.org/\"&gt;tokern&lt;/a&gt;, &lt;a href=\"https://datahubproject.io/\"&gt;datahubproject&lt;/a&gt;, &lt;a href=\"https://open-metadata.org/\"&gt;openmetadata&lt;/a&gt;).  &lt;/p&gt;\n\n&lt;p&gt;The key issue is that in addition to automatic ingestion  connectors or deriving schemas from queries, &lt;strong&gt;the tool should also have a friendly way of adding objects and their relationships from something like a csv or json file.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Context: company has been documenting all its data objects manually and has a large csv explicitly showing each data object and its predescessor/s. These aren&amp;#39;t just the standard  database/workflow/dashboard objects; these include things like power automate scripts.  I&amp;#39;m just looking for a good way to show everything in a map, visualize them, and navigate through their connections properly)&lt;/p&gt;\n\n&lt;p&gt;At this point, I&amp;#39;ll even be happy with a pure visualization engine, like for instance if I can repurpose &lt;a href=\"https://github.com/kedro-org/kedro-viz\"&gt;kedro-viz&lt;/a&gt; or &lt;a href=\"https://docs.getdbt.com/docs/build/python-models\"&gt;dbt&amp;#39;s&lt;/a&gt; lineage visualizer so that it can take a csv or json of object relationships as an input.   Or even a custom power BI visualization or python graph frontend would be fine, but I can&amp;#39;t seem to see one that works.  I&amp;#39;d also be happy if any of the aforementioned lineage tools I mentioned above have this functionality and I just missed it.&lt;/p&gt;\n\n&lt;p&gt;Thanks everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ntYU6Ou1ez-oMJPwzMpA2muqo-5TYgKpQ1CdS81RNfs.jpg?auto=webp&amp;v=enabled&amp;s=9948f8addaa2e47ee79511ae45cff1d9245dc66a", "width": 310, "height": 310}, "resolutions": [{"url": "https://external-preview.redd.it/ntYU6Ou1ez-oMJPwzMpA2muqo-5TYgKpQ1CdS81RNfs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e33b566892d18b692a63009505235b2706b86fd6", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ntYU6Ou1ez-oMJPwzMpA2muqo-5TYgKpQ1CdS81RNfs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc7224a6096e18b8bfa81e39e73c18df5bc11a3c", "width": 216, "height": 216}], "variants": {}, "id": "HAPFDFRRMoP2a9fJFsIVmEt8sTvE02WcjtCO87LuE3s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10usa5i", "is_robot_indexable": true, "report_reasons": null, "author": "efofecks", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10usa5i/looking_for_an_opensource_data_lineage_app_where/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10usa5i/looking_for_an_opensource_data_lineage_app_where/", "subreddit_subscribers": 88666, "created_utc": 1675642860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've looked in the wiki. There's a nice brief but I'm looking for a more in depth write up, video, book, etc regarding setting up a data lake. Any pointers to some? Or any personal anecdotes and lessons learned?", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have a pointer to a more technical reference for creating a data lake from scratch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ufwvb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675612935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve looked in the wiki. There&amp;#39;s a nice brief but I&amp;#39;m looking for a more in depth write up, video, book, etc regarding setting up a data lake. Any pointers to some? Or any personal anecdotes and lessons learned?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ufwvb", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ufwvb/anyone_have_a_pointer_to_a_more_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ufwvb/anyone_have_a_pointer_to_a_more_technical/", "subreddit_subscribers": 88666, "created_utc": 1675612935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\\[Databricks newbie here\\] Hello, I need to download lots (a few tens of thousands) of datasets from Databricks (I need to train a ML model), and I am looking for a way to make it fast. Does anyone have suggestions on how to proceed?\n\nAt the moment my procedure is the following:\n\n1. Retrieve dataframe using \\`spark.sql\\`\n2. Select the dataframe column which we want to consider\n3. Display dataframe and save result\n\nI also tried using \\`df1.coalesce(1, shuffle = true).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"dbfs:/FileStore/user/me/output.csv\")\\` but it's unbearably slow.\n\nWhat is the recommended way to retrieve data and export it fast? Thank you.", "author_fullname": "t2_c75nc3n7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a fast way to get lots of data from Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10udui5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675613529.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675607602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[Databricks newbie here] Hello, I need to download lots (a few tens of thousands) of datasets from Databricks (I need to train a ML model), and I am looking for a way to make it fast. Does anyone have suggestions on how to proceed?&lt;/p&gt;\n\n&lt;p&gt;At the moment my procedure is the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Retrieve dataframe using `spark.sql`&lt;/li&gt;\n&lt;li&gt;Select the dataframe column which we want to consider&lt;/li&gt;\n&lt;li&gt;Display dataframe and save result&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I also tried using `df1.coalesce(1, shuffle = true).write.format(&amp;quot;com.databricks.spark.csv&amp;quot;).option(&amp;quot;header&amp;quot;, &amp;quot;true&amp;quot;).save(&amp;quot;dbfs:/FileStore/user/me/output.csv&amp;quot;)` but it&amp;#39;s unbearably slow.&lt;/p&gt;\n\n&lt;p&gt;What is the recommended way to retrieve data and export it fast? Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10udui5", "is_robot_indexable": true, "report_reasons": null, "author": "BackgroundPass2082", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10udui5/looking_for_a_fast_way_to_get_lots_of_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10udui5/looking_for_a_fast_way_to_get_lots_of_data_from/", "subreddit_subscribers": 88666, "created_utc": 1675607602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm quite new to data engineering but I'm doing more of it at work (started out as a data analyst but the role involves more and more engineering stuff). We use AWS - most of our pipeline is made up of a bunch of python Lambdas and a script that runs in an EC2 instance, and it's orchestrated using Step Functions.\n\nI'm aware that Airflow is a standard tool for organising data pipelines but I'm confused about when I should be thinking about using it. Given I'm using  AWS Step Functions, what exactly would I get out of using Airflow? And given that most companies are using cloud platforms that presumably have similar services, why is Airflow still used? Is it that it is more transferrable if we choose to migrate to GCP or Azure or whatever?  I feel like I'm missing something here.", "author_fullname": "t2_1znkakv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow vs AWS Step Functions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ujaxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675621188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m quite new to data engineering but I&amp;#39;m doing more of it at work (started out as a data analyst but the role involves more and more engineering stuff). We use AWS - most of our pipeline is made up of a bunch of python Lambdas and a script that runs in an EC2 instance, and it&amp;#39;s orchestrated using Step Functions.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware that Airflow is a standard tool for organising data pipelines but I&amp;#39;m confused about when I should be thinking about using it. Given I&amp;#39;m using  AWS Step Functions, what exactly would I get out of using Airflow? And given that most companies are using cloud platforms that presumably have similar services, why is Airflow still used? Is it that it is more transferrable if we choose to migrate to GCP or Azure or whatever?  I feel like I&amp;#39;m missing something here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ujaxk", "is_robot_indexable": true, "report_reasons": null, "author": "Psychological-Suit-5", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ujaxk/airflow_vs_aws_step_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ujaxk/airflow_vs_aws_step_functions/", "subreddit_subscribers": 88666, "created_utc": 1675621188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a cross-post from \"r/learnpython\", but I thought this community might have more relevant experience and insights. I just started learning Python, and am trying to simulate some scenarios I might encounter in my job as a Data Engineer (mostly ETL-related). I'm ready to try to set up my first official \"Project\" for reading a certain category of flat files and loading them to a DB. Does anyone have any advice or good articles on how to structure things properly for Prod / Non-Prod activities?\n\nFor example, I'm going to build a separate module to create my SQL Alchemy engine objects that can be reused for multiple sources / targets / environments. What might be a good way to specify if I want a Dev or a Prod connection back? Do you pass a parameter into the module? Use configuration files and have the module figure out which environment it is running in? Or maybe something like different virtual environments for Dev and Prod? Any advice or insight would be greatly appreciated!", "author_fullname": "t2_14wbya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Prod / Non-Prod Configuration Advice for Python ETL Jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ucw8n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675604990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a cross-post from &amp;quot;&lt;a href=\"/r/learnpython\"&gt;r/learnpython&lt;/a&gt;&amp;quot;, but I thought this community might have more relevant experience and insights. I just started learning Python, and am trying to simulate some scenarios I might encounter in my job as a Data Engineer (mostly ETL-related). I&amp;#39;m ready to try to set up my first official &amp;quot;Project&amp;quot; for reading a certain category of flat files and loading them to a DB. Does anyone have any advice or good articles on how to structure things properly for Prod / Non-Prod activities?&lt;/p&gt;\n\n&lt;p&gt;For example, I&amp;#39;m going to build a separate module to create my SQL Alchemy engine objects that can be reused for multiple sources / targets / environments. What might be a good way to specify if I want a Dev or a Prod connection back? Do you pass a parameter into the module? Use configuration files and have the module figure out which environment it is running in? Or maybe something like different virtual environments for Dev and Prod? Any advice or insight would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ucw8n", "is_robot_indexable": true, "report_reasons": null, "author": "phobia42", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ucw8n/seeking_prod_nonprod_configuration_advice_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ucw8n/seeking_prod_nonprod_configuration_advice_for/", "subreddit_subscribers": 88666, "created_utc": 1675604990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I've been lucky enough to have received 3 data engineering internship offers for this summer which are giving me a tough time making a decision. Any advice would be appreciated.\n\nAbout me: SWE student with 2 C++ internships under my belt. While I do know basic data engineering concepts (Data lake/warehouse, ETL, APIs, data pipeline, etc.), I don't really have experience using any data tools and technologies. I'm really just looking for the option that will give me the best start towards the data engineer path.\n\n&amp;#x200B;\n\nCompany A: Medium-sized company specializing in wealth management solutions.\n\n\\- Tech stack: Python, SQL, Airflow, Db2, TFS\n\n\\- Pay: 22$/h\n\n\\- Remote\n\n\\- Small team of 2 data engineers, 1 tech lead and 1 manager. \n\n&amp;#x200B;\n\nCompany B: small-sized consulting company specializing in delivering AI solutions to businesses.\n\n\\- Tech stack: depends on the client's needs but the team said it is often Python, Spark, Databricks, Azure, Git, Kedro\n\n\\- Pay: 25$/h\n\n\\- Remote\n\n\\- Small team of 1 data engineer, 2 data scientists and 1 manager. \n\n&amp;#x200B;\n\nCompany C: large-sized company specializing in conversational AI solutions.\n\n\\- Tech stack: Java, Spark, Azure Databricks, Delta Lake, Git, Kafka, Spring\n\n\\- Pay: 25$/h\n\n\\- Remote\n\n\\- Team size: 9-10", "author_fullname": "t2_58yrswvw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for choosing between internship offers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10uke9m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675623752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve been lucky enough to have received 3 data engineering internship offers for this summer which are giving me a tough time making a decision. Any advice would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;About me: SWE student with 2 C++ internships under my belt. While I do know basic data engineering concepts (Data lake/warehouse, ETL, APIs, data pipeline, etc.), I don&amp;#39;t really have experience using any data tools and technologies. I&amp;#39;m really just looking for the option that will give me the best start towards the data engineer path.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Company A: Medium-sized company specializing in wealth management solutions.&lt;/p&gt;\n\n&lt;p&gt;- Tech stack: Python, SQL, Airflow, Db2, TFS&lt;/p&gt;\n\n&lt;p&gt;- Pay: 22$/h&lt;/p&gt;\n\n&lt;p&gt;- Remote&lt;/p&gt;\n\n&lt;p&gt;- Small team of 2 data engineers, 1 tech lead and 1 manager. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Company B: small-sized consulting company specializing in delivering AI solutions to businesses.&lt;/p&gt;\n\n&lt;p&gt;- Tech stack: depends on the client&amp;#39;s needs but the team said it is often Python, Spark, Databricks, Azure, Git, Kedro&lt;/p&gt;\n\n&lt;p&gt;- Pay: 25$/h&lt;/p&gt;\n\n&lt;p&gt;- Remote&lt;/p&gt;\n\n&lt;p&gt;- Small team of 1 data engineer, 2 data scientists and 1 manager. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Company C: large-sized company specializing in conversational AI solutions.&lt;/p&gt;\n\n&lt;p&gt;- Tech stack: Java, Spark, Azure Databricks, Delta Lake, Git, Kafka, Spring&lt;/p&gt;\n\n&lt;p&gt;- Pay: 25$/h&lt;/p&gt;\n\n&lt;p&gt;- Remote&lt;/p&gt;\n\n&lt;p&gt;- Team size: 9-10&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10uke9m", "is_robot_indexable": true, "report_reasons": null, "author": "Promettre", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10uke9m/advice_for_choosing_between_internship_offers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10uke9m/advice_for_choosing_between_internship_offers/", "subreddit_subscribers": 88666, "created_utc": 1675623752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!  \nI'm looking for a managed service/tool that can call an API and ingest it into a datawarehouse(Snowflake).  \n\n\nHope that anyone can help me in the right direction \ud83e\udd1e", "author_fullname": "t2_oeiyrmpr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managed service for API calls", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10uhri4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675617506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;br/&gt;\nI&amp;#39;m looking for a managed service/tool that can call an API and ingest it into a datawarehouse(Snowflake).  &lt;/p&gt;\n\n&lt;p&gt;Hope that anyone can help me in the right direction \ud83e\udd1e&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10uhri4", "is_robot_indexable": true, "report_reasons": null, "author": "formaldehyden", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10uhri4/managed_service_for_api_calls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10uhri4/managed_service_for_api_calls/", "subreddit_subscribers": 88666, "created_utc": 1675617506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For a data analysis job I need a OLAP database. I\u2018m considering Druid because it\u2019s scalable, real-time and can use mini.io as deep storage. Because we use min.io, this is a nice feature. \n\nDo you have any experiences with the challenges Druid puts onto you team or good advices for alternatives? From what I see, managing the cluster could be a bigger effort.", "author_fullname": "t2_s3omzbn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your OLAP Database recommendation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ulbai", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675625954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For a data analysis job I need a OLAP database. I\u2018m considering Druid because it\u2019s scalable, real-time and can use mini.io as deep storage. Because we use min.io, this is a nice feature. &lt;/p&gt;\n\n&lt;p&gt;Do you have any experiences with the challenges Druid puts onto you team or good advices for alternatives? From what I see, managing the cluster could be a bigger effort.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ulbai", "is_robot_indexable": true, "report_reasons": null, "author": "ZenCoding", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ulbai/whats_your_olap_database_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ulbai/whats_your_olap_database_recommendation/", "subreddit_subscribers": 88666, "created_utc": 1675625954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am new to the community and I am pretty sure that this has been discussed before. What do you think about the future-proof of Data Engineers? Sure the generated amount of data keeps growing year after year but with the all these automations don't you think that DE may be replaced or may become redundant role? Do you think that other software engineering positions are more future-proofed like full stack engineers (maybe with web3/blockchain) or mobile developers? (excluding ML/AI engineers as the answer there is obvious). Also with which career do you think it would be easier to create a startup company?\nFor me: I have been working as an Integrations Data Engineer for 2 years and I have been using ELT/ETL and other tools that are not so popular and widely used. So after these years I am asking myself where do I fit into the marketplace, does DE have a future, should I consider it over other career paths? Also since I am a new, young software engineer and I haven't worked in the web/mobile sectors (maybe the most popular one), I am not totally sure which career is best for me. Hence, I am wondering whether or not to give it (to other career paths) a try but at the same time I don't want to put time into something that may disappear in the near future. \nAlso IMO, the knowledge that a full stack engineer has, is broader thus it may be transferred in a lot of different area if needed. Compared to DE where it is more specialised.\n\nI appreciate every opinion and comment.", "author_fullname": "t2_h2lk2bhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE career advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10v1ash", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675670911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am new to the community and I am pretty sure that this has been discussed before. What do you think about the future-proof of Data Engineers? Sure the generated amount of data keeps growing year after year but with the all these automations don&amp;#39;t you think that DE may be replaced or may become redundant role? Do you think that other software engineering positions are more future-proofed like full stack engineers (maybe with web3/blockchain) or mobile developers? (excluding ML/AI engineers as the answer there is obvious). Also with which career do you think it would be easier to create a startup company?\nFor me: I have been working as an Integrations Data Engineer for 2 years and I have been using ELT/ETL and other tools that are not so popular and widely used. So after these years I am asking myself where do I fit into the marketplace, does DE have a future, should I consider it over other career paths? Also since I am a new, young software engineer and I haven&amp;#39;t worked in the web/mobile sectors (maybe the most popular one), I am not totally sure which career is best for me. Hence, I am wondering whether or not to give it (to other career paths) a try but at the same time I don&amp;#39;t want to put time into something that may disappear in the near future. \nAlso IMO, the knowledge that a full stack engineer has, is broader thus it may be transferred in a lot of different area if needed. Compared to DE where it is more specialised.&lt;/p&gt;\n\n&lt;p&gt;I appreciate every opinion and comment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10v1ash", "is_robot_indexable": true, "report_reasons": null, "author": "Kind-Interaction646", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10v1ash/de_career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10v1ash/de_career_advice/", "subreddit_subscribers": 88666, "created_utc": 1675670911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have any favorite resources for reviewing common spark interview questions? Things like optimization, scaling, and such. Thanks!", "author_fullname": "t2_2vuapfhl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Interview Prep Resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ux96m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675656794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any favorite resources for reviewing common spark interview questions? Things like optimization, scaling, and such. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ux96m", "is_robot_indexable": true, "report_reasons": null, "author": "TheShitStorms92", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ux96m/spark_interview_prep_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ux96m/spark_interview_prep_resources/", "subreddit_subscribers": 88666, "created_utc": 1675656794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I am extremely new to dagster and I couldn't find any information about the questions I have. Basically I want to know 2 things:  \n\n\n1. Can you trigger a dagster pipeline via an api call? Can I integrate this with other API libraries like FastAPI or Django REST?\n2. Can I pass data via path- or get-params in the api call to the asset functions themselves?\n\nMore info in this [stackoverflow post I made](https://stackoverflow.com/questions/75350111/dagster-can-you-trigger-a-job-to-run-via-an-api), but basically I have a pipeline that is written in pure python, that receives a player's username via an api call, and then runs the entire pipeline to get all the data for *that particular player only.* I want to know if this is possible, or if dagster is only good for data that doesn't require pre-defined parameters\n\nAny help is greatly appreciated, thanks!", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to trigger a dagster pipeline from an api endpoint, and also pass metadata/data via get params to some of the assets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10uok0y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675633671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I am extremely new to dagster and I couldn&amp;#39;t find any information about the questions I have. Basically I want to know 2 things:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Can you trigger a dagster pipeline via an api call? Can I integrate this with other API libraries like FastAPI or Django REST?&lt;/li&gt;\n&lt;li&gt;Can I pass data via path- or get-params in the api call to the asset functions themselves?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;More info in this &lt;a href=\"https://stackoverflow.com/questions/75350111/dagster-can-you-trigger-a-job-to-run-via-an-api\"&gt;stackoverflow post I made&lt;/a&gt;, but basically I have a pipeline that is written in pure python, that receives a player&amp;#39;s username via an api call, and then runs the entire pipeline to get all the data for &lt;em&gt;that particular player only.&lt;/em&gt; I want to know if this is possible, or if dagster is only good for data that doesn&amp;#39;t require pre-defined parameters&lt;/p&gt;\n\n&lt;p&gt;Any help is greatly appreciated, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10uok0y", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10uok0y/is_there_a_way_to_trigger_a_dagster_pipeline_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10uok0y/is_there_a_way_to_trigger_a_dagster_pipeline_from/", "subreddit_subscribers": 88666, "created_utc": 1675633671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all. \n\nLooking for some advice on job titles at work. I'm currently employed with the title of Data Analyst, but due to how the team grew, my skillset and what needed doing when I joined I've been working on data engineering projects since I joined. Mostly builting etl/elt. I recently got a job offer for a data engineer position and my current team counter offered a very large raise and a title change for the new financial year me to stay.\n\n\nThe only thing was because of team structures they can't currently give me the title of \"Data Engineer\". So I've got some input on what the title will be as its essentially being made for me. I want it to actually reflect what I do for my CV and employment history when it comes to my future career and applying elsewhere.\n\nSome thoughts I've had so far are \"analytics engineer\" or \"ETL developer\". Was wondering if anyone else had any advice or ideas? Can I just put Data Engineer on my CV anyway? \n\nThanks", "author_fullname": "t2_dojxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job title thats not \"Data engineer\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10v2kml", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675676072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all. &lt;/p&gt;\n\n&lt;p&gt;Looking for some advice on job titles at work. I&amp;#39;m currently employed with the title of Data Analyst, but due to how the team grew, my skillset and what needed doing when I joined I&amp;#39;ve been working on data engineering projects since I joined. Mostly builting etl/elt. I recently got a job offer for a data engineer position and my current team counter offered a very large raise and a title change for the new financial year me to stay.&lt;/p&gt;\n\n&lt;p&gt;The only thing was because of team structures they can&amp;#39;t currently give me the title of &amp;quot;Data Engineer&amp;quot;. So I&amp;#39;ve got some input on what the title will be as its essentially being made for me. I want it to actually reflect what I do for my CV and employment history when it comes to my future career and applying elsewhere.&lt;/p&gt;\n\n&lt;p&gt;Some thoughts I&amp;#39;ve had so far are &amp;quot;analytics engineer&amp;quot; or &amp;quot;ETL developer&amp;quot;. Was wondering if anyone else had any advice or ideas? Can I just put Data Engineer on my CV anyway? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10v2kml", "is_robot_indexable": true, "report_reasons": null, "author": "Gartlas", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10v2kml/job_title_thats_not_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10v2kml/job_title_thats_not_data_engineer/", "subreddit_subscribers": 88666, "created_utc": 1675676072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\nI am currently working as a Senior Data Engineer at a tech start up. The work is really good and I am learning a lot.\n\nWill adding ML to my tech stack help me?", "author_fullname": "t2_tyw12rnx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some feedback on ML in DE tech stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10uz8mg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675663245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI am currently working as a Senior Data Engineer at a tech start up. The work is really good and I am learning a lot.&lt;/p&gt;\n\n&lt;p&gt;Will adding ML to my tech stack help me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10uz8mg", "is_robot_indexable": true, "report_reasons": null, "author": "technophilius89", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10uz8mg/need_some_feedback_on_ml_in_de_tech_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10uz8mg/need_some_feedback_on_ml_in_de_tech_stack/", "subreddit_subscribers": 88666, "created_utc": 1675663245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm looking to switch into a career in data engineering, but I know myself well enough to know that self-study is not effective for me.\n\n* I will definitely constantly doubt whether I am studying the most optimal thing\n\n* When I get frustrated/confused, I will constantly wish I just had an experienced human being to answer my questions, instead of googling and drowning in terminology and opinions\n\n* I think I will become a generic data engineer, instead of becoming really really good at a single hyper-specific tech stack.\n\n* I personally get motivated 100x more when I have 1-on-1 accountability, whether it be fitness, study, religion, a hobby, really anything.\n\nSo I am looking for a senior-level data engineer who can give me advice. **I want someone who will essentially teach me how to become a clone of themselves.** I want someone who will tell me the exact steps to get a job similar to theirs.\n\nFor structure, I was thinking maybe 1 hour per week on Zoom to critique my weekly study progress, and maybe 3-5 text messages per week for quick questions. We can discuss money privately, but I am definitely willing to compensate you fairly for your time!", "author_fullname": "t2_fq68u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to Hire a Mentor \ud83d\ude4f", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10uz4c4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675668012.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675662873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m looking to switch into a career in data engineering, but I know myself well enough to know that self-study is not effective for me.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;I will definitely constantly doubt whether I am studying the most optimal thing&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;When I get frustrated/confused, I will constantly wish I just had an experienced human being to answer my questions, instead of googling and drowning in terminology and opinions&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I think I will become a generic data engineer, instead of becoming really really good at a single hyper-specific tech stack.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I personally get motivated 100x more when I have 1-on-1 accountability, whether it be fitness, study, religion, a hobby, really anything.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So I am looking for a senior-level data engineer who can give me advice. &lt;strong&gt;I want someone who will essentially teach me how to become a clone of themselves.&lt;/strong&gt; I want someone who will tell me the exact steps to get a job similar to theirs.&lt;/p&gt;\n\n&lt;p&gt;For structure, I was thinking maybe 1 hour per week on Zoom to critique my weekly study progress, and maybe 3-5 text messages per week for quick questions. We can discuss money privately, but I am definitely willing to compensate you fairly for your time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10uz4c4", "is_robot_indexable": true, "report_reasons": null, "author": "KimchiFitness", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10uz4c4/looking_to_hire_a_mentor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10uz4c4/looking_to_hire_a_mentor/", "subreddit_subscribers": 88666, "created_utc": 1675662873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wanted to do a POC on edgeDB. Can anyone share resources or insights throwing light on proper use cases for utilizing edgeDB over other data warehouses?", "author_fullname": "t2_eb80kwgd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use case for using edgeDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10uyrwk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675661696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wanted to do a POC on edgeDB. Can anyone share resources or insights throwing light on proper use cases for utilizing edgeDB over other data warehouses?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10uyrwk", "is_robot_indexable": true, "report_reasons": null, "author": "No-Caregiver-1204", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10uyrwk/use_case_for_using_edgedb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10uyrwk/use_case_for_using_edgedb/", "subreddit_subscribers": 88666, "created_utc": 1675661696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everybody,\n\nNeeded input on how best to transfer almost 2 TBs of data from an on-prem Teradata server to Bigquery, on a daily basis.\n\nHere are some considerations:\n\n1. The data should be transferred in a secure manner\n2. The process should have the ability to do both full and delta loads\n3. As inexpensive as possible\n\nAll feedback and comments welcome :)", "author_fullname": "t2_pxuosxo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On-prem Teradata to BQ Data Transfer pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10uvf5r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675651351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everybody,&lt;/p&gt;\n\n&lt;p&gt;Needed input on how best to transfer almost 2 TBs of data from an on-prem Teradata server to Bigquery, on a daily basis.&lt;/p&gt;\n\n&lt;p&gt;Here are some considerations:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The data should be transferred in a secure manner&lt;/li&gt;\n&lt;li&gt;The process should have the ability to do both full and delta loads&lt;/li&gt;\n&lt;li&gt;As inexpensive as possible&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;All feedback and comments welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10uvf5r", "is_robot_indexable": true, "report_reasons": null, "author": "deezelmunky", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10uvf5r/onprem_teradata_to_bq_data_transfer_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10uvf5r/onprem_teradata_to_bq_data_transfer_pipeline/", "subreddit_subscribers": 88666, "created_utc": 1675651351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# GOAL:\n\n* Have a python list as a global variable between tasks.\n* Currently it crashes at the 1st task.\n* 1.) I am trying to have a simple python list that is carried from 1 task to the next and append a few string values to it at task 2. So the goal is to have 1 shared list.\n* 2.) Even if 1 task fails it should just move on ad dotn care (obviously mark the task area failed)\n\n&amp;#x200B;\n\n# SETUP:\n\n* I am on Airflow 2.4.1\n* I use Airflow Docker and build a python environemnt that I have used many times and just works fine.\n\n# MY CODE:\n\n&amp;#x200B;\n\n    from __future__ import annotations\n    import logging\n    import os\n    import shutil\n    import sys\n    import tempfile\n    import time\n    from pprint import pprint\n    import pendulum\n    from airflow import DAG\n    from airflow.decorators import task\n    \n    log = logging.getLogger(__name__)\n    PYTHON = sys.executable\n    BASE_DIR = tempfile.gettempdir()\n    \n    my_default_args = {\n        'owner': 'me',\n        'email': ['some_email@some_email.com'],\n        'email_on_failure': True,\n        'email_on_retry': False, \n        'write_successes': [],\n    }\n    \n    with DAG(\n        dag_id='my_dag_id',\n        schedule='9 9 * * *',\n        start_date=pendulum.datetime(2022, 1, 1, tz=\"UTC\"),\n        catchup=False,\n        default_args=my_default_args,\n        tags=['a', 'b'],\n        ) as dag:\n    \n        @task.external_python(task_id=\"one\", python='/opt/airflow/venv1/bin/python3')\n        def first(**kwargs):\n            task_id=\"one\"\n            write_successes = kwargs.get('write_successes', [])\n    \n            print(write_successes)\n            write_successes.append(99)\n            print(write_successes)\n    \n    \n        @task.external_python(task_id=\"two\", python='/opt/airflow/venv1/bin/python3')\n        def second(**kwargs):\n            write_successes = kwargs.get('write_successes', [])\n    \n            print(write_successes)\n            write_successes.append(101)\n            print(write_successes)\n    \n    \n        one = first()\n        two = second()\n    \n        one &gt;&gt; two\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n# ERROR:\n\nLOG OF THE 1st failed task the second \"upstream\\_failed\"\n\n    *** Reading local file: /opt/airflow/logs/dag_id=test_global_variable/run_id=scheduled__2023-02-05T09:09:00+00:00/task_id=one/attempt=1.log\n    [2023-02-06, 12:24:43 GMT] {taskinstance.py:1165} INFO - Dependencies all met for &lt;TaskInstance: test_global_variable.one scheduled__2023-02-05T09:09:00+00:00 [queued]&gt;\n    [2023-02-06, 12:24:43 GMT] {taskinstance.py:1165} INFO - Dependencies all met for &lt;TaskInstance: test_global_variable.one scheduled__2023-02-05T09:09:00+00:00 [queued]&gt;\n    [2023-02-06, 12:24:43 GMT] {taskinstance.py:1362} INFO - \n    --------------------------------------------------------------------------------\n    [2023-02-06, 12:24:43 GMT] {taskinstance.py:1363} INFO - Starting attempt 1 of 1\n    [2023-02-06, 12:24:43 GMT] {taskinstance.py:1364} INFO - \n    --------------------------------------------------------------------------------\n    [2023-02-06, 12:24:43 GMT] {taskinstance.py:1383} INFO - Executing &lt;Task(_PythonExternalDecoratedOperator): one&gt; on 2023-02-05 09:09:00+00:00\n    [2023-02-06, 12:24:43 GMT] {standard_task_runner.py:54} INFO - Started process 239657 to run task\n    [2023-02-06, 12:24:43 GMT] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'test_global_variable', 'one', 'scheduled__2023-02-05T09:09:00+00:00', '--job-id', '72751', '--raw', '--subdir', 'DAGS_FOLDER/test_global_variable.py', '--cfg-path', '/tmp/tmpxldmrzpp']\n    [2023-02-06, 12:24:43 GMT] {standard_task_runner.py:83} INFO - Job 72751: Subtask one\n    [2023-02-06, 12:24:43 GMT] {dagbag.py:525} INFO - Filling up the DagBag from /opt/airflow/dags/test_global_variable.py\n    [2023-02-06, 12:24:43 GMT] {task_command.py:384} INFO - Running &lt;TaskInstance: test_global_variable.one scheduled__2023-02-05T09:09:00+00:00 [running]&gt; on host 4851b30aa5cf\n    [2023-02-06, 12:24:43 GMT] {taskinstance.py:1590} INFO - Exporting the following env vars:\n    AIRFLOW_CTX_DAG_OWNER=me\n    AIRFLOW_CTX_DAG_ID=test_global_variable\n    AIRFLOW_CTX_TASK_ID=one\n    AIRFLOW_CTX_EXECUTION_DATE=2023-02-05T09:09:00+00:00\n    AIRFLOW_CTX_TRY_NUMBER=1\n    AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-02-05T09:09:00+00:00\n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'next_ds' from the template is deprecated and will be removed in a future version. Please use '{{ data_interval_end | ds }}' instead.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'next_ds_nodash' from the template is deprecated and will be removed in a future version. Please use '{{ data_interval_end | ds_nodash }}' instead.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'next_execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_end' instead.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'prev_ds' from the template is deprecated and will be removed in a future version.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'prev_ds_nodash' from the template is deprecated and will be removed in a future version.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'prev_execution_date' from the template is deprecated and will be removed in a future version.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'prev_execution_date_success' from the template is deprecated and will be removed in a future version. Please use 'prev_data_interval_start_success' instead.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'tomorrow_ds' from the template is deprecated and will be removed in a future version.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'tomorrow_ds_nodash' from the template is deprecated and will be removed in a future version.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'yesterday_ds' from the template is deprecated and will be removed in a future version.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing 'yesterday_ds_nodash' from the template is deprecated and will be removed in a future version.\n      warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n    \n    [2023-02-06, 12:24:44 GMT] {taskinstance.py:1851} ERROR - Task failed with exception\n    Traceback (most recent call last):\n      File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/decorators/base.py\", line 188, in execute\n        return_value = super().execute(context)\n      File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py\", line 370, in execute\n        return super().execute(context=serializable_context)\n      File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py\", line 175, in execute\n        return_value = self.execute_callable()\n      File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py\", line 678, in execute_callable\n        return self._execute_python_callable_in_subprocess(python_path, tmp_path)\n      File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py\", line 411, in _execute_python_callable_in_subprocess\n        self._write_args(input_path)\n      File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py\", line 381, in _write_args\n        file.write_bytes(self.pickling_library.dumps({'args': self.op_args, 'kwargs': self.op_kwargs}))\n    _pickle.PicklingError: Can't pickle &lt;function first at 0x7f80ff76e4c0&gt;: it's not the same object as unusual_prefix_6cc7442bed7c02593e3a29524b0e65329d9f59da_test_global_variable.first\n    [2023-02-06, 12:24:44 GMT] {taskinstance.py:1401} INFO - Marking task as FAILED. dag_id=test_global_variable, task_id=one, execution_date=20230205T090900, start_date=20230206T122443, end_date=20230206T122444\n    [2023-02-06, 12:24:44 GMT] {standard_task_runner.py:102} ERROR - Failed to execute job 72751 for task one (Can't pickle &lt;function first at 0x7f80ff76e4c0&gt;: it's not the same object as unusual_prefix_6cc7442bed7c02593e3a29524b0e65329d9f59da_test_global_variable.first; 239657)\n    [2023-02-06, 12:24:44 GMT] {local_task_job.py:164} INFO - Task exited with return code 1\n    [2023-02-06, 12:24:44 GMT] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n# I have tried to fix it based on the following posts:\n\n\\- I have tried global python variables that did not worked at all\n\n\\- [https://stackoverflow.com/questions/58792721/global-variables-in-airflow](https://stackoverflow.com/questions/58792721/global-variables-in-airflow) \\- i have separate \"task.external\\_python\" that makes it not possible to use the following post.\n\n\\- Mine is not a class issue - [https://stackoverflow.com/questions/61705029/list-as-global-variable-inside-a-class-in-python](https://stackoverflow.com/questions/61705029/list-as-global-variable-inside-a-class-in-python)\n\n\\- might be interesting but I have separate python venve for each task - [https://stackoverflow.com/a/58804409/10270590](https://stackoverflow.com/a/58804409/10270590)\n\n\\- I could not get Airflow XCOM working", "author_fullname": "t2_qrm5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Airflow] How to use a python list as global variable with @task.external_python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10v5kmk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675687029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;GOAL:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Have a python list as a global variable between tasks.&lt;/li&gt;\n&lt;li&gt;Currently it crashes at the 1st task.&lt;/li&gt;\n&lt;li&gt;1.) I am trying to have a simple python list that is carried from 1 task to the next and append a few string values to it at task 2. So the goal is to have 1 shared list.&lt;/li&gt;\n&lt;li&gt;2.) Even if 1 task fails it should just move on ad dotn care (obviously mark the task area failed)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;SETUP:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I am on Airflow 2.4.1&lt;/li&gt;\n&lt;li&gt;I use Airflow Docker and build a python environemnt that I have used many times and just works fine.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;MY CODE:&lt;/h1&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from __future__ import annotations\nimport logging\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport time\nfrom pprint import pprint\nimport pendulum\nfrom airflow import DAG\nfrom airflow.decorators import task\n\nlog = logging.getLogger(__name__)\nPYTHON = sys.executable\nBASE_DIR = tempfile.gettempdir()\n\nmy_default_args = {\n    &amp;#39;owner&amp;#39;: &amp;#39;me&amp;#39;,\n    &amp;#39;email&amp;#39;: [&amp;#39;some_email@some_email.com&amp;#39;],\n    &amp;#39;email_on_failure&amp;#39;: True,\n    &amp;#39;email_on_retry&amp;#39;: False, \n    &amp;#39;write_successes&amp;#39;: [],\n}\n\nwith DAG(\n    dag_id=&amp;#39;my_dag_id&amp;#39;,\n    schedule=&amp;#39;9 9 * * *&amp;#39;,\n    start_date=pendulum.datetime(2022, 1, 1, tz=&amp;quot;UTC&amp;quot;),\n    catchup=False,\n    default_args=my_default_args,\n    tags=[&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;],\n    ) as dag:\n\n    @task.external_python(task_id=&amp;quot;one&amp;quot;, python=&amp;#39;/opt/airflow/venv1/bin/python3&amp;#39;)\n    def first(**kwargs):\n        task_id=&amp;quot;one&amp;quot;\n        write_successes = kwargs.get(&amp;#39;write_successes&amp;#39;, [])\n\n        print(write_successes)\n        write_successes.append(99)\n        print(write_successes)\n\n\n    @task.external_python(task_id=&amp;quot;two&amp;quot;, python=&amp;#39;/opt/airflow/venv1/bin/python3&amp;#39;)\n    def second(**kwargs):\n        write_successes = kwargs.get(&amp;#39;write_successes&amp;#39;, [])\n\n        print(write_successes)\n        write_successes.append(101)\n        print(write_successes)\n\n\n    one = first()\n    two = second()\n\n    one &amp;gt;&amp;gt; two\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;ERROR:&lt;/h1&gt;\n\n&lt;p&gt;LOG OF THE 1st failed task the second &amp;quot;upstream_failed&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;*** Reading local file: /opt/airflow/logs/dag_id=test_global_variable/run_id=scheduled__2023-02-05T09:09:00+00:00/task_id=one/attempt=1.log\n[2023-02-06, 12:24:43 GMT] {taskinstance.py:1165} INFO - Dependencies all met for &amp;lt;TaskInstance: test_global_variable.one scheduled__2023-02-05T09:09:00+00:00 [queued]&amp;gt;\n[2023-02-06, 12:24:43 GMT] {taskinstance.py:1165} INFO - Dependencies all met for &amp;lt;TaskInstance: test_global_variable.one scheduled__2023-02-05T09:09:00+00:00 [queued]&amp;gt;\n[2023-02-06, 12:24:43 GMT] {taskinstance.py:1362} INFO - \n--------------------------------------------------------------------------------\n[2023-02-06, 12:24:43 GMT] {taskinstance.py:1363} INFO - Starting attempt 1 of 1\n[2023-02-06, 12:24:43 GMT] {taskinstance.py:1364} INFO - \n--------------------------------------------------------------------------------\n[2023-02-06, 12:24:43 GMT] {taskinstance.py:1383} INFO - Executing &amp;lt;Task(_PythonExternalDecoratedOperator): one&amp;gt; on 2023-02-05 09:09:00+00:00\n[2023-02-06, 12:24:43 GMT] {standard_task_runner.py:54} INFO - Started process 239657 to run task\n[2023-02-06, 12:24:43 GMT] {standard_task_runner.py:82} INFO - Running: [&amp;#39;airflow&amp;#39;, &amp;#39;tasks&amp;#39;, &amp;#39;run&amp;#39;, &amp;#39;test_global_variable&amp;#39;, &amp;#39;one&amp;#39;, &amp;#39;scheduled__2023-02-05T09:09:00+00:00&amp;#39;, &amp;#39;--job-id&amp;#39;, &amp;#39;72751&amp;#39;, &amp;#39;--raw&amp;#39;, &amp;#39;--subdir&amp;#39;, &amp;#39;DAGS_FOLDER/test_global_variable.py&amp;#39;, &amp;#39;--cfg-path&amp;#39;, &amp;#39;/tmp/tmpxldmrzpp&amp;#39;]\n[2023-02-06, 12:24:43 GMT] {standard_task_runner.py:83} INFO - Job 72751: Subtask one\n[2023-02-06, 12:24:43 GMT] {dagbag.py:525} INFO - Filling up the DagBag from /opt/airflow/dags/test_global_variable.py\n[2023-02-06, 12:24:43 GMT] {task_command.py:384} INFO - Running &amp;lt;TaskInstance: test_global_variable.one scheduled__2023-02-05T09:09:00+00:00 [running]&amp;gt; on host 4851b30aa5cf\n[2023-02-06, 12:24:43 GMT] {taskinstance.py:1590} INFO - Exporting the following env vars:\nAIRFLOW_CTX_DAG_OWNER=me\nAIRFLOW_CTX_DAG_ID=test_global_variable\nAIRFLOW_CTX_TASK_ID=one\nAIRFLOW_CTX_EXECUTION_DATE=2023-02-05T09:09:00+00:00\nAIRFLOW_CTX_TRY_NUMBER=1\nAIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-02-05T09:09:00+00:00\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;execution_date&amp;#39; from the template is deprecated and will be removed in a future version. Please use &amp;#39;data_interval_start&amp;#39; or &amp;#39;logical_date&amp;#39; instead.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;next_ds&amp;#39; from the template is deprecated and will be removed in a future version. Please use &amp;#39;{{ data_interval_end | ds }}&amp;#39; instead.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;next_ds_nodash&amp;#39; from the template is deprecated and will be removed in a future version. Please use &amp;#39;{{ data_interval_end | ds_nodash }}&amp;#39; instead.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;next_execution_date&amp;#39; from the template is deprecated and will be removed in a future version. Please use &amp;#39;data_interval_end&amp;#39; instead.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;prev_ds&amp;#39; from the template is deprecated and will be removed in a future version.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;prev_ds_nodash&amp;#39; from the template is deprecated and will be removed in a future version.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;prev_execution_date&amp;#39; from the template is deprecated and will be removed in a future version.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;prev_execution_date_success&amp;#39; from the template is deprecated and will be removed in a future version. Please use &amp;#39;prev_data_interval_start_success&amp;#39; instead.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;tomorrow_ds&amp;#39; from the template is deprecated and will be removed in a future version.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;tomorrow_ds_nodash&amp;#39; from the template is deprecated and will be removed in a future version.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;yesterday_ds&amp;#39; from the template is deprecated and will be removed in a future version.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/context.py:204: AirflowContextDeprecationWarning: Accessing &amp;#39;yesterday_ds_nodash&amp;#39; from the template is deprecated and will be removed in a future version.\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\n\n[2023-02-06, 12:24:44 GMT] {taskinstance.py:1851} ERROR - Task failed with exception\nTraceback (most recent call last):\n  File &amp;quot;/home/airflow/.local/lib/python3.8/site-packages/airflow/decorators/base.py&amp;quot;, line 188, in execute\n    return_value = super().execute(context)\n  File &amp;quot;/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py&amp;quot;, line 370, in execute\n    return super().execute(context=serializable_context)\n  File &amp;quot;/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py&amp;quot;, line 175, in execute\n    return_value = self.execute_callable()\n  File &amp;quot;/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py&amp;quot;, line 678, in execute_callable\n    return self._execute_python_callable_in_subprocess(python_path, tmp_path)\n  File &amp;quot;/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py&amp;quot;, line 411, in _execute_python_callable_in_subprocess\n    self._write_args(input_path)\n  File &amp;quot;/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py&amp;quot;, line 381, in _write_args\n    file.write_bytes(self.pickling_library.dumps({&amp;#39;args&amp;#39;: self.op_args, &amp;#39;kwargs&amp;#39;: self.op_kwargs}))\n_pickle.PicklingError: Can&amp;#39;t pickle &amp;lt;function first at 0x7f80ff76e4c0&amp;gt;: it&amp;#39;s not the same object as unusual_prefix_6cc7442bed7c02593e3a29524b0e65329d9f59da_test_global_variable.first\n[2023-02-06, 12:24:44 GMT] {taskinstance.py:1401} INFO - Marking task as FAILED. dag_id=test_global_variable, task_id=one, execution_date=20230205T090900, start_date=20230206T122443, end_date=20230206T122444\n[2023-02-06, 12:24:44 GMT] {standard_task_runner.py:102} ERROR - Failed to execute job 72751 for task one (Can&amp;#39;t pickle &amp;lt;function first at 0x7f80ff76e4c0&amp;gt;: it&amp;#39;s not the same object as unusual_prefix_6cc7442bed7c02593e3a29524b0e65329d9f59da_test_global_variable.first; 239657)\n[2023-02-06, 12:24:44 GMT] {local_task_job.py:164} INFO - Task exited with return code 1\n[2023-02-06, 12:24:44 GMT] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;I have tried to fix it based on the following posts:&lt;/h1&gt;\n\n&lt;p&gt;- I have tried global python variables that did not worked at all&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://stackoverflow.com/questions/58792721/global-variables-in-airflow\"&gt;https://stackoverflow.com/questions/58792721/global-variables-in-airflow&lt;/a&gt; - i have separate &amp;quot;task.external_python&amp;quot; that makes it not possible to use the following post.&lt;/p&gt;\n\n&lt;p&gt;- Mine is not a class issue - &lt;a href=\"https://stackoverflow.com/questions/61705029/list-as-global-variable-inside-a-class-in-python\"&gt;https://stackoverflow.com/questions/61705029/list-as-global-variable-inside-a-class-in-python&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- might be interesting but I have separate python venve for each task - &lt;a href=\"https://stackoverflow.com/a/58804409/10270590\"&gt;https://stackoverflow.com/a/58804409/10270590&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- I could not get Airflow XCOM working&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10v5kmk", "is_robot_indexable": true, "report_reasons": null, "author": "glassAlloy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10v5kmk/airflow_how_to_use_a_python_list_as_global/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10v5kmk/airflow_how_to_use_a_python_list_as_global/", "subreddit_subscribers": 88666, "created_utc": 1675687029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_iji1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is observability? (explained in 5 mins)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": true, "name": "t3_10v5e42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6del5U8Si7PGPQfm5bXyQv8sBryra8Lm-Rvmw3bsg2o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675686475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "haydenjames.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://haydenjames.io/what-is-observability/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jElQeYdexwEO-67OAM2mrperu_ku8nyMKHiGuRT8iEw.jpg?auto=webp&amp;v=enabled&amp;s=1a3952433bde6721f3d3e64fdcad6e5ff789b016", "width": 793, "height": 495}, "resolutions": [{"url": "https://external-preview.redd.it/jElQeYdexwEO-67OAM2mrperu_ku8nyMKHiGuRT8iEw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6609f1f8bac72ac7833af6694902259f19482024", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/jElQeYdexwEO-67OAM2mrperu_ku8nyMKHiGuRT8iEw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=715eb462406e21ce9504700e94110bc4bb487679", "width": 216, "height": 134}, {"url": "https://external-preview.redd.it/jElQeYdexwEO-67OAM2mrperu_ku8nyMKHiGuRT8iEw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4aa1201e0ee664f675dfd0a2b85f40e6f9d5bd08", "width": 320, "height": 199}, {"url": "https://external-preview.redd.it/jElQeYdexwEO-67OAM2mrperu_ku8nyMKHiGuRT8iEw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06f1247d357e87ef366cc3013da517660c6c3975", "width": 640, "height": 399}], "variants": {}, "id": "uNQoP7PSu_9vgwDYgwXmhsPEHiQDcRlEPGbdpLSBMTc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10v5e42", "is_robot_indexable": true, "report_reasons": null, "author": "modelop", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10v5e42/what_is_observability_explained_in_5_mins/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://haydenjames.io/what-is-observability/", "subreddit_subscribers": 88666, "created_utc": 1675686475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working on a data pipeline that is extracting data from a web API. The data is then loaded into Google Cloud Storage as JSON Line delimited files. From there I would like to ingest the data into BigQuery. Unfortunately some field types across the many files are not constant, and BigQuery returns an error when I try loading in many files, implying that a field type does not match the schema. I would really appreciate it if someone can give me an indication of the industry best practices when it comes to processing and validating the data to ensure consistency across the different files. Would I have to create a beam pipeline that coerces all the values to fit the schema, or are there any frameworks such as great expectations to do this? If so where should I integrate great expectations or another framework within the pipeline. Thank you for your time.", "author_fullname": "t2_7s8l0yk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data validation and field type coercion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10v2u7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675677217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on a data pipeline that is extracting data from a web API. The data is then loaded into Google Cloud Storage as JSON Line delimited files. From there I would like to ingest the data into BigQuery. Unfortunately some field types across the many files are not constant, and BigQuery returns an error when I try loading in many files, implying that a field type does not match the schema. I would really appreciate it if someone can give me an indication of the industry best practices when it comes to processing and validating the data to ensure consistency across the different files. Would I have to create a beam pipeline that coerces all the values to fit the schema, or are there any frameworks such as great expectations to do this? If so where should I integrate great expectations or another framework within the pipeline. Thank you for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10v2u7v", "is_robot_indexable": true, "report_reasons": null, "author": "manfromthetaleb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10v2u7v/data_validation_and_field_type_coercion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10v2u7v/data_validation_and_field_type_coercion/", "subreddit_subscribers": 88666, "created_utc": 1675677217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hy guys, somebody please explain me the difference between bias and variance. I am soo confused between them and every website is defining them differently. \n\nI spent so much time figuring what they are, yet unsuccessful.\n\nNote: As of now i only know about linear regression model. \n\nSomebody please explain the difference between them (and what they actually mean) in plain English. Do not use complicated words please.", "author_fullname": "t2_q60xrlzp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difference between Bias and Variance (I only know about linear regression as of now)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10v0byb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675667108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hy guys, somebody please explain me the difference between bias and variance. I am soo confused between them and every website is defining them differently. &lt;/p&gt;\n\n&lt;p&gt;I spent so much time figuring what they are, yet unsuccessful.&lt;/p&gt;\n\n&lt;p&gt;Note: As of now i only know about linear regression model. &lt;/p&gt;\n\n&lt;p&gt;Somebody please explain the difference between them (and what they actually mean) in plain English. Do not use complicated words please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10v0byb", "is_robot_indexable": true, "report_reasons": null, "author": "hungry_man13", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10v0byb/difference_between_bias_and_variance_i_only_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10v0byb/difference_between_bias_and_variance_i_only_know/", "subreddit_subscribers": 88666, "created_utc": 1675667108.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}