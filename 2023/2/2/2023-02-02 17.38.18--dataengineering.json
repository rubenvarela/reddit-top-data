{"kind": "Listing", "data": {"after": "t3_10r2504", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Being hired as a Data Analyst Intern by an SME but doing database development, DWH and BI report. When I come in, they were using Power BI to read live data from ERP and did all the transformation work and nobody can tell a reason why they do any certain transformation (the intern before me only instructed by a senior person and both left that company, remained those queries that impossible to read. Stakeholder complained the unknown calculations and the endless errors in the reports. It took me six months to go through Reddit and the DWH toolkit to gain knowledge and build up an basic infrastructure with some help from the only SWE in my company. And now my leader start to complain why it took me so long to develop TWO reports (he simply divide 6 months by 2 to get the time spending). Another Data analyst benefit from my database and his report gain a lot of flowers by Senior leadership. Does Data Engineer always be the first to blame and the last to credit? I am starting to doubt myself whether this position is the right one for me.", "author_fullname": "t2_ehg2vux7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Data Engineer not suitable for me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10racds", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675295400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Being hired as a Data Analyst Intern by an SME but doing database development, DWH and BI report. When I come in, they were using Power BI to read live data from ERP and did all the transformation work and nobody can tell a reason why they do any certain transformation (the intern before me only instructed by a senior person and both left that company, remained those queries that impossible to read. Stakeholder complained the unknown calculations and the endless errors in the reports. It took me six months to go through Reddit and the DWH toolkit to gain knowledge and build up an basic infrastructure with some help from the only SWE in my company. And now my leader start to complain why it took me so long to develop TWO reports (he simply divide 6 months by 2 to get the time spending). Another Data analyst benefit from my database and his report gain a lot of flowers by Senior leadership. Does Data Engineer always be the first to blame and the last to credit? I am starting to doubt myself whether this position is the right one for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10racds", "is_robot_indexable": true, "report_reasons": null, "author": "JowwLee", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10racds/is_data_engineer_not_suitable_for_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10racds/is_data_engineer_not_suitable_for_me/", "subreddit_subscribers": 88250, "created_utc": 1675295400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you think about this news? I think this will make Azure a way better choice for Data Engineering workloads, if the managed airflow really works like I\u2019d imagine it to do. \n\nhttps://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151", "author_fullname": "t2_aqzwz9ec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing 'Managed Airflow' in Azure Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rk9tj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675324053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you think about this news? I think this will make Azure a way better choice for Data Engineering workloads, if the managed airflow really works like I\u2019d imagine it to do. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151\"&gt;https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?auto=webp&amp;v=enabled&amp;s=377641832eecbd621b310ab9dd16d381993c449f", "width": 2288, "height": 1448}, "resolutions": [{"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc4ffca7b91875a7fa3134c0d86d66dcd449c179", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75220a5e620ee298b7b2d8329044b879daca4586", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f764681c03c53d1229c8ff8e3799ea7bec9e4899", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f124a2fdd04ffd44d572a4e01089662b94a5f067", "width": 640, "height": 405}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7d3af51b0e95fbe753302b6f4c7969990339dc6", "width": 960, "height": 607}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a177ca1e48aaead34281cb5f3e145a59154b95e", "width": 1080, "height": 683}], "variants": {}, "id": "S0ZC01nTKv9edIZMquYzX-iG3I-MSj166ECzeJEKDn8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10rk9tj", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Noise-3261", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rk9tj/introducing_managed_airflow_in_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rk9tj/introducing_managed_airflow_in_azure_data_factory/", "subreddit_subscribers": 88250, "created_utc": 1675324053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a compsci student who's trying to get into this field and I really don't like the idea of using low-code/no-code tools (i.e. Tableu or Power BI) \u2014 I'm a programmer after all.  A few months ago I was planning to learn Plotly Dash because I wanted to start a carreer working with data, but most roles which involve analysis and dashboarding have as prerequisite low code tools like Excel and those aforementioned.\n\nStraight to the point, now: I want to become a DE because I like to build stuff with code and I would love to make some dashboards with Python here and there, but:\n\n1) I don't even know if dashboarding skills is something needed to this role.\n\n2) Assuming it is, how useful is to learn a \"low level\" tool like Plotly Dash since the market usually asks for low-code stuff?", "author_fullname": "t2_pme9byci", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should a data engineer aspirant learn dashboarding tools like Tableu and/or Plotly Dash?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r72qx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675287654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a compsci student who&amp;#39;s trying to get into this field and I really don&amp;#39;t like the idea of using low-code/no-code tools (i.e. Tableu or Power BI) \u2014 I&amp;#39;m a programmer after all.  A few months ago I was planning to learn Plotly Dash because I wanted to start a carreer working with data, but most roles which involve analysis and dashboarding have as prerequisite low code tools like Excel and those aforementioned.&lt;/p&gt;\n\n&lt;p&gt;Straight to the point, now: I want to become a DE because I like to build stuff with code and I would love to make some dashboards with Python here and there, but:&lt;/p&gt;\n\n&lt;p&gt;1) I don&amp;#39;t even know if dashboarding skills is something needed to this role.&lt;/p&gt;\n\n&lt;p&gt;2) Assuming it is, how useful is to learn a &amp;quot;low level&amp;quot; tool like Plotly Dash since the market usually asks for low-code stuff?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10r72qx", "is_robot_indexable": true, "report_reasons": null, "author": "mr_tellok", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r72qx/should_a_data_engineer_aspirant_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r72qx/should_a_data_engineer_aspirant_learn/", "subreddit_subscribers": 88250, "created_utc": 1675287654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a basic question with probably a simple answer, but I'm still not sure what's best practice. In my hypothetical ETL pipeline I have my extraction step where I read data from somewhere and write it to S3 as a CSV.\n\nThis first run is easy since I don't have any extracted data files from previous runs. However, the second time this pipeline runs I already have a file from the previous run, and I can choose to version it with a new S3 key or overwrite the previous key. Now the question is which should I chose?\n\nDoes this depend on whether I do incramental reads or full reads?\n\nDoes it also depend on whether my warehouse is SCD1 or SCD2?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I version extracted data files on repeated ELT runs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rcohx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675301205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a basic question with probably a simple answer, but I&amp;#39;m still not sure what&amp;#39;s best practice. In my hypothetical ETL pipeline I have my extraction step where I read data from somewhere and write it to S3 as a CSV.&lt;/p&gt;\n\n&lt;p&gt;This first run is easy since I don&amp;#39;t have any extracted data files from previous runs. However, the second time this pipeline runs I already have a file from the previous run, and I can choose to version it with a new S3 key or overwrite the previous key. Now the question is which should I chose?&lt;/p&gt;\n\n&lt;p&gt;Does this depend on whether I do incramental reads or full reads?&lt;/p&gt;\n\n&lt;p&gt;Does it also depend on whether my warehouse is SCD1 or SCD2?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10rcohx", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rcohx/should_i_version_extracted_data_files_on_repeated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rcohx/should_i_version_extracted_data_files_on_repeated/", "subreddit_subscribers": 88250, "created_utc": 1675301205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to learn the ins and outs of snowflake and data mesh. Any comprehensive tutorials out there?", "author_fullname": "t2_122ieh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Data Mesh Tutorials?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rdoqz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675303800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to learn the ins and outs of snowflake and data mesh. Any comprehensive tutorials out there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10rdoqz", "is_robot_indexable": true, "report_reasons": null, "author": "Ltothetm", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rdoqz/snowflake_data_mesh_tutorials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rdoqz/snowflake_data_mesh_tutorials/", "subreddit_subscribers": 88250, "created_utc": 1675303800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nI'm new to this so please forgive my newbie question. Trying to retrieve files from onedrive via a python script and deploying it to Google Cloud Functions. Any idea how? Do I use Microsoft Graph? Need some guide to start. Thank u!", "author_fullname": "t2_c58xvng2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How To Retrieve Files From Onedrive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rgmd5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675311934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to this so please forgive my newbie question. Trying to retrieve files from onedrive via a python script and deploying it to Google Cloud Functions. Any idea how? Do I use Microsoft Graph? Need some guide to start. Thank u!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10rgmd5", "is_robot_indexable": true, "report_reasons": null, "author": "dehydratedcatnip", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rgmd5/how_to_retrieve_files_from_onedrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rgmd5/how_to_retrieve_files_from_onedrive/", "subreddit_subscribers": 88250, "created_utc": 1675311934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As in, what the name of your production dataplatform or database? Do you have a pet-name, some name based on a company theme or is it a descriptive name? \n\nPersonally I've mostly called my production datawarehouse \"datawarehouse\". At my company we are moving to Databricks and the current questions are: \"what do we call our dbt production catalog?\" and \"should it be a descriptive name or a cool-sounding name?\" I thought it would be interesting to see how others approach this.", "author_fullname": "t2_11b4ct", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the \"name\" of your production system or project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ro2vo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675338878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As in, what the name of your production dataplatform or database? Do you have a pet-name, some name based on a company theme or is it a descriptive name? &lt;/p&gt;\n\n&lt;p&gt;Personally I&amp;#39;ve mostly called my production datawarehouse &amp;quot;datawarehouse&amp;quot;. At my company we are moving to Databricks and the current questions are: &amp;quot;what do we call our dbt production catalog?&amp;quot; and &amp;quot;should it be a descriptive name or a cool-sounding name?&amp;quot; I thought it would be interesting to see how others approach this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ro2vo", "is_robot_indexable": true, "report_reasons": null, "author": "mmammies", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ro2vo/what_is_the_name_of_your_production_system_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ro2vo/what_is_the_name_of_your_production_system_or/", "subreddit_subscribers": 88250, "created_utc": 1675338878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone\n\nI'm trying to get a new job in Data Engineering after doing Data Talks Zoomcamp, but is rough. My past two years I've worked as Web Developer/Tech Guy but I don't see my career or paycheck going up.", "author_fullname": "t2_kgod5v4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resume feedback?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10r9zeo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/oE6zteO3_yv0IzBAGKQFROoRwESfujM4Y_bU-_gLnr4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675294499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to get a new job in Data Engineering after doing Data Talks Zoomcamp, but is rough. My past two years I&amp;#39;ve worked as Web Developer/Tech Guy but I don&amp;#39;t see my career or paycheck going up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/vlr8ws1oepfa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?auto=webp&amp;v=enabled&amp;s=1e1d030c2dc02e376420e4b51cac3bdeaff70ae2", "width": 1080, "height": 1526}, "resolutions": [{"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfee1f63d7877ae8dfb54d902c2136c5979ad372", "width": 108, "height": 152}, {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f9d62687712f866eb0b8dd7d6c06c7581ff6573", "width": 216, "height": 305}, {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7e04cf35ba04b1f900baca59d9c70cddb6b58ee", "width": 320, "height": 452}, {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d4a35b7d3454a0f70dcff90c66b6da9367ce439", "width": 640, "height": 904}, {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61570f6cc42defccedd317a9993df22bcde6169d", "width": 960, "height": 1356}, {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa2c15a37a5a817f8dfb540b5be5d63aed0f194a", "width": 1080, "height": 1526}], "variants": {}, "id": "4WhbjGb0o0A5DFLu8MMbU0qM-QncwVLubWYEgHnWQ0k"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10r9zeo", "is_robot_indexable": true, "report_reasons": null, "author": "FarFaithlessness8812", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r9zeo/resume_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/vlr8ws1oepfa1.png", "subreddit_subscribers": 88250, "created_utc": 1675294499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've gone through the [Getting started with dbt Core](https://docs.getdbt.com/docs/get-started/getting-started-dbt-core) tutorial and I've connected my BigQuery instance to dbt through the use of the JSON file in the relevant directory.\n\nI've made changes to tables/models and see those reflected in the warehouse each time I run `dbt run`, but I cannot see where some of the base tables exist, namely orders.sql and customers.sql, both of which were used to define the models that were similarly named, and also the staging models.\n\nSurely, I looked under the `seeds` directory since that would make sense as they're seeding the project, but I don't see anything there.\n\nFor reference, the two staging tables toward the end of the tutorial, `stg_customers.sql` and `stg_orders.sql` are defined to source from\n\n    `dbt-tutorial`.jaffle_shop.customers    and\n    `dbt-tutorial`.jaffle_shop.orders\n\nrespectively.\n\nI've tried moving on and doing other stuff to further my learning, but I can't scratch this and it's irritating me.\n\nI figured that I probably deleted it so I ran `dbt init` again and there's nothing present that's just \\`orders.sql\\` or anything.", "author_fullname": "t2_svn12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Very much a noob question, but for dbt's tutorial, where are the order and customer tables kept?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r5wrc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675285747.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675284986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve gone through the &lt;a href=\"https://docs.getdbt.com/docs/get-started/getting-started-dbt-core\"&gt;Getting started with dbt Core&lt;/a&gt; tutorial and I&amp;#39;ve connected my BigQuery instance to dbt through the use of the JSON file in the relevant directory.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve made changes to tables/models and see those reflected in the warehouse each time I run &lt;code&gt;dbt run&lt;/code&gt;, but I cannot see where some of the base tables exist, namely orders.sql and customers.sql, both of which were used to define the models that were similarly named, and also the staging models.&lt;/p&gt;\n\n&lt;p&gt;Surely, I looked under the &lt;code&gt;seeds&lt;/code&gt; directory since that would make sense as they&amp;#39;re seeding the project, but I don&amp;#39;t see anything there.&lt;/p&gt;\n\n&lt;p&gt;For reference, the two staging tables toward the end of the tutorial, &lt;code&gt;stg_customers.sql&lt;/code&gt; and &lt;code&gt;stg_orders.sql&lt;/code&gt; are defined to source from&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;`dbt-tutorial`.jaffle_shop.customers    and\n`dbt-tutorial`.jaffle_shop.orders\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;respectively.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried moving on and doing other stuff to further my learning, but I can&amp;#39;t scratch this and it&amp;#39;s irritating me.&lt;/p&gt;\n\n&lt;p&gt;I figured that I probably deleted it so I ran &lt;code&gt;dbt init&lt;/code&gt; again and there&amp;#39;s nothing present that&amp;#39;s just `orders.sql` or anything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?auto=webp&amp;v=enabled&amp;s=bfe5e9b2927d016e953dc1100d04aa7edae028b8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac2470355dc3f9626c6f35140e0ec423549da50e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6800fda7fcc0abb4bd00f0af3c485a21b9befd60", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ff6e290ecb9d197e6339baf74f37ad4bcf47da0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59c6c188d4255d45db867f741fbf4a6fe1161f4a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56be5b52655ecb694651e6bf1bf5a025673b7cc3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8a7d892a9bafd02790b2d76b5fbbe76e9d0857f", "width": 1080, "height": 567}], "variants": {}, "id": "KBohsdqrfvkRxfqADmI_uqtotFtqgZjYu8NQbRpJlaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10r5wrc", "is_robot_indexable": true, "report_reasons": null, "author": "paxmlank", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r5wrc/very_much_a_noob_question_but_for_dbts_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r5wrc/very_much_a_noob_question_but_for_dbts_tutorial/", "subreddit_subscribers": 88250, "created_utc": 1675284986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all! I'm a women who is currently working as a librarian, mainly on metadata for digital resources. Right now my job involves a lot of data transformation from one data format to another (think JSON to CSV and back again), extracting data as needed from applications using JSON APIs, and cleaning up data using a mix of Python (especially pandas) and basic formulas in Excel/LibreOffice Calc. These technical skills are all self-taught (5 years ago, I didn't know what a terminal was.) I really love it, but wish I could do more. I'd especially love to help create and improve data pipelines for library-related applications. Ideally, I'd like a job in library tech (or other non-profit sector) that still allows me to grapple with the nitty-gritty of data while doing higher-level tasks of infrastructure building.\n\nI think my biggest strengths right now are 1) a strong ability to understand relationships between entities quickly, 2) good problem-solving skills, 3) I'm naturally extremely patient. I think my biggest weakness are 1) I don't know anything about computer science, 2) I am slightly error-prone (I have ADHD), although I always write and test my scripts with this in mind, and 3) it's hard for me to speak up sometimes, especially if I feel out of my depth.\n\n1) Based on the description above, do you think data engineering would be a good fit for me? \n\n2) Do you still end up doing a lot of Python coding in your daily work? How much is troubleshooting vs project management?\n\n3) Do you work on a team? What is that like -- are you the expert on this or one of many experts? I'd especially love to hear from women.", "author_fullname": "t2_gnmgv5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Woman interested in data engineering with Python background", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rs411", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675350741.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! I&amp;#39;m a women who is currently working as a librarian, mainly on metadata for digital resources. Right now my job involves a lot of data transformation from one data format to another (think JSON to CSV and back again), extracting data as needed from applications using JSON APIs, and cleaning up data using a mix of Python (especially pandas) and basic formulas in Excel/LibreOffice Calc. These technical skills are all self-taught (5 years ago, I didn&amp;#39;t know what a terminal was.) I really love it, but wish I could do more. I&amp;#39;d especially love to help create and improve data pipelines for library-related applications. Ideally, I&amp;#39;d like a job in library tech (or other non-profit sector) that still allows me to grapple with the nitty-gritty of data while doing higher-level tasks of infrastructure building.&lt;/p&gt;\n\n&lt;p&gt;I think my biggest strengths right now are 1) a strong ability to understand relationships between entities quickly, 2) good problem-solving skills, 3) I&amp;#39;m naturally extremely patient. I think my biggest weakness are 1) I don&amp;#39;t know anything about computer science, 2) I am slightly error-prone (I have ADHD), although I always write and test my scripts with this in mind, and 3) it&amp;#39;s hard for me to speak up sometimes, especially if I feel out of my depth.&lt;/p&gt;\n\n&lt;p&gt;1) Based on the description above, do you think data engineering would be a good fit for me? &lt;/p&gt;\n\n&lt;p&gt;2) Do you still end up doing a lot of Python coding in your daily work? How much is troubleshooting vs project management?&lt;/p&gt;\n\n&lt;p&gt;3) Do you work on a team? What is that like -- are you the expert on this or one of many experts? I&amp;#39;d especially love to hear from women.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10rs411", "is_robot_indexable": true, "report_reasons": null, "author": "elleanywhere", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rs411/woman_interested_in_data_engineering_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rs411/woman_interested_in_data_engineering_with_python/", "subreddit_subscribers": 88250, "created_utc": 1675350741.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since Databricks is \"cloud-only\" I wanted to install the stack into a Centos VM to figure out the basic stuff (I come from a \"traditional\" database background, Oracle/MSSQL etc.)\n\n* Base installation was made using [this guide](https://largecats.github.io/blog/2019/08/19/hadoop-spark-hive-wsl/) I've found. It works quite well after minor changes. \n* Metastore is a Postgresql DB running in the same VM\n* Delta Format ([delta.io](https://delta.io)) included in Spark\n* Juypiter Notebook-Server and VS Code with Python plug-ins also installed &amp; configured\n\nBig question: Where are my tables??\n\n**Step 1: Created a directory in my HDFS and uploaded a .csv file**\n\n    [admroger@spark2 testdata]$ hadoop fs -mkdir /hw_monitoring\n    [admroger@spark2 testdata]$ hadoop fs -put /home/admroger/testdata/hw_monitoring.csv /hw_monitoring\n\n**Step 2: Using a notebook (called from the Windows host) successfully loaded the .csv into a Pyspark dataframe**\n\nMake sure [delta.io](https://delta.io) is included\n\n    from delta import *\n    from pyspark.sql import SparkSession\n    \n    builder = SparkSession.builder.appName(\"MyApp\") \\\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    \n    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n\n.. then load the file\n\n    csvFile = \"/hw_monitoring/hw_monitoring.csv\"\n    \n    df = spark.read.format(\"csv\") \\\n        .option(\"inferSchema\", \"true\") \\\n        .option(\"header\", \"true\") \\\n        .load(csvFile)\n\n**Step 3: Saved the dataframe as a delta-table**\n\n    df.write.format(\"delta\").saveAsTable(\"hwtable1\")\n\nObviously the table went to HDFS (I also find its traces in the metastore's tables in Postgres)\n\n    hadoop fs -ls /user/hive/warehouse/hwtable1\n\ndrwxr-xr-x   - admroger supergroup          0 2023-02-02 10:07 /user/hive/warehouse/hwtable1/\\_delta\\_log\n\n\\-rw-r--r--   1 admroger supergroup     105026 2023-02-02 10:07 /user/hive/warehouse/hwtable1/part-00000-63b422a2-0448-40ca-827a-de5ec46d22df-c000.snappy.parquet\n\nThis seems to be the \"default\" schema\n\n**Step 4: Load the table - that's where confusion starts**\n\n    # hdfs\n    # drwxr-xr-x   - admroger supergroup          0 2023-02-02 10:07 /user/hive/warehouse/hwtable1\n    dfTable = spark.read.format(\"delta\").load(\"hwtable1\")\n\n AnalysisException: Path does not exist: hwtable1 \n\nit also won't work for default.hwtable1\n\nTo make it work, I have to use the HDFS location:\n\n    dfTable = spark.read.format(\"delta\").load(\"/user/hive/warehouse/hwtable1\")\n    (ok)\n\nLooks like the metastore isn't really used ?? Do I have a misunderstanding here? I expect this to act like a catalog...\n\n**Step 5: Tried to save the same table in a different database**\n\n    $SPARK_HOME/bin/spark-sql\n    create database test1;\n    \n    show databases;\n    (ok)\n\n(I configured [delta.io](https://delta.io) in  $SPARK\\_HOME/conf/spark-defaults.xml so it's loaded &amp; available)\n\nBack in the Notebook, this won't work\n\n    df.write.format(\"delta\").saveAsTable(\"test1.hwTable1\")\n\n AnalysisException: Database 'test1' not found \n\nHow come?\n\nIt's not an actual problem, since I only want to work with Pyspark using previously loaded files/tables and that's okay - nevertheless I wonder why my installation isn't acting like a \"catalog-based\" system. I've spotted the \"test1\" database in HDFS as well as in the metastore-table (DBS)\n\n/user/hive/warehouse/test1.db\n\nDBS.DB\\_LOCATION\\_URI: hdfs://localhost:9000/user/hive/warehouse/test1.db", "author_fullname": "t2_337g1dil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confusion with Hadoop/Hive/Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rn7a7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675335595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since Databricks is &amp;quot;cloud-only&amp;quot; I wanted to install the stack into a Centos VM to figure out the basic stuff (I come from a &amp;quot;traditional&amp;quot; database background, Oracle/MSSQL etc.)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Base installation was made using &lt;a href=\"https://largecats.github.io/blog/2019/08/19/hadoop-spark-hive-wsl/\"&gt;this guide&lt;/a&gt; I&amp;#39;ve found. It works quite well after minor changes. &lt;/li&gt;\n&lt;li&gt;Metastore is a Postgresql DB running in the same VM&lt;/li&gt;\n&lt;li&gt;Delta Format (&lt;a href=\"https://delta.io\"&gt;delta.io&lt;/a&gt;) included in Spark&lt;/li&gt;\n&lt;li&gt;Juypiter Notebook-Server and VS Code with Python plug-ins also installed &amp;amp; configured&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Big question: Where are my tables??&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 1: Created a directory in my HDFS and uploaded a .csv file&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[admroger@spark2 testdata]$ hadoop fs -mkdir /hw_monitoring\n[admroger@spark2 testdata]$ hadoop fs -put /home/admroger/testdata/hw_monitoring.csv /hw_monitoring\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 2: Using a notebook (called from the Windows host) successfully loaded the .csv into a Pyspark dataframe&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Make sure &lt;a href=\"https://delta.io\"&gt;delta.io&lt;/a&gt; is included&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from delta import *\nfrom pyspark.sql import SparkSession\n\nbuilder = SparkSession.builder.appName(&amp;quot;MyApp&amp;quot;) \\\n    .config(&amp;quot;spark.sql.extensions&amp;quot;, &amp;quot;io.delta.sql.DeltaSparkSessionExtension&amp;quot;) \\\n    .config(&amp;quot;spark.sql.catalog.spark_catalog&amp;quot;, &amp;quot;org.apache.spark.sql.delta.catalog.DeltaCatalog&amp;quot;)\n\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;.. then load the file&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;csvFile = &amp;quot;/hw_monitoring/hw_monitoring.csv&amp;quot;\n\ndf = spark.read.format(&amp;quot;csv&amp;quot;) \\\n    .option(&amp;quot;inferSchema&amp;quot;, &amp;quot;true&amp;quot;) \\\n    .option(&amp;quot;header&amp;quot;, &amp;quot;true&amp;quot;) \\\n    .load(csvFile)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 3: Saved the dataframe as a delta-table&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.write.format(&amp;quot;delta&amp;quot;).saveAsTable(&amp;quot;hwtable1&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Obviously the table went to HDFS (I also find its traces in the metastore&amp;#39;s tables in Postgres)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;hadoop fs -ls /user/hive/warehouse/hwtable1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;drwxr-xr-x   - admroger supergroup          0 2023-02-02 10:07 /user/hive/warehouse/hwtable1/_delta_log&lt;/p&gt;\n\n&lt;p&gt;-rw-r--r--   1 admroger supergroup     105026 2023-02-02 10:07 /user/hive/warehouse/hwtable1/part-00000-63b422a2-0448-40ca-827a-de5ec46d22df-c000.snappy.parquet&lt;/p&gt;\n\n&lt;p&gt;This seems to be the &amp;quot;default&amp;quot; schema&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 4: Load the table - that&amp;#39;s where confusion starts&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# hdfs\n# drwxr-xr-x   - admroger supergroup          0 2023-02-02 10:07 /user/hive/warehouse/hwtable1\ndfTable = spark.read.format(&amp;quot;delta&amp;quot;).load(&amp;quot;hwtable1&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;AnalysisException: Path does not exist: hwtable1 &lt;/p&gt;\n\n&lt;p&gt;it also won&amp;#39;t work for default.hwtable1&lt;/p&gt;\n\n&lt;p&gt;To make it work, I have to use the HDFS location:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;dfTable = spark.read.format(&amp;quot;delta&amp;quot;).load(&amp;quot;/user/hive/warehouse/hwtable1&amp;quot;)\n(ok)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Looks like the metastore isn&amp;#39;t really used ?? Do I have a misunderstanding here? I expect this to act like a catalog...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Step 5: Tried to save the same table in a different database&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-sql\ncreate database test1;\n\nshow databases;\n(ok)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;(I configured &lt;a href=\"https://delta.io\"&gt;delta.io&lt;/a&gt; in  $SPARK_HOME/conf/spark-defaults.xml so it&amp;#39;s loaded &amp;amp; available)&lt;/p&gt;\n\n&lt;p&gt;Back in the Notebook, this won&amp;#39;t work&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.write.format(&amp;quot;delta&amp;quot;).saveAsTable(&amp;quot;test1.hwTable1&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;AnalysisException: Database &amp;#39;test1&amp;#39; not found &lt;/p&gt;\n\n&lt;p&gt;How come?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not an actual problem, since I only want to work with Pyspark using previously loaded files/tables and that&amp;#39;s okay - nevertheless I wonder why my installation isn&amp;#39;t acting like a &amp;quot;catalog-based&amp;quot; system. I&amp;#39;ve spotted the &amp;quot;test1&amp;quot; database in HDFS as well as in the metastore-table (DBS)&lt;/p&gt;\n\n&lt;p&gt;/user/hive/warehouse/test1.db&lt;/p&gt;\n\n&lt;p&gt;DBS.DB_LOCATION_URI: hdfs://localhost:9000/user/hive/warehouse/test1.db&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10rn7a7", "is_robot_indexable": true, "report_reasons": null, "author": "eierwerfer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rn7a7/confusion_with_hadoophivespark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rn7a7/confusion_with_hadoophivespark/", "subreddit_subscribers": 88250, "created_utc": 1675335595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vxxrqrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming data pipelines with DuckDB and Striim", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10rk5vy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CbO09xNraej6tWy1g156vTdO8TspmSAgclxdOCxJwxU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675323623.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/pedram/p/streaming-data-pipelines-with-striim", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UCmnpR4sUewEmjdPMiV26PadDNIeymKGozG8tkjymw8.jpg?auto=webp&amp;v=enabled&amp;s=8c99c9d608b35df70cf4ed77ef1483d03ab786fb", "width": 600, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/UCmnpR4sUewEmjdPMiV26PadDNIeymKGozG8tkjymw8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b479189de0aa6863645939c92e60d30f4d0722d", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/UCmnpR4sUewEmjdPMiV26PadDNIeymKGozG8tkjymw8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5f84c5f220cf61ffe1676e754673f4ea147c9b7", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/UCmnpR4sUewEmjdPMiV26PadDNIeymKGozG8tkjymw8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7214e4c0fd838789088190cdefce869662baf530", "width": 320, "height": 320}], "variants": {}, "id": "dsntXV9g7fIEf1VqManCNFXmD7-LI1B72jzUxKWsr34"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10rk5vy", "is_robot_indexable": true, "report_reasons": null, "author": "MrMosBiggestFan", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rk5vy/streaming_data_pipelines_with_duckdb_and_striim/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/pedram/p/streaming-data-pipelines-with-striim", "subreddit_subscribers": 88250, "created_utc": 1675323623.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a CPRA analyst and data governance analyst. Is PE and DE related, or separate? \n\nHow would someone be a PE?", "author_fullname": "t2_6qmgdlvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Privacy Engineering Related to DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r6h9g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675286300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a CPRA analyst and data governance analyst. Is PE and DE related, or separate? &lt;/p&gt;\n\n&lt;p&gt;How would someone be a PE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10r6h9g", "is_robot_indexable": true, "report_reasons": null, "author": "OkScientist96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r6h9g/is_privacy_engineering_related_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r6h9g/is_privacy_engineering_related_to_de/", "subreddit_subscribers": 88250, "created_utc": 1675286300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For me, analyze the data set, create the data models, define the architecture, set up the environment, design the pipelines, deploy , migrate to production. I have listed the phases but having a hard time to estimate the efforts, I know the complexity might be the factor which is the key. But what could be the worst case scenarios.", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you crafted a project plan for big data ? What are the parameters that were considered ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r0w1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675273542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For me, analyze the data set, create the data models, define the architecture, set up the environment, design the pipelines, deploy , migrate to production. I have listed the phases but having a hard time to estimate the efforts, I know the complexity might be the factor which is the key. But what could be the worst case scenarios.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10r0w1g", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r0w1g/have_you_crafted_a_project_plan_for_big_data_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r0w1g/have_you_crafted_a_project_plan_for_big_data_what/", "subreddit_subscribers": 88250, "created_utc": 1675273542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,  \n\n\nSorry if it has already been asked previously. I recently built the EL orchestration part of our data pipeline that is made of postgres and minio, with airflow. I am fetching from tables that are already been prepared and curated by another engineer and don't have a lot of visibility on the transformation being done by him (joining multiple tables, creating new fields, filtering etc). I would like to introduce him to dbt core (we can't move on cloud), so the transformations can be documented and version controlled and better understood by other people on the team. Am I on the right path, and would dbt would be appropriated for this ? Where can I start except the tutorial videos on the dbt website.  \n\n\nThank you folks !", "author_fullname": "t2_3ysn66ii", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best resources to getting started with dbt core with airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rshfo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675351661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,  &lt;/p&gt;\n\n&lt;p&gt;Sorry if it has already been asked previously. I recently built the EL orchestration part of our data pipeline that is made of postgres and minio, with airflow. I am fetching from tables that are already been prepared and curated by another engineer and don&amp;#39;t have a lot of visibility on the transformation being done by him (joining multiple tables, creating new fields, filtering etc). I would like to introduce him to dbt core (we can&amp;#39;t move on cloud), so the transformations can be documented and version controlled and better understood by other people on the team. Am I on the right path, and would dbt would be appropriated for this ? Where can I start except the tutorial videos on the dbt website.  &lt;/p&gt;\n\n&lt;p&gt;Thank you folks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10rshfo", "is_robot_indexable": true, "report_reasons": null, "author": "Jul1ano0", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rshfo/best_resources_to_getting_started_with_dbt_core/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rshfo/best_resources_to_getting_started_with_dbt_core/", "subreddit_subscribers": 88250, "created_utc": 1675351661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a junior DE working with a small team. Recently I was shadowing a senior DE who abruptly quit. I've been given their entire work load and feel completely overwhelmed. I also found out from my manager that the information the senior DE was giving me was wrong, to the point where my manager said he thinks they were sabotaging me but doesn't know why they would do that. The senior DE also deleted all of their data/workflows/processes and code.\n\nSo now were set back in some instances nearly two years and I'm working 14-16 hour days trying to rebuild things that are completely out of my area of knowledge and at the same time I'm getting pressure from different stakeholders to deliver data and products that I haven't even had enough time to rebuild yet or even learn about.\n\nI hate to sound like a cry baby but I feel totally overwhelmed and like a duck drowning.\n\nMy manager is trying to intercept as many stakeholders as he can to give me time while nudging me along.\n\nHow do you all handle it? Any tools or tips?", "author_fullname": "t2_gsch4oaq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle increasing stress?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10rudcp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675356298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a junior DE working with a small team. Recently I was shadowing a senior DE who abruptly quit. I&amp;#39;ve been given their entire work load and feel completely overwhelmed. I also found out from my manager that the information the senior DE was giving me was wrong, to the point where my manager said he thinks they were sabotaging me but doesn&amp;#39;t know why they would do that. The senior DE also deleted all of their data/workflows/processes and code.&lt;/p&gt;\n\n&lt;p&gt;So now were set back in some instances nearly two years and I&amp;#39;m working 14-16 hour days trying to rebuild things that are completely out of my area of knowledge and at the same time I&amp;#39;m getting pressure from different stakeholders to deliver data and products that I haven&amp;#39;t even had enough time to rebuild yet or even learn about.&lt;/p&gt;\n\n&lt;p&gt;I hate to sound like a cry baby but I feel totally overwhelmed and like a duck drowning.&lt;/p&gt;\n\n&lt;p&gt;My manager is trying to intercept as many stakeholders as he can to give me time while nudging me along.&lt;/p&gt;\n\n&lt;p&gt;How do you all handle it? Any tools or tips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10rudcp", "is_robot_indexable": true, "report_reasons": null, "author": "xxEiGhTyxx", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rudcp/how_do_you_handle_increasing_stress/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rudcp/how_do_you_handle_increasing_stress/", "subreddit_subscribers": 88250, "created_utc": 1675356298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Every week we receive a batch of data of around 200MB. We want to append this data to a large table that will contain all the weekly batches. What are the best practices to do this? We use GCP so my idea was to \n\n1. store the weekly batch in Cloud Storage\n2. use a Cloud Function (Python) to append the data to the large table\n3. push the large table to BigQuery since we need to combine it with other tables later on\n\nAs the BQ table starts getting larger, should I consider doing partitions to make transformations and querying more efficient? We also want to prevent the historic data from being accidentally deleted, so would you recommend doing a weekly snapshot of the large table and store it in a Storage somewhere else? I\u2019m relatively new to this to open to any suggestions! Thanks", "author_fullname": "t2_fibr5mdx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you design a pipeline that ingests a batch of data weekly and appends it to a large historic table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rphyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675343523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every week we receive a batch of data of around 200MB. We want to append this data to a large table that will contain all the weekly batches. What are the best practices to do this? We use GCP so my idea was to &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;store the weekly batch in Cloud Storage&lt;/li&gt;\n&lt;li&gt;use a Cloud Function (Python) to append the data to the large table&lt;/li&gt;\n&lt;li&gt;push the large table to BigQuery since we need to combine it with other tables later on&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;As the BQ table starts getting larger, should I consider doing partitions to make transformations and querying more efficient? We also want to prevent the historic data from being accidentally deleted, so would you recommend doing a weekly snapshot of the large table and store it in a Storage somewhere else? I\u2019m relatively new to this to open to any suggestions! Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10rphyl", "is_robot_indexable": true, "report_reasons": null, "author": "Ancient-Bad-5612", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rphyl/how_would_you_design_a_pipeline_that_ingests_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rphyl/how_would_you_design_a_pipeline_that_ingests_a/", "subreddit_subscribers": 88250, "created_utc": 1675343523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm thinking of doing a database of 'everything' but non-instance versions. So like people and their properties but not me or you. I was thinking you could add it to other data to give a fuller picture to maybe connect patterns that haven't been seen before. Would you guys find it useful or nah?", "author_fullname": "t2_pzfud0d2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non-instantiated objects and their properties database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r0k0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675272751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of doing a database of &amp;#39;everything&amp;#39; but non-instance versions. So like people and their properties but not me or you. I was thinking you could add it to other data to give a fuller picture to maybe connect patterns that haven&amp;#39;t been seen before. Would you guys find it useful or nah?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10r0k0o", "is_robot_indexable": true, "report_reasons": null, "author": "noone_thing", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r0k0o/noninstantiated_objects_and_their_properties/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r0k0o/noninstantiated_objects_and_their_properties/", "subreddit_subscribers": 88250, "created_utc": 1675272751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are we building a Real-Time Data Cache that Powers Blazing Fast APIs in Rust?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10ruswr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1675357298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "getdozer.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://getdozer.io/blog/2023/02/02/dozer-story/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10ruswr", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ruswr/why_are_we_building_a_realtime_data_cache_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://getdozer.io/blog/2023/02/02/dozer-story/", "subreddit_subscribers": 88250, "created_utc": 1675357298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nJunior DE asking for help as I was directed towards the pandas documentation for read_csv which doesn\u2019t help.\n\nEssentially the check is expecting a field to have a str dtype but when I pass it as an obj it still returns true and I don\u2019t understand why.", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Passing a field as obj dtype to great expectations expect_column_values_to_be_of_type returns true when it\u2019s expecting a str dtype?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rr5kr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675348210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;Junior DE asking for help as I was directed towards the pandas documentation for read_csv which doesn\u2019t help.&lt;/p&gt;\n\n&lt;p&gt;Essentially the check is expecting a field to have a str dtype but when I pass it as an obj it still returns true and I don\u2019t understand why.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10rr5kr", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rr5kr/passing_a_field_as_obj_dtype_to_great/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rr5kr/passing_a_field_as_obj_dtype_to_great/", "subreddit_subscribers": 88250, "created_utc": 1675348210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello to all, long-time lurker here, I just got my first job in a Data &amp; Analytics consultancy firm, in a Data Integration team. \n\nWe work with basically Talend + some Informatica, and we're responsible for ETL/ELT and creating the data fabric for our clients. Since I've been graduated in Business, for now I'll be put in some specific Data Quality/Data Governance projects and certifications.\n\nI know that Talend is very low-code but it's giving me a lot of insights and good practical knowledge about some technical aspects, like cloud-computing and data warehousing, without having to build something from zero or with a lot of code, which would be a little bit hard for me ATM, even if I really enjoy coding and solving hard problems. \n\nIs this a good beginning and opportunity in pursuing a DE career? Or at least being an Analytics Engineer, something like that?\n\nCheers.", "author_fullname": "t2_3dyr7rm6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First Job - Am I in the right path?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rqq4w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675347028.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello to all, long-time lurker here, I just got my first job in a Data &amp;amp; Analytics consultancy firm, in a Data Integration team. &lt;/p&gt;\n\n&lt;p&gt;We work with basically Talend + some Informatica, and we&amp;#39;re responsible for ETL/ELT and creating the data fabric for our clients. Since I&amp;#39;ve been graduated in Business, for now I&amp;#39;ll be put in some specific Data Quality/Data Governance projects and certifications.&lt;/p&gt;\n\n&lt;p&gt;I know that Talend is very low-code but it&amp;#39;s giving me a lot of insights and good practical knowledge about some technical aspects, like cloud-computing and data warehousing, without having to build something from zero or with a lot of code, which would be a little bit hard for me ATM, even if I really enjoy coding and solving hard problems. &lt;/p&gt;\n\n&lt;p&gt;Is this a good beginning and opportunity in pursuing a DE career? Or at least being an Analytics Engineer, something like that?&lt;/p&gt;\n\n&lt;p&gt;Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10rqq4w", "is_robot_indexable": true, "report_reasons": null, "author": "britalianrushfan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rqq4w/first_job_am_i_in_the_right_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rqq4w/first_job_am_i_in_the_right_path/", "subreddit_subscribers": 88250, "created_utc": 1675347028.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Intro to Data Modeling (by Max Beauchemin's team)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10rps2p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KC4TcYQegRyICUQxzw4kY85F-awDzM7qYAYAEdf6cPM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675344357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "preset.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://preset.io/events/intro-to-data-modeling", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vwXNoqj0BQHAEaCD573l3YoCF9SacYC7ISyQqjJjzFc.jpg?auto=webp&amp;v=enabled&amp;s=6b3c45223b39d568a0a84bf9a07a1615c5e66480", "width": 3600, "height": 3600}, "resolutions": [{"url": "https://external-preview.redd.it/vwXNoqj0BQHAEaCD573l3YoCF9SacYC7ISyQqjJjzFc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e70913a95bb99e1b0da2aed8251e4b72bb9dd904", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/vwXNoqj0BQHAEaCD573l3YoCF9SacYC7ISyQqjJjzFc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d85d7e6ce74a66bab0c601bdc8fae84b4bd31b71", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/vwXNoqj0BQHAEaCD573l3YoCF9SacYC7ISyQqjJjzFc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a34754de1ac8c902a959020818aacf901f8d9aec", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/vwXNoqj0BQHAEaCD573l3YoCF9SacYC7ISyQqjJjzFc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=722a7cd676ea45c748ae09d2573c66b90588c0b0", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/vwXNoqj0BQHAEaCD573l3YoCF9SacYC7ISyQqjJjzFc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2e7bacbc754be884342826d29f9822672a78289", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/vwXNoqj0BQHAEaCD573l3YoCF9SacYC7ISyQqjJjzFc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d52e737012c092c6e89142e7592e34d6dd6c83bd", "width": 1080, "height": 1080}], "variants": {}, "id": "xGObo4WNt54ffILEAXd-J-l6m0xR8a1z5R4HqI6ONV4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10rps2p", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rps2p/intro_to_data_modeling_by_max_beauchemins_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://preset.io/events/intro-to-data-modeling", "subreddit_subscribers": 88250, "created_utc": 1675344357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Social media listening is the process of monitoring and analyzing social media conversations about a brand, industry, product, or topic. Here's how it can help your business:\n\nUnderstand your audience better: Social media listening allows you to understand your target audience's opinions, needs, and preferences. You can see how they engage with your brand and what topics they are interested in. This information can help you create content and campaigns that resonate with your audience.\n\nAcquire new clients: Social media listening can help you identify potential customers and prospects who are interested in your products or services. By engaging with them on social media and responding to their questions and concerns, you can build trust and establish yourself as a thought leader in your industry.\n\nMonitor Brand Perception: Social media listening allows you to monitor the perception of your brand in real-time. You can see what people are saying about your brand, what they like and dislike, and what they expect from your business. This information can help you identify areas where you need to improve and make changes to better meet the needs of your customers.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/fgejl5udgqfa1.png?width=1434&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=58e9bf222a00b6123615f2d95ca47a7249b85830", "author_fullname": "t2_jtca4f0z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can social media listening help your business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "media_metadata": {"fgejl5udgqfa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58b0dfec20190a3ba992d369cf19f42a17244781"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a0be39df4c69198e1b400df783033cc283329d9"}, {"y": 226, "x": 320, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a087eead109e458e06516f0a256dbbd98f412b40"}, {"y": 452, "x": 640, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc11ccc3efe747d1a76ebc9d83f26f6c2a8ddb5b"}, {"y": 678, "x": 960, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b78c3b429071a2358a04fe1ffcbf5922bd8f9245"}, {"y": 763, "x": 1080, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2121c8ab0e5425ec11e8a91daca697993e8a82c"}], "s": {"y": 1014, "x": 1434, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=1434&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=58e9bf222a00b6123615f2d95ca47a7249b85830"}, "id": "fgejl5udgqfa1"}}, "name": "t3_10rkl71", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bRZGP7jAQ-QzzxRn_ja-HwsC3bL5lnN_Knm5G-sKx7E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675325246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Social media listening is the process of monitoring and analyzing social media conversations about a brand, industry, product, or topic. Here&amp;#39;s how it can help your business:&lt;/p&gt;\n\n&lt;p&gt;Understand your audience better: Social media listening allows you to understand your target audience&amp;#39;s opinions, needs, and preferences. You can see how they engage with your brand and what topics they are interested in. This information can help you create content and campaigns that resonate with your audience.&lt;/p&gt;\n\n&lt;p&gt;Acquire new clients: Social media listening can help you identify potential customers and prospects who are interested in your products or services. By engaging with them on social media and responding to their questions and concerns, you can build trust and establish yourself as a thought leader in your industry.&lt;/p&gt;\n\n&lt;p&gt;Monitor Brand Perception: Social media listening allows you to monitor the perception of your brand in real-time. You can see what people are saying about your brand, what they like and dislike, and what they expect from your business. This information can help you identify areas where you need to improve and make changes to better meet the needs of your customers.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fgejl5udgqfa1.png?width=1434&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=58e9bf222a00b6123615f2d95ca47a7249b85830\"&gt;https://preview.redd.it/fgejl5udgqfa1.png?width=1434&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=58e9bf222a00b6123615f2d95ca47a7249b85830&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10rkl71", "is_robot_indexable": true, "report_reasons": null, "author": "hardik-s", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rkl71/how_can_social_media_listening_help_your_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rkl71/how_can_social_media_listening_help_your_business/", "subreddit_subscribers": 88250, "created_utc": 1675325246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "...what would you suggest?\n\nFocussing on:\n\n* Data Engineering processes (Databricks + AWS)\n* Adoption step by step\n* Best practice", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need to become an expert on enterprise databricks adoption but nothing to do with code...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r3rrr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675280872.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675280111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;...what would you suggest?&lt;/p&gt;\n\n&lt;p&gt;Focussing on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Engineering processes (Databricks + AWS)&lt;/li&gt;\n&lt;li&gt;Adoption step by step&lt;/li&gt;\n&lt;li&gt;Best practice&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10r3rrr", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r3rrr/i_need_to_become_an_expert_on_enterprise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r3rrr/i_need_to_become_an_expert_on_enterprise/", "subreddit_subscribers": 88250, "created_utc": 1675280111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lease remote resources? Kinda going over my head. Can someone give a real time example?", "author_fullname": "t2_5lve5av0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the use case of nimbus IAAS tool kit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r2504", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675276388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lease remote resources? Kinda going over my head. Can someone give a real time example?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10r2504", "is_robot_indexable": true, "report_reasons": null, "author": "noobmastersmaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r2504/what_is_the_use_case_of_nimbus_iaas_tool_kit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r2504/what_is_the_use_case_of_nimbus_iaas_tool_kit/", "subreddit_subscribers": 88250, "created_utc": 1675276388.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}