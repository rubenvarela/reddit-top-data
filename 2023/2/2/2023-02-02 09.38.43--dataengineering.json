{"kind": "Listing", "data": {"after": "t3_10qsp3w", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently interviewed with Uber and had 3 rounds with them:\n\n1. DSA - Graph based problem\n2. Spark/SQL/Scaling - Asked to write a query to find number of users who went to a same group of cities (order matters, records need to be ordered by time). Asked to give time complexity of SQL query. Asked to port that to spark, lot of cross questioning about optimisations, large amount of data handling in spark with limited resources etc.\n3. System Design - Asked to design bookmyshow. Lot of cross questioning around concurrency, fault tolerance, CAP theorem, how to choose data sources etc.\n\nMy interviews didn't went the way I hoped, so wanted to understand from more experienced folks here, how do I prepare for:\n\n1. Big O notation complexity calculation on a sql query\n2. Prepare of system design, data modeling for system design. I was stumped on choosing data sources for specific purposes (like which data source to use for storing seats availability)", "author_fullname": "t2_cu6opso3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Uber Interview Experience/Asking Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qzicp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675271957.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675270322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently interviewed with Uber and had 3 rounds with them:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;DSA - Graph based problem&lt;/li&gt;\n&lt;li&gt;Spark/SQL/Scaling - Asked to write a query to find number of users who went to a same group of cities (order matters, records need to be ordered by time). Asked to give time complexity of SQL query. Asked to port that to spark, lot of cross questioning about optimisations, large amount of data handling in spark with limited resources etc.&lt;/li&gt;\n&lt;li&gt;System Design - Asked to design bookmyshow. Lot of cross questioning around concurrency, fault tolerance, CAP theorem, how to choose data sources etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My interviews didn&amp;#39;t went the way I hoped, so wanted to understand from more experienced folks here, how do I prepare for:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Big O notation complexity calculation on a sql query&lt;/li&gt;\n&lt;li&gt;Prepare of system design, data modeling for system design. I was stumped on choosing data sources for specific purposes (like which data source to use for storing seats availability)&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10qzicp", "is_robot_indexable": true, "report_reasons": null, "author": "bha159", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qzicp/uber_interview_experienceasking_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qzicp/uber_interview_experienceasking_suggestions/", "subreddit_subscribers": 88214, "created_utc": 1675270322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Being hired as a Data Analyst Intern by an SME but doing database development, DWH and BI report. When I come in, they were using Power BI to read live data from ERP and did all the transformation work and nobody can tell a reason why they do any certain transformation (the intern before me only instructed by a senior person and both left that company, remained those queries that impossible to read. Stakeholder complained the unknown calculations and the endless errors in the reports. It took me six months to go through Reddit and the DWH toolkit to gain knowledge and build up an basic infrastructure with some help from the only SWE in my company. And now my leader start to complain why it took me so long to develop TWO reports (he simply divide 6 months by 2 to get the time spending). Another Data analyst benefit from my database and his report gain a lot of flowers by Senior leadership. Does Data Engineer always be the first to blame and the last to credit? I am starting to doubt myself whether this position is the right one for me.", "author_fullname": "t2_ehg2vux7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Data Engineer not suitable for me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10racds", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675295400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Being hired as a Data Analyst Intern by an SME but doing database development, DWH and BI report. When I come in, they were using Power BI to read live data from ERP and did all the transformation work and nobody can tell a reason why they do any certain transformation (the intern before me only instructed by a senior person and both left that company, remained those queries that impossible to read. Stakeholder complained the unknown calculations and the endless errors in the reports. It took me six months to go through Reddit and the DWH toolkit to gain knowledge and build up an basic infrastructure with some help from the only SWE in my company. And now my leader start to complain why it took me so long to develop TWO reports (he simply divide 6 months by 2 to get the time spending). Another Data analyst benefit from my database and his report gain a lot of flowers by Senior leadership. Does Data Engineer always be the first to blame and the last to credit? I am starting to doubt myself whether this position is the right one for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10racds", "is_robot_indexable": true, "report_reasons": null, "author": "JowwLee", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10racds/is_data_engineer_not_suitable_for_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10racds/is_data_engineer_not_suitable_for_me/", "subreddit_subscribers": 88214, "created_utc": 1675295400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you provide access to end users to the RDBMS (say Postgres) warehouse if they want to use (only) python? Do you build custom python SDK which include predefined queries? Or maybe use something like PostgREST? Or SQLAlchemy? Or any other  way?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Providing end users access to the database using Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qv2x3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675259128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you provide access to end users to the RDBMS (say Postgres) warehouse if they want to use (only) python? Do you build custom python SDK which include predefined queries? Or maybe use something like PostgREST? Or SQLAlchemy? Or any other  way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qv2x3", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qv2x3/providing_end_users_access_to_the_database_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qv2x3/providing_end_users_access_to_the_database_using/", "subreddit_subscribers": 88214, "created_utc": 1675259128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I think the general idea of Airflow is to orchestrate but I came across two deployments in the last week that are running thousands of tasks concurrently using dynamic tasks - a sort of map reduce process using airflow.\n\nThe data is being passed around using the file system. We can also see many examples online of carrying the heavy lifting inside Airflow.\n\nHave you noticed that? It\u2019s kind of tempting - especially with the dynamic tasks I\u2019m thinking yeah sure why not? Then I don\u2019t have to deal with any other tooling to process my data.\n\nEspecially if you deploy to Kubernetes you can keep scaling without too much issues", "author_fullname": "t2_htptc13q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Airflow Anti-Patterns the Norm Now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qtv21", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675255548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think the general idea of Airflow is to orchestrate but I came across two deployments in the last week that are running thousands of tasks concurrently using dynamic tasks - a sort of map reduce process using airflow.&lt;/p&gt;\n\n&lt;p&gt;The data is being passed around using the file system. We can also see many examples online of carrying the heavy lifting inside Airflow.&lt;/p&gt;\n\n&lt;p&gt;Have you noticed that? It\u2019s kind of tempting - especially with the dynamic tasks I\u2019m thinking yeah sure why not? Then I don\u2019t have to deal with any other tooling to process my data.&lt;/p&gt;\n\n&lt;p&gt;Especially if you deploy to Kubernetes you can keep scaling without too much issues&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qtv21", "is_robot_indexable": true, "report_reasons": null, "author": "Main_Tap_1256", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qtv21/are_airflow_antipatterns_the_norm_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qtv21/are_airflow_antipatterns_the_norm_now/", "subreddit_subscribers": 88214, "created_utc": 1675255548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to learn the ins and outs of snowflake and data mesh. Any comprehensive tutorials out there?", "author_fullname": "t2_122ieh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Data Mesh Tutorials?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rdoqz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675303800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to learn the ins and outs of snowflake and data mesh. Any comprehensive tutorials out there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10rdoqz", "is_robot_indexable": true, "report_reasons": null, "author": "Ltothetm", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rdoqz/snowflake_data_mesh_tutorials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rdoqz/snowflake_data_mesh_tutorials/", "subreddit_subscribers": 88214, "created_utc": 1675303800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the actual use cases of the data contracts? I just read about them and from what I understand, it is used mainly for defining data schema for streaming data? I.e. you define schema you expect your data to adhere when reading such data from e.g. Kafka? And it is useful because e.g. protobuf can be implemented using different languages? Or are data contracts used in some other way? Do they make any sense for batch data processing?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data contracts actual use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qrefc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675250797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the actual use cases of the data contracts? I just read about them and from what I understand, it is used mainly for defining data schema for streaming data? I.e. you define schema you expect your data to adhere when reading such data from e.g. Kafka? And it is useful because e.g. protobuf can be implemented using different languages? Or are data contracts used in some other way? Do they make any sense for batch data processing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qrefc", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qrefc/data_contracts_actual_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qrefc/data_contracts_actual_use_cases/", "subreddit_subscribers": 88214, "created_utc": 1675250797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a basic question with probably a simple answer, but I'm still not sure what's best practice. In my hypothetical ETL pipeline I have my extraction step where I read data from somewhere and write it to S3 as a CSV.\n\nThis first run is easy since I don't have any extracted data files from previous runs. However, the second time this pipeline runs I already have a file from the previous run, and I can choose to version it with a new S3 key or overwrite the previous key. Now the question is which should I chose?\n\nDoes this depend on whether I do incramental reads or full reads?\n\nDoes it also depend on whether my warehouse is SCD1 or SCD2?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I version extracted data files on repeated ELT runs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rcohx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675301205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a basic question with probably a simple answer, but I&amp;#39;m still not sure what&amp;#39;s best practice. In my hypothetical ETL pipeline I have my extraction step where I read data from somewhere and write it to S3 as a CSV.&lt;/p&gt;\n\n&lt;p&gt;This first run is easy since I don&amp;#39;t have any extracted data files from previous runs. However, the second time this pipeline runs I already have a file from the previous run, and I can choose to version it with a new S3 key or overwrite the previous key. Now the question is which should I chose?&lt;/p&gt;\n\n&lt;p&gt;Does this depend on whether I do incramental reads or full reads?&lt;/p&gt;\n\n&lt;p&gt;Does it also depend on whether my warehouse is SCD1 or SCD2?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10rcohx", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rcohx/should_i_version_extracted_data_files_on_repeated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rcohx/should_i_version_extracted_data_files_on_repeated/", "subreddit_subscribers": 88214, "created_utc": 1675301205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a compsci student who's trying to get into this field and I really don't like the idea of using low-code/no-code tools (i.e. Tableu or Power BI) \u2014 I'm a programmer after all.  A few months ago I was planning to learn Plotly Dash because I wanted to start a carreer working with data, but most roles which involve analysis and dashboarding have as prerequisite low code tools like Excel and those aforementioned.\n\nStraight to the point, now: I want to become a DE because I like to build stuff with code and I would love to make some dashboards with Python here and there, but:\n\n1) I don't even know if dashboarding skills is something needed to this role.\n\n2) Assuming it is, how useful is to learn a \"low level\" tool like Plotly Dash since the market usually asks for low-code stuff?", "author_fullname": "t2_pme9byci", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should a data engineer aspirant learn dashboarding tools like Tableu and/or Plotly Dash?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r72qx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675287654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a compsci student who&amp;#39;s trying to get into this field and I really don&amp;#39;t like the idea of using low-code/no-code tools (i.e. Tableu or Power BI) \u2014 I&amp;#39;m a programmer after all.  A few months ago I was planning to learn Plotly Dash because I wanted to start a carreer working with data, but most roles which involve analysis and dashboarding have as prerequisite low code tools like Excel and those aforementioned.&lt;/p&gt;\n\n&lt;p&gt;Straight to the point, now: I want to become a DE because I like to build stuff with code and I would love to make some dashboards with Python here and there, but:&lt;/p&gt;\n\n&lt;p&gt;1) I don&amp;#39;t even know if dashboarding skills is something needed to this role.&lt;/p&gt;\n\n&lt;p&gt;2) Assuming it is, how useful is to learn a &amp;quot;low level&amp;quot; tool like Plotly Dash since the market usually asks for low-code stuff?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10r72qx", "is_robot_indexable": true, "report_reasons": null, "author": "mr_tellok", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r72qx/should_a_data_engineer_aspirant_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r72qx/should_a_data_engineer_aspirant_learn/", "subreddit_subscribers": 88214, "created_utc": 1675287654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Share Data with Snowflake Data Sharing (with examples)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_10qwozi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": "transparent", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1q8IakFKMNzTQCObe9Q7ElwGo3r5_mvFu3G_w6BEk5c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675263429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "prequel.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.prequel.co/blog/how-to-share-data-with-snowflake-data-sharing", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?auto=webp&amp;v=enabled&amp;s=da978d0a443e9b734d24029be2a75603b0c9768d", "width": 1920, "height": 820}, "resolutions": [{"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebc0cf3491161978a928a0bd40b30156c504f74a", "width": 108, "height": 46}, {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf0dd251abed2b4a5f6d434eaedd8d107064b7d8", "width": 216, "height": 92}, {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=015a9589470108d9aad7758462d684bf484f4ca0", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=692b96c58baac0764e69d3cb39f2e9fa756f91c0", "width": 640, "height": 273}, {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8efad1fbb093e4f14fd609c95bff616808dd1e4f", "width": 960, "height": 410}, {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d813c4a516adb779c9a2b40df18a63c70e83584", "width": 1080, "height": 461}], "variants": {}, "id": "hW2pCvJeKg5crzWPDHlylm0VFGaSzyon859YfBjDf8M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10qwozi", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10qwozi/how_to_share_data_with_snowflake_data_sharing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.prequel.co/blog/how-to-share-data-with-snowflake-data-sharing", "subreddit_subscribers": 88214, "created_utc": 1675263429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nI'm new to this so please forgive my newbie question. Trying to retrieve files from onedrive via a python script and deploying it to Google Cloud Functions. Any idea how? Do I use Microsoft Graph? Need some guide to start. Thank u!", "author_fullname": "t2_c58xvng2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How To Retrieve Files From Onedrive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10rgmd5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675311934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to this so please forgive my newbie question. Trying to retrieve files from onedrive via a python script and deploying it to Google Cloud Functions. Any idea how? Do I use Microsoft Graph? Need some guide to start. Thank u!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10rgmd5", "is_robot_indexable": true, "report_reasons": null, "author": "dehydratedcatnip", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rgmd5/how_to_retrieve_files_from_onedrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rgmd5/how_to_retrieve_files_from_onedrive/", "subreddit_subscribers": 88214, "created_utc": 1675311934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you handle a destructive change to the table used in production? Let's say we have a Postgres as production warehouse and applications query the data from it (obviously). And now we want to change some table in the WH and let's say we want to delete some columns. How do you manage this process? You create a clone table with the same data and give some period to all applications to adapt to the new table and then discontinue the old one? Or what is the best practice in such scenario?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling table schema change in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qs3or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675252031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you handle a destructive change to the table used in production? Let&amp;#39;s say we have a Postgres as production warehouse and applications query the data from it (obviously). And now we want to change some table in the WH and let&amp;#39;s say we want to delete some columns. How do you manage this process? You create a clone table with the same data and give some period to all applications to adapt to the new table and then discontinue the old one? Or what is the best practice in such scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qs3or", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qs3or/handling_table_schema_change_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qs3or/handling_table_schema_change_in_production/", "subreddit_subscribers": 88214, "created_utc": 1675252031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone\n\nI'm trying to get a new job in Data Engineering after doing Data Talks Zoomcamp, but is rough. My past two years I've worked as Web Developer/Tech Guy but I don't see my career or paycheck going up.", "author_fullname": "t2_kgod5v4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resume feedback?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10r9zeo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/oE6zteO3_yv0IzBAGKQFROoRwESfujM4Y_bU-_gLnr4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675294499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to get a new job in Data Engineering after doing Data Talks Zoomcamp, but is rough. My past two years I&amp;#39;ve worked as Web Developer/Tech Guy but I don&amp;#39;t see my career or paycheck going up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/vlr8ws1oepfa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?auto=webp&amp;v=enabled&amp;s=1e1d030c2dc02e376420e4b51cac3bdeaff70ae2", "width": 1080, "height": 1526}, "resolutions": [{"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfee1f63d7877ae8dfb54d902c2136c5979ad372", "width": 108, "height": 152}, {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f9d62687712f866eb0b8dd7d6c06c7581ff6573", "width": 216, "height": 305}, {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7e04cf35ba04b1f900baca59d9c70cddb6b58ee", "width": 320, "height": 452}, {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d4a35b7d3454a0f70dcff90c66b6da9367ce439", "width": 640, "height": 904}, {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61570f6cc42defccedd317a9993df22bcde6169d", "width": 960, "height": 1356}, {"url": "https://preview.redd.it/vlr8ws1oepfa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa2c15a37a5a817f8dfb540b5be5d63aed0f194a", "width": 1080, "height": 1526}], "variants": {}, "id": "4WhbjGb0o0A5DFLu8MMbU0qM-QncwVLubWYEgHnWQ0k"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10r9zeo", "is_robot_indexable": true, "report_reasons": null, "author": "FarFaithlessness8812", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r9zeo/resume_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/vlr8ws1oepfa1.png", "subreddit_subscribers": 88214, "created_utc": 1675294499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently a new DE at my company and we\u2019re designing and implementing a data lakehouse using Databricks and Unity Catalog. \n\nMy coworkers were telling me about the design of the landing container. We have several source systems and they will transfer their data to the ADLS bucket on an hourly basis to folders like source_system/domain/item/YYYY/MM/DD/HH. The data are CDC logs in JSON format. \n\nThis part is straightforward and I can understand it.\n\nThe next part is the part I have questions about: we have jobs that will read the raw files from the landing container and basically turn them into delta format. We have a design that involves 3 folders \u201clanding\u201d, \u201cworking\u201d, \u201carchived\u201d. Whenever you start a job that involves that file, you would move that file on the landing container from a \u201clanding\u201d folder to a \u201cworking\u201d folder. Then once the job is completed, we are supposed to move the file from the \u201cworking\u201d folder to an \u201carchived\u201d folder.\n\nA problem arises when we have several jobs that involves the same file. Let\u2019s say both job A and job B uses that raw file, and they\u2019re sequential. When job A is done with the file, it moves it to an \u201carchived\u201d folder. Now job B, which expects the file to be in the \u201clanding\u201d folder, can no longer locate the file, because it\u2019s been moved to the \u201carchived\u201d folder. It is even a bigger problem when both job A and job B are happening at the same time. \n\nMy coworkers tell me that this is intentional and what should actually happen is that we are supposed to create 2 copies of the raw file, or in the case of 3 jobs being dependent on the file, then 3 copies. We would have to rename the item to something like item_copy1, item_copy2. \n\nThe reason is that this is a best practice because now the state of where the file is will reflect the status of the job. Upon a job success, files are always moved to the \u201carchived\u201d folder. The \u201clanding\u201d and \u201cworking\u201d folders should always be empty after a job. If they are not empty, it will help indicate to us that some error happened to the job.\n\nI\u2019ve never seen a design like this before and I\u2019m used to the design where once a file is in the landing container, we don\u2019t move it and we only read from it. We use Airflow to track the success of individual jobs instead of the file location. But my coworkers told me this would be brittle design as now I am too reliant of Airflow for information on the status of the job. \n\nI was wondering if anyone could educate me or point me to resources that explain this type of design to me more. Or correct me if this is some well-known best practice that I\u2019m misunderstanding and butchering. I want to understand the rationale behind creating copies and moving files to an \u201carchived\u201d folder and its benefits.", "author_fullname": "t2_13lfsb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lakehouse architecture - design of landing container", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qzlma", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675270728.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675270547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently a new DE at my company and we\u2019re designing and implementing a data lakehouse using Databricks and Unity Catalog. &lt;/p&gt;\n\n&lt;p&gt;My coworkers were telling me about the design of the landing container. We have several source systems and they will transfer their data to the ADLS bucket on an hourly basis to folders like source_system/domain/item/YYYY/MM/DD/HH. The data are CDC logs in JSON format. &lt;/p&gt;\n\n&lt;p&gt;This part is straightforward and I can understand it.&lt;/p&gt;\n\n&lt;p&gt;The next part is the part I have questions about: we have jobs that will read the raw files from the landing container and basically turn them into delta format. We have a design that involves 3 folders \u201clanding\u201d, \u201cworking\u201d, \u201carchived\u201d. Whenever you start a job that involves that file, you would move that file on the landing container from a \u201clanding\u201d folder to a \u201cworking\u201d folder. Then once the job is completed, we are supposed to move the file from the \u201cworking\u201d folder to an \u201carchived\u201d folder.&lt;/p&gt;\n\n&lt;p&gt;A problem arises when we have several jobs that involves the same file. Let\u2019s say both job A and job B uses that raw file, and they\u2019re sequential. When job A is done with the file, it moves it to an \u201carchived\u201d folder. Now job B, which expects the file to be in the \u201clanding\u201d folder, can no longer locate the file, because it\u2019s been moved to the \u201carchived\u201d folder. It is even a bigger problem when both job A and job B are happening at the same time. &lt;/p&gt;\n\n&lt;p&gt;My coworkers tell me that this is intentional and what should actually happen is that we are supposed to create 2 copies of the raw file, or in the case of 3 jobs being dependent on the file, then 3 copies. We would have to rename the item to something like item_copy1, item_copy2. &lt;/p&gt;\n\n&lt;p&gt;The reason is that this is a best practice because now the state of where the file is will reflect the status of the job. Upon a job success, files are always moved to the \u201carchived\u201d folder. The \u201clanding\u201d and \u201cworking\u201d folders should always be empty after a job. If they are not empty, it will help indicate to us that some error happened to the job.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve never seen a design like this before and I\u2019m used to the design where once a file is in the landing container, we don\u2019t move it and we only read from it. We use Airflow to track the success of individual jobs instead of the file location. But my coworkers told me this would be brittle design as now I am too reliant of Airflow for information on the status of the job. &lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone could educate me or point me to resources that explain this type of design to me more. Or correct me if this is some well-known best practice that I\u2019m misunderstanding and butchering. I want to understand the rationale behind creating copies and moving files to an \u201carchived\u201d folder and its benefits.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qzlma", "is_robot_indexable": true, "report_reasons": null, "author": "pizzanub", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qzlma/data_lakehouse_architecture_design_of_landing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qzlma/data_lakehouse_architecture_design_of_landing/", "subreddit_subscribers": 88214, "created_utc": 1675270547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today I found out about AWS Quicksight Q which helps answering business questions using NLP based on reports created in it (like what was my sales for the last week?)\n\nI want to know if there is a solution could be integrated via slack bots to help people get quick answers to such queries. Currently Quicksight Q doesn\u2019t allow slack integration I think. \n\nFeel like this feature can be helpful for quick answers rather than searching through a report.", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NLP support for reports", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qp8dc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675243625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today I found out about AWS Quicksight Q which helps answering business questions using NLP based on reports created in it (like what was my sales for the last week?)&lt;/p&gt;\n\n&lt;p&gt;I want to know if there is a solution could be integrated via slack bots to help people get quick answers to such queries. Currently Quicksight Q doesn\u2019t allow slack integration I think. &lt;/p&gt;\n\n&lt;p&gt;Feel like this feature can be helpful for quick answers rather than searching through a report.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10qp8dc", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qp8dc/nlp_support_for_reports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qp8dc/nlp_support_for_reports/", "subreddit_subscribers": 88214, "created_utc": 1675243625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you think about this news? I think this will make Azure a way better choice for Data Engineering workloads, if the managed airflow really works like I\u2019d imagine it to do. \n\nhttps://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151", "author_fullname": "t2_aqzwz9ec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing 'Managed Airflow' in Azure Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10rk9tj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675324053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you think about this news? I think this will make Azure a way better choice for Data Engineering workloads, if the managed airflow really works like I\u2019d imagine it to do. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151\"&gt;https://techcommunity.microsoft.com/t5/azure-data-factory-blog/introducing-managed-airflow-in-azure-data-factory/ba-p/3730151&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?auto=webp&amp;v=enabled&amp;s=377641832eecbd621b310ab9dd16d381993c449f", "width": 2288, "height": 1448}, "resolutions": [{"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc4ffca7b91875a7fa3134c0d86d66dcd449c179", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75220a5e620ee298b7b2d8329044b879daca4586", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f764681c03c53d1229c8ff8e3799ea7bec9e4899", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f124a2fdd04ffd44d572a4e01089662b94a5f067", "width": 640, "height": 405}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7d3af51b0e95fbe753302b6f4c7969990339dc6", "width": 960, "height": 607}, {"url": "https://external-preview.redd.it/n6wIcVLTyCX0Hkixi_F_yyCCss0LwNNXlZp3hmCmoRU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a177ca1e48aaead34281cb5f3e145a59154b95e", "width": 1080, "height": 683}], "variants": {}, "id": "S0ZC01nTKv9edIZMquYzX-iG3I-MSj166ECzeJEKDn8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10rk9tj", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Noise-3261", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rk9tj/introducing_managed_airflow_in_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rk9tj/introducing_managed_airflow_in_azure_data_factory/", "subreddit_subscribers": 88214, "created_utc": 1675324053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a CPRA analyst and data governance analyst. Is PE and DE related, or separate? \n\nHow would someone be a PE?", "author_fullname": "t2_6qmgdlvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Privacy Engineering Related to DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r6h9g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675286300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a CPRA analyst and data governance analyst. Is PE and DE related, or separate? &lt;/p&gt;\n\n&lt;p&gt;How would someone be a PE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10r6h9g", "is_robot_indexable": true, "report_reasons": null, "author": "OkScientist96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r6h9g/is_privacy_engineering_related_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r6h9g/is_privacy_engineering_related_to_de/", "subreddit_subscribers": 88214, "created_utc": 1675286300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've gone through the [Getting started with dbt Core](https://docs.getdbt.com/docs/get-started/getting-started-dbt-core) tutorial and I've connected my BigQuery instance to dbt through the use of the JSON file in the relevant directory.\n\nI've made changes to tables/models and see those reflected in the warehouse each time I run `dbt run`, but I cannot see where some of the base tables exist, namely orders.sql and customers.sql, both of which were used to define the models that were similarly named, and also the staging models.\n\nSurely, I looked under the `seeds` directory since that would make sense as they're seeding the project, but I don't see anything there.\n\nFor reference, the two staging tables toward the end of the tutorial, `stg_customers.sql` and `stg_orders.sql` are defined to source from\n\n    `dbt-tutorial`.jaffle_shop.customers    and\n    `dbt-tutorial`.jaffle_shop.orders\n\nrespectively.\n\nI've tried moving on and doing other stuff to further my learning, but I can't scratch this and it's irritating me.\n\nI figured that I probably deleted it so I ran `dbt init` again and there's nothing present that's just \\`orders.sql\\` or anything.", "author_fullname": "t2_svn12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Very much a noob question, but for dbt's tutorial, where are the order and customer tables kept?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r5wrc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675285747.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675284986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve gone through the &lt;a href=\"https://docs.getdbt.com/docs/get-started/getting-started-dbt-core\"&gt;Getting started with dbt Core&lt;/a&gt; tutorial and I&amp;#39;ve connected my BigQuery instance to dbt through the use of the JSON file in the relevant directory.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve made changes to tables/models and see those reflected in the warehouse each time I run &lt;code&gt;dbt run&lt;/code&gt;, but I cannot see where some of the base tables exist, namely orders.sql and customers.sql, both of which were used to define the models that were similarly named, and also the staging models.&lt;/p&gt;\n\n&lt;p&gt;Surely, I looked under the &lt;code&gt;seeds&lt;/code&gt; directory since that would make sense as they&amp;#39;re seeding the project, but I don&amp;#39;t see anything there.&lt;/p&gt;\n\n&lt;p&gt;For reference, the two staging tables toward the end of the tutorial, &lt;code&gt;stg_customers.sql&lt;/code&gt; and &lt;code&gt;stg_orders.sql&lt;/code&gt; are defined to source from&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;`dbt-tutorial`.jaffle_shop.customers    and\n`dbt-tutorial`.jaffle_shop.orders\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;respectively.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried moving on and doing other stuff to further my learning, but I can&amp;#39;t scratch this and it&amp;#39;s irritating me.&lt;/p&gt;\n\n&lt;p&gt;I figured that I probably deleted it so I ran &lt;code&gt;dbt init&lt;/code&gt; again and there&amp;#39;s nothing present that&amp;#39;s just `orders.sql` or anything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?auto=webp&amp;v=enabled&amp;s=bfe5e9b2927d016e953dc1100d04aa7edae028b8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac2470355dc3f9626c6f35140e0ec423549da50e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6800fda7fcc0abb4bd00f0af3c485a21b9befd60", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ff6e290ecb9d197e6339baf74f37ad4bcf47da0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59c6c188d4255d45db867f741fbf4a6fe1161f4a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56be5b52655ecb694651e6bf1bf5a025673b7cc3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8a7d892a9bafd02790b2d76b5fbbe76e9d0857f", "width": 1080, "height": 567}], "variants": {}, "id": "KBohsdqrfvkRxfqADmI_uqtotFtqgZjYu8NQbRpJlaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10r5wrc", "is_robot_indexable": true, "report_reasons": null, "author": "paxmlank", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r5wrc/very_much_a_noob_question_but_for_dbts_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r5wrc/very_much_a_noob_question_but_for_dbts_tutorial/", "subreddit_subscribers": 88214, "created_utc": 1675284986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For me, analyze the data set, create the data models, define the architecture, set up the environment, design the pipelines, deploy , migrate to production. I have listed the phases but having a hard time to estimate the efforts, I know the complexity might be the factor which is the key. But what could be the worst case scenarios.", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you crafted a project plan for big data ? What are the parameters that were considered ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r0w1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675273542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For me, analyze the data set, create the data models, define the architecture, set up the environment, design the pipelines, deploy , migrate to production. I have listed the phases but having a hard time to estimate the efforts, I know the complexity might be the factor which is the key. But what could be the worst case scenarios.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10r0w1g", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r0w1g/have_you_crafted_a_project_plan_for_big_data_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r0w1g/have_you_crafted_a_project_plan_for_big_data_what/", "subreddit_subscribers": 88214, "created_utc": 1675273542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1675270813.475, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qzpp1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675270813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qzpp1", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qzpp1/monthly_general_discussion/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/10qzpp1/monthly_general_discussion/", "subreddit_subscribers": 88214, "created_utc": 1675270813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Social media listening is the process of monitoring and analyzing social media conversations about a brand, industry, product, or topic. Here's how it can help your business:\n\nUnderstand your audience better: Social media listening allows you to understand your target audience's opinions, needs, and preferences. You can see how they engage with your brand and what topics they are interested in. This information can help you create content and campaigns that resonate with your audience.\n\nAcquire new clients: Social media listening can help you identify potential customers and prospects who are interested in your products or services. By engaging with them on social media and responding to their questions and concerns, you can build trust and establish yourself as a thought leader in your industry.\n\nMonitor Brand Perception: Social media listening allows you to monitor the perception of your brand in real-time. You can see what people are saying about your brand, what they like and dislike, and what they expect from your business. This information can help you identify areas where you need to improve and make changes to better meet the needs of your customers.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/fgejl5udgqfa1.png?width=1434&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=58e9bf222a00b6123615f2d95ca47a7249b85830", "author_fullname": "t2_jtca4f0z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can social media listening help your business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": true, "media_metadata": {"fgejl5udgqfa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58b0dfec20190a3ba992d369cf19f42a17244781"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a0be39df4c69198e1b400df783033cc283329d9"}, {"y": 226, "x": 320, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a087eead109e458e06516f0a256dbbd98f412b40"}, {"y": 452, "x": 640, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc11ccc3efe747d1a76ebc9d83f26f6c2a8ddb5b"}, {"y": 678, "x": 960, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b78c3b429071a2358a04fe1ffcbf5922bd8f9245"}, {"y": 763, "x": 1080, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2121c8ab0e5425ec11e8a91daca697993e8a82c"}], "s": {"y": 1014, "x": 1434, "u": "https://preview.redd.it/fgejl5udgqfa1.png?width=1434&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=58e9bf222a00b6123615f2d95ca47a7249b85830"}, "id": "fgejl5udgqfa1"}}, "name": "t3_10rkl71", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bRZGP7jAQ-QzzxRn_ja-HwsC3bL5lnN_Knm5G-sKx7E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675325246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Social media listening is the process of monitoring and analyzing social media conversations about a brand, industry, product, or topic. Here&amp;#39;s how it can help your business:&lt;/p&gt;\n\n&lt;p&gt;Understand your audience better: Social media listening allows you to understand your target audience&amp;#39;s opinions, needs, and preferences. You can see how they engage with your brand and what topics they are interested in. This information can help you create content and campaigns that resonate with your audience.&lt;/p&gt;\n\n&lt;p&gt;Acquire new clients: Social media listening can help you identify potential customers and prospects who are interested in your products or services. By engaging with them on social media and responding to their questions and concerns, you can build trust and establish yourself as a thought leader in your industry.&lt;/p&gt;\n\n&lt;p&gt;Monitor Brand Perception: Social media listening allows you to monitor the perception of your brand in real-time. You can see what people are saying about your brand, what they like and dislike, and what they expect from your business. This information can help you identify areas where you need to improve and make changes to better meet the needs of your customers.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fgejl5udgqfa1.png?width=1434&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=58e9bf222a00b6123615f2d95ca47a7249b85830\"&gt;https://preview.redd.it/fgejl5udgqfa1.png?width=1434&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=58e9bf222a00b6123615f2d95ca47a7249b85830&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "10rkl71", "is_robot_indexable": true, "report_reasons": null, "author": "hardik-s", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rkl71/how_can_social_media_listening_help_your_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10rkl71/how_can_social_media_listening_help_your_business/", "subreddit_subscribers": 88214, "created_utc": 1675325246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vxxrqrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming data pipelines with DuckDB and Striim", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_10rk5vy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CbO09xNraej6tWy1g156vTdO8TspmSAgclxdOCxJwxU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675323623.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/pedram/p/streaming-data-pipelines-with-striim", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UCmnpR4sUewEmjdPMiV26PadDNIeymKGozG8tkjymw8.jpg?auto=webp&amp;v=enabled&amp;s=8c99c9d608b35df70cf4ed77ef1483d03ab786fb", "width": 600, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/UCmnpR4sUewEmjdPMiV26PadDNIeymKGozG8tkjymw8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b479189de0aa6863645939c92e60d30f4d0722d", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/UCmnpR4sUewEmjdPMiV26PadDNIeymKGozG8tkjymw8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5f84c5f220cf61ffe1676e754673f4ea147c9b7", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/UCmnpR4sUewEmjdPMiV26PadDNIeymKGozG8tkjymw8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7214e4c0fd838789088190cdefce869662baf530", "width": 320, "height": 320}], "variants": {}, "id": "dsntXV9g7fIEf1VqManCNFXmD7-LI1B72jzUxKWsr34"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10rk5vy", "is_robot_indexable": true, "report_reasons": null, "author": "MrMosBiggestFan", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10rk5vy/streaming_data_pipelines_with_duckdb_and_striim/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/pedram/p/streaming-data-pipelines-with-striim", "subreddit_subscribers": 88214, "created_utc": 1675323623.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm thinking of doing a database of 'everything' but non-instance versions. So like people and their properties but not me or you. I was thinking you could add it to other data to give a fuller picture to maybe connect patterns that haven't been seen before. Would you guys find it useful or nah?", "author_fullname": "t2_pzfud0d2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non-instantiated objects and their properties database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r0k0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675272751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of doing a database of &amp;#39;everything&amp;#39; but non-instance versions. So like people and their properties but not me or you. I was thinking you could add it to other data to give a fuller picture to maybe connect patterns that haven&amp;#39;t been seen before. Would you guys find it useful or nah?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10r0k0o", "is_robot_indexable": true, "report_reasons": null, "author": "noone_thing", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r0k0o/noninstantiated_objects_and_their_properties/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r0k0o/noninstantiated_objects_and_their_properties/", "subreddit_subscribers": 88214, "created_utc": 1675272751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "...what would you suggest?\n\nFocussing on:\n\n* Data Engineering processes (Databricks + AWS)\n* Adoption step by step\n* Best practice", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need to become an expert on enterprise databricks adoption but nothing to do with code...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r3rrr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675280872.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675280111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;...what would you suggest?&lt;/p&gt;\n\n&lt;p&gt;Focussing on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Engineering processes (Databricks + AWS)&lt;/li&gt;\n&lt;li&gt;Adoption step by step&lt;/li&gt;\n&lt;li&gt;Best practice&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10r3rrr", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r3rrr/i_need_to_become_an_expert_on_enterprise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r3rrr/i_need_to_become_an_expert_on_enterprise/", "subreddit_subscribers": 88214, "created_utc": 1675280111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lease remote resources? Kinda going over my head. Can someone give a real time example?", "author_fullname": "t2_5lve5av0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the use case of nimbus IAAS tool kit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r2504", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675276388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lease remote resources? Kinda going over my head. Can someone give a real time example?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10r2504", "is_robot_indexable": true, "report_reasons": null, "author": "noobmastersmaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r2504/what_is_the_use_case_of_nimbus_iaas_tool_kit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r2504/what_is_the_use_case_of_nimbus_iaas_tool_kit/", "subreddit_subscribers": 88214, "created_utc": 1675276388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi.\n\nI am trying to arrange all my data pipeline with Spark SQL, so Scala/Python knowledge is required.\n\nNow, suppose I have my final dataset, I want to write it to a new place (some new folder, no files yet), how to do that using SQL?", "author_fullname": "t2_dwipmn93", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[QUESTION]: Spark SQL to write data to storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qsp3w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675253115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;I am trying to arrange all my data pipeline with Spark SQL, so Scala/Python knowledge is required.&lt;/p&gt;\n\n&lt;p&gt;Now, suppose I have my final dataset, I want to write it to a new place (some new folder, no files yet), how to do that using SQL?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qsp3w", "is_robot_indexable": true, "report_reasons": null, "author": "knkydud", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qsp3w/question_spark_sql_to_write_data_to_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qsp3w/question_spark_sql_to_write_data_to_storage/", "subreddit_subscribers": 88214, "created_utc": 1675253115.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}