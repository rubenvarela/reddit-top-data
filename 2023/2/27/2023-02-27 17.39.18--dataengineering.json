{"kind": "Listing", "data": {"after": "t3_11dbkjv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This weekend, for one of our larger clients, we had an \u201caccount memory allocation limit reached - reach out to support for assistance\u201d bug that spontaneously showed up and caused all of our global refreshes to begin failing in all environments. We did as the error said and reached out to support.\n\nAfter hours of no response, we were forced to work the weekend and migrate our scheduled jobs off of DBT cloud to resume data for our customers to get them out of unscheduled downtime.\n\nAfter 32 hours (basically a day after successfully migrating and resuming our jobs on our own, outside of DBT Cloud altogether) we finally received a response from DBT Cloud\u2026..letting us know they don\u2019t work weekends.\n\nSo unless you\u2019re working with a product where you have customers who are totally fine with 48h+ of downtime, I\u2019d recommend not relying on their scheduler for anything.\n\nJust posting here in case it saves anyone the headache of finding that out the hard way.\n\nEdit: some are pointing out that DBT advertises 24x5 support on their pricing page. I\u2019ve been using dbt cloud since 2019 and don\u2019t frequent their faq/pricing pages \u2014 according to wayback machine this was added sometime in 2020. Over the past few years, the support experience has degraded significantly as they\u2019ve grown. \n\nIt\u2019s worth noting that this is the desired and intended level of support for basic DBT cloud customers.", "author_fullname": "t2_n0d7a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: we learned the hard way DBT Cloud support doesn\u2019t work weekends\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11d4kuk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 153, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 153, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677514975.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677478516.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This weekend, for one of our larger clients, we had an \u201caccount memory allocation limit reached - reach out to support for assistance\u201d bug that spontaneously showed up and caused all of our global refreshes to begin failing in all environments. We did as the error said and reached out to support.&lt;/p&gt;\n\n&lt;p&gt;After hours of no response, we were forced to work the weekend and migrate our scheduled jobs off of DBT cloud to resume data for our customers to get them out of unscheduled downtime.&lt;/p&gt;\n\n&lt;p&gt;After 32 hours (basically a day after successfully migrating and resuming our jobs on our own, outside of DBT Cloud altogether) we finally received a response from DBT Cloud\u2026..letting us know they don\u2019t work weekends.&lt;/p&gt;\n\n&lt;p&gt;So unless you\u2019re working with a product where you have customers who are totally fine with 48h+ of downtime, I\u2019d recommend not relying on their scheduler for anything.&lt;/p&gt;\n\n&lt;p&gt;Just posting here in case it saves anyone the headache of finding that out the hard way.&lt;/p&gt;\n\n&lt;p&gt;Edit: some are pointing out that DBT advertises 24x5 support on their pricing page. I\u2019ve been using dbt cloud since 2019 and don\u2019t frequent their faq/pricing pages \u2014 according to wayback machine this was added sometime in 2020. Over the past few years, the support experience has degraded significantly as they\u2019ve grown. &lt;/p&gt;\n\n&lt;p&gt;It\u2019s worth noting that this is the desired and intended level of support for basic DBT cloud customers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11d4kuk", "is_robot_indexable": true, "report_reasons": null, "author": "aormiston", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11d4kuk/psa_we_learned_the_hard_way_dbt_cloud_support/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11d4kuk/psa_we_learned_the_hard_way_dbt_cloud_support/", "subreddit_subscribers": 91304, "created_utc": 1677478516.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently can be best described as an Analytics Engineer for my current company and I use SQL, Python(mostly for airflow), DBT and Fivetran.  I notice a lot of interview posts on here relate to spark questions.  Am I better off waiting till I can get spark experience to start applying for DE roles, or would you say I\u2019m decently ready now?", "author_fullname": "t2_dqlps", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is spark necessary starting out?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11czv25", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677463670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently can be best described as an Analytics Engineer for my current company and I use SQL, Python(mostly for airflow), DBT and Fivetran.  I notice a lot of interview posts on here relate to spark questions.  Am I better off waiting till I can get spark experience to start applying for DE roles, or would you say I\u2019m decently ready now?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11czv25", "is_robot_indexable": true, "report_reasons": null, "author": "dynamex1097", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11czv25/is_spark_necessary_starting_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11czv25/is_spark_necessary_starting_out/", "subreddit_subscribers": 91304, "created_utc": 1677463670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I see this course recommended in this sub and lots of other places around the internet, but I am finding the environment set up to be incredibly frustrating. I have deleted my GCP instance and started over from scratch about 13 times so far and I keep having various issues. This is absolutely maddening. Is there like an easy version for dummies, or a guide to set this up on a windows machine without using gitbash?", "author_fullname": "t2_4nbvm2s0m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Zoomcamp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11crlpv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677443060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see this course recommended in this sub and lots of other places around the internet, but I am finding the environment set up to be incredibly frustrating. I have deleted my GCP instance and started over from scratch about 13 times so far and I keep having various issues. This is absolutely maddening. Is there like an easy version for dummies, or a guide to set this up on a windows machine without using gitbash?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11crlpv", "is_robot_indexable": true, "report_reasons": null, "author": "Visible-Tennis4144", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11crlpv/data_engineering_zoomcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11crlpv/data_engineering_zoomcamp/", "subreddit_subscribers": 91304, "created_utc": 1677443060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI head towards data science before 1 year ago which ended up with realizing after six month that the field is enourmously saturated. I was 2 years experienced with mobile development (Java), so working with software was a lot more engaging and efficient for me so I decided to go on maybe Data Engineering.\n\nAs far as I see, data engineering is not much hyped as being a crazy data scientist or AI/ML engineer. I think people generally stay away from data engineering because of the need of software development background, and they step into data science because they do not aware of the holy math behind the scene.\n\nThose are my guess. I wonder if it is true.\n\nToday junior data analyst positions are full for most regions. However i encounter with jr data engineer positions.\n\nI ask if there is life in data engineering :) If it is over-saturated too, I will focus on cloud engineering before I finish the university (kinda sophomore student).", "author_fullname": "t2_t1xjvr55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is data engineering over-hyped too like data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ctwrk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677448410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I head towards data science before 1 year ago which ended up with realizing after six month that the field is enourmously saturated. I was 2 years experienced with mobile development (Java), so working with software was a lot more engaging and efficient for me so I decided to go on maybe Data Engineering.&lt;/p&gt;\n\n&lt;p&gt;As far as I see, data engineering is not much hyped as being a crazy data scientist or AI/ML engineer. I think people generally stay away from data engineering because of the need of software development background, and they step into data science because they do not aware of the holy math behind the scene.&lt;/p&gt;\n\n&lt;p&gt;Those are my guess. I wonder if it is true.&lt;/p&gt;\n\n&lt;p&gt;Today junior data analyst positions are full for most regions. However i encounter with jr data engineer positions.&lt;/p&gt;\n\n&lt;p&gt;I ask if there is life in data engineering :) If it is over-saturated too, I will focus on cloud engineering before I finish the university (kinda sophomore student).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11ctwrk", "is_robot_indexable": true, "report_reasons": null, "author": "gxslash", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ctwrk/is_data_engineering_overhyped_too_like_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ctwrk/is_data_engineering_overhyped_too_like_data/", "subreddit_subscribers": 91304, "created_utc": 1677448410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, sorry for a lengthy post. I am looking to have a discussion with you guys about designing data-marts for business reporting where the data for the marts are extracted and transformed from a centralized Data Lake.\n\nWe have an Extract and Load process from our sources into an Azure Data Lake. Data in the lake is currently only used in ML or data-driven applications, not really for traditional BI uses. However, as we are advancing our platform to increase our value to the business, we are needing to enable these types of traditional BI reporting capabilities on top of the data we ingest into our data lake. Everything we develop is project-specific and must be approved by a portfolio board. So, the plan is to develop project-based data-marts in an Azure dedicated SQL pool (Azure Data warehouse).\n\n[Extract and Load into the Data Store\\/Central Repository. Parquets stored in Azure Data Lake Storage](https://preview.redd.it/muy6ytyxhkka1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8d835743a515c4c89058bf970c5eaa403770350a)\n\n&gt;*I will preface that I am in an awkward position, because I come from BI and analytics, not architecture or engineering. Nonetheless, I transitioned into an IT role last year, where we do not have a senior data engineer, or any dedicated engineer at all that can help with this advancement. I am looking to fill that piece of the team, but am basically a junior DE with no senior DE to learn from... I have been running the POC's and want to make sure the way I plan the data-marts at least make sense. I know I won't create the perfect solution the first-time around, but I want to at least start in the right direction. I don't love having to go to reddit to have these discussions, but it is really my only option at this point...*\n\nThe Data Lake will serve as the central repository/data store for all downstream use-cases. The transformation process will be Dockerized python-based data pipelines running on a Kubernetes cluster. The pipeline will extract data from the lake, perform the data transformations needed for the business use-case (business rules, joins, conditions, filters, etc...), and load the transformed data into a data-mart in the warehouse.\n\n**I am just really confused as to how these data-marts should be modeled in the warehouse. Some options I can think of are:**\n\n1. The transformation results in a denormalized OBT structure that is stored in a specific schema. In this option, 1 database schema = 1 data mart, and the data mart is just a single OBT. New data are loaded into the lake, transformed with python (Docker + Kubernetes), and records are then inserted into the OBT/mart in the warehouse. Although this makes sense to me, I feel like it isn't best, as every mart would just a single table in a specific database schema. If the business needs lots of potential dimensions at their immediate disposal in the semantic model, this method doesn't seem to be practical.\n\n[1 data warehouse schema = 1 data mart, where each mart is just a denormalized OBT](https://preview.redd.it/v8wkifdnhkka1.jpg?width=1362&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=bbcbb646a00bd8a24e4787f550fa320049576907)\n\n2. The transformation results in a star-schema structure that is stored in a specific database schema. With this option, 1 data mart = 1 database schema, and the table schema resembles a star-schema. The issue with this is I think I would just be duplicating dimensions in different database schemas. Maybe this is okay? If I create a mart to track delivery performance for logistics, I would need to source the customers table from the lake. If later I create another mart to track sales, I would also need to source the customer table from the lake. Now I have two customer tables in different schemas of the warehouse. This honestly might not be a big deal since the dimensions are sourced from the sample data in the data lake.\n\n[1 data warehouse schema = 1 data mart, where each mart is a single fact table with multiple dimensions](https://preview.redd.it/bgusekfphkka1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=abd7ac824d2ba3844ac27856b17bc8ab835fafcb)\n\n3. A combination of 1 and 2. I can imagine some marts are better off as a OBT structure, while others would benefit from a star schema. Could also create a constellation structure if multiple facts are required.\n\n4. I am way off all together, and I am not thinking about this correctly (which honestly may be the case and explains why I am so confused). Like what if I need to have a combination of non-aggregated fact tables alongside aggregated tables for easier analysis? Would I just throw all of these inside the same data warehouse schema?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Structuring Data Marts from a Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 46, "top_awarded_type": null, "hide_score": false, "media_metadata": {"muy6ytyxhkka1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/muy6ytyxhkka1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95bce96dac5e270970c084124a1d0859886c5ef6"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/muy6ytyxhkka1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=113c6c4a3beb5caea7e6ded48ec99c81b09d97a7"}, {"y": 106, "x": 320, "u": "https://preview.redd.it/muy6ytyxhkka1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=349d949cedab9301e2a6a982492617bcc17f6211"}, {"y": 212, "x": 640, "u": "https://preview.redd.it/muy6ytyxhkka1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d92a0117e40e910b8ce2d91211ab749f0916a6e"}, {"y": 318, "x": 960, "u": "https://preview.redd.it/muy6ytyxhkka1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d20e8ece0e3641b3c705d93eed2452cc2f445c5"}, {"y": 358, "x": 1080, "u": "https://preview.redd.it/muy6ytyxhkka1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=531f4a3eed6cbcd6236b3dc2d6e93e75b836f08b"}], "s": {"y": 425, "x": 1280, "u": "https://preview.redd.it/muy6ytyxhkka1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8d835743a515c4c89058bf970c5eaa403770350a"}, "id": "muy6ytyxhkka1"}, "v8wkifdnhkka1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/v8wkifdnhkka1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e79104064177a718ca9ab583e3588211d3bcc6cf"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/v8wkifdnhkka1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03885bfbcf1e3fb7f697d76e499026bd6780d265"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/v8wkifdnhkka1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36e565016e2838b7c15c012fb629b5bccb078af9"}, {"y": 378, "x": 640, "u": "https://preview.redd.it/v8wkifdnhkka1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e431e121d004ccc261a5c81406d3b96ccb012b6"}, {"y": 568, "x": 960, "u": "https://preview.redd.it/v8wkifdnhkka1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59002e9fee89774b8fda6c0156eca7955ae4a24b"}, {"y": 639, "x": 1080, "u": "https://preview.redd.it/v8wkifdnhkka1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ad26f9eca5f6eaeb0514784871810f16fa30af2"}], "s": {"y": 806, "x": 1362, "u": "https://preview.redd.it/v8wkifdnhkka1.jpg?width=1362&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=bbcbb646a00bd8a24e4787f550fa320049576907"}, "id": "v8wkifdnhkka1"}, "bgusekfphkka1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/bgusekfphkka1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d33a3932fdffe47c214cc5e2f22d9ebd46ef5785"}, {"y": 133, "x": 216, "u": "https://preview.redd.it/bgusekfphkka1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2eac2caf21c94d44b4900657fb165355021bf23b"}, {"y": 198, "x": 320, "u": "https://preview.redd.it/bgusekfphkka1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08f918d4e1f8637846eb10f213133bcc905d747e"}, {"y": 396, "x": 640, "u": "https://preview.redd.it/bgusekfphkka1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36e2e95b834ca4fff739b8860af110d168fd2087"}, {"y": 594, "x": 960, "u": "https://preview.redd.it/bgusekfphkka1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c09af06c4048cced0de8c78c059b44496e0b20df"}, {"y": 669, "x": 1080, "u": "https://preview.redd.it/bgusekfphkka1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f1360c260994dd057961c33591da59c9c17f58b"}], "s": {"y": 793, "x": 1280, "u": "https://preview.redd.it/bgusekfphkka1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=abd7ac824d2ba3844ac27856b17bc8ab835fafcb"}, "id": "bgusekfphkka1"}}, "name": "t3_11cneq2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/E3et4EkiFQ4cyykMDvikWt6rKtGfeLUQlquJ_QMQCmk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677433180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, sorry for a lengthy post. I am looking to have a discussion with you guys about designing data-marts for business reporting where the data for the marts are extracted and transformed from a centralized Data Lake.&lt;/p&gt;\n\n&lt;p&gt;We have an Extract and Load process from our sources into an Azure Data Lake. Data in the lake is currently only used in ML or data-driven applications, not really for traditional BI uses. However, as we are advancing our platform to increase our value to the business, we are needing to enable these types of traditional BI reporting capabilities on top of the data we ingest into our data lake. Everything we develop is project-specific and must be approved by a portfolio board. So, the plan is to develop project-based data-marts in an Azure dedicated SQL pool (Azure Data warehouse).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/muy6ytyxhkka1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8d835743a515c4c89058bf970c5eaa403770350a\"&gt;Extract and Load into the Data Store/Central Repository. Parquets stored in Azure Data Lake Storage&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;I will preface that I am in an awkward position, because I come from BI and analytics, not architecture or engineering. Nonetheless, I transitioned into an IT role last year, where we do not have a senior data engineer, or any dedicated engineer at all that can help with this advancement. I am looking to fill that piece of the team, but am basically a junior DE with no senior DE to learn from... I have been running the POC&amp;#39;s and want to make sure the way I plan the data-marts at least make sense. I know I won&amp;#39;t create the perfect solution the first-time around, but I want to at least start in the right direction. I don&amp;#39;t love having to go to reddit to have these discussions, but it is really my only option at this point...&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The Data Lake will serve as the central repository/data store for all downstream use-cases. The transformation process will be Dockerized python-based data pipelines running on a Kubernetes cluster. The pipeline will extract data from the lake, perform the data transformations needed for the business use-case (business rules, joins, conditions, filters, etc...), and load the transformed data into a data-mart in the warehouse.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I am just really confused as to how these data-marts should be modeled in the warehouse. Some options I can think of are:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The transformation results in a denormalized OBT structure that is stored in a specific schema. In this option, 1 database schema = 1 data mart, and the data mart is just a single OBT. New data are loaded into the lake, transformed with python (Docker + Kubernetes), and records are then inserted into the OBT/mart in the warehouse. Although this makes sense to me, I feel like it isn&amp;#39;t best, as every mart would just a single table in a specific database schema. If the business needs lots of potential dimensions at their immediate disposal in the semantic model, this method doesn&amp;#39;t seem to be practical.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v8wkifdnhkka1.jpg?width=1362&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bbcbb646a00bd8a24e4787f550fa320049576907\"&gt;1 data warehouse schema = 1 data mart, where each mart is just a denormalized OBT&lt;/a&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The transformation results in a star-schema structure that is stored in a specific database schema. With this option, 1 data mart = 1 database schema, and the table schema resembles a star-schema. The issue with this is I think I would just be duplicating dimensions in different database schemas. Maybe this is okay? If I create a mart to track delivery performance for logistics, I would need to source the customers table from the lake. If later I create another mart to track sales, I would also need to source the customer table from the lake. Now I have two customer tables in different schemas of the warehouse. This honestly might not be a big deal since the dimensions are sourced from the sample data in the data lake.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bgusekfphkka1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=abd7ac824d2ba3844ac27856b17bc8ab835fafcb\"&gt;1 data warehouse schema = 1 data mart, where each mart is a single fact table with multiple dimensions&lt;/a&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;A combination of 1 and 2. I can imagine some marts are better off as a OBT structure, while others would benefit from a star schema. Could also create a constellation structure if multiple facts are required.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I am way off all together, and I am not thinking about this correctly (which honestly may be the case and explains why I am so confused). Like what if I need to have a combination of non-aggregated fact tables alongside aggregated tables for easier analysis? Would I just throw all of these inside the same data warehouse schema?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11cneq2", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11cneq2/structuring_data_marts_from_a_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11cneq2/structuring_data_marts_from_a_data_lake/", "subreddit_subscribers": 91304, "created_utc": 1677433180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1jx9xua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding BigQuery Table Usage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11daoo1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/slx9MVlAJX0KMEv8OuO3EKbQMXlHiQEj0l1tmYcXDJc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677500917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech.loveholidays.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tech.loveholidays.com/the-life-changing-magic-of-t%CC%B6i%CC%B6d%CC%B6y%CC%B6i%CC%B6n%CC%B6g%CC%B6-%CC%B6u%CC%B6p%CC%B6-understanding-bigquery-table-usage-a-76aae6006d01", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Yg-BA2isCavyycyu2UockhxlBp4nNIMrYaSXA2iIfVQ.jpg?auto=webp&amp;v=enabled&amp;s=60fc29fcba9d4ac91feced4d08e3112fce4c3831", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/Yg-BA2isCavyycyu2UockhxlBp4nNIMrYaSXA2iIfVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88cd2088efb1e45d078bb990ea11bb1af031f92c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Yg-BA2isCavyycyu2UockhxlBp4nNIMrYaSXA2iIfVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=28d4bd5340a33983a4302f8f38ead383ecbf4cf3", "width": 216, "height": 216}], "variants": {}, "id": "1dNvQidZQZlP4iW4BacCKwpmxpYb0i1gpKQDKOMrvbU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11daoo1", "is_robot_indexable": true, "report_reasons": null, "author": "dropber", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11daoo1/understanding_bigquery_table_usage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tech.loveholidays.com/the-life-changing-magic-of-t%CC%B6i%CC%B6d%CC%B6y%CC%B6i%CC%B6n%CC%B6g%CC%B6-%CC%B6u%CC%B6p%CC%B6-understanding-bigquery-table-usage-a-76aae6006d01", "subreddit_subscribers": 91304, "created_utc": 1677500917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We all know that CAP theorem is designed for distributed file systems(collection of interconnected nodes).CAP Theorem also known as Brewer\u2019s theorem and used to distributed consistency.It contains follwing three technical terms for distributed systems.\n\n* **C**\u00a0\u2013 Consistency\n* **A**\u00a0\u2013 Availability\n* **P**\u00a0\u2013 Partition Tolerance\n\nHDFS is a Hadoop Distributed File System so of course it is partition tolerant.Users can only access the system through Namenode so it is consistency.\n\nIn Hadoop 2.0, they handle SPOF by developing HDFS High Availability with using Secondary Namenode.\n\nThen does HDFS become a CAP system (based on CAP theorem) or am I misunderstand something here?", "author_fullname": "t2_9y8fc63a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDFS in CAP theorem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11d06tt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677474244.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677464641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We all know that CAP theorem is designed for distributed file systems(collection of interconnected nodes).CAP Theorem also known as Brewer\u2019s theorem and used to distributed consistency.It contains follwing three technical terms for distributed systems.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;C&lt;/strong&gt;\u00a0\u2013 Consistency&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;\u00a0\u2013 Availability&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;P&lt;/strong&gt;\u00a0\u2013 Partition Tolerance&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;HDFS is a Hadoop Distributed File System so of course it is partition tolerant.Users can only access the system through Namenode so it is consistency.&lt;/p&gt;\n\n&lt;p&gt;In Hadoop 2.0, they handle SPOF by developing HDFS High Availability with using Secondary Namenode.&lt;/p&gt;\n\n&lt;p&gt;Then does HDFS become a CAP system (based on CAP theorem) or am I misunderstand something here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11d06tt", "is_robot_indexable": true, "report_reasons": null, "author": "ImportantWater71", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11d06tt/hdfs_in_cap_theorem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11d06tt/hdfs_in_cap_theorem/", "subreddit_subscribers": 91304, "created_utc": 1677464641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, currently I am in some identity crisis. Below is my achievement in this particular company as a Junior Data Engineer for 8 months.  \n \n\n* Built and implemented a data infrastructure, including ETL/ELT tools, a data warehouse, and data visualization tools.\n* Improved the ETL/ELT pipeline by researching and selecting the most suitable tools, reducing manual processing time by 90% and increasing reliability.\n* Created a data warehouse that provided downstream users access to clean data from a single, reliable source.\n* Ensured data accuracy from source to data warehouse to visualization, maintaining accuracy above 90% consistently.\n* Applied data governance measures to the infrastructure and users to ensure data security and accuracy.\n* Designed and implemented various data models to meet the needs of stakeholders.\n* Developed and maintained SQL queries using DBT Core-  Built and implemented a data infrastructure, including ETL/ELT tools, a  data warehouse, and data visualization tools. - Improved the ETL/ELT pipeline by researching and selecting the most suitable tools, reducing manual processing time by 90% and increasing reliability. - Created a data warehouse that provided downstream users access to clean data from a single, reliable source. \n* Ensured data accuracy from source to data warehouse to visualization,  maintaining accuracy above 90% consistently. - Applied data governance measures to the infrastructure and users to ensure data security and accuracy.\n* Designed and implemented various data models to meet the needs of stakeholders.\n* Developed and maintained SQL queries using DBT Core     \n\n**Skills:** dbt  core \u00b7 Tableau \u00b7 Tableau Online \u00b7 Hevo Data \u00b7 ELT Tools \u00b7 Amazon  Redshift \u00b7 Data Visualization \u00b7 SQL \u00b7 Data Pipelines \u00b7 Data Warehousing \u00b7  ETL Tools  \n\n\nTLDR: Basically I introduce OLAP to the company and enable analytics, but currently didn't bring any value as there is no metadata being documented the company wants to move fast so I experience burnout and want to find a new job but which area should I focus, I used no code solution for the ELT process, did setup and design data lakehouse and make some simple dashboard.  What kind of title I should be searching for? Most data engineering open positions are focused on hive/spark technology, but for data analyst I haven't applied techniques such as linear regression or features selection since uni time. So yeah I need some advice.", "author_fullname": "t2_k7g9mhou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some career advise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11d2f0p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677471342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, currently I am in some identity crisis. Below is my achievement in this particular company as a Junior Data Engineer for 8 months.  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Built and implemented a data infrastructure, including ETL/ELT tools, a data warehouse, and data visualization tools.&lt;/li&gt;\n&lt;li&gt;Improved the ETL/ELT pipeline by researching and selecting the most suitable tools, reducing manual processing time by 90% and increasing reliability.&lt;/li&gt;\n&lt;li&gt;Created a data warehouse that provided downstream users access to clean data from a single, reliable source.&lt;/li&gt;\n&lt;li&gt;Ensured data accuracy from source to data warehouse to visualization, maintaining accuracy above 90% consistently.&lt;/li&gt;\n&lt;li&gt;Applied data governance measures to the infrastructure and users to ensure data security and accuracy.&lt;/li&gt;\n&lt;li&gt;Designed and implemented various data models to meet the needs of stakeholders.&lt;/li&gt;\n&lt;li&gt;Developed and maintained SQL queries using DBT Core-  Built and implemented a data infrastructure, including ETL/ELT tools, a  data warehouse, and data visualization tools. - Improved the ETL/ELT pipeline by researching and selecting the most suitable tools, reducing manual processing time by 90% and increasing reliability. - Created a data warehouse that provided downstream users access to clean data from a single, reliable source. &lt;/li&gt;\n&lt;li&gt;Ensured data accuracy from source to data warehouse to visualization,  maintaining accuracy above 90% consistently. - Applied data governance measures to the infrastructure and users to ensure data security and accuracy.&lt;/li&gt;\n&lt;li&gt;Designed and implemented various data models to meet the needs of stakeholders.&lt;/li&gt;\n&lt;li&gt;Developed and maintained SQL queries using DBT Core&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Skills:&lt;/strong&gt; dbt  core \u00b7 Tableau \u00b7 Tableau Online \u00b7 Hevo Data \u00b7 ELT Tools \u00b7 Amazon  Redshift \u00b7 Data Visualization \u00b7 SQL \u00b7 Data Pipelines \u00b7 Data Warehousing \u00b7  ETL Tools  &lt;/p&gt;\n\n&lt;p&gt;TLDR: Basically I introduce OLAP to the company and enable analytics, but currently didn&amp;#39;t bring any value as there is no metadata being documented the company wants to move fast so I experience burnout and want to find a new job but which area should I focus, I used no code solution for the ELT process, did setup and design data lakehouse and make some simple dashboard.  What kind of title I should be searching for? Most data engineering open positions are focused on hive/spark technology, but for data analyst I haven&amp;#39;t applied techniques such as linear regression or features selection since uni time. So yeah I need some advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11d2f0p", "is_robot_indexable": true, "report_reasons": null, "author": "FlorexOng_a1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11d2f0p/need_some_career_advise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11d2f0p/need_some_career_advise/", "subreddit_subscribers": 91304, "created_utc": 1677471342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that data engineers work mostly oriented with data. Building pipelines, scheduling tasks, manipluating data, mining it etc. What I dont know is what is it that cloud engineers really do.\n\nNow, while I was looking for intern positions on data engineering I've realized that the job ads mostly include those:\n\nCloud software Aws etc, containers (docker etc), scheduling (airflow etc), parallel computation frameworks (hdfs, spark etc), bashscript, linux server knowledge.\n\nHowever it seems to me that 70% of requirements do match with cloud engineering requirements.\n\nI dont know where to ask this question. It matters both fields. But is it easy to switch between cloud and data engineering? Does they do mostly the same job. If it is, what are the most differing parts?", "author_fullname": "t2_t1xjvr55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weird Question: What Really Differs Data Engineers from Cloud Engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ctnxh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677447827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that data engineers work mostly oriented with data. Building pipelines, scheduling tasks, manipluating data, mining it etc. What I dont know is what is it that cloud engineers really do.&lt;/p&gt;\n\n&lt;p&gt;Now, while I was looking for intern positions on data engineering I&amp;#39;ve realized that the job ads mostly include those:&lt;/p&gt;\n\n&lt;p&gt;Cloud software Aws etc, containers (docker etc), scheduling (airflow etc), parallel computation frameworks (hdfs, spark etc), bashscript, linux server knowledge.&lt;/p&gt;\n\n&lt;p&gt;However it seems to me that 70% of requirements do match with cloud engineering requirements.&lt;/p&gt;\n\n&lt;p&gt;I dont know where to ask this question. It matters both fields. But is it easy to switch between cloud and data engineering? Does they do mostly the same job. If it is, what are the most differing parts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11ctnxh", "is_robot_indexable": true, "report_reasons": null, "author": "gxslash", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ctnxh/weird_question_what_really_differs_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ctnxh/weird_question_what_really_differs_data_engineers/", "subreddit_subscribers": 91304, "created_utc": 1677447827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It's the somewhat industry standard now for building models and data transformations.\n\nMy company is evaluating the need for it for our analysts.\n\nThis sub has users of dbt and I've seen many posts. Can you help advise me :\n\n\\- What does dbt do really well?\n\n\\- What are the shortcomings and how do you overcome them?\n\n\\- What can dbt Cloud give that you can't build on your own?\n\n\\- Has anyone thought of rewriting or forking to improve it? Is there a better dbt out there?", "author_fullname": "t2_w0uodzp2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions around using dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ct7o9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677446773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s the somewhat industry standard now for building models and data transformations.&lt;/p&gt;\n\n&lt;p&gt;My company is evaluating the need for it for our analysts.&lt;/p&gt;\n\n&lt;p&gt;This sub has users of dbt and I&amp;#39;ve seen many posts. Can you help advise me :&lt;/p&gt;\n\n&lt;p&gt;- What does dbt do really well?&lt;/p&gt;\n\n&lt;p&gt;- What are the shortcomings and how do you overcome them?&lt;/p&gt;\n\n&lt;p&gt;- What can dbt Cloud give that you can&amp;#39;t build on your own?&lt;/p&gt;\n\n&lt;p&gt;- Has anyone thought of rewriting or forking to improve it? Is there a better dbt out there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11ct7o9", "is_robot_indexable": true, "report_reasons": null, "author": "Accurate-Peak4856", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ct7o9/questions_around_using_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ct7o9/questions_around_using_dbt/", "subreddit_subscribers": 91304, "created_utc": 1677446773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR: How do I setup a basic ETL pipeline with Python and SQL Server in Azure?\n\nI am a recent undergraduate student in Data Science who works in a company with no other persons with a CS or DS background. I have been building basic reports and applications in Python and R with data pulled on the fly from API's.  \nThe company now wants to develop a database storing the data instead of pulling from API's everytime we want a report, visualization etc.\n\nWe have various data sources, but I would like to start simple and develop an ETL pipeline from API pull to SQL database. I have build API wrappers in Python and are doing transformation in Python as well, however I am struggling to ingest the data into a database.\n\nHow do I best create a simple ETL pipeline with data extracted and transformed in Python from an API to SQL Server database in Azure?", "author_fullname": "t2_226yoed5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple ETL pipeline as undergraduate as only CS/DS person in company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11d7kdm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677489752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: How do I setup a basic ETL pipeline with Python and SQL Server in Azure?&lt;/p&gt;\n\n&lt;p&gt;I am a recent undergraduate student in Data Science who works in a company with no other persons with a CS or DS background. I have been building basic reports and applications in Python and R with data pulled on the fly from API&amp;#39;s.&lt;br/&gt;\nThe company now wants to develop a database storing the data instead of pulling from API&amp;#39;s everytime we want a report, visualization etc.&lt;/p&gt;\n\n&lt;p&gt;We have various data sources, but I would like to start simple and develop an ETL pipeline from API pull to SQL database. I have build API wrappers in Python and are doing transformation in Python as well, however I am struggling to ingest the data into a database.&lt;/p&gt;\n\n&lt;p&gt;How do I best create a simple ETL pipeline with data extracted and transformed in Python from an API to SQL Server database in Azure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11d7kdm", "is_robot_indexable": true, "report_reasons": null, "author": "C_Ronsholt", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11d7kdm/simple_etl_pipeline_as_undergraduate_as_only_csds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11d7kdm/simple_etl_pipeline_as_undergraduate_as_only_csds/", "subreddit_subscribers": 91304, "created_utc": 1677489752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been in a complex dilemma of getting low-latency data available into the data lake. While I understand that low latency data should be either handled into an application itself or an engine/motor that enables fast aggregations like Apache Druid - I've been trying to get into a sweet spot where I can both have a low latency + ok reading performance, but I progressed little and without much confidence. \n\nWhat you guys have been using? How do you manage small files without having latency over a minute or having to interrupt the streaming ingestion to run an optimize/aggregation operation?", "author_fullname": "t2_10pv8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Low latency streaming data into Datalake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11cv28g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677451110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been in a complex dilemma of getting low-latency data available into the data lake. While I understand that low latency data should be either handled into an application itself or an engine/motor that enables fast aggregations like Apache Druid - I&amp;#39;ve been trying to get into a sweet spot where I can both have a low latency + ok reading performance, but I progressed little and without much confidence. &lt;/p&gt;\n\n&lt;p&gt;What you guys have been using? How do you manage small files without having latency over a minute or having to interrupt the streaming ingestion to run an optimize/aggregation operation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11cv28g", "is_robot_indexable": true, "report_reasons": null, "author": "naniviaa", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11cv28g/low_latency_streaming_data_into_datalake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11cv28g/low_latency_streaming_data_into_datalake/", "subreddit_subscribers": 91304, "created_utc": 1677451110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a consulting firm and have been looking for technical opportunities, specifically more traditional software engineering opportunities. That unfortunately hasn\u2019t worked out, so I took a project for data engineering. My main responsibilities are supposed to be PySpark developer. The thing is, I have a foundational understanding of Spark, but I have pretty much no real world experience with it outside of a short stint creating some basic jobs in Databricks for a few days on my last project. Also, my SQL skills are okay. I was mostly a data analyst on my last project and used it on a daily basis, but most queries were basic select statements with a filter or 2 and maybe a simple join as well. I have faith in myself to pickup new technologies, but I also need to be realistic with myself here. I plan on doing some PySpark crash courses this weekend, but I wanted to get some feedback from the community if y\u2019all think I\u2019m crazy for hopping on this project (which starts in 2 weeks)", "author_fullname": "t2_1fco9rqs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I setting myself up for failure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11colje", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677435868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a consulting firm and have been looking for technical opportunities, specifically more traditional software engineering opportunities. That unfortunately hasn\u2019t worked out, so I took a project for data engineering. My main responsibilities are supposed to be PySpark developer. The thing is, I have a foundational understanding of Spark, but I have pretty much no real world experience with it outside of a short stint creating some basic jobs in Databricks for a few days on my last project. Also, my SQL skills are okay. I was mostly a data analyst on my last project and used it on a daily basis, but most queries were basic select statements with a filter or 2 and maybe a simple join as well. I have faith in myself to pickup new technologies, but I also need to be realistic with myself here. I plan on doing some PySpark crash courses this weekend, but I wanted to get some feedback from the community if y\u2019all think I\u2019m crazy for hopping on this project (which starts in 2 weeks)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11colje", "is_robot_indexable": true, "report_reasons": null, "author": "gencoupethrowaway69", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11colje/am_i_setting_myself_up_for_failure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11colje/am_i_setting_myself_up_for_failure/", "subreddit_subscribers": 91304, "created_utc": 1677435868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am an ETL developer with 1.5 years of experience. I primarily work with informatica power centre, Alteryx, SQL. I am familiar with python as interned as a backend developer where I used flask extensively. I am planning to learn databricks as my company is expecting projects based on databricks. I also worked with pandas and numpy is it enough to master databricks?\n\nI have started to learn databricks from the databricks academy. \nI found the course to be confusing. Please guide me on how to approach learning databricks.\n\nWhat are the prerequisites?\nI have not worked with spark. I know that the databricks platform uses spark. Is it necessary to learn it before learning databricks. \n\nShould I learn Java or Scala as i heard it is faster than python \n\nIs there any udemy course or YouTube channel recommendations for databricks or spark.\n\nThanks in Advance!", "author_fullname": "t2_bff9x1i8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ddtsj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677509896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an ETL developer with 1.5 years of experience. I primarily work with informatica power centre, Alteryx, SQL. I am familiar with python as interned as a backend developer where I used flask extensively. I am planning to learn databricks as my company is expecting projects based on databricks. I also worked with pandas and numpy is it enough to master databricks?&lt;/p&gt;\n\n&lt;p&gt;I have started to learn databricks from the databricks academy. \nI found the course to be confusing. Please guide me on how to approach learning databricks.&lt;/p&gt;\n\n&lt;p&gt;What are the prerequisites?\nI have not worked with spark. I know that the databricks platform uses spark. Is it necessary to learn it before learning databricks. &lt;/p&gt;\n\n&lt;p&gt;Should I learn Java or Scala as i heard it is faster than python &lt;/p&gt;\n\n&lt;p&gt;Is there any udemy course or YouTube channel recommendations for databricks or spark.&lt;/p&gt;\n\n&lt;p&gt;Thanks in Advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11ddtsj", "is_robot_indexable": true, "report_reasons": null, "author": "ninjanoob_16", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ddtsj/learning_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ddtsj/learning_databricks/", "subreddit_subscribers": 91304, "created_utc": 1677509896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\ntoday I was watching [this video](https://www.youtube.com/watch?v=4Spo2QRTz1k&amp;t=989s) from Maxime Beauchemin in which he suggests an alternative strategy to handle SCD, different from type 2. The gist of it is to snapshot all the dim table every day and partition by date.\n\nIn the video he mentions the several complexities about handling SCD2, in specific one concept I was not aware about is \"surrogate key lookup\".\n\nI researched a bit and landed on this [example here](https://aws.amazon.com/blogs/big-data/build-slowly-changing-dimensions-type-2-scd2-with-apache-spark-and-apache-hudi-on-amazon-emr/) (scroll until **Customer dimension key lookup**) from AWS, which shows that the synthetic pkey from the dimension table is added to the fact table.\n\nWhy is this? Can't one just use the dimension_id and the valid date range to join a fact and dim table, if historical values are needed?\n\nE.g.\n\n```sql\nSELECT *\nFROM fact a\nJOIN dim b ON\n    a.dim_id = b.dim_id AND\n    a.date BETWEEN b.valid_from and b.valid_to\n```\n\nLet me know what you think!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SCD2 surrogate key lookup, is it necessary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dd8q8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677508342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;today I was watching &lt;a href=\"https://www.youtube.com/watch?v=4Spo2QRTz1k&amp;amp;t=989s\"&gt;this video&lt;/a&gt; from Maxime Beauchemin in which he suggests an alternative strategy to handle SCD, different from type 2. The gist of it is to snapshot all the dim table every day and partition by date.&lt;/p&gt;\n\n&lt;p&gt;In the video he mentions the several complexities about handling SCD2, in specific one concept I was not aware about is &amp;quot;surrogate key lookup&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I researched a bit and landed on this &lt;a href=\"https://aws.amazon.com/blogs/big-data/build-slowly-changing-dimensions-type-2-scd2-with-apache-spark-and-apache-hudi-on-amazon-emr/\"&gt;example here&lt;/a&gt; (scroll until &lt;strong&gt;Customer dimension key lookup&lt;/strong&gt;) from AWS, which shows that the synthetic pkey from the dimension table is added to the fact table.&lt;/p&gt;\n\n&lt;p&gt;Why is this? Can&amp;#39;t one just use the dimension_id and the valid date range to join a fact and dim table, if historical values are needed?&lt;/p&gt;\n\n&lt;p&gt;E.g.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql\nSELECT *\nFROM fact a\nJOIN dim b ON\n    a.dim_id = b.dim_id AND\n    a.date BETWEEN b.valid_from and b.valid_to\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7cv2ZTQ6dDJGaFbCmICEfBF5eUzEbeiLlcOxz8tudSI.jpg?auto=webp&amp;v=enabled&amp;s=f0a05e4168f1db6cf27b0178ae180af6bfa36e9d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/7cv2ZTQ6dDJGaFbCmICEfBF5eUzEbeiLlcOxz8tudSI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb48c268996a5f8b8498abd00637090660b35a24", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/7cv2ZTQ6dDJGaFbCmICEfBF5eUzEbeiLlcOxz8tudSI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02db52765b249d63f0be1a63ee941c5fffb146e0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/7cv2ZTQ6dDJGaFbCmICEfBF5eUzEbeiLlcOxz8tudSI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b46d7e2a85ebac61a2aad3d4a85ad0715fb8218", "width": 320, "height": 240}], "variants": {}, "id": "W4OjmCzoyBuwkuy_25X6VLeKRelJkH5FdeX79YCbFAY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11dd8q8", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dd8q8/scd2_surrogate_key_lookup_is_it_necessary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dd8q8/scd2_surrogate_key_lookup_is_it_necessary/", "subreddit_subscribers": 91304, "created_utc": 1677508342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,  \nI am looking into building data pipelines in a way that is as pythonic as possible, I found out about this Streamz project and it integrates well with Dask. Does anyone have any experience using this? My main use would be for fairly simple accumulator pipelines.  \nhttps://github.com/python-streamz/streamz", "author_fullname": "t2_ytvdx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experiences using Streamz?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11d9umo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677498114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nI am looking into building data pipelines in a way that is as pythonic as possible, I found out about this Streamz project and it integrates well with Dask. Does anyone have any experience using this? My main use would be for fairly simple accumulator pipelines.&lt;br/&gt;\n&lt;a href=\"https://github.com/python-streamz/streamz\"&gt;https://github.com/python-streamz/streamz&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?auto=webp&amp;v=enabled&amp;s=c7f2507a5945e4140ec5b7c378d8263a00d9c1f9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e60719913a37addce2db8b588b1fb3c3e46f324a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99948364664600ec4a213029cfb081cbd36796b2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0cd49140a6295a19415a408f089ad9049983d1db", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=515434dd12129d0e76dd8cc8bdd99c350e69b51d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=150fa150e8e696cee11d48e0fdd301dee390fc10", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d5f5886a4ddf06742906f6a045b5649feb3ef9c", "width": 1080, "height": 540}], "variants": {}, "id": "tumjq3w6bN8Iody0VGURd8uVr6WPWLCP71LCovZCr2k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11d9umo", "is_robot_indexable": true, "report_reasons": null, "author": "carnivorousdrew", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11d9umo/experiences_using_streamz/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11d9umo/experiences_using_streamz/", "subreddit_subscribers": 91304, "created_utc": 1677498114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have many jobs I have to run on a recurring basis (once a week/day/hour etc). These are all\n\n1. Containerized (Docker)\n2. Take a few minutes - few days to complete (and need to have timeouts to avoid infinite runs - differs by job type)\n3. Need a specified amount of CPUs and RAM to complete (differs by job type)\n\nCurrently, I have enough server capacity to handle the max load and run the Docker containers with cron jobs. But the load varies a lot, sometimes all running containers need just 8GB RAM altogether, sometimes 100GB - so this is not very cost-efficient. It also needs some monitoring and maintenance that I think could be simplified.\n\nWhat is the best way of going about this? Can I set this up using Kubernetes (I haven't used it before), or is there a more focused tool for a similar problem?", "author_fullname": "t2_b9odplx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best scalable way to run Docker containers for recurring data jobs with a start, finish and timeout?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11d69u9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677484822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many jobs I have to run on a recurring basis (once a week/day/hour etc). These are all&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Containerized (Docker)&lt;/li&gt;\n&lt;li&gt;Take a few minutes - few days to complete (and need to have timeouts to avoid infinite runs - differs by job type)&lt;/li&gt;\n&lt;li&gt;Need a specified amount of CPUs and RAM to complete (differs by job type)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Currently, I have enough server capacity to handle the max load and run the Docker containers with cron jobs. But the load varies a lot, sometimes all running containers need just 8GB RAM altogether, sometimes 100GB - so this is not very cost-efficient. It also needs some monitoring and maintenance that I think could be simplified.&lt;/p&gt;\n\n&lt;p&gt;What is the best way of going about this? Can I set this up using Kubernetes (I haven&amp;#39;t used it before), or is there a more focused tool for a similar problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11d69u9", "is_robot_indexable": true, "report_reasons": null, "author": "vasarmilan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11d69u9/best_scalable_way_to_run_docker_containers_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11d69u9/best_scalable_way_to_run_docker_containers_for/", "subreddit_subscribers": 91304, "created_utc": 1677484822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'll get to the point quick. I wish to create a database where each item is associated with a set of information and values, which in turn is associated with a set of information and values\n\nA simplified representation:\n\nA:{a:{1,x},b:{2,y}c:{3,z}}\n\nB:{d:{1,z}e:{2,z}f:{3,y}}\n\nC:{g:{1,y},h:{2,x},i{3,x}}\n\nThe sets within the largest set can be seen as a sequence (by which I mean that the data is ordered according to the same metadata (I think in SQL they call this a 'key') always). This is all a simplification, but I would expect every top tier set to contain about 2000 sets and 10 values and each of those 2nd tier sets to contain about 20 sets and 10 values and for each of those 20 sets to be associated with 20 values.\n\nI'm not entirely sure if a relational database is best suited for this; and my Google searches haven't helped. Many thanks in advance!\n\n(If you want to know what this to be used for, it's for a linguistic corpus where every work is divided into sentences and every sentence is divided into words and each sentence and words is associated with certain values. I understand that there are already software for corpora, but they do not work well for either the language I research or the questions I ask, so that's why I ask the question in relation to sets)", "author_fullname": "t2_graxcz2q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Quick Question] Best way to make a database of databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11cmmbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677431474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll get to the point quick. I wish to create a database where each item is associated with a set of information and values, which in turn is associated with a set of information and values&lt;/p&gt;\n\n&lt;p&gt;A simplified representation:&lt;/p&gt;\n\n&lt;p&gt;A:{a:{1,x},b:{2,y}c:{3,z}}&lt;/p&gt;\n\n&lt;p&gt;B:{d:{1,z}e:{2,z}f:{3,y}}&lt;/p&gt;\n\n&lt;p&gt;C:{g:{1,y},h:{2,x},i{3,x}}&lt;/p&gt;\n\n&lt;p&gt;The sets within the largest set can be seen as a sequence (by which I mean that the data is ordered according to the same metadata (I think in SQL they call this a &amp;#39;key&amp;#39;) always). This is all a simplification, but I would expect every top tier set to contain about 2000 sets and 10 values and each of those 2nd tier sets to contain about 20 sets and 10 values and for each of those 20 sets to be associated with 20 values.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not entirely sure if a relational database is best suited for this; and my Google searches haven&amp;#39;t helped. Many thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;(If you want to know what this to be used for, it&amp;#39;s for a linguistic corpus where every work is divided into sentences and every sentence is divided into words and each sentence and words is associated with certain values. I understand that there are already software for corpora, but they do not work well for either the language I research or the questions I ask, so that&amp;#39;s why I ask the question in relation to sets)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11cmmbk", "is_robot_indexable": true, "report_reasons": null, "author": "alpolvovolvere", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11cmmbk/quick_question_best_way_to_make_a_database_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11cmmbk/quick_question_best_way_to_make_a_database_of/", "subreddit_subscribers": 91304, "created_utc": 1677431474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to break into data engineering roles. I have experience in dot net and data analysis and a MS in Data Science and worked on dot net, Python, SQL, Tableau, SSIS/SSRS, VBA etc.\n\nHowever, what I'm finding is that there is literally no consistency among what skills companies are asking for DE roles. The data engineer has become a catch-all term for anything from simple data analysis, database dev, BI dev to ML/stats to actual pipelines development to a tools ninja.\n\nThere seems to be a flood of tools in the DE space and each job posting is asking hands-on experience in a different combination of tools.\n\nI'm scratching my head as to how should I spend my time learning what tools and skills?\n\nIt's impossible to have hands-on experience on all/most of these tools, even in a regular ACTUAL DE job. For example, below is the list of frequently asked tools I've curated from job postings -----------------------------------------------------------------\n\n**Programming Languages and Tools:** Python, SQL, C#, YAML, Unix Shell Scripting, CLI, DBT, REST APIs\n\n**Data Formats:** Relational, Unstructured, Semi-structured (XML, JSON, CSV), Parquet, time-series\n\n**Cloud Computing:**  Snowflake, Databricks, Amazon S3, EC2, AWS CloudFormation, Python Boto3 SDK, Amazon DMS (Database Migration Service), AWS Glue, Amazon Redshift, AWS Athena, Amazon QuickSight, SNS, KMS, CDK, Azure Storage, Azure Data Factory, Azure Synapse, Azure SQL DB, Azure DevOps, Google BigQuery (GCP), Google Cloud Dataflow (GCP), Terraform\n\n**Tools:** Apache/Confluent Kafka, PySpark, Apache Airflow, (DevOps and CI/CD) Docker, Kubernetes, Jenkins, Github Actions, SQL Server, Oracle, MySQL, PostgreSQL, MongoDb, Azure CosmosDB, AWS Dynamo, Tableau, SSIS\n\n**Big Data:** Hadoop, Hive, Pig, HBase, Cassandra Amazon EMR, Spark, PySpark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink\n\n\\----------------------------------------------------------------\n\nAlso, **recruiters won't bother to contact you unless you tell them that you have X years of experience in Y technology**. So I have had to watch some tutorials about the tools and make up stories about having worked on them. This does not fill me confidence.\n\nSo how do I go about navigating through this mess? I'm literally overwhelmed right now. Anyone facing similar issue? Any suggestions are appreciated. Thanks.", "author_fullname": "t2_jyng23u3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineer job hunt is a mess!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11dgjv2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677517329.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677516776.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to break into data engineering roles. I have experience in dot net and data analysis and a MS in Data Science and worked on dot net, Python, SQL, Tableau, SSIS/SSRS, VBA etc.&lt;/p&gt;\n\n&lt;p&gt;However, what I&amp;#39;m finding is that there is literally no consistency among what skills companies are asking for DE roles. The data engineer has become a catch-all term for anything from simple data analysis, database dev, BI dev to ML/stats to actual pipelines development to a tools ninja.&lt;/p&gt;\n\n&lt;p&gt;There seems to be a flood of tools in the DE space and each job posting is asking hands-on experience in a different combination of tools.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m scratching my head as to how should I spend my time learning what tools and skills?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s impossible to have hands-on experience on all/most of these tools, even in a regular ACTUAL DE job. For example, below is the list of frequently asked tools I&amp;#39;ve curated from job postings -----------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Programming Languages and Tools:&lt;/strong&gt; Python, SQL, C#, YAML, Unix Shell Scripting, CLI, DBT, REST APIs&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Formats:&lt;/strong&gt; Relational, Unstructured, Semi-structured (XML, JSON, CSV), Parquet, time-series&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cloud Computing:&lt;/strong&gt;  Snowflake, Databricks, Amazon S3, EC2, AWS CloudFormation, Python Boto3 SDK, Amazon DMS (Database Migration Service), AWS Glue, Amazon Redshift, AWS Athena, Amazon QuickSight, SNS, KMS, CDK, Azure Storage, Azure Data Factory, Azure Synapse, Azure SQL DB, Azure DevOps, Google BigQuery (GCP), Google Cloud Dataflow (GCP), Terraform&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Tools:&lt;/strong&gt; Apache/Confluent Kafka, PySpark, Apache Airflow, (DevOps and CI/CD) Docker, Kubernetes, Jenkins, Github Actions, SQL Server, Oracle, MySQL, PostgreSQL, MongoDb, Azure CosmosDB, AWS Dynamo, Tableau, SSIS&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Big Data:&lt;/strong&gt; Hadoop, Hive, Pig, HBase, Cassandra Amazon EMR, Spark, PySpark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink&lt;/p&gt;\n\n&lt;p&gt;----------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;Also, &lt;strong&gt;recruiters won&amp;#39;t bother to contact you unless you tell them that you have X years of experience in Y technology&lt;/strong&gt;. So I have had to watch some tutorials about the tools and make up stories about having worked on them. This does not fill me confidence.&lt;/p&gt;\n\n&lt;p&gt;So how do I go about navigating through this mess? I&amp;#39;m literally overwhelmed right now. Anyone facing similar issue? Any suggestions are appreciated. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11dgjv2", "is_robot_indexable": true, "report_reasons": null, "author": "Fabulous_Weekend330", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dgjv2/data_engineer_job_hunt_is_a_mess/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dgjv2/data_engineer_job_hunt_is_a_mess/", "subreddit_subscribers": 91304, "created_utc": 1677516776.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking into lake house solutions to use with AWS S3, really trying to stay as open source as possible (mostly for cost and avoiding vendor lock).  I really like a lot about Delta Lake, Apache Hudi, Apache Iceberg, but I can't find a lot of information about table access control i.e. how to control access to individual columns within the data based on credentials of the user querying it. It looks like all the column level (or row/cell level) access control solutions come bundled in paid products like Databricks , Privacera, Immuta, etc.   \n\n\n**Is there a way to get this type of access control on lake house tables in S3 using only open source tools?**   \n\n\nS3 Lake Formation has Governed Tables that I don't think would increase any costs over the other lake house table formats but I worry about being locked into that format long term.", "author_fullname": "t2_54h1iub7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source solutions to lake house access control?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11delba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677511836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking into lake house solutions to use with AWS S3, really trying to stay as open source as possible (mostly for cost and avoiding vendor lock).  I really like a lot about Delta Lake, Apache Hudi, Apache Iceberg, but I can&amp;#39;t find a lot of information about table access control i.e. how to control access to individual columns within the data based on credentials of the user querying it. It looks like all the column level (or row/cell level) access control solutions come bundled in paid products like Databricks , Privacera, Immuta, etc.   &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there a way to get this type of access control on lake house tables in S3 using only open source tools?&lt;/strong&gt;   &lt;/p&gt;\n\n&lt;p&gt;S3 Lake Formation has Governed Tables that I don&amp;#39;t think would increase any costs over the other lake house table formats but I worry about being locked into that format long term.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11delba", "is_robot_indexable": true, "report_reasons": null, "author": "Jtmyer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11delba/open_source_solutions_to_lake_house_access_control/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11delba/open_source_solutions_to_lake_house_access_control/", "subreddit_subscribers": 91304, "created_utc": 1677511836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Using (Azure) Databricks Autoloader we pick up files from an Azure blob storage and process them. I want to implement some monitoring functionality and as part of that I want to know if we are loading empty rows. Knowing that streaming dataframes are a special thing, I feel like I can't just simply call a simple check on it. Do you have any best practices?", "author_fullname": "t2_hp7r8vez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Structured Streaming: how to catch if we are loading empty rows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ddt3g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677509846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Using (Azure) Databricks Autoloader we pick up files from an Azure blob storage and process them. I want to implement some monitoring functionality and as part of that I want to know if we are loading empty rows. Knowing that streaming dataframes are a special thing, I feel like I can&amp;#39;t just simply call a simple check on it. Do you have any best practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11ddt3g", "is_robot_indexable": true, "report_reasons": null, "author": "Labanc_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11ddt3g/spark_structured_streaming_how_to_catch_if_we_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ddt3g/spark_structured_streaming_how_to_catch_if_we_are/", "subreddit_subscribers": 91304, "created_utc": 1677509846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Now that the dramatic title is out there - do you think it\u2019s the case that analytics engineering, albeit still brand new, also risks being dead end?\n\nSure, this person can learn dbt core over dbt cloud and get further into CICD and devops/cloud type tasks (this is me right now), but where do you go from there?\n\nSeems like the logical progression is to data engineer (meaning, a more python heavy role). By contrast, I could see how an analytics engineer would benefit from learning some ML to implement (mainly with the way dbt is going with python support).\n\nWhat\u2019s your opinion? What do you speculate will happen to the role?", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is analytics engineering a dead end career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ddhe5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677508993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now that the dramatic title is out there - do you think it\u2019s the case that analytics engineering, albeit still brand new, also risks being dead end?&lt;/p&gt;\n\n&lt;p&gt;Sure, this person can learn dbt core over dbt cloud and get further into CICD and devops/cloud type tasks (this is me right now), but where do you go from there?&lt;/p&gt;\n\n&lt;p&gt;Seems like the logical progression is to data engineer (meaning, a more python heavy role). By contrast, I could see how an analytics engineer would benefit from learning some ML to implement (mainly with the way dbt is going with python support).&lt;/p&gt;\n\n&lt;p&gt;What\u2019s your opinion? What do you speculate will happen to the role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11ddhe5", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ddhe5/is_analytics_engineering_a_dead_end_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ddhe5/is_analytics_engineering_a_dead_end_career/", "subreddit_subscribers": 91304, "created_utc": 1677508993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a central data team serving different between BU with their analytics team or is there a central analytics team as well?\n\nAnd whom do you report to? CDP, CIO, CFO or COO?", "author_fullname": "t2_xap78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your org structure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dcxc9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677507491.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a central data team serving different between BU with their analytics team or is there a central analytics team as well?&lt;/p&gt;\n\n&lt;p&gt;And whom do you report to? CDP, CIO, CFO or COO?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11dcxc9", "is_robot_indexable": true, "report_reasons": null, "author": "totalsports1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11dcxc9/whats_your_org_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dcxc9/whats_your_org_structure/", "subreddit_subscribers": 91304, "created_utc": 1677507491.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Same as title", "author_fullname": "t2_57e44nxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "which MLOPs framework is best in terms of reusability of code? What all mlops framework do you guys use and why ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dbxf9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677504738.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Same as title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11dbxf9", "is_robot_indexable": true, "report_reasons": null, "author": "RstarPhoneix", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dbxf9/which_mlops_framework_is_best_in_terms_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dbxf9/which_mlops_framework_is_best_in_terms_of/", "subreddit_subscribers": 91304, "created_utc": 1677504738.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was passively looking for a job for a bit, but now want to turn it up and actively get some applications out there. I'm targeting senior data/BI/analytics engineer, and manager-level roles as that's exactly where I'm at right now, and happy with a lateral as long as it comes with a pay bump.\n\nI've seen a few staffing agencies come up on LinkedIn, but I'm hesitant on reaching out to them. So I wanted to ask the group here if any of you all have used any, and if there are any you would recommend or not.", "author_fullname": "t2_2pyy4c8f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used data-focused staffing or recruiting agencies? Are there any you would recommend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dbkjv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677503668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was passively looking for a job for a bit, but now want to turn it up and actively get some applications out there. I&amp;#39;m targeting senior data/BI/analytics engineer, and manager-level roles as that&amp;#39;s exactly where I&amp;#39;m at right now, and happy with a lateral as long as it comes with a pay bump.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen a few staffing agencies come up on LinkedIn, but I&amp;#39;m hesitant on reaching out to them. So I wanted to ask the group here if any of you all have used any, and if there are any you would recommend or not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11dbkjv", "is_robot_indexable": true, "report_reasons": null, "author": "opabm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dbkjv/has_anyone_used_datafocused_staffing_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dbkjv/has_anyone_used_datafocused_staffing_or/", "subreddit_subscribers": 91304, "created_utc": 1677503668.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}