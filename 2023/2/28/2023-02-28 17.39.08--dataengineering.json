{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve got a monolith AWS lambda function that needs to reach out to and query Athena and also do some writes to various s3 buckets after processing what it got from Athena. \n\nTrying to set up pytest and moto but getting stuck because I\u2019m also using awswrangler. \n\nNot even sure where to start finding info about building unit tests here and want to avoid deploying to AWS just to test minor code changes. \n\nDoes anyone have any good resources in where to start?", "author_fullname": "t2_d5gr1nxb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for resources for building unit testing for boto3 code and mocking AWS services in pytest", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11drsam", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677544195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve got a monolith AWS lambda function that needs to reach out to and query Athena and also do some writes to various s3 buckets after processing what it got from Athena. &lt;/p&gt;\n\n&lt;p&gt;Trying to set up pytest and moto but getting stuck because I\u2019m also using awswrangler. &lt;/p&gt;\n\n&lt;p&gt;Not even sure where to start finding info about building unit tests here and want to avoid deploying to AWS just to test minor code changes. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any good resources in where to start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11drsam", "is_robot_indexable": true, "report_reasons": null, "author": "renok_archnmy", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11drsam/looking_for_resources_for_building_unit_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11drsam/looking_for_resources_for_building_unit_testing/", "subreddit_subscribers": 91410, "created_utc": 1677544195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you structure your pandas ETL jobs? Do you have some predefined template/structure how do you apply steps to your dataframes? Do you apply the transformations on the dataframes right away or you create a separate functions for each transforming step? Do you use some micro-frameworks like for example [Hamilton](https://github.com/dagworks-inc/hamilton)?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pandas workflow template", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dlu99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677529508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you structure your pandas ETL jobs? Do you have some predefined template/structure how do you apply steps to your dataframes? Do you apply the transformations on the dataframes right away or you create a separate functions for each transforming step? Do you use some micro-frameworks like for example &lt;a href=\"https://github.com/dagworks-inc/hamilton\"&gt;Hamilton&lt;/a&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?auto=webp&amp;v=enabled&amp;s=7e647c2fe032b2efe1c8e86fadcd33540ed3318a", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9023c5cd1aa30963e66aa3d8c30ca94545f9d93", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d747f449845b4276445506cd5f33dcceda89bb6d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=861ecaa2e1b8b03513b7ed62e1839b6fe9743295", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fcca2b33479e11f211dda359a9b7538ae447662", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75561d3b421a585f0947c8a260caaf7142f15f08", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d0a1b54ba8e8e0434783d28301591bb3e7f2aadd", "width": 1080, "height": 540}], "variants": {}, "id": "3YR5ZTzaTkBoH-5mexRRNuUftSAaFay-5D83JdeOsTM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11dlu99", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dlu99/pandas_workflow_template/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dlu99/pandas_workflow_template/", "subreddit_subscribers": 91410, "created_utc": 1677529508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.recordlydata.com/blog/introduction-to-streamlit-and-data-applications-on-snowflake-with-winter-sports-examples](https://www.recordlydata.com/blog/introduction-to-streamlit-and-data-applications-on-snowflake-with-winter-sports-examples)\n\nHi, I wrote a longer blog that you can use to explain to your data engineering team why Streamlit is an excellent choice for example replacing spreadsheets and/or visualizing your ML models. The examples are simple in terms that it shows how easy it's for example to write data back into Snowflake or to visualize a map data. \n\nHope you enjoy and had as much fun reading as I had writing this.   \n\n\nhttps://preview.redd.it/sgnvh0116xka1.png?width=1920&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2305e0a3080a28066343d0cb236af6ac587f38c5", "author_fullname": "t2_sxd6cnuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Streamlit and Data applications on Snowflake with Winter Sports examples", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "media_metadata": {"sgnvh0116xka1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 49, "x": 108, "u": "https://preview.redd.it/sgnvh0116xka1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aba633b89ce8a4ad4b6ad478e398b87b98701892"}, {"y": 98, "x": 216, "u": "https://preview.redd.it/sgnvh0116xka1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7168ad79cf7bc0e89f737d02fd9a5da08b2685d0"}, {"y": 145, "x": 320, "u": "https://preview.redd.it/sgnvh0116xka1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=250dd2fc5d66dbfa5d6541df378d5147b21972a0"}, {"y": 291, "x": 640, "u": "https://preview.redd.it/sgnvh0116xka1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7318e143a74a678c35b71e4407a6c9cf62567db"}, {"y": 437, "x": 960, "u": "https://preview.redd.it/sgnvh0116xka1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3b41d90f5e2d93b3cbc91ff19c3544c59d6fc2a"}, {"y": 492, "x": 1080, "u": "https://preview.redd.it/sgnvh0116xka1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b6f7e1c23ca1a7cb5902486f4bb8b88f334e3da"}], "s": {"y": 875, "x": 1920, "u": "https://preview.redd.it/sgnvh0116xka1.png?width=1920&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2305e0a3080a28066343d0cb236af6ac587f38c5"}, "id": "sgnvh0116xka1"}}, "name": "t3_11e4bv4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WNUlYBzmq5_xjOvDF9FNroXOFVBeHFjCtdrFBVGl18g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677585694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.recordlydata.com/blog/introduction-to-streamlit-and-data-applications-on-snowflake-with-winter-sports-examples\"&gt;https://www.recordlydata.com/blog/introduction-to-streamlit-and-data-applications-on-snowflake-with-winter-sports-examples&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi, I wrote a longer blog that you can use to explain to your data engineering team why Streamlit is an excellent choice for example replacing spreadsheets and/or visualizing your ML models. The examples are simple in terms that it shows how easy it&amp;#39;s for example to write data back into Snowflake or to visualize a map data. &lt;/p&gt;\n\n&lt;p&gt;Hope you enjoy and had as much fun reading as I had writing this.   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/sgnvh0116xka1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2305e0a3080a28066343d0cb236af6ac587f38c5\"&gt;https://preview.redd.it/sgnvh0116xka1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2305e0a3080a28066343d0cb236af6ac587f38c5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11e4bv4", "is_robot_indexable": true, "report_reasons": null, "author": "Recordly_MHeino", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11e4bv4/introduction_to_streamlit_and_data_applications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11e4bv4/introduction_to_streamlit_and_data_applications/", "subreddit_subscribers": 91410, "created_utc": 1677585694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any benefit on using this technology nowadays? Is it still worth investing into? If not, which compelling arguments could i use to convince my managers to finally adopt cloud technologies?\nClient's management is considering to migrate hadoop jobs to kudu reason being it's a mess to manage updates in hdfs.\nI strongly feel like it could be a big mistake to invest in such niche and unpolular technologies. Considering possible time and effort required for the migration could be big, in my client's interest i would like to suggest a more future-proof and why not, more pleasant to work with technologies to accomplish our requirements (basically NRT reads from a kafka topic via apache spark and NRT updates of master data tables in a data store, currently done in full batch on hive tables over hdfs).\n\nThanks in advance", "author_fullname": "t2_3xvidrz9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone use Apache Kudu?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11do3cw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677534898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any benefit on using this technology nowadays? Is it still worth investing into? If not, which compelling arguments could i use to convince my managers to finally adopt cloud technologies?\nClient&amp;#39;s management is considering to migrate hadoop jobs to kudu reason being it&amp;#39;s a mess to manage updates in hdfs.\nI strongly feel like it could be a big mistake to invest in such niche and unpolular technologies. Considering possible time and effort required for the migration could be big, in my client&amp;#39;s interest i would like to suggest a more future-proof and why not, more pleasant to work with technologies to accomplish our requirements (basically NRT reads from a kafka topic via apache spark and NRT updates of master data tables in a data store, currently done in full batch on hive tables over hdfs).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11do3cw", "is_robot_indexable": true, "report_reasons": null, "author": "RAT-000", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11do3cw/does_anyone_use_apache_kudu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11do3cw/does_anyone_use_apache_kudu/", "subreddit_subscribers": 91410, "created_utc": 1677534898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are either going to build or buy a Lakehouse. One thing that is unclear to me is how we can achieve a solid permissioning story on a single multi-tenant cluster.\n\nOur usage patterns\n\n* We expect 80% of use cases to be querying via SQL (tbd but something like Trino, spark thrift, etc).\n* The other 20% will be submitting spark jobs.\n\nOur requirements\n\n1. As few clusters as possible. I don't want to have to spin up one cluster per permission boundary. Ideally we have one cluster that autoscales with load. I'm open to push-back on this one. If managing a bunch of clusters isn't that bad using Databricks I'm open to hearing that argument.\n2. Users should only be able to access data they have permission to. It's fine if we *define* this outside of the lakehouse as long as there's some mechanism to *enforce* it within the cluster/jobs.\n3. We can build or buy or adopt, but if we build or adopt it'll be in AWS\n\nThings we've looked at\n\n* Databricks. Per [this doc](https://docs.databricks.com/workflows/jobs/jobs.html), jobs submitted via spark-submit do not support cluster autoscaling?! And then if you use the unity catalog, the cluster needs to be Single User access mode. Both seem like big downsides. Seems like to make this work we'd need to have separate clusters for the spark jobs, one cluster per permission boundary, and size them manually to handle the workloads. \n* Maybe we can [Kerberize an EMR cluster and use Apache Ranger](https://aws.amazon.com/blogs/big-data/introducing-amazon-emr-integration-with-apache-ranger/)? \n\n**Does anyone have a multi-tenant cluster that allows spark and SQL-like workloads and properly enforces permissions? What is your setup?**", "author_fullname": "t2_tic2ae1k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-tenant permissions on a Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11djd7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677523547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are either going to build or buy a Lakehouse. One thing that is unclear to me is how we can achieve a solid permissioning story on a single multi-tenant cluster.&lt;/p&gt;\n\n&lt;p&gt;Our usage patterns&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We expect 80% of use cases to be querying via SQL (tbd but something like Trino, spark thrift, etc).&lt;/li&gt;\n&lt;li&gt;The other 20% will be submitting spark jobs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Our requirements&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;As few clusters as possible. I don&amp;#39;t want to have to spin up one cluster per permission boundary. Ideally we have one cluster that autoscales with load. I&amp;#39;m open to push-back on this one. If managing a bunch of clusters isn&amp;#39;t that bad using Databricks I&amp;#39;m open to hearing that argument.&lt;/li&gt;\n&lt;li&gt;Users should only be able to access data they have permission to. It&amp;#39;s fine if we &lt;em&gt;define&lt;/em&gt; this outside of the lakehouse as long as there&amp;#39;s some mechanism to &lt;em&gt;enforce&lt;/em&gt; it within the cluster/jobs.&lt;/li&gt;\n&lt;li&gt;We can build or buy or adopt, but if we build or adopt it&amp;#39;ll be in AWS&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Things we&amp;#39;ve looked at&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Databricks. Per &lt;a href=\"https://docs.databricks.com/workflows/jobs/jobs.html\"&gt;this doc&lt;/a&gt;, jobs submitted via spark-submit do not support cluster autoscaling?! And then if you use the unity catalog, the cluster needs to be Single User access mode. Both seem like big downsides. Seems like to make this work we&amp;#39;d need to have separate clusters for the spark jobs, one cluster per permission boundary, and size them manually to handle the workloads. &lt;/li&gt;\n&lt;li&gt;Maybe we can &lt;a href=\"https://aws.amazon.com/blogs/big-data/introducing-amazon-emr-integration-with-apache-ranger/\"&gt;Kerberize an EMR cluster and use Apache Ranger&lt;/a&gt;? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Does anyone have a multi-tenant cluster that allows spark and SQL-like workloads and properly enforces permissions? What is your setup?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11djd7c", "is_robot_indexable": true, "report_reasons": null, "author": "databolica", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11djd7c/multitenant_permissions_on_a_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11djd7c/multitenant_permissions_on_a_lakehouse/", "subreddit_subscribers": 91410, "created_utc": 1677523547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "## Or how to build a light weight data platform for a small organization from scratch? \n\nWe are an organization with a scale of around 300 people, and our administrative team wants to establish a data platform for aggregating, analyzing, and displaying some internal operational data. Our data volume is not large, so we do not need heavyweight distributed technologies. From the hardware requirements perspective, a bare metal server may be sufficient.\n\nHowever, on the other hand, we need the ability to extract, store, and ultimately present data from multiple sources on a reporting platform.  What I want to know is whether a lakehouse solution like Delta Lake is an ideal choice.\n\nWe do not have relevant implementation experience and hope to get some advice or recommended reference materials to help us better understand the relevant technologies.\n\n\n### Update\nI try to draw out a potential solution based on the discussion and knowledge we have.\n\n| Domain | Solution | Note |\n|-|-|-|\n| Data Warehouse | Clickhouse or PgSql | Lakehouse is overkill |\n| BI / Data App | SuperSet and Dash |  |\n| ETL | AirByte | Any suggestions? |", "author_fullname": "t2_m0cc1w3g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to deploy a lakehouse solution on a bare metal server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ea84j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677604352.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677596960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;Or how to build a light weight data platform for a small organization from scratch?&lt;/h2&gt;\n\n&lt;p&gt;We are an organization with a scale of around 300 people, and our administrative team wants to establish a data platform for aggregating, analyzing, and displaying some internal operational data. Our data volume is not large, so we do not need heavyweight distributed technologies. From the hardware requirements perspective, a bare metal server may be sufficient.&lt;/p&gt;\n\n&lt;p&gt;However, on the other hand, we need the ability to extract, store, and ultimately present data from multiple sources on a reporting platform.  What I want to know is whether a lakehouse solution like Delta Lake is an ideal choice.&lt;/p&gt;\n\n&lt;p&gt;We do not have relevant implementation experience and hope to get some advice or recommended reference materials to help us better understand the relevant technologies.&lt;/p&gt;\n\n&lt;h3&gt;Update&lt;/h3&gt;\n\n&lt;p&gt;I try to draw out a potential solution based on the discussion and knowledge we have.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Domain&lt;/th&gt;\n&lt;th&gt;Solution&lt;/th&gt;\n&lt;th&gt;Note&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;Data Warehouse&lt;/td&gt;\n&lt;td&gt;Clickhouse or PgSql&lt;/td&gt;\n&lt;td&gt;Lakehouse is overkill&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;BI / Data App&lt;/td&gt;\n&lt;td&gt;SuperSet and Dash&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;ETL&lt;/td&gt;\n&lt;td&gt;AirByte&lt;/td&gt;\n&lt;td&gt;Any suggestions?&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11ea84j", "is_robot_indexable": true, "report_reasons": null, "author": "_link89_", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ea84j/is_it_possible_to_deploy_a_lakehouse_solution_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ea84j/is_it_possible_to_deploy_a_lakehouse_solution_on/", "subreddit_subscribers": 91410, "created_utc": 1677596960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone i applied recently to a data engineering job but got rejected after passing technical interview and getting \u2248 80% on codility test,\n\nMy current job is python developer/data engineer basically where i am they had a project for about a year and a half and its done. \n\nI feel down and last time i practiced what i did as a data engineer was about 8 months ago, idk how to find a data engineering job anymore and more importantly how to practice what i know or need to know to actually land a job. \n\nI hope you help me find my way out of this", "author_fullname": "t2_i62lmiqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help getting on track", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dlisz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677528739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone i applied recently to a data engineering job but got rejected after passing technical interview and getting \u2248 80% on codility test,&lt;/p&gt;\n\n&lt;p&gt;My current job is python developer/data engineer basically where i am they had a project for about a year and a half and its done. &lt;/p&gt;\n\n&lt;p&gt;I feel down and last time i practiced what i did as a data engineer was about 8 months ago, idk how to find a data engineering job anymore and more importantly how to practice what i know or need to know to actually land a job. &lt;/p&gt;\n\n&lt;p&gt;I hope you help me find my way out of this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11dlisz", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Resolution-1025", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dlisz/help_getting_on_track/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dlisz/help_getting_on_track/", "subreddit_subscribers": 91410, "created_utc": 1677528739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking at docs and it is saying that mounting external cloud storage isn\u2019t best practice? If that\u2019s the case how do you guys work daily? I am new to databricks and not sure where notebooks, data should be stored and best practice working with cloud storage.\n\n1. If you have data in a data lake like s3/adls to u connect and pull it in or work with remotely?\n1a. If processing that data where do you write it? Back to s3 or dbfs?\n2. Purpose of dbfs if they say not to put data in it?\n3. Do you just mount your external cloud storage anyways?", "author_fullname": "t2_fqm3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice with storage and databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11ec67v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677599940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking at docs and it is saying that mounting external cloud storage isn\u2019t best practice? If that\u2019s the case how do you guys work daily? I am new to databricks and not sure where notebooks, data should be stored and best practice working with cloud storage.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If you have data in a data lake like s3/adls to u connect and pull it in or work with remotely?\n1a. If processing that data where do you write it? Back to s3 or dbfs?&lt;/li&gt;\n&lt;li&gt;Purpose of dbfs if they say not to put data in it?&lt;/li&gt;\n&lt;li&gt;Do you just mount your external cloud storage anyways?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11ec67v", "is_robot_indexable": true, "report_reasons": null, "author": "Bigchip01", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ec67v/best_practice_with_storage_and_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ec67v/best_practice_with_storage_and_databricks/", "subreddit_subscribers": 91410, "created_utc": 1677599940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have an upcoming interview for a mid-level data engineer role at a data consultancy in London. I have to do a multiple choice SQL test that runs against time, it does not involve coding. I looked online for mock tests but I don't know which one would be the equivalent in terms of difficulty. I browsed through the w3 schools quiz but its entry-level stuff. Any advice or links to a good mock test?", "author_fullname": "t2_r7bmwiee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Test", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11edtlt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677603806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have an upcoming interview for a mid-level data engineer role at a data consultancy in London. I have to do a multiple choice SQL test that runs against time, it does not involve coding. I looked online for mock tests but I don&amp;#39;t know which one would be the equivalent in terms of difficulty. I browsed through the w3 schools quiz but its entry-level stuff. Any advice or links to a good mock test?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "11edtlt", "is_robot_indexable": true, "report_reasons": null, "author": "va1kyrja-kara", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11edtlt/interview_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11edtlt/interview_test/", "subreddit_subscribers": 91410, "created_utc": 1677603806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have just started tiny Data Analytics consulting firm. Currently I am the only employee, working full-time  at one client but planning to provide end to end service of Staff Aug as well complete Data Engineering project implementation service. Recently I came across Data conference [https://www.datacouncil.ai/austin](https://www.datacouncil.ai/austin). I have never been to this type of event in individual capacity. I think it should be great learning but do you know if this can be helpful get some connection to get client in future ? How should I use events like these ? If anyone has such experience and advice, that will be of great help.", "author_fullname": "t2_1u2bikk0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Conference", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11edhmg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677603024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have just started tiny Data Analytics consulting firm. Currently I am the only employee, working full-time  at one client but planning to provide end to end service of Staff Aug as well complete Data Engineering project implementation service. Recently I came across Data conference &lt;a href=\"https://www.datacouncil.ai/austin\"&gt;https://www.datacouncil.ai/austin&lt;/a&gt;. I have never been to this type of event in individual capacity. I think it should be great learning but do you know if this can be helpful get some connection to get client in future ? How should I use events like these ? If anyone has such experience and advice, that will be of great help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_F5GUrm7PGzf7vLlLgeDXrjMiY-EMTAdHljTEzGNgEg.jpg?auto=webp&amp;v=enabled&amp;s=54e72a190e3a4e0e3e7412697d111a1d4164245f", "width": 1434, "height": 1434}, "resolutions": [{"url": "https://external-preview.redd.it/_F5GUrm7PGzf7vLlLgeDXrjMiY-EMTAdHljTEzGNgEg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=712b54f3ba3432e501cc4ba9fba72776b722543a", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/_F5GUrm7PGzf7vLlLgeDXrjMiY-EMTAdHljTEzGNgEg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9eeb54f2d3f0388636f0b6c30d23acaaf892ab9d", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/_F5GUrm7PGzf7vLlLgeDXrjMiY-EMTAdHljTEzGNgEg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4724ad2f2c8c22bcf6f3d29edf73f5975a527d96", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/_F5GUrm7PGzf7vLlLgeDXrjMiY-EMTAdHljTEzGNgEg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e46045182cf4ea2bd1725215182a69ceafaf3ba7", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/_F5GUrm7PGzf7vLlLgeDXrjMiY-EMTAdHljTEzGNgEg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=652c5d744d177eabea48e38193f0a9985d19e198", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/_F5GUrm7PGzf7vLlLgeDXrjMiY-EMTAdHljTEzGNgEg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18c77681ac295cf1d6523ab0a3856cb09db838ca", "width": 1080, "height": 1080}], "variants": {}, "id": "Fc1fxjSKNc0AhT9MqwkoW1xP4i7uEi4B67sxKyjzJl8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11edhmg", "is_robot_indexable": true, "report_reasons": null, "author": "ks4701", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11edhmg/data_conference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11edhmg/data_conference/", "subreddit_subscribers": 91410, "created_utc": 1677603024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I would very appreciate your help here, an important question about architecture modification.  \nWe use the ETL process to fetch data from external services (for example, Github).\n\n1. Extract the data (e.g. issues and PRs) and create raw\\_data objects\n2. Transform the data to our unique objects - let's call them assets.\n3. Load the assets as JSONs (jsonb field) to postgres DB (general assets table).\n\nWe have a few problems with this approach and now we are considering to change our pipeline.\n\n* **Bidirectional asset connection:** our assets have a direct relationship between them, for example, every user has groups, and every group has users.Currently we manually fill the data from one of the sides again and again (before loading the data to Postgres). We manage this in-memory (scale has entered the meeting)\n* **Very slow analysis:** we need the ability to present the whole JSON, but we also need to make an analysis between all of the assets.The problem is that if we have a lot of assets (a lot of JSONs inside of the assets table), the analysis is very slow. For example, 'find all issues that are related to the pull request', in this case we need to iterate over all of the internal JSONs and search with regex on a specific field.\n\nWhat would you recommend in this case?Suggestions:\n\n1. Manage it in Postgres, use functions to convert JSONs into tables, and create triggers to fill the Bidirectional relation.\n2. Data warehouse? I lack knowledge on the subject, but in general not sure it will be ideal, first of all because of OLAP vs OLTP, we need to make analysis but also the possibility to present the whole rows. And how would we fill the bidirectional connection?\n3. GraphDB? Sounds ideal for the bidirectional asset, but not sure about scaling problems.\n\nWhat do you think guys? What would you do in this case?", "author_fullname": "t2_64lh2w2sq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline modification - JSONs and graphs driven", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11ebv7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677599291.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I would very appreciate your help here, an important question about architecture modification.&lt;br/&gt;\nWe use the ETL process to fetch data from external services (for example, Github).&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Extract the data (e.g. issues and PRs) and create raw_data objects&lt;/li&gt;\n&lt;li&gt;Transform the data to our unique objects - let&amp;#39;s call them assets.&lt;/li&gt;\n&lt;li&gt;Load the assets as JSONs (jsonb field) to postgres DB (general assets table).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We have a few problems with this approach and now we are considering to change our pipeline.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Bidirectional asset connection:&lt;/strong&gt; our assets have a direct relationship between them, for example, every user has groups, and every group has users.Currently we manually fill the data from one of the sides again and again (before loading the data to Postgres). We manage this in-memory (scale has entered the meeting)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Very slow analysis:&lt;/strong&gt; we need the ability to present the whole JSON, but we also need to make an analysis between all of the assets.The problem is that if we have a lot of assets (a lot of JSONs inside of the assets table), the analysis is very slow. For example, &amp;#39;find all issues that are related to the pull request&amp;#39;, in this case we need to iterate over all of the internal JSONs and search with regex on a specific field.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What would you recommend in this case?Suggestions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Manage it in Postgres, use functions to convert JSONs into tables, and create triggers to fill the Bidirectional relation.&lt;/li&gt;\n&lt;li&gt;Data warehouse? I lack knowledge on the subject, but in general not sure it will be ideal, first of all because of OLAP vs OLTP, we need to make analysis but also the possibility to present the whole rows. And how would we fill the bidirectional connection?&lt;/li&gt;\n&lt;li&gt;GraphDB? Sounds ideal for the bidirectional asset, but not sure about scaling problems.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you think guys? What would you do in this case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11ebv7v", "is_robot_indexable": true, "report_reasons": null, "author": "ewenField", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ebv7v/data_pipeline_modification_jsons_and_graphs_driven/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ebv7v/data_pipeline_modification_jsons_and_graphs_driven/", "subreddit_subscribers": 91410, "created_utc": 1677599291.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I've been working with full files for quite some time now and was wondering what if the each file contain only a subset of total data, how can someone handle a situation where records are duplicated in 2 different files.", "author_fullname": "t2_t1rd5ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling delta files in data lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11e3wgn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677584262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been working with full files for quite some time now and was wondering what if the each file contain only a subset of total data, how can someone handle a situation where records are duplicated in 2 different files.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11e3wgn", "is_robot_indexable": true, "report_reasons": null, "author": "PR0K1NG", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11e3wgn/handling_delta_files_in_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11e3wgn/handling_delta_files_in_data_lake/", "subreddit_subscribers": 91410, "created_utc": 1677584262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm storing metrics table in delta lake.   \nIs there any way to ingest this data through databricks (or any other tool) to sharepoint.  \nAnd then use it to visualize reports.  \n\n\nI don't want to go with power BI or other pricing tool. Is there any free way directly in sharepoint ?", "author_fullname": "t2_anqnfg2m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sharepoint as Reporting tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dx0tg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677559185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m storing metrics table in delta lake.&lt;br/&gt;\nIs there any way to ingest this data through databricks (or any other tool) to sharepoint.&lt;br/&gt;\nAnd then use it to visualize reports.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to go with power BI or other pricing tool. Is there any free way directly in sharepoint ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11dx0tg", "is_robot_indexable": true, "report_reasons": null, "author": "saurrb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dx0tg/sharepoint_as_reporting_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dx0tg/sharepoint_as_reporting_tool/", "subreddit_subscribers": 91410, "created_utc": 1677559185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is chaos engineering applicable to data pipelines? Yeah, nay, meh? \n\n[https://towardsdatascience.com/the-chaos-data-engineering-manifesto-5dc09a182e85?source=friends\\_link&amp;sk=05d2a17c0e1a736853cffd5f4a4d9482](https://towardsdatascience.com/the-chaos-data-engineering-manifesto-5dc09a182e85?source=friends_link&amp;sk=05d2a17c0e1a736853cffd5f4a4d9482)", "author_fullname": "t2_a49okn69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Chaos Data Engineering Manifesto", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11doz9v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677537066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is chaos engineering applicable to data pipelines? Yeah, nay, meh? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://towardsdatascience.com/the-chaos-data-engineering-manifesto-5dc09a182e85?source=friends_link&amp;amp;sk=05d2a17c0e1a736853cffd5f4a4d9482\"&gt;https://towardsdatascience.com/the-chaos-data-engineering-manifesto-5dc09a182e85?source=friends_link&amp;amp;sk=05d2a17c0e1a736853cffd5f4a4d9482&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?auto=webp&amp;v=enabled&amp;s=9766b1150f219f0b8162d974d8e88030977b44fe", "width": 1200, "height": 799}, "resolutions": [{"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2f27f4fc40dee189afc186bb197f342e473a042", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=01ed1779226a29b9442ebd68a2510e20c364a8cd", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d99aa18e186bead7c4f836457c61547b49a8cd3", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e66fdd19c7a5dc72646964ad2e33657f9895157", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09fe81c9ecf434ff4dd4ef6d8ceac9bae59708af", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa8bdf5c1ed093a70ac7d2418f5d8d586a02cf0d", "width": 1080, "height": 719}], "variants": {}, "id": "kOfxTDZ_6hCNb8d7op6kp8aMstwvUTHmmqWb0nuDz8M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11doz9v", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Substance2185", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11doz9v/chaos_data_engineering_manifesto/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11doz9v/chaos_data_engineering_manifesto/", "subreddit_subscribers": 91410, "created_utc": 1677537066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it true that relational databases are not good for reads of a lot of sequential data? I mean e.g. transaction logs consisting only of updates = you need to read all the transactions in the order? Is it too \u201cdifficult\u201d for the database? If technology is needed I mean for example TimescaleDB.\n\nAnd if that is the case - what would be the best way to read such data? Only reading pure files like parquet/avro?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RDBMS sequential reads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dk5is", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677525422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it true that relational databases are not good for reads of a lot of sequential data? I mean e.g. transaction logs consisting only of updates = you need to read all the transactions in the order? Is it too \u201cdifficult\u201d for the database? If technology is needed I mean for example TimescaleDB.&lt;/p&gt;\n\n&lt;p&gt;And if that is the case - what would be the best way to read such data? Only reading pure files like parquet/avro?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11dk5is", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dk5is/rdbms_sequential_reads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dk5is/rdbms_sequential_reads/", "subreddit_subscribers": 91410, "created_utc": 1677525422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) are two different approaches for data integration in data warehousing.\n\nIn ETL, data is extracted from various sources, transformed to fit the target schema, and then loaded into the data warehouse. In contrast, ELT loads the raw data into the data warehouse and then applies transformations as needed.\n\nThe main advantage of ETL is that it can handle complex transformations and can be more efficient when dealing with large datasets. ELT, on the other hand, allows for faster loading of data and greater flexibility in performing transformations.\n\nUltimately, the choice between ETL and ELT will depend on the specific requirements and constraints of each data integration project.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/tm49toucpwka1.png?width=1466&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=509c3a4dc6d555f22a7f7927ff2c2d694ab34267", "author_fullname": "t2_jtca4f0z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL vs ELT: Check out the major differences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "media_metadata": {"tm49toucpwka1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 79, "x": 108, "u": "https://preview.redd.it/tm49toucpwka1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cf11bffa252465348ae77cc9d8c374833cb0c6b"}, {"y": 159, "x": 216, "u": "https://preview.redd.it/tm49toucpwka1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be57ca76fbb1ce14ca71284a043e314bbebcce90"}, {"y": 236, "x": 320, "u": "https://preview.redd.it/tm49toucpwka1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd79d4fcd69c61adcec4d0967f6d76d5ae712075"}, {"y": 473, "x": 640, "u": "https://preview.redd.it/tm49toucpwka1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f92c26b5ced72a5faa9f41bc174c393160780ea8"}, {"y": 709, "x": 960, "u": "https://preview.redd.it/tm49toucpwka1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f76e46536971112ff37adc4ca668a3b58d8a4cf"}, {"y": 798, "x": 1080, "u": "https://preview.redd.it/tm49toucpwka1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49c565df9a45a5d2662b67bb8c6b11e7e87733c4"}], "s": {"y": 1084, "x": 1466, "u": "https://preview.redd.it/tm49toucpwka1.png?width=1466&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=509c3a4dc6d555f22a7f7927ff2c2d694ab34267"}, "id": "tm49toucpwka1"}}, "name": "t3_11e2rqj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VmyF4Cx_rlvpJp8Ola9mFsLJW0SprwZA1cVkc6gHQgk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677580103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) are two different approaches for data integration in data warehousing.&lt;/p&gt;\n\n&lt;p&gt;In ETL, data is extracted from various sources, transformed to fit the target schema, and then loaded into the data warehouse. In contrast, ELT loads the raw data into the data warehouse and then applies transformations as needed.&lt;/p&gt;\n\n&lt;p&gt;The main advantage of ETL is that it can handle complex transformations and can be more efficient when dealing with large datasets. ELT, on the other hand, allows for faster loading of data and greater flexibility in performing transformations.&lt;/p&gt;\n\n&lt;p&gt;Ultimately, the choice between ETL and ELT will depend on the specific requirements and constraints of each data integration project.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tm49toucpwka1.png?width=1466&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=509c3a4dc6d555f22a7f7927ff2c2d694ab34267\"&gt;https://preview.redd.it/tm49toucpwka1.png?width=1466&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=509c3a4dc6d555f22a7f7927ff2c2d694ab34267&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11e2rqj", "is_robot_indexable": true, "report_reasons": null, "author": "hardik-s", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11e2rqj/etl_vs_elt_check_out_the_major_differences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11e2rqj/etl_vs_elt_check_out_the_major_differences/", "subreddit_subscribers": 91410, "created_utc": 1677580103.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}