{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Now that the dramatic title is out there - do you think it\u2019s the case that analytics engineering, albeit still brand new, also risks being dead end?\n\nSure, this person can learn dbt core over dbt cloud and get further into CICD and devops/cloud type tasks (this is me right now), but where do you go from there?\n\nSeems like the logical progression is to data engineer (meaning, a more python heavy role). By contrast, I could see how an analytics engineer would benefit from learning some ML to implement (mainly with the way dbt is going with python support).\n\nWhat\u2019s your opinion? What do you speculate will happen to the role?", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is analytics engineering a dead end career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ddhe5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677508993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now that the dramatic title is out there - do you think it\u2019s the case that analytics engineering, albeit still brand new, also risks being dead end?&lt;/p&gt;\n\n&lt;p&gt;Sure, this person can learn dbt core over dbt cloud and get further into CICD and devops/cloud type tasks (this is me right now), but where do you go from there?&lt;/p&gt;\n\n&lt;p&gt;Seems like the logical progression is to data engineer (meaning, a more python heavy role). By contrast, I could see how an analytics engineer would benefit from learning some ML to implement (mainly with the way dbt is going with python support).&lt;/p&gt;\n\n&lt;p&gt;What\u2019s your opinion? What do you speculate will happen to the role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11ddhe5", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ddhe5/is_analytics_engineering_a_dead_end_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ddhe5/is_analytics_engineering_a_dead_end_career/", "subreddit_subscribers": 91365, "created_utc": 1677508993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to break into data engineering roles. I have experience in dot net and data analysis and a MS in Data Science and worked on dot net, Python, SQL, Tableau, SSIS/SSRS, VBA etc.\n\nHowever, what I'm finding is that there is literally no consistency among what skills companies are asking for DE roles. The data engineer has become a catch-all term for anything from simple data analysis, database dev, BI dev to ML/stats to actual pipelines development to a tools ninja.\n\nThere seems to be a flood of tools in the DE space and each job posting is asking hands-on experience in a different combination of tools.\n\nI'm scratching my head as to how should I spend my time learning what tools and skills?\n\nIt's impossible to have hands-on experience on all/most of these tools, even in a regular ACTUAL DE job. For example, below is the list of frequently asked tools I've curated from job postings -----------------------------------------------------------------\n\n**Programming Languages and Tools:** Python, SQL, C#, YAML, Unix Shell Scripting, CLI, DBT, REST APIs\n\n**Data Formats:** Relational, Unstructured, Semi-structured (XML, JSON, CSV), Parquet, time-series\n\n**Cloud Computing:**  Snowflake, Databricks, Amazon S3, EC2, AWS CloudFormation, Python Boto3 SDK, Amazon DMS (Database Migration Service), AWS Glue, Amazon Redshift, AWS Athena, Amazon QuickSight, SNS, KMS, CDK, Azure Storage, Azure Data Factory, Azure Synapse, Azure SQL DB, Azure DevOps, Google BigQuery (GCP), Google Cloud Dataflow (GCP), Terraform\n\n**Tools:** Apache/Confluent Kafka, PySpark, Apache Airflow, (DevOps and CI/CD) Docker, Kubernetes, Jenkins, Github Actions, SQL Server, Oracle, MySQL, PostgreSQL, MongoDb, Azure CosmosDB, AWS Dynamo, Tableau, SSIS\n\n**Big Data:** Hadoop, Hive, Pig, HBase, Cassandra Amazon EMR, Spark, PySpark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink\n\n\\----------------------------------------------------------------\n\nAlso, **recruiters won't bother to contact you unless you tell them that you have X years of experience in Y technology**. So I have had to watch some tutorials about the tools and make up stories about having worked on them. This does not fill me confidence.\n\nSo how do I go about navigating through this mess? I'm literally overwhelmed right now. Anyone facing similar issue? Any suggestions are appreciated. Thanks.", "author_fullname": "t2_jyng23u3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineer job hunt is a mess!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dgjv2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677517329.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677516776.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to break into data engineering roles. I have experience in dot net and data analysis and a MS in Data Science and worked on dot net, Python, SQL, Tableau, SSIS/SSRS, VBA etc.&lt;/p&gt;\n\n&lt;p&gt;However, what I&amp;#39;m finding is that there is literally no consistency among what skills companies are asking for DE roles. The data engineer has become a catch-all term for anything from simple data analysis, database dev, BI dev to ML/stats to actual pipelines development to a tools ninja.&lt;/p&gt;\n\n&lt;p&gt;There seems to be a flood of tools in the DE space and each job posting is asking hands-on experience in a different combination of tools.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m scratching my head as to how should I spend my time learning what tools and skills?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s impossible to have hands-on experience on all/most of these tools, even in a regular ACTUAL DE job. For example, below is the list of frequently asked tools I&amp;#39;ve curated from job postings -----------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Programming Languages and Tools:&lt;/strong&gt; Python, SQL, C#, YAML, Unix Shell Scripting, CLI, DBT, REST APIs&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Formats:&lt;/strong&gt; Relational, Unstructured, Semi-structured (XML, JSON, CSV), Parquet, time-series&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cloud Computing:&lt;/strong&gt;  Snowflake, Databricks, Amazon S3, EC2, AWS CloudFormation, Python Boto3 SDK, Amazon DMS (Database Migration Service), AWS Glue, Amazon Redshift, AWS Athena, Amazon QuickSight, SNS, KMS, CDK, Azure Storage, Azure Data Factory, Azure Synapse, Azure SQL DB, Azure DevOps, Google BigQuery (GCP), Google Cloud Dataflow (GCP), Terraform&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Tools:&lt;/strong&gt; Apache/Confluent Kafka, PySpark, Apache Airflow, (DevOps and CI/CD) Docker, Kubernetes, Jenkins, Github Actions, SQL Server, Oracle, MySQL, PostgreSQL, MongoDb, Azure CosmosDB, AWS Dynamo, Tableau, SSIS&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Big Data:&lt;/strong&gt; Hadoop, Hive, Pig, HBase, Cassandra Amazon EMR, Spark, PySpark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink&lt;/p&gt;\n\n&lt;p&gt;----------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;Also, &lt;strong&gt;recruiters won&amp;#39;t bother to contact you unless you tell them that you have X years of experience in Y technology&lt;/strong&gt;. So I have had to watch some tutorials about the tools and make up stories about having worked on them. This does not fill me confidence.&lt;/p&gt;\n\n&lt;p&gt;So how do I go about navigating through this mess? I&amp;#39;m literally overwhelmed right now. Anyone facing similar issue? Any suggestions are appreciated. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11dgjv2", "is_robot_indexable": true, "report_reasons": null, "author": "Fabulous_Weekend330", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dgjv2/data_engineer_job_hunt_is_a_mess/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dgjv2/data_engineer_job_hunt_is_a_mess/", "subreddit_subscribers": 91365, "created_utc": 1677516776.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve got a monolith AWS lambda function that needs to reach out to and query Athena and also do some writes to various s3 buckets after processing what it got from Athena. \n\nTrying to set up pytest and moto but getting stuck because I\u2019m also using awswrangler. \n\nNot even sure where to start finding info about building unit tests here and want to avoid deploying to AWS just to test minor code changes. \n\nDoes anyone have any good resources in where to start?", "author_fullname": "t2_d5gr1nxb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for resources for building unit testing for boto3 code and mocking AWS services in pytest", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11drsam", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677544195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve got a monolith AWS lambda function that needs to reach out to and query Athena and also do some writes to various s3 buckets after processing what it got from Athena. &lt;/p&gt;\n\n&lt;p&gt;Trying to set up pytest and moto but getting stuck because I\u2019m also using awswrangler. &lt;/p&gt;\n\n&lt;p&gt;Not even sure where to start finding info about building unit tests here and want to avoid deploying to AWS just to test minor code changes. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any good resources in where to start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11drsam", "is_robot_indexable": true, "report_reasons": null, "author": "renok_archnmy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11drsam/looking_for_resources_for_building_unit_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11drsam/looking_for_resources_for_building_unit_testing/", "subreddit_subscribers": 91365, "created_utc": 1677544195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any benefit on using this technology nowadays? Is it still worth investing into? If not, which compelling arguments could i use to convince my managers to finally adopt cloud technologies?\nClient's management is considering to migrate hadoop jobs to kudu reason being it's a mess to manage updates in hdfs.\nI strongly feel like it could be a big mistake to invest in such niche and unpolular technologies. Considering possible time and effort required for the migration could be big, in my client's interest i would like to suggest a more future-proof and why not, more pleasant to work with technologies to accomplish our requirements (basically NRT reads from a kafka topic via apache spark and NRT updates of master data tables in a data store, currently done in full batch on hive tables over hdfs).\n\nThanks in advance", "author_fullname": "t2_3xvidrz9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone use Apache Kudu?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11do3cw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677534898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any benefit on using this technology nowadays? Is it still worth investing into? If not, which compelling arguments could i use to convince my managers to finally adopt cloud technologies?\nClient&amp;#39;s management is considering to migrate hadoop jobs to kudu reason being it&amp;#39;s a mess to manage updates in hdfs.\nI strongly feel like it could be a big mistake to invest in such niche and unpolular technologies. Considering possible time and effort required for the migration could be big, in my client&amp;#39;s interest i would like to suggest a more future-proof and why not, more pleasant to work with technologies to accomplish our requirements (basically NRT reads from a kafka topic via apache spark and NRT updates of master data tables in a data store, currently done in full batch on hive tables over hdfs).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11do3cw", "is_robot_indexable": true, "report_reasons": null, "author": "RAT-000", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11do3cw/does_anyone_use_apache_kudu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11do3cw/does_anyone_use_apache_kudu/", "subreddit_subscribers": 91365, "created_utc": 1677534898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you structure your pandas ETL jobs? Do you have some predefined template/structure how do you apply steps to your dataframes? Do you apply the transformations on the dataframes right away or you create a separate functions for each transforming step? Do you use some micro-frameworks like for example [Hamilton](https://github.com/dagworks-inc/hamilton)?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pandas workflow template", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dlu99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677529508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you structure your pandas ETL jobs? Do you have some predefined template/structure how do you apply steps to your dataframes? Do you apply the transformations on the dataframes right away or you create a separate functions for each transforming step? Do you use some micro-frameworks like for example &lt;a href=\"https://github.com/dagworks-inc/hamilton\"&gt;Hamilton&lt;/a&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?auto=webp&amp;v=enabled&amp;s=7e647c2fe032b2efe1c8e86fadcd33540ed3318a", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9023c5cd1aa30963e66aa3d8c30ca94545f9d93", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d747f449845b4276445506cd5f33dcceda89bb6d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=861ecaa2e1b8b03513b7ed62e1839b6fe9743295", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fcca2b33479e11f211dda359a9b7538ae447662", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75561d3b421a585f0947c8a260caaf7142f15f08", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/7lCtm758KvYl3NoNGokuMhmoEhM7aWXWDgVd5kqhPpE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d0a1b54ba8e8e0434783d28301591bb3e7f2aadd", "width": 1080, "height": 540}], "variants": {}, "id": "3YR5ZTzaTkBoH-5mexRRNuUftSAaFay-5D83JdeOsTM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11dlu99", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dlu99/pandas_workflow_template/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dlu99/pandas_workflow_template/", "subreddit_subscribers": 91365, "created_utc": 1677529508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1jx9xua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding BigQuery Table Usage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11daoo1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/slx9MVlAJX0KMEv8OuO3EKbQMXlHiQEj0l1tmYcXDJc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677500917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech.loveholidays.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tech.loveholidays.com/the-life-changing-magic-of-t%CC%B6i%CC%B6d%CC%B6y%CC%B6i%CC%B6n%CC%B6g%CC%B6-%CC%B6u%CC%B6p%CC%B6-understanding-bigquery-table-usage-a-76aae6006d01", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Yg-BA2isCavyycyu2UockhxlBp4nNIMrYaSXA2iIfVQ.jpg?auto=webp&amp;v=enabled&amp;s=60fc29fcba9d4ac91feced4d08e3112fce4c3831", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/Yg-BA2isCavyycyu2UockhxlBp4nNIMrYaSXA2iIfVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88cd2088efb1e45d078bb990ea11bb1af031f92c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Yg-BA2isCavyycyu2UockhxlBp4nNIMrYaSXA2iIfVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=28d4bd5340a33983a4302f8f38ead383ecbf4cf3", "width": 216, "height": 216}], "variants": {}, "id": "1dNvQidZQZlP4iW4BacCKwpmxpYb0i1gpKQDKOMrvbU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11daoo1", "is_robot_indexable": true, "report_reasons": null, "author": "dropber", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11daoo1/understanding_bigquery_table_usage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tech.loveholidays.com/the-life-changing-magic-of-t%CC%B6i%CC%B6d%CC%B6y%CC%B6i%CC%B6n%CC%B6g%CC%B6-%CC%B6u%CC%B6p%CC%B6-understanding-bigquery-table-usage-a-76aae6006d01", "subreddit_subscribers": 91365, "created_utc": 1677500917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are either going to build or buy a Lakehouse. One thing that is unclear to me is how we can achieve a solid permissioning story on a single multi-tenant cluster.\n\nOur usage patterns\n\n* We expect 80% of use cases to be querying via SQL (tbd but something like Trino, spark thrift, etc).\n* The other 20% will be submitting spark jobs.\n\nOur requirements\n\n1. As few clusters as possible. I don't want to have to spin up one cluster per permission boundary. Ideally we have one cluster that autoscales with load. I'm open to push-back on this one. If managing a bunch of clusters isn't that bad using Databricks I'm open to hearing that argument.\n2. Users should only be able to access data they have permission to. It's fine if we *define* this outside of the lakehouse as long as there's some mechanism to *enforce* it within the cluster/jobs.\n3. We can build or buy or adopt, but if we build or adopt it'll be in AWS\n\nThings we've looked at\n\n* Databricks. Per [this doc](https://docs.databricks.com/workflows/jobs/jobs.html), jobs submitted via spark-submit do not support cluster autoscaling?! And then if you use the unity catalog, the cluster needs to be Single User access mode. Both seem like big downsides. Seems like to make this work we'd need to have separate clusters for the spark jobs, one cluster per permission boundary, and size them manually to handle the workloads. \n* Maybe we can [Kerberize an EMR cluster and use Apache Ranger](https://aws.amazon.com/blogs/big-data/introducing-amazon-emr-integration-with-apache-ranger/)? \n\n**Does anyone have a multi-tenant cluster that allows spark and SQL-like workloads and properly enforces permissions? What is your setup?**", "author_fullname": "t2_tic2ae1k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-tenant permissions on a Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11djd7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677523547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are either going to build or buy a Lakehouse. One thing that is unclear to me is how we can achieve a solid permissioning story on a single multi-tenant cluster.&lt;/p&gt;\n\n&lt;p&gt;Our usage patterns&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We expect 80% of use cases to be querying via SQL (tbd but something like Trino, spark thrift, etc).&lt;/li&gt;\n&lt;li&gt;The other 20% will be submitting spark jobs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Our requirements&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;As few clusters as possible. I don&amp;#39;t want to have to spin up one cluster per permission boundary. Ideally we have one cluster that autoscales with load. I&amp;#39;m open to push-back on this one. If managing a bunch of clusters isn&amp;#39;t that bad using Databricks I&amp;#39;m open to hearing that argument.&lt;/li&gt;\n&lt;li&gt;Users should only be able to access data they have permission to. It&amp;#39;s fine if we &lt;em&gt;define&lt;/em&gt; this outside of the lakehouse as long as there&amp;#39;s some mechanism to &lt;em&gt;enforce&lt;/em&gt; it within the cluster/jobs.&lt;/li&gt;\n&lt;li&gt;We can build or buy or adopt, but if we build or adopt it&amp;#39;ll be in AWS&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Things we&amp;#39;ve looked at&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Databricks. Per &lt;a href=\"https://docs.databricks.com/workflows/jobs/jobs.html\"&gt;this doc&lt;/a&gt;, jobs submitted via spark-submit do not support cluster autoscaling?! And then if you use the unity catalog, the cluster needs to be Single User access mode. Both seem like big downsides. Seems like to make this work we&amp;#39;d need to have separate clusters for the spark jobs, one cluster per permission boundary, and size them manually to handle the workloads. &lt;/li&gt;\n&lt;li&gt;Maybe we can &lt;a href=\"https://aws.amazon.com/blogs/big-data/introducing-amazon-emr-integration-with-apache-ranger/\"&gt;Kerberize an EMR cluster and use Apache Ranger&lt;/a&gt;? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Does anyone have a multi-tenant cluster that allows spark and SQL-like workloads and properly enforces permissions? What is your setup?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11djd7c", "is_robot_indexable": true, "report_reasons": null, "author": "databolica", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11djd7c/multitenant_permissions_on_a_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11djd7c/multitenant_permissions_on_a_lakehouse/", "subreddit_subscribers": 91365, "created_utc": 1677523547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR: How do I setup a basic ETL pipeline with Python and SQL Server in Azure?\n\nI am a recent undergraduate student in Data Science who works in a company with no other persons with a CS or DS background. I have been building basic reports and applications in Python and R with data pulled on the fly from API's.  \nThe company now wants to develop a database storing the data instead of pulling from API's everytime we want a report, visualization etc.\n\nWe have various data sources, but I would like to start simple and develop an ETL pipeline from API pull to SQL database. I have build API wrappers in Python and are doing transformation in Python as well, however I am struggling to ingest the data into a database.\n\nHow do I best create a simple ETL pipeline with data extracted and transformed in Python from an API to SQL Server database in Azure?", "author_fullname": "t2_226yoed5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple ETL pipeline as undergraduate as only CS/DS person in company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11d7kdm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677489752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: How do I setup a basic ETL pipeline with Python and SQL Server in Azure?&lt;/p&gt;\n\n&lt;p&gt;I am a recent undergraduate student in Data Science who works in a company with no other persons with a CS or DS background. I have been building basic reports and applications in Python and R with data pulled on the fly from API&amp;#39;s.&lt;br/&gt;\nThe company now wants to develop a database storing the data instead of pulling from API&amp;#39;s everytime we want a report, visualization etc.&lt;/p&gt;\n\n&lt;p&gt;We have various data sources, but I would like to start simple and develop an ETL pipeline from API pull to SQL database. I have build API wrappers in Python and are doing transformation in Python as well, however I am struggling to ingest the data into a database.&lt;/p&gt;\n\n&lt;p&gt;How do I best create a simple ETL pipeline with data extracted and transformed in Python from an API to SQL Server database in Azure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11d7kdm", "is_robot_indexable": true, "report_reasons": null, "author": "C_Ronsholt", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11d7kdm/simple_etl_pipeline_as_undergraduate_as_only_csds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11d7kdm/simple_etl_pipeline_as_undergraduate_as_only_csds/", "subreddit_subscribers": 91365, "created_utc": 1677489752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have many jobs I have to run on a recurring basis (once a week/day/hour etc). These are all\n\n1. Containerized (Docker)\n2. Take a few minutes - few days to complete (and need to have timeouts to avoid infinite runs - differs by job type)\n3. Need a specified amount of CPUs and RAM to complete (differs by job type)\n\nCurrently, I have enough server capacity to handle the max load and run the Docker containers with cron jobs. But the load varies a lot, sometimes all running containers need just 8GB RAM altogether, sometimes 100GB - so this is not very cost-efficient. It also needs some monitoring and maintenance that I think could be simplified.\n\nWhat is the best way of going about this? Can I set this up using Kubernetes (I haven't used it before), or is there a more focused tool for a similar problem?", "author_fullname": "t2_b9odplx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best scalable way to run Docker containers for recurring data jobs with a start, finish and timeout?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11d69u9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677484822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many jobs I have to run on a recurring basis (once a week/day/hour etc). These are all&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Containerized (Docker)&lt;/li&gt;\n&lt;li&gt;Take a few minutes - few days to complete (and need to have timeouts to avoid infinite runs - differs by job type)&lt;/li&gt;\n&lt;li&gt;Need a specified amount of CPUs and RAM to complete (differs by job type)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Currently, I have enough server capacity to handle the max load and run the Docker containers with cron jobs. But the load varies a lot, sometimes all running containers need just 8GB RAM altogether, sometimes 100GB - so this is not very cost-efficient. It also needs some monitoring and maintenance that I think could be simplified.&lt;/p&gt;\n\n&lt;p&gt;What is the best way of going about this? Can I set this up using Kubernetes (I haven&amp;#39;t used it before), or is there a more focused tool for a similar problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11d69u9", "is_robot_indexable": true, "report_reasons": null, "author": "vasarmilan", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11d69u9/best_scalable_way_to_run_docker_containers_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11d69u9/best_scalable_way_to_run_docker_containers_for/", "subreddit_subscribers": 91365, "created_utc": 1677484822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm storing metrics table in delta lake.   \nIs there any way to ingest this data through databricks (or any other tool) to sharepoint.  \nAnd then use it to visualize reports.  \n\n\nI don't want to go with power BI or other pricing tool. Is there any free way directly in sharepoint ?", "author_fullname": "t2_anqnfg2m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sharepoint as Reporting tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dx0tg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677559185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m storing metrics table in delta lake.&lt;br/&gt;\nIs there any way to ingest this data through databricks (or any other tool) to sharepoint.&lt;br/&gt;\nAnd then use it to visualize reports.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to go with power BI or other pricing tool. Is there any free way directly in sharepoint ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11dx0tg", "is_robot_indexable": true, "report_reasons": null, "author": "saurrb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dx0tg/sharepoint_as_reporting_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dx0tg/sharepoint_as_reporting_tool/", "subreddit_subscribers": 91365, "created_utc": 1677559185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is chaos engineering applicable to data pipelines? Yeah, nay, meh? \n\n[https://towardsdatascience.com/the-chaos-data-engineering-manifesto-5dc09a182e85?source=friends\\_link&amp;sk=05d2a17c0e1a736853cffd5f4a4d9482](https://towardsdatascience.com/the-chaos-data-engineering-manifesto-5dc09a182e85?source=friends_link&amp;sk=05d2a17c0e1a736853cffd5f4a4d9482)", "author_fullname": "t2_a49okn69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Chaos Data Engineering Manifesto", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11doz9v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677537066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is chaos engineering applicable to data pipelines? Yeah, nay, meh? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://towardsdatascience.com/the-chaos-data-engineering-manifesto-5dc09a182e85?source=friends_link&amp;amp;sk=05d2a17c0e1a736853cffd5f4a4d9482\"&gt;https://towardsdatascience.com/the-chaos-data-engineering-manifesto-5dc09a182e85?source=friends_link&amp;amp;sk=05d2a17c0e1a736853cffd5f4a4d9482&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?auto=webp&amp;v=enabled&amp;s=9766b1150f219f0b8162d974d8e88030977b44fe", "width": 1200, "height": 799}, "resolutions": [{"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2f27f4fc40dee189afc186bb197f342e473a042", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=01ed1779226a29b9442ebd68a2510e20c364a8cd", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d99aa18e186bead7c4f836457c61547b49a8cd3", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e66fdd19c7a5dc72646964ad2e33657f9895157", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09fe81c9ecf434ff4dd4ef6d8ceac9bae59708af", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/H_xuLJefUXCliZPNQK1-HuyCK9o5cNmhOVhICJGHBSM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa8bdf5c1ed093a70ac7d2418f5d8d586a02cf0d", "width": 1080, "height": 719}], "variants": {}, "id": "kOfxTDZ_6hCNb8d7op6kp8aMstwvUTHmmqWb0nuDz8M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11doz9v", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Substance2185", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11doz9v/chaos_data_engineering_manifesto/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11doz9v/chaos_data_engineering_manifesto/", "subreddit_subscribers": 91365, "created_utc": 1677537066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am an ETL developer with 1.5 years of experience. I primarily work with informatica power centre, Alteryx, SQL. I am familiar with python as interned as a backend developer where I used flask extensively. I am planning to learn databricks as my company is expecting projects based on databricks. I also worked with pandas and numpy is it enough to master databricks?\n\nI have started to learn databricks from the databricks academy. \nI found the course to be confusing. Please guide me on how to approach learning databricks.\n\nWhat are the prerequisites?\nI have not worked with spark. I know that the databricks platform uses spark. Is it necessary to learn it before learning databricks. \n\nShould I learn Java or Scala as i heard it is faster than python \n\nIs there any udemy course or YouTube channel recommendations for databricks or spark.\n\nThanks in Advance!", "author_fullname": "t2_bff9x1i8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ddtsj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677509896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an ETL developer with 1.5 years of experience. I primarily work with informatica power centre, Alteryx, SQL. I am familiar with python as interned as a backend developer where I used flask extensively. I am planning to learn databricks as my company is expecting projects based on databricks. I also worked with pandas and numpy is it enough to master databricks?&lt;/p&gt;\n\n&lt;p&gt;I have started to learn databricks from the databricks academy. \nI found the course to be confusing. Please guide me on how to approach learning databricks.&lt;/p&gt;\n\n&lt;p&gt;What are the prerequisites?\nI have not worked with spark. I know that the databricks platform uses spark. Is it necessary to learn it before learning databricks. &lt;/p&gt;\n\n&lt;p&gt;Should I learn Java or Scala as i heard it is faster than python &lt;/p&gt;\n\n&lt;p&gt;Is there any udemy course or YouTube channel recommendations for databricks or spark.&lt;/p&gt;\n\n&lt;p&gt;Thanks in Advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11ddtsj", "is_robot_indexable": true, "report_reasons": null, "author": "ninjanoob_16", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ddtsj/learning_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ddtsj/learning_databricks/", "subreddit_subscribers": 91365, "created_utc": 1677509896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\ntoday I was watching [this video](https://www.youtube.com/watch?v=4Spo2QRTz1k&amp;t=989s) from Maxime Beauchemin in which he suggests an alternative strategy to handle SCD, different from type 2. The gist of it is to snapshot all the dim table every day and partition by date.\n\nIn the video he mentions the several complexities about handling SCD2, in specific one concept I was not aware about is \"surrogate key lookup\".\n\nI researched a bit and landed on this [example here](https://aws.amazon.com/blogs/big-data/build-slowly-changing-dimensions-type-2-scd2-with-apache-spark-and-apache-hudi-on-amazon-emr/) (scroll until **Customer dimension key lookup**) from AWS, which shows that the synthetic pkey from the dimension table is added to the fact table.\n\nWhy is this? Can't one just use the dimension_id and the valid date range to join a fact and dim table, if historical values are needed?\n\nE.g.\n\n```sql\nSELECT *\nFROM fact a\nJOIN dim b ON\n    a.dim_id = b.dim_id AND\n    a.date BETWEEN b.valid_from and b.valid_to\n```\n\nLet me know what you think!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SCD2 surrogate key lookup, is it necessary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dd8q8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677508342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;today I was watching &lt;a href=\"https://www.youtube.com/watch?v=4Spo2QRTz1k&amp;amp;t=989s\"&gt;this video&lt;/a&gt; from Maxime Beauchemin in which he suggests an alternative strategy to handle SCD, different from type 2. The gist of it is to snapshot all the dim table every day and partition by date.&lt;/p&gt;\n\n&lt;p&gt;In the video he mentions the several complexities about handling SCD2, in specific one concept I was not aware about is &amp;quot;surrogate key lookup&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I researched a bit and landed on this &lt;a href=\"https://aws.amazon.com/blogs/big-data/build-slowly-changing-dimensions-type-2-scd2-with-apache-spark-and-apache-hudi-on-amazon-emr/\"&gt;example here&lt;/a&gt; (scroll until &lt;strong&gt;Customer dimension key lookup&lt;/strong&gt;) from AWS, which shows that the synthetic pkey from the dimension table is added to the fact table.&lt;/p&gt;\n\n&lt;p&gt;Why is this? Can&amp;#39;t one just use the dimension_id and the valid date range to join a fact and dim table, if historical values are needed?&lt;/p&gt;\n\n&lt;p&gt;E.g.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql\nSELECT *\nFROM fact a\nJOIN dim b ON\n    a.dim_id = b.dim_id AND\n    a.date BETWEEN b.valid_from and b.valid_to\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7cv2ZTQ6dDJGaFbCmICEfBF5eUzEbeiLlcOxz8tudSI.jpg?auto=webp&amp;v=enabled&amp;s=f0a05e4168f1db6cf27b0178ae180af6bfa36e9d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/7cv2ZTQ6dDJGaFbCmICEfBF5eUzEbeiLlcOxz8tudSI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb48c268996a5f8b8498abd00637090660b35a24", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/7cv2ZTQ6dDJGaFbCmICEfBF5eUzEbeiLlcOxz8tudSI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02db52765b249d63f0be1a63ee941c5fffb146e0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/7cv2ZTQ6dDJGaFbCmICEfBF5eUzEbeiLlcOxz8tudSI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b46d7e2a85ebac61a2aad3d4a85ad0715fb8218", "width": 320, "height": 240}], "variants": {}, "id": "W4OjmCzoyBuwkuy_25X6VLeKRelJkH5FdeX79YCbFAY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11dd8q8", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dd8q8/scd2_surrogate_key_lookup_is_it_necessary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dd8q8/scd2_surrogate_key_lookup_is_it_necessary/", "subreddit_subscribers": 91365, "created_utc": 1677508342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a central data team serving different between BU with their analytics team or is there a central analytics team as well?\n\nAnd whom do you report to? CDP, CIO, CFO or COO?", "author_fullname": "t2_xap78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your org structure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dcxc9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677507491.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a central data team serving different between BU with their analytics team or is there a central analytics team as well?&lt;/p&gt;\n\n&lt;p&gt;And whom do you report to? CDP, CIO, CFO or COO?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11dcxc9", "is_robot_indexable": true, "report_reasons": null, "author": "totalsports1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11dcxc9/whats_your_org_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dcxc9/whats_your_org_structure/", "subreddit_subscribers": 91365, "created_utc": 1677507491.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Same as title", "author_fullname": "t2_57e44nxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "which MLOPs framework is best in terms of reusability of code? What all mlops framework do you guys use and why ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dbxf9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677504738.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Same as title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11dbxf9", "is_robot_indexable": true, "report_reasons": null, "author": "RstarPhoneix", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dbxf9/which_mlops_framework_is_best_in_terms_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dbxf9/which_mlops_framework_is_best_in_terms_of/", "subreddit_subscribers": 91365, "created_utc": 1677504738.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was passively looking for a job for a bit, but now want to turn it up and actively get some applications out there. I'm targeting senior data/BI/analytics engineer, and manager-level roles as that's exactly where I'm at right now, and happy with a lateral as long as it comes with a pay bump.\n\nI've seen a few staffing agencies come up on LinkedIn, but I'm hesitant on reaching out to them. So I wanted to ask the group here if any of you all have used any, and if there are any you would recommend or not.", "author_fullname": "t2_2pyy4c8f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used data-focused staffing or recruiting agencies? Are there any you would recommend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dbkjv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677503668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was passively looking for a job for a bit, but now want to turn it up and actively get some applications out there. I&amp;#39;m targeting senior data/BI/analytics engineer, and manager-level roles as that&amp;#39;s exactly where I&amp;#39;m at right now, and happy with a lateral as long as it comes with a pay bump.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen a few staffing agencies come up on LinkedIn, but I&amp;#39;m hesitant on reaching out to them. So I wanted to ask the group here if any of you all have used any, and if there are any you would recommend or not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11dbkjv", "is_robot_indexable": true, "report_reasons": null, "author": "opabm", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dbkjv/has_anyone_used_datafocused_staffing_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dbkjv/has_anyone_used_datafocused_staffing_or/", "subreddit_subscribers": 91365, "created_utc": 1677503668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,  \nI am looking into building data pipelines in a way that is as pythonic as possible, I found out about this Streamz project and it integrates well with Dask. Does anyone have any experience using this? My main use would be for fairly simple accumulator pipelines.  \nhttps://github.com/python-streamz/streamz", "author_fullname": "t2_ytvdx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experiences using Streamz?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11d9umo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677498114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nI am looking into building data pipelines in a way that is as pythonic as possible, I found out about this Streamz project and it integrates well with Dask. Does anyone have any experience using this? My main use would be for fairly simple accumulator pipelines.&lt;br/&gt;\n&lt;a href=\"https://github.com/python-streamz/streamz\"&gt;https://github.com/python-streamz/streamz&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?auto=webp&amp;v=enabled&amp;s=c7f2507a5945e4140ec5b7c378d8263a00d9c1f9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e60719913a37addce2db8b588b1fb3c3e46f324a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99948364664600ec4a213029cfb081cbd36796b2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0cd49140a6295a19415a408f089ad9049983d1db", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=515434dd12129d0e76dd8cc8bdd99c350e69b51d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=150fa150e8e696cee11d48e0fdd301dee390fc10", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/3MiMH-Pr2aaGsXv59XDlIfDzEVMoRsU_rlgzZVJhNqk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d5f5886a4ddf06742906f6a045b5649feb3ef9c", "width": 1080, "height": 540}], "variants": {}, "id": "tumjq3w6bN8Iody0VGURd8uVr6WPWLCP71LCovZCr2k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11d9umo", "is_robot_indexable": true, "report_reasons": null, "author": "carnivorousdrew", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11d9umo/experiences_using_streamz/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11d9umo/experiences_using_streamz/", "subreddit_subscribers": 91365, "created_utc": 1677498114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got masters degree in ME as a HVAC designer, but after 5 years, i am sick of it and i want to transition to IT and after some research, DE caught my eye.\nI started learning SQL and Python for now.\nWhat should i learn next? Is there any realistic chance for me to get Junior DE role in 1 year from now? If its hard to get DE role straight away, what roles should i look for first, so its easier to transition to DE.\nIf anyone has been in similar situation, any advice is welcome!", "author_fullname": "t2_12jygu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Want to transition to DE from ME background", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11dy9mq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677563233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got masters degree in ME as a HVAC designer, but after 5 years, i am sick of it and i want to transition to IT and after some research, DE caught my eye.\nI started learning SQL and Python for now.\nWhat should i learn next? Is there any realistic chance for me to get Junior DE role in 1 year from now? If its hard to get DE role straight away, what roles should i look for first, so its easier to transition to DE.\nIf anyone has been in similar situation, any advice is welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11dy9mq", "is_robot_indexable": true, "report_reasons": null, "author": "1las", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dy9mq/want_to_transition_to_de_from_me_background/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dy9mq/want_to_transition_to_de_from_me_background/", "subreddit_subscribers": 91365, "created_utc": 1677563233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone i applied recently to a data engineering job but got rejected after passing technical interview and getting \u2248 80% on codility test,\n\nMy current job is python developer/data engineer basically where i am they had a project for about a year and a half and its done. \n\nI feel down and last time i practiced what i did as a data engineer was about 8 months ago, idk how to find a data engineering job anymore and more importantly how to practice what i know or need to know to actually land a job. \n\nI hope you help me find my way out of this", "author_fullname": "t2_i62lmiqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help getting on track", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dlisz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677528739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone i applied recently to a data engineering job but got rejected after passing technical interview and getting \u2248 80% on codility test,&lt;/p&gt;\n\n&lt;p&gt;My current job is python developer/data engineer basically where i am they had a project for about a year and a half and its done. &lt;/p&gt;\n\n&lt;p&gt;I feel down and last time i practiced what i did as a data engineer was about 8 months ago, idk how to find a data engineering job anymore and more importantly how to practice what i know or need to know to actually land a job. &lt;/p&gt;\n\n&lt;p&gt;I hope you help me find my way out of this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11dlisz", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Resolution-1025", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dlisz/help_getting_on_track/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dlisz/help_getting_on_track/", "subreddit_subscribers": 91365, "created_utc": 1677528739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it true that relational databases are not good for reads of a lot of sequential data? I mean e.g. transaction logs consisting only of updates = you need to read all the transactions in the order? Is it too \u201cdifficult\u201d for the database? If technology is needed I mean for example TimescaleDB.\n\nAnd if that is the case - what would be the best way to read such data? Only reading pure files like parquet/avro?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RDBMS sequential reads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11dk5is", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677525422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it true that relational databases are not good for reads of a lot of sequential data? I mean e.g. transaction logs consisting only of updates = you need to read all the transactions in the order? Is it too \u201cdifficult\u201d for the database? If technology is needed I mean for example TimescaleDB.&lt;/p&gt;\n\n&lt;p&gt;And if that is the case - what would be the best way to read such data? Only reading pure files like parquet/avro?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11dk5is", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11dk5is/rdbms_sequential_reads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11dk5is/rdbms_sequential_reads/", "subreddit_subscribers": 91365, "created_utc": 1677525422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking into lake house solutions to use with AWS S3, really trying to stay as open source as possible (mostly for cost and avoiding vendor lock).  I really like a lot about Delta Lake, Apache Hudi, Apache Iceberg, but I can't find a lot of information about table access control i.e. how to control access to individual columns within the data based on credentials of the user querying it. It looks like all the column level (or row/cell level) access control solutions come bundled in paid products like Databricks , Privacera, Immuta, etc.   \n\n\n**Is there a way to get this type of access control on lake house tables in S3 using only open source tools?**   \n\n\nS3 Lake Formation has Governed Tables that I don't think would increase any costs over the other lake house table formats but I worry about being locked into that format long term.", "author_fullname": "t2_54h1iub7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source solutions to lake house access control?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11delba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677511836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking into lake house solutions to use with AWS S3, really trying to stay as open source as possible (mostly for cost and avoiding vendor lock).  I really like a lot about Delta Lake, Apache Hudi, Apache Iceberg, but I can&amp;#39;t find a lot of information about table access control i.e. how to control access to individual columns within the data based on credentials of the user querying it. It looks like all the column level (or row/cell level) access control solutions come bundled in paid products like Databricks , Privacera, Immuta, etc.   &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there a way to get this type of access control on lake house tables in S3 using only open source tools?&lt;/strong&gt;   &lt;/p&gt;\n\n&lt;p&gt;S3 Lake Formation has Governed Tables that I don&amp;#39;t think would increase any costs over the other lake house table formats but I worry about being locked into that format long term.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11delba", "is_robot_indexable": true, "report_reasons": null, "author": "Jtmyer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11delba/open_source_solutions_to_lake_house_access_control/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11delba/open_source_solutions_to_lake_house_access_control/", "subreddit_subscribers": 91365, "created_utc": 1677511836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Using (Azure) Databricks Autoloader we pick up files from an Azure blob storage and process them. I want to implement some monitoring functionality and as part of that I want to know if we are loading empty rows. Knowing that streaming dataframes are a special thing, I feel like I can't just simply call a simple check on it. Do you have any best practices?", "author_fullname": "t2_hp7r8vez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Structured Streaming: how to catch if we are loading empty rows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ddt3g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677509846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Using (Azure) Databricks Autoloader we pick up files from an Azure blob storage and process them. I want to implement some monitoring functionality and as part of that I want to know if we are loading empty rows. Knowing that streaming dataframes are a special thing, I feel like I can&amp;#39;t just simply call a simple check on it. Do you have any best practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11ddt3g", "is_robot_indexable": true, "report_reasons": null, "author": "Labanc_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11ddt3g/spark_structured_streaming_how_to_catch_if_we_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ddt3g/spark_structured_streaming_how_to_catch_if_we_are/", "subreddit_subscribers": 91365, "created_utc": 1677509846.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}