{"kind": "Listing", "data": {"after": "t3_10z22gn", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So after building a near real time pipeline with super complex parsing and data quality in record speed your dashboard gal/guy sends an email to C-level showing her/his awesome dashboard and just cc you in the email.\n\n\n\nNobody would know \n\n\u2014 time spent to find optimal parallelism\n\n\n\u2014 time spent for solving weird dq issues\n\n\n\u2014 time spent to make the pipeline dynamically handle schema change\n\n\n\u2014 time spent for threading it all together in orchestrator\n\n\nAfter all these years still hurts on how thankless this job is.\n\nOnly lesson : Thank your plumber next time you meet him.", "author_fullname": "t2_5ifseipu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thanking my plumber every time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ybytx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 176, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 176, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675990984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So after building a near real time pipeline with super complex parsing and data quality in record speed your dashboard gal/guy sends an email to C-level showing her/his awesome dashboard and just cc you in the email.&lt;/p&gt;\n\n&lt;p&gt;Nobody would know &lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent to find optimal parallelism&lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent for solving weird dq issues&lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent to make the pipeline dynamically handle schema change&lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent for threading it all together in orchestrator&lt;/p&gt;\n\n&lt;p&gt;After all these years still hurts on how thankless this job is.&lt;/p&gt;\n\n&lt;p&gt;Only lesson : Thank your plumber next time you meet him.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ybytx", "is_robot_indexable": true, "report_reasons": null, "author": "Smart-Weird", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ybytx/thanking_my_plumber_every_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ybytx/thanking_my_plumber_every_time/", "subreddit_subscribers": 89119, "created_utc": 1675990984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019m a manager at Big 4 consulting and I\u2019m trying to find resources which I can show my engineers to help them understand best practices &amp; common patterns in developing production ready applications / code bases. \n\nSome things I can think of that I\u2019ve had to teach them:\n\n1. Git\n2. Environment specific parameters\n3. Writing Unittests, mocking cloud services\n4. What is CI/CD", "author_fullname": "t2_ay99iuoj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do we learn SWE best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ye0ji", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675996673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m a manager at Big 4 consulting and I\u2019m trying to find resources which I can show my engineers to help them understand best practices &amp;amp; common patterns in developing production ready applications / code bases. &lt;/p&gt;\n\n&lt;p&gt;Some things I can think of that I\u2019ve had to teach them:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Git&lt;/li&gt;\n&lt;li&gt;Environment specific parameters&lt;/li&gt;\n&lt;li&gt;Writing Unittests, mocking cloud services&lt;/li&gt;\n&lt;li&gt;What is CI/CD&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ye0ji", "is_robot_indexable": true, "report_reasons": null, "author": "Rich_Repair", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ye0ji/how_do_we_learn_swe_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ye0ji/how_do_we_learn_swe_best_practices/", "subreddit_subscribers": 89119, "created_utc": 1675996673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_brkxjomi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Valentine's for your data sweetheart \ud83e\udef6", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"biduu0mtffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/biduu0mtffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79381bd9167be6bc4a6801a37db2b2c399155875"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/biduu0mtffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03da64f559ff0b77ce2202f22cd238438cfa7274"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/biduu0mtffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09ecbb1fd34068c8d8760d287e619947bc1fd49c"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/biduu0mtffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83231bcaf3e67deb8fb3c67189c7891f09e47ab2"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/biduu0mtffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12b432ecd6093ff860444963d6dc65d13409546d"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/biduu0mtffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=afe55d32aed951d2a501dee5d2343482da26cbb6"}, "id": "biduu0mtffha1"}, "ryo2izltffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/ryo2izltffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf6105f3c39c2eb2f3dc6e9211cdb0bd3ed04d41"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/ryo2izltffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7787c7546a5ca67acc869986ae7ca30d26c530c8"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/ryo2izltffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12b45e98685f26d1688e1900aec957a5e318d9a6"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/ryo2izltffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf130fe2d485801b60b1a6f806f7cb4c731072e6"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/ryo2izltffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7dd73d7d82d9269117b82ccc09f9837c6be64459"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/ryo2izltffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=74ae962e6fffe5593810c429220809ada4d34d36"}, "id": "ryo2izltffha1"}, "xj1kc2mtffha1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f519c75e14dc49b1f0ff605527ca50a60757883"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9aaf79cdeec3be1f685f0458862ef50ebe3e6cbe"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e682512352eeb61f1c5994ff667c2ecc13bcc99a"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73b4563feeeb86c6b21cd0efca1b4fadd08df04e"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=999e144157716bc6c62dd03f600dc0ad06e5ffb9"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=adaeca8dcb921f37da10cdcb3431878ca00f6623"}, "id": "xj1kc2mtffha1"}, "17fehyltffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/17fehyltffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac24ecb8bc5aad89c5a20c3960b2ae4e791c88f7"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/17fehyltffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67bc4621be7bda93147c7203e74c142eecd35465"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/17fehyltffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f066215eb3f86dd0ad5ed938ff00caf9880d22c"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/17fehyltffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21f7b6fb93b603495641b15ce5aedac34b847bb3"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/17fehyltffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0475e63182927b6c7bbfea6461cb540189a1e6c9"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/17fehyltffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=80a98dfcb93576438446c70cd56b955d95ae62f4"}, "id": "17fehyltffha1"}, "atd821mtffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/atd821mtffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=193c7cfe15eb4c07ad44ab8ca9bc7bfe59dfa5ab"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/atd821mtffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=513c05e5e4182710264771882d427dbf4f09fcfc"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/atd821mtffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6024b102abb3bc7da079e72117920c698b5e0e07"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/atd821mtffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fef394e19adf67defd1e61f85f9674bf4aad2da"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/atd821mtffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7642046c5955c3ff4fcff245be24db0dd61148b"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/atd821mtffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=11f273bc54db2d84120dc5537dd1ffc261434fbb"}, "id": "atd821mtffha1"}, "n9jjt1mtffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc475144c683e58f59a61985e6c1841d80a732bd"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9aafead2bd7b2106ec3d3488c08c32885bae5ab6"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=010168a10ee1b3d4292f0d5cf025906097f5b998"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0084fd2c5f0ff5de7079dc81efcd72e942cce43f"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=171505d0fca2097db573ffef84aa9f6f608c7b8d"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7b4c9a1c9e4858df2a986bc98bd93d43052da8be"}, "id": "n9jjt1mtffha1"}}, "name": "t3_10z37l5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 26, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "ryo2izltffha1", "id": 239274768}, {"media_id": "17fehyltffha1", "id": 239274769}, {"media_id": "biduu0mtffha1", "id": 239274770}, {"media_id": "atd821mtffha1", "id": 239274771}, {"media_id": "n9jjt1mtffha1", "id": 239274772}, {"media_id": "xj1kc2mtffha1", "id": 239274773}]}, "link_flair_text": "Meme", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/akDVWEtAYUZrcqWztA1PtaB5z4ncRPFVMAhMrdocQL4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676063522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/10z37l5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "10z37l5", "is_robot_indexable": true, "report_reasons": null, "author": "Straight_House8628", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z37l5/valentines_for_your_data_sweetheart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/10z37l5", "subreddit_subscribers": 89119, "created_utc": 1676063522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Clickbaity title but this former Google BigQuery engineer has some really interesting things to say about why most companies do not or should not utilize big data.", "author_fullname": "t2_t3ffq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big Data is Dead", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_10z1ft9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1pO0RBiV6vwhGezwNmiqCZro0ztqzvrl3hQ2Ib5SB8A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676059302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "motherduck.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Clickbaity title but this former Google BigQuery engineer has some really interesting things to say about why most companies do not or should not utilize big data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://motherduck.com/blog/big-data-is-dead/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?auto=webp&amp;v=enabled&amp;s=422507ff3e1d876f1565f9afa286ac2a3b12b70d", "width": 1024, "height": 535}, "resolutions": [{"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eea8d6f5a4b37bc287fcc1bd5d4b8364089b4589", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84293dc6fd5ff654378e1403be79188e69070a05", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8c1be6a0050d06000aac1c8326148e53643be6b", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1af5f5964cf9dc80b6d7c498f369c8505d1a4dc3", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1ab1c4a64b03a27503a012ea8235765e4de0220", "width": 960, "height": 501}], "variants": {}, "id": "C3v2JVQcD5hONfZJ0FsXvcBoNEQPQWKeM2eQkWh2PMw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10z1ft9", "is_robot_indexable": true, "report_reasons": null, "author": "FortunOfficial", "discussion_type": null, "num_comments": 8, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10z1ft9/big_data_is_dead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://motherduck.com/blog/big-data-is-dead/", "subreddit_subscribers": 89119, "created_utc": 1676059302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,  \nWe used to migrate data with a CDC based architecture. Then one day, someone said:\n\n\"What if we discover some contaminated data? We'd need to fix the data in the source, and then replay the migration from that point. Does your architecture handle that?\"  \n\n\n\\*gulp\\*  \n\n\nQuestion: How does CDC fit into your company? Is it an *extra* migration pipeline that feeds real-time solutions? Do you still have an \"SQL-based\" pipeline just in case?", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC and Backfill", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ytfl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676039499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;br/&gt;\nWe used to migrate data with a CDC based architecture. Then one day, someone said:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;What if we discover some contaminated data? We&amp;#39;d need to fix the data in the source, and then replay the migration from that point. Does your architecture handle that?&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;*gulp*  &lt;/p&gt;\n\n&lt;p&gt;Question: How does CDC fit into your company? Is it an &lt;em&gt;extra&lt;/em&gt; migration pipeline that feeds real-time solutions? Do you still have an &amp;quot;SQL-based&amp;quot; pipeline just in case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ytfl3", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ytfl3/cdc_and_backfill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ytfl3/cdc_and_backfill/", "subreddit_subscribers": 89119, "created_utc": 1676039499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it good enough to receive terabytes of data from api using simple python request module in glue and spark to store it in s3 with delta format? The project is to process daily data from company db to s3, and the company will provide an api to access the data.\n\nMy current concern is size limit of data that api can handle, is there any limit for an api? I don't know what kind of api will be given because this is for a project in poc phase. \n\nI am fairly new in data engineering so feel free to give any advice, thank you.", "author_fullname": "t2_ityodnp4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processing terabytes from api in aws glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yuub4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676043015.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it good enough to receive terabytes of data from api using simple python request module in glue and spark to store it in s3 with delta format? The project is to process daily data from company db to s3, and the company will provide an api to access the data.&lt;/p&gt;\n\n&lt;p&gt;My current concern is size limit of data that api can handle, is there any limit for an api? I don&amp;#39;t know what kind of api will be given because this is for a project in poc phase. &lt;/p&gt;\n\n&lt;p&gt;I am fairly new in data engineering so feel free to give any advice, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10yuub4", "is_robot_indexable": true, "report_reasons": null, "author": "LimeDine", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yuub4/processing_terabytes_from_api_in_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yuub4/processing_terabytes_from_api_in_aws_glue/", "subreddit_subscribers": 89119, "created_utc": 1676043015.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm getting quite confused about this. So it's very typical in a transactional system to have header line data, like purchase order and purchase line.\n\nWhen I bring this into a data warehouse and I'm designing the fact tables, how do I model this relationship? If I add columns for the header detail I'll have tonnes of redundant, repeated data. But having 2 linked facts is also not correct.\n\nAny help is appreciated thanks!", "author_fullname": "t2_32y0tqx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to model header line data in a DWH without duplicating data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yxiy6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676049784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m getting quite confused about this. So it&amp;#39;s very typical in a transactional system to have header line data, like purchase order and purchase line.&lt;/p&gt;\n\n&lt;p&gt;When I bring this into a data warehouse and I&amp;#39;m designing the fact tables, how do I model this relationship? If I add columns for the header detail I&amp;#39;ll have tonnes of redundant, repeated data. But having 2 linked facts is also not correct.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10yxiy6", "is_robot_indexable": true, "report_reasons": null, "author": "MrWriter1234", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yxiy6/how_to_model_header_line_data_in_a_dwh_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yxiy6/how_to_model_header_line_data_in_a_dwh_without/", "subreddit_subscribers": 89119, "created_utc": 1676049784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I left an SSIS ETL gig about 8 years ago to join a much bigger company as a Business Intelligence (glorified reporting guy). I have kept working on side gigs using SSIS and DBT\u2026 now, because I have a giant gap of not related Data engineering positions.  I get an automatic rejection from HR and I\u2019m at lost.  I hate doing tableau and making things pretty for idiotic reasons.  I need to get back home and I need some feedback on how to fix my resume.", "author_fullname": "t2_4i6f73l2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get back into data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yssm1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676037807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I left an SSIS ETL gig about 8 years ago to join a much bigger company as a Business Intelligence (glorified reporting guy). I have kept working on side gigs using SSIS and DBT\u2026 now, because I have a giant gap of not related Data engineering positions.  I get an automatic rejection from HR and I\u2019m at lost.  I hate doing tableau and making things pretty for idiotic reasons.  I need to get back home and I need some feedback on how to fix my resume.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10yssm1", "is_robot_indexable": true, "report_reasons": null, "author": "SadDogOwner27", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yssm1/how_to_get_back_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yssm1/how_to_get_back_into_data_engineering/", "subreddit_subscribers": 89119, "created_utc": 1676037807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Aggregate Combinators in ClickHouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_10ymgce", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RFnbn_Th7B7VHl0xfx7w8R9Igi5UxtiPf6cmlU6-AFs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676021073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "clickhouse.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://clickhouse.com/blog/aggregate-functions-combinators-in-clickhouse-for-arrays-maps-and-states", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?auto=webp&amp;v=enabled&amp;s=cd239d606e97421b4ecf0fbe89750920297358e4", "width": 1576, "height": 888}, "resolutions": [{"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3268a14fd20c70a15f1049a9cc190eb4e7137715", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a15d0c264885ec7e06180f237faebe2d7e404416", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=999dc6e15cd6329e185392ace1625d6f512934a6", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=863abd8d5a71065339ac5d671a03f1dce0b7126e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2079f3c7d5c8d3408a80d28ec7fc7dd8f5a2a374", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b594b457ea42efb2329dcf45c8f397ed53c650e", "width": 1080, "height": 608}], "variants": {}, "id": "EszfP3ZeRjCe_uXMGl4zs8W6bXQkIcIFjknp0SByRjw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10ymgce", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ymgce/using_aggregate_combinators_in_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://clickhouse.com/blog/aggregate-functions-combinators-in-clickhouse-for-arrays-maps-and-states", "subreddit_subscribers": 89119, "created_utc": 1676021073.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I am going through the understanding of  end to end data engineering process. I understand how things work, however, I have not found one resource to understand how the data is saved in such cloud storage by the enterprises?\n\nI mean, I understand data can come from anywhere like, website, mobile application or any app that is storing users details like facebooks user details. Now how does these data like, connected into (injested) into cloud storage? How are they mounted? A few real world examples or cases can help to understand. \n\nFor example: I like a post, or create a user account in Facebook, this data is saved in some database which is mounted to cloud storage? is this is it? and we go on with next from there?", "author_fullname": "t2_lngymjsb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the idle scenario where a data in stored in azure blog or any cloud storage like S3.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yb413", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675988622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I am going through the understanding of  end to end data engineering process. I understand how things work, however, I have not found one resource to understand how the data is saved in such cloud storage by the enterprises?&lt;/p&gt;\n\n&lt;p&gt;I mean, I understand data can come from anywhere like, website, mobile application or any app that is storing users details like facebooks user details. Now how does these data like, connected into (injested) into cloud storage? How are they mounted? A few real world examples or cases can help to understand. &lt;/p&gt;\n\n&lt;p&gt;For example: I like a post, or create a user account in Facebook, this data is saved in some database which is mounted to cloud storage? is this is it? and we go on with next from there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10yb413", "is_robot_indexable": true, "report_reasons": null, "author": "iamdhage", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yb413/what_is_the_idle_scenario_where_a_data_in_stored/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yb413/what_is_the_idle_scenario_where_a_data_in_stored/", "subreddit_subscribers": 89119, "created_utc": 1675988622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_zus64vk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data imputation in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_10y9rwp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/upxsp65nnsAGtTc-gAoMDIllQnv_ZRo1odB1CqnmtpQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675985209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hubs.li", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://hubs.li/Q01BW4NT0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?auto=webp&amp;v=enabled&amp;s=7e35df19320e1aad03aadd059f628000aeead487", "width": 800, "height": 526}, "resolutions": [{"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6ca36cd4a5a93b0c47d6e70a1c198501108da83", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=416c8300fc41b7723d9ebe3786248688a509aeb1", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e656629a9830f76ea20131566c7d5cb268153ca", "width": 320, "height": 210}, {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f68309c60f31f46fdfc36020d446292538d58788", "width": 640, "height": 420}], "variants": {}, "id": "nGnUWldmoOF3Mysh8RqBH_abXJv3jq3i_pyrYZ3w6oc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10y9rwp", "is_robot_indexable": true, "report_reasons": null, "author": "oli_k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y9rwp/data_imputation_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://hubs.li/Q01BW4NT0", "subreddit_subscribers": 89119, "created_utc": 1675985209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So our security division is extracting data from a big cloud security system into our analytics server and there is a lot of beef between our data team and their team, they are messing our analytics server performance, delaying our subscriptions and so on, and our rules of bouncing every query that is taking more than 2 hours is messing up with their scripts.(Tbf not even sure why their queries are taking more than 2 hours)\n\nI need to mention that we use the data extracted for analytics as well, but they also use it for monitoring and dashboarding.\n\nNow I want to take this issue in my hands and do something about it, but for that I need to convince my manager.\n\nNow I am thinking this should be a pretty easy fix, we either create another server specifically for the security team where they can use it for whatever they want, and we connect to their server to extract the data to build our dwh, or we move them on our etl server, but I am looking more into the first option.\n\nThat\u2019s why Im posting here, I would like some opinions from more seasoned Data Engineers.\n\nHope you guys can help me with some insights.", "author_fullname": "t2_4lcvdsdn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beef between security and data team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yvm1y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676044891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So our security division is extracting data from a big cloud security system into our analytics server and there is a lot of beef between our data team and their team, they are messing our analytics server performance, delaying our subscriptions and so on, and our rules of bouncing every query that is taking more than 2 hours is messing up with their scripts.(Tbf not even sure why their queries are taking more than 2 hours)&lt;/p&gt;\n\n&lt;p&gt;I need to mention that we use the data extracted for analytics as well, but they also use it for monitoring and dashboarding.&lt;/p&gt;\n\n&lt;p&gt;Now I want to take this issue in my hands and do something about it, but for that I need to convince my manager.&lt;/p&gt;\n\n&lt;p&gt;Now I am thinking this should be a pretty easy fix, we either create another server specifically for the security team where they can use it for whatever they want, and we connect to their server to extract the data to build our dwh, or we move them on our etl server, but I am looking more into the first option.&lt;/p&gt;\n\n&lt;p&gt;That\u2019s why Im posting here, I would like some opinions from more seasoned Data Engineers.&lt;/p&gt;\n\n&lt;p&gt;Hope you guys can help me with some insights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10yvm1y", "is_robot_indexable": true, "report_reasons": null, "author": "Emotional_Key", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yvm1y/beef_between_security_and_data_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yvm1y/beef_between_security_and_data_team/", "subreddit_subscribers": 89119, "created_utc": 1676044891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to ask what skills can I acquire now and put on my resume to get noticed by hiring managers and recruiters. I graduated with a CS bachelors degree in 2021, tried to get a data science job, couldn't get one, enrolled in masters but will be dropping out in the middle because I need to find a job asap. I look at data engineer job postings and some of them have an overwhelming number of tools listed for an entry level job. At the same time, the most recurring skills I keep hearing are python, sql and data warehousing.Can I honestly score a DE position with the knowledge of just these 3? My dead end search for a data science position taught me that DS isn't really an area where recent graduates are accepted. That is why I'm looking for DE roles now because of transferable skills and newbie friendliness but it looks like I'm going down the same road again. Plus, I am also at a stalemate for DE projects. For DS, I've made web apps that take input and predict a result. But I'm not able to come up with ideas for DE projects. Can scraping data off the web and loading it into a CSV qualify? Please help.", "author_fullname": "t2_e6tzy246", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting into DE straight after college?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yqih5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676031338.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to ask what skills can I acquire now and put on my resume to get noticed by hiring managers and recruiters. I graduated with a CS bachelors degree in 2021, tried to get a data science job, couldn&amp;#39;t get one, enrolled in masters but will be dropping out in the middle because I need to find a job asap. I look at data engineer job postings and some of them have an overwhelming number of tools listed for an entry level job. At the same time, the most recurring skills I keep hearing are python, sql and data warehousing.Can I honestly score a DE position with the knowledge of just these 3? My dead end search for a data science position taught me that DS isn&amp;#39;t really an area where recent graduates are accepted. That is why I&amp;#39;m looking for DE roles now because of transferable skills and newbie friendliness but it looks like I&amp;#39;m going down the same road again. Plus, I am also at a stalemate for DE projects. For DS, I&amp;#39;ve made web apps that take input and predict a result. But I&amp;#39;m not able to come up with ideas for DE projects. Can scraping data off the web and loading it into a CSV qualify? Please help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10yqih5", "is_robot_indexable": true, "report_reasons": null, "author": "al8k", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yqih5/getting_into_de_straight_after_college/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yqih5/getting_into_de_straight_after_college/", "subreddit_subscribers": 89119, "created_utc": 1676031338.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently out of a job because I recently moved to a new country and sorting out a work permit.\n\nI'm currently volunteering at an NGO, they do not have a data warehouse. They manually manage data on their records management system. I see some ways I can leverage open-source modern data stack tools to automate the processes.\n\nShould finance be an issue for cloud storage, then I can do with them provisioning me with a server-like machine for the on-prem Postgres warehouse. \n\nMy concern however is, since this NGO is funded by the government, I have concerns about how the project can be scalable and won\u2019t eventually become an irrelevant project in the long run.\n\nJust to save me from \u201cif it ain\u2019t broke, don\u2019t fix it\u201d.\n\nI\u2019ll appreciate related experience and/or advice.", "author_fullname": "t2_c36b59fk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Open-Source Data Stack for the NGO I\u2019m volunteering at - it it worth the effort?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yhj68", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676007903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently out of a job because I recently moved to a new country and sorting out a work permit.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently volunteering at an NGO, they do not have a data warehouse. They manually manage data on their records management system. I see some ways I can leverage open-source modern data stack tools to automate the processes.&lt;/p&gt;\n\n&lt;p&gt;Should finance be an issue for cloud storage, then I can do with them provisioning me with a server-like machine for the on-prem Postgres warehouse. &lt;/p&gt;\n\n&lt;p&gt;My concern however is, since this NGO is funded by the government, I have concerns about how the project can be scalable and won\u2019t eventually become an irrelevant project in the long run.&lt;/p&gt;\n\n&lt;p&gt;Just to save me from \u201cif it ain\u2019t broke, don\u2019t fix it\u201d.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll appreciate related experience and/or advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10yhj68", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-Flow220", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yhj68/building_a_opensource_data_stack_for_the_ngo_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yhj68/building_a_opensource_data_stack_for_the_ngo_im/", "subreddit_subscribers": 89119, "created_utc": 1676007903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Passing Google Cloud Certified Professional Data Engineer Exam in 2023\n\n\\[English version\\] [Passing Google Cloud Professional Data Engineer Exam in 2023](https://medium.com/aidatatools/passing-google-cloud-professional-data-engineer-exam-in-2023-7eb2e2e8ea0e)\n\n\u901a\u904eGCP\u8a8d\u8b49\u8cc7\u6599\u5de5\u7a0b\u5e2b\u7684\u8003\u8a66\u5fc3\u5f97\u5206\u4eab\n\n\\[\u96d9\u8a9e\u7248\\] [Passing Google Cloud Professional Data Engineer Exam in 2023](https://jasonchuang.substack.com/p/passing-google-cloud-professional)", "author_fullname": "t2_7uik3gn1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Passing Google Cloud Certified Professional Data Engineer Exam in 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10z47fd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676066157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Passing Google Cloud Certified Professional Data Engineer Exam in 2023&lt;/p&gt;\n\n&lt;p&gt;[English version] &lt;a href=\"https://medium.com/aidatatools/passing-google-cloud-professional-data-engineer-exam-in-2023-7eb2e2e8ea0e\"&gt;Passing Google Cloud Professional Data Engineer Exam in 2023&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\u901a\u904eGCP\u8a8d\u8b49\u8cc7\u6599\u5de5\u7a0b\u5e2b\u7684\u8003\u8a66\u5fc3\u5f97\u5206\u4eab&lt;/p&gt;\n\n&lt;p&gt;[\u96d9\u8a9e\u7248] &lt;a href=\"https://jasonchuang.substack.com/p/passing-google-cloud-professional\"&gt;Passing Google Cloud Professional Data Engineer Exam in 2023&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?auto=webp&amp;v=enabled&amp;s=19257639f54d68848b831958e26796285bcac00e", "width": 617, "height": 472}, "resolutions": [{"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65d915362523383343976db338ed4d5b1836875b", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c4297d241bd67bdd69ac6b6ad35acd0ccae6b67", "width": 216, "height": 165}, {"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=276c1e31603cffeea2d0cada8f6fdcd4f9f7615d", "width": 320, "height": 244}], "variants": {}, "id": "HbdGQG9XOZg3TP_DSmXkz4yQjrRAIxAn4tROL699ahY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10z47fd", "is_robot_indexable": true, "report_reasons": null, "author": "Few_Knee1141", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z47fd/passing_google_cloud_certified_professional_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z47fd/passing_google_cloud_certified_professional_data/", "subreddit_subscribers": 89119, "created_utc": 1676066157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say I want to design a data warehouse to support BI workloads, where a BI Developer can create semantic models (sourced from the warehouse) to support various business reports. In the design of the warehouse, I know I can create multiple schemas to organize groups of tables with CREATE SCHEMA followed by several CREATE TABLE's to add tables within a particular schema. \n\nLet's say that the data engineers at an organization favor to load only denormalized tables (OBT) into their data warehouse for the BI workloads. My question is: would the data engineers create individual schemas in the warehouse for each one of these denormalized tables (Each schema has only 1 wide table)? So it is like a 1:1 mapping of schema to table.\n\nIf a different data engineering team favored the star schema instead, would there just be a single star schema inside of each schema?\n\nWe can also assume that there is no need to create schemas for security/permission reasons. All BI developers will have access to the entire warehouse.\n\nIs there ever a scenario where schema separation is irrelevant in a data warehouse?\n\nThanks in advanced for any help!", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused about Data Warehouse Schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z19dl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676058884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I want to design a data warehouse to support BI workloads, where a BI Developer can create semantic models (sourced from the warehouse) to support various business reports. In the design of the warehouse, I know I can create multiple schemas to organize groups of tables with CREATE SCHEMA followed by several CREATE TABLE&amp;#39;s to add tables within a particular schema. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say that the data engineers at an organization favor to load only denormalized tables (OBT) into their data warehouse for the BI workloads. My question is: would the data engineers create individual schemas in the warehouse for each one of these denormalized tables (Each schema has only 1 wide table)? So it is like a 1:1 mapping of schema to table.&lt;/p&gt;\n\n&lt;p&gt;If a different data engineering team favored the star schema instead, would there just be a single star schema inside of each schema?&lt;/p&gt;\n\n&lt;p&gt;We can also assume that there is no need to create schemas for security/permission reasons. All BI developers will have access to the entire warehouse.&lt;/p&gt;\n\n&lt;p&gt;Is there ever a scenario where schema separation is irrelevant in a data warehouse?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advanced for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10z19dl", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z19dl/confused_about_data_warehouse_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z19dl/confused_about_data_warehouse_schema/", "subreddit_subscribers": 89119, "created_utc": 1676058884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9lgq0ecy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer with 2 YOE. Planning to switch job and could use some feedback on my resume. Thanks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10yzbes", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2Y_0eQhgojj2j2fUXstl9GFd8dj8SoLazIrDfk2_-aA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676054271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rk2hpmv8oeha1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rk2hpmv8oeha1.jpg?auto=webp&amp;v=enabled&amp;s=b171d707c1a2e73333dadf28a78aba7b31326f60", "width": 2448, "height": 3168}, "resolutions": [{"url": "https://preview.redd.it/rk2hpmv8oeha1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2245c1513fe2b24b5ab8fe434461cbac0f9708f", "width": 108, "height": 139}, {"url": "https://preview.redd.it/rk2hpmv8oeha1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=432323d4d8f6f4ee1bd390fc4fb6e20911b329a3", "width": 216, "height": 279}, {"url": "https://preview.redd.it/rk2hpmv8oeha1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63fac91e58370395915433b01e12bd505b767f20", "width": 320, "height": 414}, {"url": "https://preview.redd.it/rk2hpmv8oeha1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70135d9062705597112dbe132edabc43e992ba85", "width": 640, "height": 828}, {"url": "https://preview.redd.it/rk2hpmv8oeha1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b24744cef3725732e11797790c9a7e82e045e6a", "width": 960, "height": 1242}, {"url": "https://preview.redd.it/rk2hpmv8oeha1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fddfc6e2b158e20fe09fc0cc682a2617d666174d", "width": 1080, "height": 1397}], "variants": {}, "id": "7peITHwvur8hWLbPyVoHk48EMh-la5ctDhFU5pDTyfw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10yzbes", "is_robot_indexable": true, "report_reasons": null, "author": "deeLi007", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yzbes/data_engineer_with_2_yoe_planning_to_switch_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rk2hpmv8oeha1.jpg", "subreddit_subscribers": 89119, "created_utc": 1676054271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We'd like to use Production data to test performance in our dev environment. But the company doesn't allow us to use production data in dev. So we're looking to replace characters in specific fields in the parquet to the point where it's not considered production data so we can test it in dev. The requirement is that the data structure/characteristics must remain the same. Meaning, if we change \"John\" to \"Alex\" then all \"John\" must change to \"Alex\", same for addresses, phone numbers, IP addresses etc.\n\nCan we easily achieve this somehow with parquet files?  \n\n\nAfter doing a bit of research, seems like the best way to do this is:  \n1. Convert parquet to a CSV  \n2. Make changes to CSV  \n3. Create parquet files based on CSV", "author_fullname": "t2_1tbzh2ae", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacing characters &amp; numbers in parquet files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yy44g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676051311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;d like to use Production data to test performance in our dev environment. But the company doesn&amp;#39;t allow us to use production data in dev. So we&amp;#39;re looking to replace characters in specific fields in the parquet to the point where it&amp;#39;s not considered production data so we can test it in dev. The requirement is that the data structure/characteristics must remain the same. Meaning, if we change &amp;quot;John&amp;quot; to &amp;quot;Alex&amp;quot; then all &amp;quot;John&amp;quot; must change to &amp;quot;Alex&amp;quot;, same for addresses, phone numbers, IP addresses etc.&lt;/p&gt;\n\n&lt;p&gt;Can we easily achieve this somehow with parquet files?  &lt;/p&gt;\n\n&lt;p&gt;After doing a bit of research, seems like the best way to do this is:&lt;br/&gt;\n1. Convert parquet to a CSV&lt;br/&gt;\n2. Make changes to CSV&lt;br/&gt;\n3. Create parquet files based on CSV&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10yy44g", "is_robot_indexable": true, "report_reasons": null, "author": "TeslaMecca", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yy44g/replacing_characters_numbers_in_parquet_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yy44g/replacing_characters_numbers_in_parquet_files/", "subreddit_subscribers": 89119, "created_utc": 1676051311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What should I prepare for new grad application and interview?", "author_fullname": "t2_cl5t0lrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer New Grad Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yfnqa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676001660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What should I prepare for new grad application and interview?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10yfnqa", "is_robot_indexable": true, "report_reasons": null, "author": "Unlucky-Difficulty86", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yfnqa/data_engineer_new_grad_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yfnqa/data_engineer_new_grad_interview/", "subreddit_subscribers": 89119, "created_utc": 1676001660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to use DMS to capture change logs from the SQL server and write them to S3. I have set up a long polling period of 6 hours. (AWS recommends &gt; 1 hour). DMS fails with the below error when the database is idle for a few hours during the night.\n\nDMS Error: Last Error AlwaysOn BACKUP-ed data is not available Task error notification received from subtask 0, thread 0\n\nError from cloud watch - Failed to access LSN '000033fc:00005314:01e6' in the backup log sets since BACKUP/LOG-s are not available\n\nI am currently using DMS version 3.4.6 with multi-az.\n\nI always thought the DMS reads the change data immediately after updating the T log with the DML changes. Why do we see this error even with a long polling period? Can someone explain why this issue is caused? how we can handle this ?", "author_fullname": "t2_2adeipr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to resolve DMS failure to access LSN issue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10z4ysc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676068366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use DMS to capture change logs from the SQL server and write them to S3. I have set up a long polling period of 6 hours. (AWS recommends &amp;gt; 1 hour). DMS fails with the below error when the database is idle for a few hours during the night.&lt;/p&gt;\n\n&lt;p&gt;DMS Error: Last Error AlwaysOn BACKUP-ed data is not available Task error notification received from subtask 0, thread 0&lt;/p&gt;\n\n&lt;p&gt;Error from cloud watch - Failed to access LSN &amp;#39;000033fc:00005314:01e6&amp;#39; in the backup log sets since BACKUP/LOG-s are not available&lt;/p&gt;\n\n&lt;p&gt;I am currently using DMS version 3.4.6 with multi-az.&lt;/p&gt;\n\n&lt;p&gt;I always thought the DMS reads the change data immediately after updating the T log with the DML changes. Why do we see this error even with a long polling period? Can someone explain why this issue is caused? how we can handle this ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10z4ysc", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Outlandishness-74", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z4ysc/how_to_resolve_dms_failure_to_access_lsn_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z4ysc/how_to_resolve_dms_failure_to_access_lsn_issue/", "subreddit_subscribers": 89119, "created_utc": 1676068366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking to fill in any Data Engineering skills gaps with project #2 and could use some help on what to do next, and how to improve my resume.\n\nBasically with project #2, I extracted some data from an API endpoint and a website's HTML, ran extract and process scripts on AWS lambda, and orchestrated them using AWS step functions (didn't have the funds for MWAA). I loaded the rows these into Postgres and visualized them in Google Data Studio. \n\nhttps://preview.redd.it/k3liutqfofha1.png?width=2550&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=677aa8d11ff9216844748579ae13c073f0db404d", "author_fullname": "t2_f2plcmjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering roles, looking to make the official move", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"k3liutqfofha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 139, "x": 108, "u": "https://preview.redd.it/k3liutqfofha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d75e5ea7631b7eeaffd680705afb52cafe195640"}, {"y": 279, "x": 216, "u": "https://preview.redd.it/k3liutqfofha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aebb89135d505811cf1d3d2f5e1307625b295f76"}, {"y": 414, "x": 320, "u": "https://preview.redd.it/k3liutqfofha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ac5d47bf9b836896908d6ed556b318c60902d8a"}, {"y": 828, "x": 640, "u": "https://preview.redd.it/k3liutqfofha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ad2fcc5985fd4657f5ad766aeda88b94bf6c4bd"}, {"y": 1242, "x": 960, "u": "https://preview.redd.it/k3liutqfofha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cbab4c24e5d35377550b056803ccd8685011b619"}, {"y": 1397, "x": 1080, "u": "https://preview.redd.it/k3liutqfofha1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d7d5b0690ce4bb9d64b51a635d9b2293fabaa5e"}], "s": {"y": 3300, "x": 2550, "u": "https://preview.redd.it/k3liutqfofha1.png?width=2550&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=677aa8d11ff9216844748579ae13c073f0db404d"}, "id": "k3liutqfofha1"}}, "name": "t3_10z4grm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/mGhQ5I7cuKDPqRYlQTL3W5vOp771P6VG-q3ew2aQ1y4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676066888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to fill in any Data Engineering skills gaps with project #2 and could use some help on what to do next, and how to improve my resume.&lt;/p&gt;\n\n&lt;p&gt;Basically with project #2, I extracted some data from an API endpoint and a website&amp;#39;s HTML, ran extract and process scripts on AWS lambda, and orchestrated them using AWS step functions (didn&amp;#39;t have the funds for MWAA). I loaded the rows these into Postgres and visualized them in Google Data Studio. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/k3liutqfofha1.png?width=2550&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=677aa8d11ff9216844748579ae13c073f0db404d\"&gt;https://preview.redd.it/k3liutqfofha1.png?width=2550&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=677aa8d11ff9216844748579ae13c073f0db404d&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10z4grm", "is_robot_indexable": true, "report_reasons": null, "author": "Tough_Bag_458", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z4grm/data_engineering_roles_looking_to_make_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z4grm/data_engineering_roles_looking_to_make_the/", "subreddit_subscribers": 89119, "created_utc": 1676066888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is maybe a dumb question.\n\nOf the three Data Lakehouse solutions - Iceberg, Hudi, Delta Lake - can any be setup without Spark?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lakehouse non-Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10z4ae3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676066401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is maybe a dumb question.&lt;/p&gt;\n\n&lt;p&gt;Of the three Data Lakehouse solutions - Iceberg, Hudi, Delta Lake - can any be setup without Spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10z4ae3", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z4ae3/data_lakehouse_nonspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z4ae3/data_lakehouse_nonspark/", "subreddit_subscribers": 89119, "created_utc": 1676066401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started taking this three quarter long certificate course for data engineering. I am currently in the second quarter, and it has been okay but it's more about learning histories of big data and a little bit of learning to use spark dataframe and spark sql and scala. There was also some assignment using hbase and cassandra. \n\nBut overall I'm wondering if I should continue. I don't have any computer science background. I don't even have any background data analysis. Although my current job involves a little bit of data handling so I have used python and pandas. I guess I had high hopes from this course that it will make me hireable as a junior data engineer. I thought this would be something like a bootcamp but this is just once a week, 3hour lecture and some weekly assignment that takes 2-3hours.\n\nThe reason why I started was from recommendation from a friend. He told me that since I already have some experience using python for handling some data,  pursuing data engineering might be worth it. However, since then I've started taking second bachelor's degree for computer science. And I feel like I'll rather just focus on getting that bachelor's degree in computer science rather than trying to become a data engineer through a certificate.\n\nSo far, I feel like I'm just getting a taste of what data engineering is. Additionally, it feels like this course is more for already working software engineers who just want to specialize in data engineering. So I'm wondering if I should continue taking this course?\n\nDo you think that just knowing how to use some of these tools will make me hireable as a junior data engineer?\n\nI feel like I have this sunken cost, I just need to finish one more quarter. However, I think that this data engineering certificate will be worthless on my resume. I haven't decided what I want to do once I get the computer science degree. Likely to interview will ask me why I have the certificate but I feel like I don't have a good reason.\n\nPerhaps if I took this course in the future, I might have liked it more.", "author_fullname": "t2_89odwyx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I give up on Data engineering certificate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10z3w7i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676065256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started taking this three quarter long certificate course for data engineering. I am currently in the second quarter, and it has been okay but it&amp;#39;s more about learning histories of big data and a little bit of learning to use spark dataframe and spark sql and scala. There was also some assignment using hbase and cassandra. &lt;/p&gt;\n\n&lt;p&gt;But overall I&amp;#39;m wondering if I should continue. I don&amp;#39;t have any computer science background. I don&amp;#39;t even have any background data analysis. Although my current job involves a little bit of data handling so I have used python and pandas. I guess I had high hopes from this course that it will make me hireable as a junior data engineer. I thought this would be something like a bootcamp but this is just once a week, 3hour lecture and some weekly assignment that takes 2-3hours.&lt;/p&gt;\n\n&lt;p&gt;The reason why I started was from recommendation from a friend. He told me that since I already have some experience using python for handling some data,  pursuing data engineering might be worth it. However, since then I&amp;#39;ve started taking second bachelor&amp;#39;s degree for computer science. And I feel like I&amp;#39;ll rather just focus on getting that bachelor&amp;#39;s degree in computer science rather than trying to become a data engineer through a certificate.&lt;/p&gt;\n\n&lt;p&gt;So far, I feel like I&amp;#39;m just getting a taste of what data engineering is. Additionally, it feels like this course is more for already working software engineers who just want to specialize in data engineering. So I&amp;#39;m wondering if I should continue taking this course?&lt;/p&gt;\n\n&lt;p&gt;Do you think that just knowing how to use some of these tools will make me hireable as a junior data engineer?&lt;/p&gt;\n\n&lt;p&gt;I feel like I have this sunken cost, I just need to finish one more quarter. However, I think that this data engineering certificate will be worthless on my resume. I haven&amp;#39;t decided what I want to do once I get the computer science degree. Likely to interview will ask me why I have the certificate but I feel like I don&amp;#39;t have a good reason.&lt;/p&gt;\n\n&lt;p&gt;Perhaps if I took this course in the future, I might have liked it more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10z3w7i", "is_robot_indexable": true, "report_reasons": null, "author": "wsb_degen_number9999", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z3w7i/should_i_give_up_on_data_engineering_certificate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z3w7i/should_i_give_up_on_data_engineering_certificate/", "subreddit_subscribers": 89119, "created_utc": 1676065256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6hz5qk0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Use dbt With Snowpark Python to Implement Sentiment Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_10z35fe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1Zk8X9mW1UjRExYc8VOmaw-to607AvYbVA0S5ufqZ0o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676063367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "phdata.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.phdata.io/blog/how-to-use-dbt-with-snowpark-python-to-implement-sentiment-analysis/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?auto=webp&amp;v=enabled&amp;s=3ebb0325aacaa6e222c6f43986ef8f9d5c87b7b4", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8ea15e9306fd11888c65aaf5687e2342c006f53", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b37907e4ba460dba067872818766a4ecfbe9f52e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e76a85a3bcc92b92f26db83c65c6491c9c315492", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7609acf14b5263a4d5ed6eff6a08a20478e39186", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4583e1bee0e72219f0ebf0f798b08326bdbd71e9", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20e692234089d8a181a1408f904b286257e628bc", "width": 1080, "height": 565}], "variants": {}, "id": "9LtNOZb3NItXy3SHc0Bi4CxfR3vcwSkmIZJbXRCJPXY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10z35fe", "is_robot_indexable": true, "report_reasons": null, "author": "OptimizedGradient", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z35fe/how_to_use_dbt_with_snowpark_python_to_implement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.phdata.io/blog/how-to-use-dbt-with-snowpark-python-to-implement-sentiment-analysis/", "subreddit_subscribers": 89119, "created_utc": 1676063367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,We are building out a Data Lake in S3. That much is straight forward. We will pipe data from a Postgres DB to S3 in Parquet format. But something confuses me. I'll explain.\n\nPostgres is \"dynamic\". If I update a record there, the correct record will have the new value. But files in S3 are \"static\". After a file is written there, it's there forever. If in the source (Postgres), a record is updated, a NEW file will be written to S3.\n\nQuestion: if all that is true, how can I meaningfully query this data without having duplicate IDs (records appearing more than once, but with different 'updated' timestamps)?\n\nSome ideas:\n\n1. a separate deduplication ETL pipeline?\n2. perhaps Athena can \"just do it\"?\n3. my premise is not accurate.\n\nPlease and thank you for your advice.  \nEDIT: clarified something in my question.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "S3 Data Lake and duplicate entries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z22gn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676061486.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676060773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,We are building out a Data Lake in S3. That much is straight forward. We will pipe data from a Postgres DB to S3 in Parquet format. But something confuses me. I&amp;#39;ll explain.&lt;/p&gt;\n\n&lt;p&gt;Postgres is &amp;quot;dynamic&amp;quot;. If I update a record there, the correct record will have the new value. But files in S3 are &amp;quot;static&amp;quot;. After a file is written there, it&amp;#39;s there forever. If in the source (Postgres), a record is updated, a NEW file will be written to S3.&lt;/p&gt;\n\n&lt;p&gt;Question: if all that is true, how can I meaningfully query this data without having duplicate IDs (records appearing more than once, but with different &amp;#39;updated&amp;#39; timestamps)?&lt;/p&gt;\n\n&lt;p&gt;Some ideas:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;a separate deduplication ETL pipeline?&lt;/li&gt;\n&lt;li&gt;perhaps Athena can &amp;quot;just do it&amp;quot;?&lt;/li&gt;\n&lt;li&gt;my premise is not accurate.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please and thank you for your advice.&lt;br/&gt;\nEDIT: clarified something in my question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10z22gn", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z22gn/s3_data_lake_and_duplicate_entries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z22gn/s3_data_lake_and_duplicate_entries/", "subreddit_subscribers": 89119, "created_utc": 1676060773.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}