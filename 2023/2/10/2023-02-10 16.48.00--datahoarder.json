{"kind": "Listing", "data": {"after": "t3_10y02my", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "imagine -- \n\nyou have a beautiful, curated media library of every TV show and film in the world. every one! tagged, posters, cast, subtitles, 4K, 5.1, all of it. \n\nany of these media files can be transmitted almost instantly to any computer in the world at little to no cost. it is a miracle of modern technology. \n\n~~~\n\nand look where we are now.\n\nif someone's in atlanta instead of charlotte, for 32 days instead of 28, they get a 'fuck you' notice if they try to watch a film. \n\nbeing able to preserve the media i love, a buffer against these sick fucking bean counting fucks, is pure joy and love. thank you all.", "author_fullname": "t2_bazgovvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Philosophically, the netflix situation is making me sick", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y9wcy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 380, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 380, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675985507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;imagine -- &lt;/p&gt;\n\n&lt;p&gt;you have a beautiful, curated media library of every TV show and film in the world. every one! tagged, posters, cast, subtitles, 4K, 5.1, all of it. &lt;/p&gt;\n\n&lt;p&gt;any of these media files can be transmitted almost instantly to any computer in the world at little to no cost. it is a miracle of modern technology. &lt;/p&gt;\n\n&lt;p&gt;~~~&lt;/p&gt;\n\n&lt;p&gt;and look where we are now.&lt;/p&gt;\n\n&lt;p&gt;if someone&amp;#39;s in atlanta instead of charlotte, for 32 days instead of 28, they get a &amp;#39;fuck you&amp;#39; notice if they try to watch a film. &lt;/p&gt;\n\n&lt;p&gt;being able to preserve the media i love, a buffer against these sick fucking bean counting fucks, is pure joy and love. thank you all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10y9wcy", "is_robot_indexable": true, "report_reasons": null, "author": "spacewalk__", "discussion_type": null, "num_comments": 162, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y9wcy/philosophically_the_netflix_situation_is_making/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y9wcy/philosophically_the_netflix_situation_is_making/", "subreddit_subscribers": 669313, "created_utc": 1675985507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Western Digital has 2x 14TB Red Plus drives for $420, must buy 2. Solid deal for actual Red Plus drives with 3 year warranty instead of shucking whites from Easystores\n\nhttps://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd#WD140EFGX", "author_fullname": "t2_4kjc5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2x WD Red Plus 14TB for $420 ($15/tb) - Direct from WD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y5106", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 163, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 163, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1675974257.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675973968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Western Digital has 2x 14TB Red Plus drives for $420, must buy 2. Solid deal for actual Red Plus drives with 3 year warranty instead of shucking whites from Easystores&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd#WD140EFGX\"&gt;https://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd#WD140EFGX&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?auto=webp&amp;v=enabled&amp;s=935ae08de307a340e006f3282858aab1b61d23e6", "width": 1680, "height": 1680}, "resolutions": [{"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b240e0224391a3132feb0e9600dfbd4d6dc72c94", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76ed630b7b2452ac8f4bea418cb80b31b56a2bdf", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ece20b5a0a3a466228d46c1f7127da11367f4bbd", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e1e1b91d58e5a475fdeec3614747050965837b2", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e218935112ef9780f443a6c2a0a9da6a1d7a63bc", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f606446b08b91e2affa4f7ae489797ae018af4d6", "width": 1080, "height": 1080}], "variants": {}, "id": "RzQmR5PjZ5asfOMDi4yuzhpBG_CJQYRO8YaVhzUtLUA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "240TB + 28TB Parity", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10y5106", "is_robot_indexable": true, "report_reasons": null, "author": "r34p3rex", "discussion_type": null, "num_comments": 61, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10y5106/2x_wd_red_plus_14tb_for_420_15tb_direct_from_wd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y5106/2x_wd_red_plus_14tb_for_420_15tb_direct_from_wd/", "subreddit_subscribers": 669313, "created_utc": 1675973968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there any modern open-source alternative to HTTrack?\n\n&amp;#x200B;\n\nThough HTTrack has more features than wget, but the scraping features are full of major bugs.\n\n(e.g. link travelling configs are obviously broken, since more than a decade ago)\n\nIn addition, the download engine does not seems promising, so it is probably not worth the time to fix the scraping defects on it.\n\n====\n\nwget has few features for scraping and is barely usable.", "author_fullname": "t2_44i2u8pq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to HTTrack (website copier) as of 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yjkgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676015463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any modern open-source alternative to HTTrack?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Though HTTrack has more features than wget, but the scraping features are full of major bugs.&lt;/p&gt;\n\n&lt;p&gt;(e.g. link travelling configs are obviously broken, since more than a decade ago)&lt;/p&gt;\n\n&lt;p&gt;In addition, the download engine does not seems promising, so it is probably not worth the time to fix the scraping defects on it.&lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;wget has few features for scraping and is barely usable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10yjkgm", "is_robot_indexable": true, "report_reasons": null, "author": "FreshP_0325X", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yjkgm/alternative_to_httrack_website_copier_as_of_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yjkgm/alternative_to_httrack_website_copier_as_of_2023/", "subreddit_subscribers": 669313, "created_utc": 1676015463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a Synology NAS right now with 5x4TB drives using SHR, I'm looking to increase the storage capacity and I thought I heard off hand that larger sizes (16-20 etc.) are not ideal?  For context, my usage is mostly as network storage for Plex media (server is on a VM elsewhere) and various backups.", "author_fullname": "t2_ff0wt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there such a thing as too big? (NAS Hard drives)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y39fx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675969944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Synology NAS right now with 5x4TB drives using SHR, I&amp;#39;m looking to increase the storage capacity and I thought I heard off hand that larger sizes (16-20 etc.) are not ideal?  For context, my usage is mostly as network storage for Plex media (server is on a VM elsewhere) and various backups.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y39fx", "is_robot_indexable": true, "report_reasons": null, "author": "landsverka", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y39fx/is_there_such_a_thing_as_too_big_nas_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y39fx/is_there_such_a_thing_as_too_big_nas_hard_drives/", "subreddit_subscribers": 669313, "created_utc": 1675969944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently bought a DVD/Blu-Ray collection from someone on eBay in like new condition, with most of the movies having not been touched at all. It was sold as a collection to, it\u2019s not a lot of various movies. I want to back them up to my media server, but I\u2019m having a problem with one of the Blu-rays.\n\nI\u2019m using MakeMKV to rip them, and on one of them I get a MEDIUM error when trying to rip it, in the same exact spot. Apparently that means that the disc failed to read, and it\u2019s damaged/scratched somewhere. I\u2019ve tried like 5 times now, I\u2019ve cleaned it very good with a micro fiber cloth, and it didn\u2019t appear to have any signs of wear at all when I opened it anyway. The bonus features ripped perfectly, what\u2019s frustrating is that the main movie is the one that\u2019s failing.\n\nAll of the other movies worked perfectly, so I don\u2019t know what could be the problem, or what I can do. The disc is in perfect condition, and the movie *is* on there, so nothing should be wrong. The disc doesn\u2019t open on my computer, but neither do any of the other films I own so I assume it\u2019s me not knowing how to do that, and I don\u2019t have a blu-ray player to test it on my television.\n\nSince it stops in the same spot every time, is there a way I can make it skip that certain section? Another way I can rip with another software that\u2019s more forgiving? Would it be petty to request a refund when it\u2019s just one movie in the collection that has enough wear that it doesn\u2019t work on one part, even though the description of it was like new? I\u2019ve already done all this work ripping the other movies, so I just don\u2019t know if it would be worth the trouble. Thanks.", "author_fullname": "t2_attayt20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Blu-Ray won\u2019t rip due to MEDIUM ERROR in MakeMKV, what options do I have?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yff3s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676000907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought a DVD/Blu-Ray collection from someone on eBay in like new condition, with most of the movies having not been touched at all. It was sold as a collection to, it\u2019s not a lot of various movies. I want to back them up to my media server, but I\u2019m having a problem with one of the Blu-rays.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m using MakeMKV to rip them, and on one of them I get a MEDIUM error when trying to rip it, in the same exact spot. Apparently that means that the disc failed to read, and it\u2019s damaged/scratched somewhere. I\u2019ve tried like 5 times now, I\u2019ve cleaned it very good with a micro fiber cloth, and it didn\u2019t appear to have any signs of wear at all when I opened it anyway. The bonus features ripped perfectly, what\u2019s frustrating is that the main movie is the one that\u2019s failing.&lt;/p&gt;\n\n&lt;p&gt;All of the other movies worked perfectly, so I don\u2019t know what could be the problem, or what I can do. The disc is in perfect condition, and the movie &lt;em&gt;is&lt;/em&gt; on there, so nothing should be wrong. The disc doesn\u2019t open on my computer, but neither do any of the other films I own so I assume it\u2019s me not knowing how to do that, and I don\u2019t have a blu-ray player to test it on my television.&lt;/p&gt;\n\n&lt;p&gt;Since it stops in the same spot every time, is there a way I can make it skip that certain section? Another way I can rip with another software that\u2019s more forgiving? Would it be petty to request a refund when it\u2019s just one movie in the collection that has enough wear that it doesn\u2019t work on one part, even though the description of it was like new? I\u2019ve already done all this work ripping the other movies, so I just don\u2019t know if it would be worth the trouble. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10yff3s", "is_robot_indexable": true, "report_reasons": null, "author": "AlternateWitness", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yff3s/new_bluray_wont_rip_due_to_medium_error_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yff3s/new_bluray_wont_rip_due_to_medium_error_in/", "subreddit_subscribers": 669313, "created_utc": 1676000907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The deets:  \nLong time lurker, beginner hoarder here. I currently have a PC case that can handle a max of 11x 3.5 drives, but right now I need a PC more than I need a server. Even so, I still want to start hoarding data immediately. My current plan is just to stick the HDD's as regular drives to a PC, and probably get an LSI to expand the SATA slots on the MOBO. Is there any serious downside to storing the data in a regular PC vs a 24/7 server? Will it be difficult to migrate to a dedicated server when I have more money? My thanks to the kind strangers who will reply to this post.\n\nTLDR:   \nI'm poor and can't afford a dedicated server right now. What's the downside to hoarding on a PC?", "author_fullname": "t2_12q0qi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I use my PC to hoard data instead of using a server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ybnkr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675990164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The deets:&lt;br/&gt;\nLong time lurker, beginner hoarder here. I currently have a PC case that can handle a max of 11x 3.5 drives, but right now I need a PC more than I need a server. Even so, I still want to start hoarding data immediately. My current plan is just to stick the HDD&amp;#39;s as regular drives to a PC, and probably get an LSI to expand the SATA slots on the MOBO. Is there any serious downside to storing the data in a regular PC vs a 24/7 server? Will it be difficult to migrate to a dedicated server when I have more money? My thanks to the kind strangers who will reply to this post.&lt;/p&gt;\n\n&lt;p&gt;TLDR:&lt;br/&gt;\nI&amp;#39;m poor and can&amp;#39;t afford a dedicated server right now. What&amp;#39;s the downside to hoarding on a PC?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ybnkr", "is_robot_indexable": true, "report_reasons": null, "author": "faplesspotato", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ybnkr/can_i_use_my_pc_to_hoard_data_instead_of_using_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ybnkr/can_i_use_my_pc_to_hoard_data_instead_of_using_a/", "subreddit_subscribers": 669313, "created_utc": 1675990164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi. I want to automate my backup but I'm not looking for software that doesn't involve creating enclosed archives that need to be read with the same software for restore (like Veeam). I want the backup to simply be a copy-paste of the selected target on my system, that can be easily browsed later from the target destination. Which free software would you recommend?", "author_fullname": "t2_e2be8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm looking for a simple and basic backup software for Windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ysbud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676036609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I want to automate my backup but I&amp;#39;m not looking for software that doesn&amp;#39;t involve creating enclosed archives that need to be read with the same software for restore (like Veeam). I want the backup to simply be a copy-paste of the selected target on my system, that can be easily browsed later from the target destination. Which free software would you recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ysbud", "is_robot_indexable": true, "report_reasons": null, "author": "xevizero", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ysbud/im_looking_for_a_simple_and_basic_backup_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ysbud/im_looking_for_a_simple_and_basic_backup_software/", "subreddit_subscribers": 669313, "created_utc": 1676036609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey I am looking for way to mass retrieve the url of Tiktok I saved in specifics collections. I don't know to download them I just need their url. I tried to get the famous JSON file from my profile but the file is mainly useless.", "author_fullname": "t2_opo9usc7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get the url of all my collections videos from tiktok", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y6obg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675977904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey I am looking for way to mass retrieve the url of Tiktok I saved in specifics collections. I don&amp;#39;t know to download them I just need their url. I tried to get the famous JSON file from my profile but the file is mainly useless.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y6obg", "is_robot_indexable": true, "report_reasons": null, "author": "Jaydayaim", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y6obg/get_the_url_of_all_my_collections_videos_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y6obg/get_the_url_of_all_my_collections_videos_from/", "subreddit_subscribers": 669313, "created_utc": 1675977904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_rrt44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a reason I'm missing that these are as cheap as they are? Am new to this.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y9qos", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1675985127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "amazon.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.amazon.com/AV-GP-Intellipower-Cache-Drive-WD20EURX/dp/B00DXOJJQQ/ref=sr_1_4?crid=QBMUPKAVIEG4&amp;keywords=2tb+server+hard+drive&amp;qid=1675984717&amp;sprefix=2tb+server+hard+drive%2Caps%2C98&amp;sr=8-4#customerReviews", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y9qos", "is_robot_indexable": true, "report_reasons": null, "author": "alvogel122", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y9qos/is_there_a_reason_im_missing_that_these_are_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.amazon.com/AV-GP-Intellipower-Cache-Drive-WD20EURX/dp/B00DXOJJQQ/ref=sr_1_4?crid=QBMUPKAVIEG4&amp;keywords=2tb+server+hard+drive&amp;qid=1675984717&amp;sprefix=2tb+server+hard+drive%2Caps%2C98&amp;sr=8-4#customerReviews", "subreddit_subscribers": 669313, "created_utc": 1675985127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\\*Not, sure if this is the right subreddit, but this is to you backup enthusiasts out there. Apologies if OT\\*\n\n&amp;#x200B;\n\nHello,\n\nI'm in a bit of a pickle:  I had set up (years ago) a differential image backup solution called \"Flashback\" by a company called \"Xeroweight\" on my and a number of my family members' PCs (am the family tech support). As an aside: \"Flashback\" was an onward development of AX64 Backup.\n\nThe software diligently ran its differential backups and regularly onto a local NAS. Unfortunately one of my family members has managed to a) trigger Bitlocker recovery at startup (how, don't know) and b) mislaid the prinout of the Bitlocker recovery key, which I had so carefully printed out for them.\n\nThe Flashback allows/allowed to create a startup USB stick / DVD-R, drawing on the WIM file that is part of the system. Way back, I also created both these recovery media and have them here.\n\nTo my shock, both these recovery media don't boot anymore. I've tried turning off Secure Boot (or allowing 3rd party CAs) in the BIOS settings. Also tried playing with turning TPM on or off. Tried to boot from DVD(-R) also, as an alternative to booting from the stick.\n\nSystem is a 2017 Microsoft Surface Pro, Win 10 Home.\n\n&amp;#x200B;\n\nIn 2017, I did an image restore via the USB stick on an older PC, on which the startup USB stick worked without any problems. I assume something must have happened around secure boot and/or UEFI standards, which is now preventing the boot process from the recovery windows system, which is on the recovery/startup USB stick, that runs the Flashback recovery UI.\n\nThe software has been discontinued; the company behind it no longer exists. All discussion forums of back then (2016?) are offline.\n\nDoes anyone have any hints for me on\n\na) what explains this change in behaviour?\n\nb) regarding any suggestions on a workaround, which I could try, in order to restore the image on the NAS to the PC?\n\n&amp;#x200B;\n\nThanks in advance\n\nPS: I know I'm an idiot for not having checked on this \"working system\" earlier and having waited so long to upgrade to solutions, which are still on the market, but... hey... that's the way it is.", "author_fullname": "t2_aev8vyca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help? - Old Backup done with \"Xeroweight Flashback\" - Unable to boot from Startup USB and/or CD-Rom", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yrzx1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676035702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;*Not, sure if this is the right subreddit, but this is to you backup enthusiasts out there. Apologies if OT*&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in a bit of a pickle:  I had set up (years ago) a differential image backup solution called &amp;quot;Flashback&amp;quot; by a company called &amp;quot;Xeroweight&amp;quot; on my and a number of my family members&amp;#39; PCs (am the family tech support). As an aside: &amp;quot;Flashback&amp;quot; was an onward development of AX64 Backup.&lt;/p&gt;\n\n&lt;p&gt;The software diligently ran its differential backups and regularly onto a local NAS. Unfortunately one of my family members has managed to a) trigger Bitlocker recovery at startup (how, don&amp;#39;t know) and b) mislaid the prinout of the Bitlocker recovery key, which I had so carefully printed out for them.&lt;/p&gt;\n\n&lt;p&gt;The Flashback allows/allowed to create a startup USB stick / DVD-R, drawing on the WIM file that is part of the system. Way back, I also created both these recovery media and have them here.&lt;/p&gt;\n\n&lt;p&gt;To my shock, both these recovery media don&amp;#39;t boot anymore. I&amp;#39;ve tried turning off Secure Boot (or allowing 3rd party CAs) in the BIOS settings. Also tried playing with turning TPM on or off. Tried to boot from DVD(-R) also, as an alternative to booting from the stick.&lt;/p&gt;\n\n&lt;p&gt;System is a 2017 Microsoft Surface Pro, Win 10 Home.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In 2017, I did an image restore via the USB stick on an older PC, on which the startup USB stick worked without any problems. I assume something must have happened around secure boot and/or UEFI standards, which is now preventing the boot process from the recovery windows system, which is on the recovery/startup USB stick, that runs the Flashback recovery UI.&lt;/p&gt;\n\n&lt;p&gt;The software has been discontinued; the company behind it no longer exists. All discussion forums of back then (2016?) are offline.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any hints for me on&lt;/p&gt;\n\n&lt;p&gt;a) what explains this change in behaviour?&lt;/p&gt;\n\n&lt;p&gt;b) regarding any suggestions on a workaround, which I could try, in order to restore the image on the NAS to the PC?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n\n&lt;p&gt;PS: I know I&amp;#39;m an idiot for not having checked on this &amp;quot;working system&amp;quot; earlier and having waited so long to upgrade to solutions, which are still on the market, but... hey... that&amp;#39;s the way it is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10yrzx1", "is_robot_indexable": true, "report_reasons": null, "author": "MightyMikeyT", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yrzx1/help_old_backup_done_with_xeroweight_flashback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yrzx1/help_old_backup_done_with_xeroweight_flashback/", "subreddit_subscribers": 669313, "created_utc": 1676035702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Have your own local copy of all the text on the internet - if you have 500TB to spare!\n\nhttps://commoncrawl.org/connect/blog/\n\nThere's also the 750GB [C4 dataset](https://huggingface.co/datasets/c4), which is a heavily filtered subset containing less text - but more interesting text.", "author_fullname": "t2_cd9nt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anybody downloaded the Common Crawl Dataset?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yegzm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675998005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have your own local copy of all the text on the internet - if you have 500TB to spare!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://commoncrawl.org/connect/blog/\"&gt;https://commoncrawl.org/connect/blog/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also the 750GB &lt;a href=\"https://huggingface.co/datasets/c4\"&gt;C4 dataset&lt;/a&gt;, which is a heavily filtered subset containing less text - but more interesting text.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10yegzm", "is_robot_indexable": true, "report_reasons": null, "author": "currentscurrents", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yegzm/has_anybody_downloaded_the_common_crawl_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yegzm/has_anybody_downloaded_the_common_crawl_dataset/", "subreddit_subscribers": 669313, "created_utc": 1675998005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for a way to download the text from a post.  \nthe `--write-metadata` option downloads everything.  \n\n\nIn the configuration file, I tried:  \n\n\n`{`\n\n`\"extractor\": {`\n\n`\"twitter\": {`\n\n`\"text-tweets\":true,`\n\n`\"quoted\":true,`\n\n`\"retweets\":true,`\n\n`\"postprocessor\": [`\n\n`{`\n\n`\"command\": \"echo {content} &gt; {filename}.txt\"`\n\n`}`\n\n`],`\n\n`\"directory\": {`\n\n`\"\": [\"{author[name]}\"]`\n\n`},`\n\n`\"filename\": {`\n\n`\"\": \"{author[name]}/{tweet_id}_{num}\"`\n\n`}`\n\n`}`\n\n`}`\n\n`}`\n\n  \nI would like text only tweet to be downloaded as .txt and the text of images posts be downloaded too.   \nSo if an image is downloaded another file with same name+.txt would contains the tweet content", "author_fullname": "t2_3zmmj0so", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gallery-dl download texts only", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ya1l0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675985873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a way to download the text from a post.&lt;br/&gt;\nthe &lt;code&gt;--write-metadata&lt;/code&gt; option downloads everything.  &lt;/p&gt;\n\n&lt;p&gt;In the configuration file, I tried:  &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;extractor&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;twitter&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;text-tweets&amp;quot;:true,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;quoted&amp;quot;:true,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;retweets&amp;quot;:true,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;postprocessor&amp;quot;: [&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;command&amp;quot;: &amp;quot;echo {content} &amp;gt; {filename}.txt&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;],&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;directory&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;&amp;quot;: [&amp;quot;{author[name]}&amp;quot;]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;},&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;filename&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;&amp;quot;: &amp;quot;{author[name]}/{tweet_id}_{num}&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I would like text only tweet to be downloaded as .txt and the text of images posts be downloaded too.&lt;br/&gt;\nSo if an image is downloaded another file with same name+.txt would contains the tweet content&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ya1l0", "is_robot_indexable": true, "report_reasons": null, "author": "Tyranoc4", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ya1l0/gallerydl_download_texts_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ya1l0/gallerydl_download_texts_only/", "subreddit_subscribers": 669313, "created_utc": 1675985873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What do you all use? I've tried HTTrack before but always struggle to get it to work correctly, or I should say how I want it. To just do all links on the site and not follow links on forever. I am most likely setting it up wrong.\n\nAway tomorrow for a week and wanted to grab\n\n[https://automatetheboringstuff.com/2e/](https://automatetheboringstuff.com/2e/)\n\nSo I can view it offline.", "author_fullname": "t2_3xqpnc72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading websites for offline viewing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10yv10h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676043454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you all use? I&amp;#39;ve tried HTTrack before but always struggle to get it to work correctly, or I should say how I want it. To just do all links on the site and not follow links on forever. I am most likely setting it up wrong.&lt;/p&gt;\n\n&lt;p&gt;Away tomorrow for a week and wanted to grab&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://automatetheboringstuff.com/2e/\"&gt;https://automatetheboringstuff.com/2e/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So I can view it offline.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10yv10h", "is_robot_indexable": true, "report_reasons": null, "author": "steviefaux", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yv10h/downloading_websites_for_offline_viewing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yv10h/downloading_websites_for_offline_viewing/", "subreddit_subscribers": 669313, "created_utc": 1676043454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So my mother (love her to death) has captured every moment since her children's birth with digital cameras, and eventually, the iPhone, and keeps a 2TB iCloud storage option to store all said family memories. We keep another terabyte or so on physical images, and I am working on digitizing and storing on ours family RAID 5 server with total 6 TB capacity.\n\nI was sitting there today loading on 2TB of high school football game recordings when I thought about the future of our families data. When my siblings are out of the house, and we have families and there are grandchildren, family gatherings, more data, wanting to look through old data, etc, etc, I had a data existential crisis.\n\nI am the techie in the family, so it is going to be my responsibility to store all of our data long term, and I ideally don't want to keep expanding my server, because then I'll make it an excuse to finally buy a Storinator, but I digress.\n\n**TL;DR: how the hell should I responsibly, securely, and reliably store (and archive) multiple TBs of family memories so they have a long life, and we can look at the memories in the future?** LTO? AWS plan with Snowball? Keep it on spinning? Help!", "author_fullname": "t2_4e5cpd3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 (and growing) terabytes of data!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yeeo4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675998080.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675997810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my mother (love her to death) has captured every moment since her children&amp;#39;s birth with digital cameras, and eventually, the iPhone, and keeps a 2TB iCloud storage option to store all said family memories. We keep another terabyte or so on physical images, and I am working on digitizing and storing on ours family RAID 5 server with total 6 TB capacity.&lt;/p&gt;\n\n&lt;p&gt;I was sitting there today loading on 2TB of high school football game recordings when I thought about the future of our families data. When my siblings are out of the house, and we have families and there are grandchildren, family gatherings, more data, wanting to look through old data, etc, etc, I had a data existential crisis.&lt;/p&gt;\n\n&lt;p&gt;I am the techie in the family, so it is going to be my responsibility to store all of our data long term, and I ideally don&amp;#39;t want to keep expanding my server, because then I&amp;#39;ll make it an excuse to finally buy a Storinator, but I digress.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR: how the hell should I responsibly, securely, and reliably store (and archive) multiple TBs of family memories so they have a long life, and we can look at the memories in the future?&lt;/strong&gt; LTO? AWS plan with Snowball? Keep it on spinning? Help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10yeeo4", "is_robot_indexable": true, "report_reasons": null, "author": "fuuny_doe", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yeeo4/3_and_growing_terabytes_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yeeo4/3_and_growing_terabytes_of_data/", "subreddit_subscribers": 669313, "created_utc": 1675997810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm sure they use something sensible and modern. Comparing nopic and with pics versions suggests about half the file size of the full version is images.\n\nAnyone know where I can read about the image compression/file type used by. Zim files?", "author_fullname": "t2_jr9a5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What image file formats do the .zim Wikipedia Dumps use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ycvw9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675993526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sure they use something sensible and modern. Comparing nopic and with pics versions suggests about half the file size of the full version is images.&lt;/p&gt;\n\n&lt;p&gt;Anyone know where I can read about the image compression/file type used by. Zim files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10ycvw9", "is_robot_indexable": true, "report_reasons": null, "author": "verrucagnome", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ycvw9/what_image_file_formats_do_the_zim_wikipedia/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ycvw9/what_image_file_formats_do_the_zim_wikipedia/", "subreddit_subscribers": 669313, "created_utc": 1675993526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, \nI frequent several art-focused subreddits and would love to archive the images that are posted there. They are typically posted as image posts with the artist and name of the art in the post title and the image is hosted on reddit, artstation, imgur, etc. So ideally a downloader would pull both the image and the post title so the name of the art is preserved. \n\nIs there a good downloader that would allow me to download posts from these subreddits? I found a tool called RedditDownloader on GitHub but I'm not sure how good it is or how easy it is to use. Would I be able to automate it so it runs every month and downloads all the posts since the last time it ran?  What sort of computer would I need to run this? (right now I have a business-grade laptop that's a few years old, but I'm planning to build a new pc that would be primarily focused on gaming, but would also have the storage space to do these kinds of things.) \n\nSorry if these questions are common or easy to answer, I'm very knew to this kind of thing.", "author_fullname": "t2_5mr4b04t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to download posts from art-focused subreddits?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1o8w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675966368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, \nI frequent several art-focused subreddits and would love to archive the images that are posted there. They are typically posted as image posts with the artist and name of the art in the post title and the image is hosted on reddit, artstation, imgur, etc. So ideally a downloader would pull both the image and the post title so the name of the art is preserved. &lt;/p&gt;\n\n&lt;p&gt;Is there a good downloader that would allow me to download posts from these subreddits? I found a tool called RedditDownloader on GitHub but I&amp;#39;m not sure how good it is or how easy it is to use. Would I be able to automate it so it runs every month and downloads all the posts since the last time it ran?  What sort of computer would I need to run this? (right now I have a business-grade laptop that&amp;#39;s a few years old, but I&amp;#39;m planning to build a new pc that would be primarily focused on gaming, but would also have the storage space to do these kinds of things.) &lt;/p&gt;\n\n&lt;p&gt;Sorry if these questions are common or easy to answer, I&amp;#39;m very knew to this kind of thing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y1o8w", "is_robot_indexable": true, "report_reasons": null, "author": "Lastdudealive46", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y1o8w/best_way_to_download_posts_from_artfocused/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y1o8w/best_way_to_download_posts_from_artfocused/", "subreddit_subscribers": 669313, "created_utc": 1675966368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Wanting to record streaming content via ROKU to DVD or HDD. What's the most cost effective way to do this? Thanks", "author_fullname": "t2_b0p0a1f2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most cost effective way to record streaming content to DVD or HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1ic9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675965987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wanting to record streaming content via ROKU to DVD or HDD. What&amp;#39;s the most cost effective way to do this? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10y1ic9", "is_robot_indexable": true, "report_reasons": null, "author": "galaxy18r", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y1ic9/most_cost_effective_way_to_record_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y1ic9/most_cost_effective_way_to_record_streaming/", "subreddit_subscribers": 669313, "created_utc": 1675965987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Decided to try a couple of refurbished drives. How would I check to see how many hours the drive has been powered on with HD Sentinel or similar, or does clearing smart data reset all of that?", "author_fullname": "t2_zqrvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to check refurbished drive stats", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yn1kl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676022166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Decided to try a couple of refurbished drives. How would I check to see how many hours the drive has been powered on with HD Sentinel or similar, or does clearing smart data reset all of that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10yn1kl", "is_robot_indexable": true, "report_reasons": null, "author": "themayor1975", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yn1kl/how_to_check_refurbished_drive_stats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yn1kl/how_to_check_refurbished_drive_stats/", "subreddit_subscribers": 669313, "created_utc": 1676022166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone, here is my backup situation right now:\n\n1. 2tb main drive\n2. 2tb backup drive\n3. backed up important stuff to iCloud\n\nI just once a month wipe my backup drive and copy the entire main drive to it which is responsible for 90% of my file management times and I\u2019m hoping to cut that down.\n\nOften times it\u2019s just like &lt;1gb of data change and it seems inefficient doing that. \n\nIs there a way to set up my backup drive so it just scans for changes in the main drive and just implements those? On Mac if that impacts the answer\n\nThank you", "author_fullname": "t2_tqnq009f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about updating backup drive more efficiently", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ycgdd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675992333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, here is my backup situation right now:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;2tb main drive&lt;/li&gt;\n&lt;li&gt;2tb backup drive&lt;/li&gt;\n&lt;li&gt;backed up important stuff to iCloud&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I just once a month wipe my backup drive and copy the entire main drive to it which is responsible for 90% of my file management times and I\u2019m hoping to cut that down.&lt;/p&gt;\n\n&lt;p&gt;Often times it\u2019s just like &amp;lt;1gb of data change and it seems inefficient doing that. &lt;/p&gt;\n\n&lt;p&gt;Is there a way to set up my backup drive so it just scans for changes in the main drive and just implements those? On Mac if that impacts the answer&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ycgdd", "is_robot_indexable": true, "report_reasons": null, "author": "ihadtomakeajoke", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ycgdd/question_about_updating_backup_drive_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ycgdd/question_about_updating_backup_drive_more/", "subreddit_subscribers": 669313, "created_utc": 1675992333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI need an external HDD that will meet those requirements:\n\n1. Not less than 10-12tb\n2. Not SMR\n3. Has decent write/read speeds\n4. Will be connected through my Asus AX86U router\\*\\*\n\n&amp;#x200B;\n\nWhat I discovered recently is that if I plug in my existing slow-ass 2tb hdd to my router through usb, I can use my Mac to download movies through QBittorrent straight to that drive that's connected to router and then on my LG C7 tv watch it directly from disk connected to router (it sees it as a audio/video source). I need an external HDD that I will connect to router, keep it connected at all times, download movies straight to it (movies I download are usually 20-50GB in single file - 4k HDR Dolby Atmos rips) and then watch 'em on my TV.\n\nI bought this: [https://www.amazon.com/dp/B08KTQWV7Z?psc=1&amp;ref=ppx\\_yo2ov\\_dt\\_b\\_product\\_details](https://www.amazon.com/dp/B08KTQWV7Z?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details)\n\nand it seemed fine at first, but my downloads to it were fine for the first 100-200megs (download speed around 8MBps) then it would slow down drastically, to around 1-200kBps. I read some reviews and people said that hdd is SMR so I figured that could be the issue.\n\nAlso, would be a plus if I could also set a limited data folder for Time Machine backups of my Mac (that router let's me set up such folder on usb drive).\n\nI guess I could buy something like Synology with two 12tb Ironwolves but my TV won't be able to read movies from that :(\n\nAny suggestions, ideas, tips or advice where else I should do my own research?\n\n&amp;#x200B;\n\nThanks and sorry if this is not the right place to ask/is a dumb question.", "author_fullname": "t2_tmkvdi7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advise choosing external HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yby6m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675990938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I need an external HDD that will meet those requirements:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Not less than 10-12tb&lt;/li&gt;\n&lt;li&gt;Not SMR&lt;/li&gt;\n&lt;li&gt;Has decent write/read speeds&lt;/li&gt;\n&lt;li&gt;Will be connected through my Asus AX86U router**&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What I discovered recently is that if I plug in my existing slow-ass 2tb hdd to my router through usb, I can use my Mac to download movies through QBittorrent straight to that drive that&amp;#39;s connected to router and then on my LG C7 tv watch it directly from disk connected to router (it sees it as a audio/video source). I need an external HDD that I will connect to router, keep it connected at all times, download movies straight to it (movies I download are usually 20-50GB in single file - 4k HDR Dolby Atmos rips) and then watch &amp;#39;em on my TV.&lt;/p&gt;\n\n&lt;p&gt;I bought this: &lt;a href=\"https://www.amazon.com/dp/B08KTQWV7Z?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details\"&gt;https://www.amazon.com/dp/B08KTQWV7Z?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;and it seemed fine at first, but my downloads to it were fine for the first 100-200megs (download speed around 8MBps) then it would slow down drastically, to around 1-200kBps. I read some reviews and people said that hdd is SMR so I figured that could be the issue.&lt;/p&gt;\n\n&lt;p&gt;Also, would be a plus if I could also set a limited data folder for Time Machine backups of my Mac (that router let&amp;#39;s me set up such folder on usb drive).&lt;/p&gt;\n\n&lt;p&gt;I guess I could buy something like Synology with two 12tb Ironwolves but my TV won&amp;#39;t be able to read movies from that :(&lt;/p&gt;\n\n&lt;p&gt;Any suggestions, ideas, tips or advice where else I should do my own research?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks and sorry if this is not the right place to ask/is a dumb question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10yby6m", "is_robot_indexable": true, "report_reasons": null, "author": "CR7KRUL", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yby6m/need_advise_choosing_external_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yby6m/need_advise_choosing_external_hdd/", "subreddit_subscribers": 669313, "created_utc": 1675990938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone personally record or know of a resource that datahoards historical news broadcasts?\n\nI was recognized in a short clip about the Surf City Marathon in Huntington Beach on 2/5/23 and would like to find a copy of it. I was told it was the 9:00 PM broadcast on KTLA 5; it's probably a 30-second clip. Any help would be greatly appreciated.", "author_fullname": "t2_b7oog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "KTLA 5 Los Angeles News Broadcasts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ybdch", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675989345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone personally record or know of a resource that datahoards historical news broadcasts?&lt;/p&gt;\n\n&lt;p&gt;I was recognized in a short clip about the Surf City Marathon in Huntington Beach on 2/5/23 and would like to find a copy of it. I was told it was the 9:00 PM broadcast on KTLA 5; it&amp;#39;s probably a 30-second clip. Any help would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ybdch", "is_robot_indexable": true, "report_reasons": null, "author": "blinkin2000", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ybdch/ktla_5_los_angeles_news_broadcasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ybdch/ktla_5_los_angeles_news_broadcasts/", "subreddit_subscribers": 669313, "created_utc": 1675989345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The other day someone on reddit posted a great program that will take large folders of videos and do batch conversions on them to retain quality but save space.  \n\n\nIt wasnt handbrake or ffmpeg.", "author_fullname": "t2_4u6au", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help remembering a Video Converting Program that is designed to convert entire librarys to more efficent codecs (Not Handbrake or FFMPEG)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y8ha8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675982137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The other day someone on reddit posted a great program that will take large folders of videos and do batch conversions on them to retain quality but save space.  &lt;/p&gt;\n\n&lt;p&gt;It wasnt handbrake or ffmpeg.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y8ha8", "is_robot_indexable": true, "report_reasons": null, "author": "dec1mus", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y8ha8/need_help_remembering_a_video_converting_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y8ha8/need_help_remembering_a_video_converting_program/", "subreddit_subscribers": 669313, "created_utc": 1675982137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys, I just found this subreddit and it seems the best place to ask for info. I have a Samsung 980 PRO EVO 2TB NVME and also a Samsung 870 PRO EVO 2TB SSD (both updated to the latest firmware). I'm currently in the process of migrating OS from SSD to NVME.\n\nThe main purpose is quick swapping drives in case of failure as I use the machine for work. I have offsite backup for important data but I need this to minimise downtime. \n\nI had an SSD suddenly die and I don't want to go through that again. \n\nAny advice would be appreciated, I've never set up a RAID config before so I don't know where to start. I have a few questions.\n\n1. Is it better to use software or bios RAID?\n\n2. If software, what do you recommend? (preferably free software)\n\n3. Will I have any issues running RAID 1 with this setup, or would it slow my NVME down to the same speed as the SSD?\n\n4. Will it cause any issues using overprovision or other settings available within Samsung Magician?", "author_fullname": "t2_1yz5nqq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID 1 with NVME main drive and SSD secondary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y5frd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675974942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I just found this subreddit and it seems the best place to ask for info. I have a Samsung 980 PRO EVO 2TB NVME and also a Samsung 870 PRO EVO 2TB SSD (both updated to the latest firmware). I&amp;#39;m currently in the process of migrating OS from SSD to NVME.&lt;/p&gt;\n\n&lt;p&gt;The main purpose is quick swapping drives in case of failure as I use the machine for work. I have offsite backup for important data but I need this to minimise downtime. &lt;/p&gt;\n\n&lt;p&gt;I had an SSD suddenly die and I don&amp;#39;t want to go through that again. &lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated, I&amp;#39;ve never set up a RAID config before so I don&amp;#39;t know where to start. I have a few questions.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Is it better to use software or bios RAID?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If software, what do you recommend? (preferably free software)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Will I have any issues running RAID 1 with this setup, or would it slow my NVME down to the same speed as the SSD?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Will it cause any issues using overprovision or other settings available within Samsung Magician?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y5frd", "is_robot_indexable": true, "report_reasons": null, "author": "spboss91", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y5frd/raid_1_with_nvme_main_drive_and_ssd_secondary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y5frd/raid_1_with_nvme_main_drive_and_ssd_secondary/", "subreddit_subscribers": 669313, "created_utc": 1675974942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "what i am trying to do? host a static website of a dynamic website. \n\nthere are some old websites written in php and mysql backend. about 1500 pages. the site is pretty heavy because of sql queries and stuff.\n\n\nhow do i take a snapshot of the website that i can host? with all links intact. i tried archivebox but it is not self hostable for the public without archivebox interface coming in between.", "author_fullname": "t2_v1kgt9f8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to take a host-able snapshot?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1bcp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675965572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;what i am trying to do? host a static website of a dynamic website. &lt;/p&gt;\n\n&lt;p&gt;there are some old websites written in php and mysql backend. about 1500 pages. the site is pretty heavy because of sql queries and stuff.&lt;/p&gt;\n\n&lt;p&gt;how do i take a snapshot of the website that i can host? with all links intact. i tried archivebox but it is not self hostable for the public without archivebox interface coming in between.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y1bcp", "is_robot_indexable": true, "report_reasons": null, "author": "noodleswind", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y1bcp/how_to_take_a_hostable_snapshot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y1bcp/how_to_take_a_hostable_snapshot/", "subreddit_subscribers": 669313, "created_utc": 1675965572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi - I was wondering if anyone had hoard of Pluralsight/ACG which they downloaded when it was free which could be shared with the community?", "author_fullname": "t2_3f188dy0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pluralsight/ACG hoard", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y02my", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675962721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi - I was wondering if anyone had hoard of Pluralsight/ACG which they downloaded when it was free which could be shared with the community?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y02my", "is_robot_indexable": true, "report_reasons": null, "author": "billysew", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y02my/pluralsightacg_hoard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y02my/pluralsightacg_hoard/", "subreddit_subscribers": 669313, "created_utc": 1675962721.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}