{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So after building a near real time pipeline with super complex parsing and data quality in record speed your dashboard gal/guy sends an email to C-level showing her/his awesome dashboard and just cc you in the email.\n\n\n\nNobody would know \n\n\u2014 time spent to find optimal parallelism\n\n\n\u2014 time spent for solving weird dq issues\n\n\n\u2014 time spent to make the pipeline dynamically handle schema change\n\n\n\u2014 time spent for threading it all together in orchestrator\n\n\nAfter all these years still hurts on how thankless this job is.\n\nOnly lesson : Thank your plumber next time you meet him.", "author_fullname": "t2_5ifseipu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thanking my plumber every time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ybytx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 127, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 127, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675990984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So after building a near real time pipeline with super complex parsing and data quality in record speed your dashboard gal/guy sends an email to C-level showing her/his awesome dashboard and just cc you in the email.&lt;/p&gt;\n\n&lt;p&gt;Nobody would know &lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent to find optimal parallelism&lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent for solving weird dq issues&lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent to make the pipeline dynamically handle schema change&lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent for threading it all together in orchestrator&lt;/p&gt;\n\n&lt;p&gt;After all these years still hurts on how thankless this job is.&lt;/p&gt;\n\n&lt;p&gt;Only lesson : Thank your plumber next time you meet him.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ybytx", "is_robot_indexable": true, "report_reasons": null, "author": "Smart-Weird", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ybytx/thanking_my_plumber_every_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ybytx/thanking_my_plumber_every_time/", "subreddit_subscribers": 89080, "created_utc": 1675990984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019m a manager at Big 4 consulting and I\u2019m trying to find resources which I can show my engineers to help them understand best practices &amp; common patterns in developing production ready applications / code bases. \n\nSome things I can think of that I\u2019ve had to teach them:\n\n1. Git\n2. Environment specific parameters\n3. Writing Unittests, mocking cloud services\n4. What is CI/CD", "author_fullname": "t2_ay99iuoj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do we learn SWE best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ye0ji", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675996673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m a manager at Big 4 consulting and I\u2019m trying to find resources which I can show my engineers to help them understand best practices &amp;amp; common patterns in developing production ready applications / code bases. &lt;/p&gt;\n\n&lt;p&gt;Some things I can think of that I\u2019ve had to teach them:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Git&lt;/li&gt;\n&lt;li&gt;Environment specific parameters&lt;/li&gt;\n&lt;li&gt;Writing Unittests, mocking cloud services&lt;/li&gt;\n&lt;li&gt;What is CI/CD&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ye0ji", "is_robot_indexable": true, "report_reasons": null, "author": "Rich_Repair", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ye0ji/how_do_we_learn_swe_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ye0ji/how_do_we_learn_swe_best_practices/", "subreddit_subscribers": 89080, "created_utc": 1675996673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI need to build an Azure data infrastructure, which ingests data from different on-prem source systems (but mainly from relational databases and from text files), loads the data to some sort of data lake (it could be something else too, like delta lake, all that matters is to have a platform from which various departments can access the data for various reasons), makes it possible to do transformations on this data (we are possibly talking about big amounts), then load the data to some kind of SQL Database / DWH System. This final platform needs to be something that provides seamless integration with cloud Dynamics 365 and Power BI.\n\nMy question for you is what kind of architecture would you use in this situation?\n\nSo far, I have explored the following options:\n\n- Data Factory / Azure Synapse Pipelines for copying data from on-prem to the data lake, and copying data from data lake to SQL Database. Also a possible option for starting scheduled Databricks jobs\n- Azure Data Lake Gen 2 as a data lake -&gt; this would be where raw, ingested data is stored (mainly text files, parquet files)\n- Azure Databricks  -&gt; For doing transformations on the ingested data. I have also explored the possibility of using Azure Delta Lake options. I am not sure what I the best practice in this situation. Is it worth to maintain a Data Lake AND a Delta Lake?\n- Azure Synapse Serverless / Dedicated SQL Pool OR SQL database / managed instance -&gt; this would be the platform which is the backend of the Dynamics 365 system. I'm really not sure if I should use Synapse for this whole process. It might be overkill for my needs.\n\nI understand this is probably a bit chaotic, Azure architecture is new to me. I'd appreciate any advice, any tips on best practices which help me decide between my options. \n\nTL;DR: Need to set up an Azure infrastructure for integrating data from on-prem DB and text files, storing raw data available for multiple use cases, enabling scheduled transformations, and providing some sort of SQL DB which can easily be integrated with Dynamics 365.", "author_fullname": "t2_vun99h9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for bringing data to Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xvxpw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675952523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I need to build an Azure data infrastructure, which ingests data from different on-prem source systems (but mainly from relational databases and from text files), loads the data to some sort of data lake (it could be something else too, like delta lake, all that matters is to have a platform from which various departments can access the data for various reasons), makes it possible to do transformations on this data (we are possibly talking about big amounts), then load the data to some kind of SQL Database / DWH System. This final platform needs to be something that provides seamless integration with cloud Dynamics 365 and Power BI.&lt;/p&gt;\n\n&lt;p&gt;My question for you is what kind of architecture would you use in this situation?&lt;/p&gt;\n\n&lt;p&gt;So far, I have explored the following options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Factory / Azure Synapse Pipelines for copying data from on-prem to the data lake, and copying data from data lake to SQL Database. Also a possible option for starting scheduled Databricks jobs&lt;/li&gt;\n&lt;li&gt;Azure Data Lake Gen 2 as a data lake -&amp;gt; this would be where raw, ingested data is stored (mainly text files, parquet files)&lt;/li&gt;\n&lt;li&gt;Azure Databricks  -&amp;gt; For doing transformations on the ingested data. I have also explored the possibility of using Azure Delta Lake options. I am not sure what I the best practice in this situation. Is it worth to maintain a Data Lake AND a Delta Lake?&lt;/li&gt;\n&lt;li&gt;Azure Synapse Serverless / Dedicated SQL Pool OR SQL database / managed instance -&amp;gt; this would be the platform which is the backend of the Dynamics 365 system. I&amp;#39;m really not sure if I should use Synapse for this whole process. It might be overkill for my needs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I understand this is probably a bit chaotic, Azure architecture is new to me. I&amp;#39;d appreciate any advice, any tips on best practices which help me decide between my options. &lt;/p&gt;\n\n&lt;p&gt;TL;DR: Need to set up an Azure infrastructure for integrating data from on-prem DB and text files, storing raw data available for multiple use cases, enabling scheduled transformations, and providing some sort of SQL DB which can easily be integrated with Dynamics 365.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xvxpw", "is_robot_indexable": true, "report_reasons": null, "author": "justadataengineer", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xvxpw/best_practices_for_bringing_data_to_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xvxpw/best_practices_for_bringing_data_to_azure/", "subreddit_subscribers": 89080, "created_utc": 1675952523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Aggregate Combinators in ClickHouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_10ymgce", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RFnbn_Th7B7VHl0xfx7w8R9Igi5UxtiPf6cmlU6-AFs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676021073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "clickhouse.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://clickhouse.com/blog/aggregate-functions-combinators-in-clickhouse-for-arrays-maps-and-states", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?auto=webp&amp;v=enabled&amp;s=cd239d606e97421b4ecf0fbe89750920297358e4", "width": 1576, "height": 888}, "resolutions": [{"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3268a14fd20c70a15f1049a9cc190eb4e7137715", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a15d0c264885ec7e06180f237faebe2d7e404416", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=999dc6e15cd6329e185392ace1625d6f512934a6", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=863abd8d5a71065339ac5d671a03f1dce0b7126e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2079f3c7d5c8d3408a80d28ec7fc7dd8f5a2a374", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/vd2eervNd-SJGKQQ5cTNRSg-Bk2bWu_TVeO6xRH_RnQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b594b457ea42efb2329dcf45c8f397ed53c650e", "width": 1080, "height": 608}], "variants": {}, "id": "EszfP3ZeRjCe_uXMGl4zs8W6bXQkIcIFjknp0SByRjw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10ymgce", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ymgce/using_aggregate_combinators_in_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://clickhouse.com/blog/aggregate-functions-combinators-in-clickhouse-for-arrays-maps-and-states", "subreddit_subscribers": 89080, "created_utc": 1676021073.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I am going through the understanding of  end to end data engineering process. I understand how things work, however, I have not found one resource to understand how the data is saved in such cloud storage by the enterprises?\n\nI mean, I understand data can come from anywhere like, website, mobile application or any app that is storing users details like facebooks user details. Now how does these data like, connected into (injested) into cloud storage? How are they mounted? A few real world examples or cases can help to understand. \n\nFor example: I like a post, or create a user account in Facebook, this data is saved in some database which is mounted to cloud storage? is this is it? and we go on with next from there?", "author_fullname": "t2_lngymjsb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the idle scenario where a data in stored in azure blog or any cloud storage like S3.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yb413", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675988622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I am going through the understanding of  end to end data engineering process. I understand how things work, however, I have not found one resource to understand how the data is saved in such cloud storage by the enterprises?&lt;/p&gt;\n\n&lt;p&gt;I mean, I understand data can come from anywhere like, website, mobile application or any app that is storing users details like facebooks user details. Now how does these data like, connected into (injested) into cloud storage? How are they mounted? A few real world examples or cases can help to understand. &lt;/p&gt;\n\n&lt;p&gt;For example: I like a post, or create a user account in Facebook, this data is saved in some database which is mounted to cloud storage? is this is it? and we go on with next from there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10yb413", "is_robot_indexable": true, "report_reasons": null, "author": "iamdhage", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yb413/what_is_the_idle_scenario_where_a_data_in_stored/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yb413/what_is_the_idle_scenario_where_a_data_in_stored/", "subreddit_subscribers": 89080, "created_utc": 1675988622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_zus64vk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data imputation in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_10y9rwp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/upxsp65nnsAGtTc-gAoMDIllQnv_ZRo1odB1CqnmtpQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675985209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hubs.li", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://hubs.li/Q01BW4NT0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?auto=webp&amp;v=enabled&amp;s=7e35df19320e1aad03aadd059f628000aeead487", "width": 800, "height": 526}, "resolutions": [{"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6ca36cd4a5a93b0c47d6e70a1c198501108da83", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=416c8300fc41b7723d9ebe3786248688a509aeb1", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e656629a9830f76ea20131566c7d5cb268153ca", "width": 320, "height": 210}, {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f68309c60f31f46fdfc36020d446292538d58788", "width": 640, "height": 420}], "variants": {}, "id": "nGnUWldmoOF3Mysh8RqBH_abXJv3jq3i_pyrYZ3w6oc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10y9rwp", "is_robot_indexable": true, "report_reasons": null, "author": "oli_k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y9rwp/data_imputation_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://hubs.li/Q01BW4NT0", "subreddit_subscribers": 89080, "created_utc": 1675985209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to ask what skills can I acquire now and put on my resume to get noticed by hiring managers and recruiters. I graduated with a CS bachelors degree in 2021, tried to get a data science job, couldn't get one, enrolled in masters but will be dropping out in the middle because I need to find a job asap. I look at data engineer job postings and some of them have an overwhelming number of tools listed for an entry level job. At the same time, the most recurring skills I keep hearing are python, sql and data warehousing.Can I honestly score a DE position with the knowledge of just these 3? My dead end search for a data science position taught me that DS isn't really an area where recent graduates are accepted. That is why I'm looking for DE roles now because of transferable skills and newbie friendliness but it looks like I'm going down the same road again. Plus, I am also at a stalemate for DE projects. For DS, I've made web apps that take input and predict a result. But I'm not able to come up with ideas for DE projects. Can scraping data off the web and loading it into a CSV qualify? Please help.", "author_fullname": "t2_e6tzy246", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting into DE straight after college?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yqih5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676031338.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to ask what skills can I acquire now and put on my resume to get noticed by hiring managers and recruiters. I graduated with a CS bachelors degree in 2021, tried to get a data science job, couldn&amp;#39;t get one, enrolled in masters but will be dropping out in the middle because I need to find a job asap. I look at data engineer job postings and some of them have an overwhelming number of tools listed for an entry level job. At the same time, the most recurring skills I keep hearing are python, sql and data warehousing.Can I honestly score a DE position with the knowledge of just these 3? My dead end search for a data science position taught me that DS isn&amp;#39;t really an area where recent graduates are accepted. That is why I&amp;#39;m looking for DE roles now because of transferable skills and newbie friendliness but it looks like I&amp;#39;m going down the same road again. Plus, I am also at a stalemate for DE projects. For DS, I&amp;#39;ve made web apps that take input and predict a result. But I&amp;#39;m not able to come up with ideas for DE projects. Can scraping data off the web and loading it into a CSV qualify? Please help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10yqih5", "is_robot_indexable": true, "report_reasons": null, "author": "al8k", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yqih5/getting_into_de_straight_after_college/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yqih5/getting_into_de_straight_after_college/", "subreddit_subscribers": 89080, "created_utc": 1676031338.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What should I prepare for new grad application and interview?", "author_fullname": "t2_cl5t0lrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer New Grad Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yfnqa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676001660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What should I prepare for new grad application and interview?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10yfnqa", "is_robot_indexable": true, "report_reasons": null, "author": "Unlucky-Difficulty86", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yfnqa/data_engineer_new_grad_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yfnqa/data_engineer_new_grad_interview/", "subreddit_subscribers": 89080, "created_utc": 1676001660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I have been working with DE for about 3 years and my current company after a long discussion decided to settle for SSIS, does anyone have some good courses or source material on it? I know it can\u2019t be that hard to learn for someone with SQL and coding background but any help is appreciated.", "author_fullname": "t2_592klrfg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSIS material", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y6hhl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675977451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I have been working with DE for about 3 years and my current company after a long discussion decided to settle for SSIS, does anyone have some good courses or source material on it? I know it can\u2019t be that hard to learn for someone with SQL and coding background but any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10y6hhl", "is_robot_indexable": true, "report_reasons": null, "author": "pvic234", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y6hhl/ssis_material/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10y6hhl/ssis_material/", "subreddit_subscribers": 89080, "created_utc": 1675977451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been recently asked to look into optimizing some notebooks written using spark sql and those notebooks sometime have 4 levels of nesting in the from clause. Does this effect performance? vs writing the logic in different data frames considering Spark optimizes the query plan anyway. If so how?", "author_fullname": "t2_s33vjakh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "sub queries in spark - bad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1kwq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675966154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been recently asked to look into optimizing some notebooks written using spark sql and those notebooks sometime have 4 levels of nesting in the from clause. Does this effect performance? vs writing the logic in different data frames considering Spark optimizes the query plan anyway. If so how?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10y1kwq", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded_Box7963", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y1kwq/sub_queries_in_spark_bad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10y1kwq/sub_queries_in_spark_bad/", "subreddit_subscribers": 89080, "created_utc": 1675966154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I left an SSIS ETL gig about 8 years ago to join a much bigger company as a Business Intelligence (glorified reporting guy). I have kept working on side gigs using SSIS and DBT\u2026 now, because I have a giant gap of not related Data engineering positions.  I get an automatic rejection from HR and I\u2019m at lost.  I hate doing tableau and making things pretty for idiotic reasons.  I need to get back home and I need some feedback on how to fix my resume.", "author_fullname": "t2_4i6f73l2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get back into data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10yssm1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676037807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I left an SSIS ETL gig about 8 years ago to join a much bigger company as a Business Intelligence (glorified reporting guy). I have kept working on side gigs using SSIS and DBT\u2026 now, because I have a giant gap of not related Data engineering positions.  I get an automatic rejection from HR and I\u2019m at lost.  I hate doing tableau and making things pretty for idiotic reasons.  I need to get back home and I need some feedback on how to fix my resume.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10yssm1", "is_robot_indexable": true, "report_reasons": null, "author": "SadDogOwner27", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yssm1/how_to_get_back_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yssm1/how_to_get_back_into_data_engineering/", "subreddit_subscribers": 89080, "created_utc": 1676037807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'm new here.\n\n**tl;dr: need help creating a pipeline, not a data engineer**\n\n&amp;#x200B;\n\nI work solely on the front side, data governance/management, with some modeling, analysis, and visualization. Above average interest in the back-end, infrastructure, systems design, studied CS before B-school. After doing research on job openings for a manager/leadership role, in 80% one of the things I should have is essential knowledge and experience with data pipelines. Not to actually do pipeline-related tasks, but to understand the entire journey from source to delivery.\n\nSince I'm interested in that side anyway, figured it would be a great challenge to create my own pipeline, nothing fancy with a gazillion tasks, lines, jobs, requiring high-tier cloud subscriptions, but something simple that ticks off the box on common phases.\n\nWhat I like to make is this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/fpkwwrgorbha1.png?width=1623&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=19cd66c464b0d6de96fbcfcb54f20ff353833128\n\nThree major blocks.\n\n\\- Starting with the one on the right, I have experience with PowerBI and know how to connect a data source.  \n\\- In the center, I have experience with SQL databases, but *not* connecting them  \n\\- On the left, this is the part I do not have experience in at all. I googled on creating random data and found Python tutorials, somewhat able to read and understand the code.\n\nMust-haves:\n\n\\- first two blocks in the cloud, providers with free tier is highly preferred  \n\\- automation of three daily jobs (as far as I can tell); creating the data, storing it, sending it and store it in the second database\n\nNice-to-have\n\n\\- having the data creation and first storage in the same app\n\nAbsolutely-not-have\n\n\\- anything that requires a Command Line, I still have a trauma on this from CS school\n\n&amp;#x200B;\n\n**Objective**\n\nHaving a daily feed of new data in the second database, fully automated\n\n**Goal**\n\nAbility to refresh my BI dashboard/reports on a weekly/daily basis with new and unknown data, ready to be analyzed.\n\n&amp;#x200B;\n\n**Need help with**\n\n\\- does my flow makes sense?  \n\\- cloud app with free tier recommendations for the first phase, if Python is great for this, the app would need both Python and SQL, not sure if this kind of app exist; if you think there is a better solution, would love to learn about it  \n\\- how do I get the data from block 1 to block 2, all automated? what kind of app do I need for this, how are these apps called? something like Hevo or Keboola, or am I confused on what they do?\n\nIn case I'm missing things, feel free to mention.\n\nThanks in advance.", "author_fullname": "t2_kh3ubtce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a pipeline as non-engineer - Need Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "media_metadata": {"fpkwwrgorbha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/fpkwwrgorbha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=835eeb1bddf21c38d55b2341caf4d18d9fa5755f"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/fpkwwrgorbha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b7053441140ae6e9a9be23b18bc8211110e59db"}, {"y": 165, "x": 320, "u": "https://preview.redd.it/fpkwwrgorbha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3087ef3d4bfa601a8840cddd3d82b386d134a631"}, {"y": 330, "x": 640, "u": "https://preview.redd.it/fpkwwrgorbha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94f34b46da302456874e65e0f3fc94e61feffd7b"}, {"y": 496, "x": 960, "u": "https://preview.redd.it/fpkwwrgorbha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13028159d458fa31b1b0b351862bee48e7cf6a0f"}, {"y": 558, "x": 1080, "u": "https://preview.redd.it/fpkwwrgorbha1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d8ef44cf47992a189c6037e4683bd0baf271ac1"}], "s": {"y": 839, "x": 1623, "u": "https://preview.redd.it/fpkwwrgorbha1.png?width=1623&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=19cd66c464b0d6de96fbcfcb54f20ff353833128"}, "id": "fpkwwrgorbha1"}}, "name": "t3_10ym8zp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fmqM1uf6pHirZaZn4ECVC6Fc40uXxgDgImD9oNnW3yI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676020678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new here.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;tl;dr: need help creating a pipeline, not a data engineer&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I work solely on the front side, data governance/management, with some modeling, analysis, and visualization. Above average interest in the back-end, infrastructure, systems design, studied CS before B-school. After doing research on job openings for a manager/leadership role, in 80% one of the things I should have is essential knowledge and experience with data pipelines. Not to actually do pipeline-related tasks, but to understand the entire journey from source to delivery.&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;m interested in that side anyway, figured it would be a great challenge to create my own pipeline, nothing fancy with a gazillion tasks, lines, jobs, requiring high-tier cloud subscriptions, but something simple that ticks off the box on common phases.&lt;/p&gt;\n\n&lt;p&gt;What I like to make is this:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fpkwwrgorbha1.png?width=1623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=19cd66c464b0d6de96fbcfcb54f20ff353833128\"&gt;https://preview.redd.it/fpkwwrgorbha1.png?width=1623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=19cd66c464b0d6de96fbcfcb54f20ff353833128&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Three major blocks.&lt;/p&gt;\n\n&lt;p&gt;- Starting with the one on the right, I have experience with PowerBI and know how to connect a data source.&lt;br/&gt;\n- In the center, I have experience with SQL databases, but &lt;em&gt;not&lt;/em&gt; connecting them&lt;br/&gt;\n- On the left, this is the part I do not have experience in at all. I googled on creating random data and found Python tutorials, somewhat able to read and understand the code.&lt;/p&gt;\n\n&lt;p&gt;Must-haves:&lt;/p&gt;\n\n&lt;p&gt;- first two blocks in the cloud, providers with free tier is highly preferred&lt;br/&gt;\n- automation of three daily jobs (as far as I can tell); creating the data, storing it, sending it and store it in the second database&lt;/p&gt;\n\n&lt;p&gt;Nice-to-have&lt;/p&gt;\n\n&lt;p&gt;- having the data creation and first storage in the same app&lt;/p&gt;\n\n&lt;p&gt;Absolutely-not-have&lt;/p&gt;\n\n&lt;p&gt;- anything that requires a Command Line, I still have a trauma on this from CS school&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Having a daily feed of new data in the second database, fully automated&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Ability to refresh my BI dashboard/reports on a weekly/daily basis with new and unknown data, ready to be analyzed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Need help with&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- does my flow makes sense?&lt;br/&gt;\n- cloud app with free tier recommendations for the first phase, if Python is great for this, the app would need both Python and SQL, not sure if this kind of app exist; if you think there is a better solution, would love to learn about it&lt;br/&gt;\n- how do I get the data from block 1 to block 2, all automated? what kind of app do I need for this, how are these apps called? something like Hevo or Keboola, or am I confused on what they do?&lt;/p&gt;\n\n&lt;p&gt;In case I&amp;#39;m missing things, feel free to mention.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ym8zp", "is_robot_indexable": true, "report_reasons": null, "author": "SquidsAndMartians", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ym8zp/creating_a_pipeline_as_nonengineer_need_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ym8zp/creating_a_pipeline_as_nonengineer_need_help/", "subreddit_subscribers": 89080, "created_utc": 1676020678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently out of a job because I recently moved to a new country and sorting out a work permit.\n\nI'm currently volunteering at an NGO, they do not have a data warehouse. They manually manage data on their records management system. I see some ways I can leverage open-source modern data stack tools to automate the processes.\n\nShould finance be an issue for cloud storage, then I can do with them provisioning me with a server-like machine for the on-prem Postgres warehouse. \n\nMy concern however is, since this NGO is funded by the government, I have concerns about how the project can be scalable and won\u2019t eventually become an irrelevant project in the long run.\n\nJust to save me from \u201cif it ain\u2019t broke, don\u2019t fix it\u201d.\n\nI\u2019ll appreciate related experience and/or advice.", "author_fullname": "t2_c36b59fk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Open-Source Data Stack for the NGO I\u2019m volunteering at - it it worth the effort?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yhj68", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676007903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently out of a job because I recently moved to a new country and sorting out a work permit.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently volunteering at an NGO, they do not have a data warehouse. They manually manage data on their records management system. I see some ways I can leverage open-source modern data stack tools to automate the processes.&lt;/p&gt;\n\n&lt;p&gt;Should finance be an issue for cloud storage, then I can do with them provisioning me with a server-like machine for the on-prem Postgres warehouse. &lt;/p&gt;\n\n&lt;p&gt;My concern however is, since this NGO is funded by the government, I have concerns about how the project can be scalable and won\u2019t eventually become an irrelevant project in the long run.&lt;/p&gt;\n\n&lt;p&gt;Just to save me from \u201cif it ain\u2019t broke, don\u2019t fix it\u201d.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll appreciate related experience and/or advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10yhj68", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-Flow220", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yhj68/building_a_opensource_data_stack_for_the_ngo_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yhj68/building_a_opensource_data_stack_for_the_ngo_im/", "subreddit_subscribers": 89080, "created_utc": 1676007903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I regularly receive files from 3rd parties that I import into our database as temporary tables.  I'm able to join most of the records with queries, matching on people's names and other fields.  But a small percentage need to be done manually.  Is there a tool out there for filtering/searching two data sets and using the mouse to drag and drop to match the remaining records together?  I'm getting tired of copying and pasting keys into an update statement to match these records.\n\nWhen I google this problem, there are lots of fuzzy-logic tools, but I'm looking for human-operated manual matching.", "author_fullname": "t2_71qwor1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a GUI tool for manually matching and setting foreign keys?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xvucj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675955029.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675952277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I regularly receive files from 3rd parties that I import into our database as temporary tables.  I&amp;#39;m able to join most of the records with queries, matching on people&amp;#39;s names and other fields.  But a small percentage need to be done manually.  Is there a tool out there for filtering/searching two data sets and using the mouse to drag and drop to match the remaining records together?  I&amp;#39;m getting tired of copying and pasting keys into an update statement to match these records.&lt;/p&gt;\n\n&lt;p&gt;When I google this problem, there are lots of fuzzy-logic tools, but I&amp;#39;m looking for human-operated manual matching.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xvucj", "is_robot_indexable": true, "report_reasons": null, "author": "Jz7t6ak5", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xvucj/is_there_a_gui_tool_for_manually_matching_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xvucj/is_there_a_gui_tool_for_manually_matching_and/", "subreddit_subscribers": 89080, "created_utc": 1675952277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Situation: I have a pyspark job that needs to be deployed on a cluster that has SparkonK8s support. A pipeline is in place that builds an image and pushes an image to azure container registry and now there is Airflow job that refers to that image having a DAG that runs this pyspark job periodically. \nI have secrets that are being utilised by this job that need to be populated to the environment as environment varibales. These secrets are placed in azure ado library as a variable group connected to a keyvault and have been authorised for pipeline usage.\n\nProblem: Now I have an issue populating the environment variables with these secrets in the environment variables.\n\nMy approach:\n1.  I have tried populating the secrets through the pipeline itself using Bash @3 task/Powershell@2 task. I try to print them in the pipeline and they are successfully printed as well. But when i run kubectl apply to my spark config file to check if the environment has variables I don't see them.\n2. Created a shell script that would login through the azure service principal, connect to the key vault and export the secrets using export command in bash(This approach works really well in local). I trigger this script through python file it populates the environment but my job which is in a spark session isn't able to get the environment variables.\n\nQuestions\n1.Where I should be assigning the secrets as an environment variables?\n2. What should the ideal flow.\n3. If I am triggering the python script why is the pyspark job not able to get the secrets?\n\nDue to this issue the entire deployment of jobs is kind of stuck.\nPlease help, any kind of discussions or help is greatly appreciated?\n\n#pyspark #airflow #kubernetes #sparkonk8s #secrets", "author_fullname": "t2_389zs7ih", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "But it works in my local\ud83d\ude29.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xvk5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675951526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Situation: I have a pyspark job that needs to be deployed on a cluster that has SparkonK8s support. A pipeline is in place that builds an image and pushes an image to azure container registry and now there is Airflow job that refers to that image having a DAG that runs this pyspark job periodically. \nI have secrets that are being utilised by this job that need to be populated to the environment as environment varibales. These secrets are placed in azure ado library as a variable group connected to a keyvault and have been authorised for pipeline usage.&lt;/p&gt;\n\n&lt;p&gt;Problem: Now I have an issue populating the environment variables with these secrets in the environment variables.&lt;/p&gt;\n\n&lt;p&gt;My approach:\n1.  I have tried populating the secrets through the pipeline itself using Bash @3 task/Powershell@2 task. I try to print them in the pipeline and they are successfully printed as well. But when i run kubectl apply to my spark config file to check if the environment has variables I don&amp;#39;t see them.\n2. Created a shell script that would login through the azure service principal, connect to the key vault and export the secrets using export command in bash(This approach works really well in local). I trigger this script through python file it populates the environment but my job which is in a spark session isn&amp;#39;t able to get the environment variables.&lt;/p&gt;\n\n&lt;p&gt;Questions\n1.Where I should be assigning the secrets as an environment variables?\n2. What should the ideal flow.\n3. If I am triggering the python script why is the pyspark job not able to get the secrets?&lt;/p&gt;\n\n&lt;p&gt;Due to this issue the entire deployment of jobs is kind of stuck.\nPlease help, any kind of discussions or help is greatly appreciated?&lt;/p&gt;\n\n&lt;h1&gt;pyspark #airflow #kubernetes #sparkonk8s #secrets&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xvk5g", "is_robot_indexable": true, "report_reasons": null, "author": "shrey2204", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xvk5g/but_it_works_in_my_local/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xvk5g/but_it_works_in_my_local/", "subreddit_subscribers": 89080, "created_utc": 1675951526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fb83g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Querying 1 Billion Rows of AWS Cost Data 100X Faster with DuckDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "name": "t3_10xw9f0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-0aQIqmXqp2my4VxPbjhZXmebP2OS68ue_K6fPhCpTc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675953372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "vantage.sh", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.vantage.sh/blog/querying-aws-cost-data-duckdb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?auto=webp&amp;v=enabled&amp;s=9f0238778e1ee72ab50629878190a93a3adca59c", "width": 2540, "height": 1520}, "resolutions": [{"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=786e823d297bfd470e7032bb81c80bb9e519ad49", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81134544132584f87ad324085ed7baab9e82d8fb", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=745ede3c1a390215a1a14e700fd51f25995aa7c1", "width": 320, "height": 191}, {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d43866963ec48799e7e8a961fc44ee5942742e25", "width": 640, "height": 382}, {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97c71936c8524fd624420049d7ce1d066da23f0d", "width": 960, "height": 574}, {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=24f013a94ee32dbf47ef52e053864a386807b084", "width": 1080, "height": 646}], "variants": {}, "id": "fFFOps9uNyZKaT6C1uHqRiZJAv21eiUxoFEL-ZkhJHM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10xw9f0", "is_robot_indexable": true, "report_reasons": null, "author": "include_stdio_h", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xw9f0/querying_1_billion_rows_of_aws_cost_data_100x/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.vantage.sh/blog/querying-aws-cost-data-duckdb", "subreddit_subscribers": 89080, "created_utc": 1675953372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have you ever wished you could ask any question to your data and get a fast, contextual answer?\n\nWe have some exciting news for you. Today we're launching a private beta\u00a0for \u2728 Secoda AI: the first AI suite of search, catalog, lineage, and documentation solutions, to enhance your data workspace.\n\nOur new AI Assistant \ud83e\udd16\u00a0will save you hours of manual labor by:\n\n\u2705\u00a0Surfacing insights through natural language search  \n\u2705\u00a0Automatically generating documentation  \n\u2705\u00a0Turning text into SQL\n\nLearn more about Secoda AI and how to get access to the private beta in the comments blow.\n\nTry it out today and see how it can improve your data discovery experience! \n\nLearn more about Secoda AI: [https://www.secoda.co/blog/learn-about-secoda-ai](https://www.secoda.co/blog/learn-about-secoda-ai)", "author_fullname": "t2_aiinah9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing: Secoda AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1eba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675965732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you ever wished you could ask any question to your data and get a fast, contextual answer?&lt;/p&gt;\n\n&lt;p&gt;We have some exciting news for you. Today we&amp;#39;re launching a private beta\u00a0for \u2728 Secoda AI: the first AI suite of search, catalog, lineage, and documentation solutions, to enhance your data workspace.&lt;/p&gt;\n\n&lt;p&gt;Our new AI Assistant \ud83e\udd16\u00a0will save you hours of manual labor by:&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Surfacing insights through natural language search&lt;br/&gt;\n\u2705\u00a0Automatically generating documentation&lt;br/&gt;\n\u2705\u00a0Turning text into SQL&lt;/p&gt;\n\n&lt;p&gt;Learn more about Secoda AI and how to get access to the private beta in the comments blow.&lt;/p&gt;\n\n&lt;p&gt;Try it out today and see how it can improve your data discovery experience! &lt;/p&gt;\n\n&lt;p&gt;Learn more about Secoda AI: &lt;a href=\"https://www.secoda.co/blog/learn-about-secoda-ai\"&gt;https://www.secoda.co/blog/learn-about-secoda-ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?auto=webp&amp;v=enabled&amp;s=8c5b8cb57e5526d06edcfa692d557d8de635a1c7", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f609b1dff87c4361f4eb611b4e0516ee8be17d67", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab583607a5d69a1fb66ed01b404ad79fd1cb4ab8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93e1fe995115293cf4c58f9f966aae2e1a97dbf1", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b68e67e7d00848f42033d8553d997ae83372e1e6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e6fd0043d6d5b3a3c2a7a189aaee5cd2c9291a81", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc0ff6d857747ec0e900d773fbb565ce45012064", "width": 1080, "height": 607}], "variants": {}, "id": "7WY8fl_h7RsynzfO-QX-YTUd7xXR4M4ad5IO3mvCPLI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10y1eba", "is_robot_indexable": true, "report_reasons": null, "author": "secodaHQ", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y1eba/introducing_secoda_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10y1eba/introducing_secoda_ai/", "subreddit_subscribers": 89080, "created_utc": 1675965732.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}