{"kind": "Listing", "data": {"after": "t3_10y8ha8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nsorry if this is not in the scope of this sub, I looked at the rules and found none against this. \n\nBertelsmann in Germany, which owns both the Gruner + Jahr publishing house and the RTL network of stations recently announced that they are shutting down many of their print titles. Among them is GEO Epoche, a German magazine specializing in history that has been around since 1999 and has an excellent reputation among readers; it's well researched, well written and has generally a very high standard for a \"mainstream\" magazine catering to history buffs.\n\nGEO Epoche, along with a few other titles are spin-offs of the main GEO magazine, which Gruner + Jahr wants to keep. They said they are shutting down all spin-off/adjacent titles, because if they sold them, they basically have no way of \"keeping a unified brand image\", which is an incredibly stupid reason if you ask me, but noting can be done about that.\n\nThere is a complementary online service, \"GEO Epoche Plus\", which for \u20ac4.99 a month offers access to the full archive (safe the two most recent editions) dating back to 1999 through both their website and an app. All the individual articles from all issues are available, plus there is an e-reader version that has basically scans of each magazine page that can be thumbed through. There is a free 30 day trial available, which I just.\n\nThere are 119 regular issues, 26 specials dealing with art history, 23 issues with mostly photographs and short texts, and 30 issues that are basically best-of collections of previous articles about a certain topic.\n\nI am currently looking at the best way to archive all of this content before it is gone. My best bet probably is using some sort of wget/curl script to crawl the page, download everything, and figure out how to structure it afterwards?\n\nIf anybody (especially if you speak German and have an interest in history) wants to take a look, the site is at [https://www.geo-epoche.de/](https://www.geo-epoche.de/), even though you probably need the subscription to get a look at the whole archive.", "author_fullname": "t2_w8wvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archiving \"GEO Epoche\" before it closes down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xsyr1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 132, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 132, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675943790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;sorry if this is not in the scope of this sub, I looked at the rules and found none against this. &lt;/p&gt;\n\n&lt;p&gt;Bertelsmann in Germany, which owns both the Gruner + Jahr publishing house and the RTL network of stations recently announced that they are shutting down many of their print titles. Among them is GEO Epoche, a German magazine specializing in history that has been around since 1999 and has an excellent reputation among readers; it&amp;#39;s well researched, well written and has generally a very high standard for a &amp;quot;mainstream&amp;quot; magazine catering to history buffs.&lt;/p&gt;\n\n&lt;p&gt;GEO Epoche, along with a few other titles are spin-offs of the main GEO magazine, which Gruner + Jahr wants to keep. They said they are shutting down all spin-off/adjacent titles, because if they sold them, they basically have no way of &amp;quot;keeping a unified brand image&amp;quot;, which is an incredibly stupid reason if you ask me, but noting can be done about that.&lt;/p&gt;\n\n&lt;p&gt;There is a complementary online service, &amp;quot;GEO Epoche Plus&amp;quot;, which for \u20ac4.99 a month offers access to the full archive (safe the two most recent editions) dating back to 1999 through both their website and an app. All the individual articles from all issues are available, plus there is an e-reader version that has basically scans of each magazine page that can be thumbed through. There is a free 30 day trial available, which I just.&lt;/p&gt;\n\n&lt;p&gt;There are 119 regular issues, 26 specials dealing with art history, 23 issues with mostly photographs and short texts, and 30 issues that are basically best-of collections of previous articles about a certain topic.&lt;/p&gt;\n\n&lt;p&gt;I am currently looking at the best way to archive all of this content before it is gone. My best bet probably is using some sort of wget/curl script to crawl the page, download everything, and figure out how to structure it afterwards?&lt;/p&gt;\n\n&lt;p&gt;If anybody (especially if you speak German and have an interest in history) wants to take a look, the site is at &lt;a href=\"https://www.geo-epoche.de/\"&gt;https://www.geo-epoche.de/&lt;/a&gt;, even though you probably need the subscription to get a look at the whole archive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10xsyr1", "is_robot_indexable": true, "report_reasons": null, "author": "-Darkguy-", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10xsyr1/archiving_geo_epoche_before_it_closes_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10xsyr1/archiving_geo_epoche_before_it_closes_down/", "subreddit_subscribers": 669281, "created_utc": 1675943790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Western Digital has 2x 14TB Red Plus drives for $420, must buy 2. Solid deal for actual Red Plus drives with 3 year warranty instead of shucking whites from Easystores\n\nhttps://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd#WD140EFGX", "author_fullname": "t2_4kjc5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2x WD Red Plus 14TB for $420 ($15/tb) - Direct from WD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y5106", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 126, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 126, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1675974257.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675973968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Western Digital has 2x 14TB Red Plus drives for $420, must buy 2. Solid deal for actual Red Plus drives with 3 year warranty instead of shucking whites from Easystores&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd#WD140EFGX\"&gt;https://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd#WD140EFGX&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?auto=webp&amp;v=enabled&amp;s=935ae08de307a340e006f3282858aab1b61d23e6", "width": 1680, "height": 1680}, "resolutions": [{"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b240e0224391a3132feb0e9600dfbd4d6dc72c94", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76ed630b7b2452ac8f4bea418cb80b31b56a2bdf", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ece20b5a0a3a466228d46c1f7127da11367f4bbd", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e1e1b91d58e5a475fdeec3614747050965837b2", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e218935112ef9780f443a6c2a0a9da6a1d7a63bc", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/_3F-YYvLZE5X1OJehuGjdMyQt7OlYdY1ovxErXFBkAI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f606446b08b91e2affa4f7ae489797ae018af4d6", "width": 1080, "height": 1080}], "variants": {}, "id": "RzQmR5PjZ5asfOMDi4yuzhpBG_CJQYRO8YaVhzUtLUA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "240TB + 28TB Parity", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10y5106", "is_robot_indexable": true, "report_reasons": null, "author": "r34p3rex", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10y5106/2x_wd_red_plus_14tb_for_420_15tb_direct_from_wd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y5106/2x_wd_red_plus_14tb_for_420_15tb_direct_from_wd/", "subreddit_subscribers": 669281, "created_utc": 1675973968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_p12csd2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Here's a recent video on how to digitize VHS Tapes using OBS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_10xnso8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 90, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tk-n7IlrXI4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to convert VHS videotape to 60p digital video (2023)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to convert VHS videotape to 60p digital video (2023)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tk-n7IlrXI4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to convert VHS videotape to 60p digital video (2023)\"&gt;&lt;/iframe&gt;", "author_name": "The Oldskool PC", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tk-n7IlrXI4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheOldskoolPC"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tk-n7IlrXI4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to convert VHS videotape to 60p digital video (2023)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/10xnso8", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 90, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pwZMO04MfpL3gFrqMwA4bkvPjvS_g2_M9BNGt5zR-Ic.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675924764.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtube.com/watch?v=tk-n7IlrXI4&amp;feature=share", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/owshpG4G9cycG-VzEtF7JGb8rIOuiaxTUk90rW3jHMg.jpg?auto=webp&amp;v=enabled&amp;s=b5231a8df6f6f4e12483f3a6fb73a22ca3173cb6", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/owshpG4G9cycG-VzEtF7JGb8rIOuiaxTUk90rW3jHMg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffb3b53620e90faf8b9adaaad01432552cbb3cd4", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/owshpG4G9cycG-VzEtF7JGb8rIOuiaxTUk90rW3jHMg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a4e3043db615392e95bef687e7b4d6c3cb845d7", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/owshpG4G9cycG-VzEtF7JGb8rIOuiaxTUk90rW3jHMg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f74b4a5b4b2c4239faae885c026733e3d598200c", "width": 320, "height": 240}], "variants": {}, "id": "TNw3I0ufneO6qTbuzfxsAwXvakKOQwPKYRAirzVNYxc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10xnso8", "is_robot_indexable": true, "report_reasons": null, "author": "boisosm", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10xnso8/heres_a_recent_video_on_how_to_digitize_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtube.com/watch?v=tk-n7IlrXI4&amp;feature=share", "subreddit_subscribers": 669281, "created_utc": 1675924764.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to convert VHS videotape to 60p digital video (2023)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tk-n7IlrXI4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to convert VHS videotape to 60p digital video (2023)\"&gt;&lt;/iframe&gt;", "author_name": "The Oldskool PC", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tk-n7IlrXI4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheOldskoolPC"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks,\n\nMy parents have hundreds of 35mm negative strips sitting in a box, most of which they printed and laid out in their photo albums over the years, but I offered to scan them all so that we'd have a digital copy.\n\nI already have a setup at home with an Espon v600, which while not the best scanner in the world, does the job and I'm happy with the quality that it gives for my own film photography. The software I use is VueScan.\n\n&amp;#x200B;\n\nHave any of yous done such a large scanning and backup job? Any advice you could spare?", "author_fullname": "t2_3o4wxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "About to scan/archive hundreds of 35mm negatives (family photos). How do you organise and archive your 35mm scans?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xrrvd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675939523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;My parents have hundreds of 35mm negative strips sitting in a box, most of which they printed and laid out in their photo albums over the years, but I offered to scan them all so that we&amp;#39;d have a digital copy.&lt;/p&gt;\n\n&lt;p&gt;I already have a setup at home with an Espon v600, which while not the best scanner in the world, does the job and I&amp;#39;m happy with the quality that it gives for my own film photography. The software I use is VueScan.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Have any of yous done such a large scanning and backup job? Any advice you could spare?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10xrrvd", "is_robot_indexable": true, "report_reasons": null, "author": "HipstCapitalist", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10xrrvd/about_to_scanarchive_hundreds_of_35mm_negatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10xrrvd/about_to_scanarchive_hundreds_of_35mm_negatives/", "subreddit_subscribers": 669281, "created_utc": 1675939523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "imagine -- \n\nyou have a beautiful, curated media library of every TV show and film in the world. every one! tagged, posters, cast, subtitles, 4K, 5.1, all of it. \n\nany of these media files can be transmitted almost instantly to any computer in the world at little to no cost. it is a miracle of modern technology. \n\n~~~\n\nand look where we are now.\n\nif someone's in atlanta instead of charlotte, for 32 days instead of 28, they get a 'fuck you' notice if they try to watch a film. \n\nbeing able to preserve the media i love, a buffer against these sick fucking bean counting fucks, is pure joy and love. thank you all.", "author_fullname": "t2_bazgovvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Philosophically, the netflix situation is making me sick", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y9wcy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675985507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;imagine -- &lt;/p&gt;\n\n&lt;p&gt;you have a beautiful, curated media library of every TV show and film in the world. every one! tagged, posters, cast, subtitles, 4K, 5.1, all of it. &lt;/p&gt;\n\n&lt;p&gt;any of these media files can be transmitted almost instantly to any computer in the world at little to no cost. it is a miracle of modern technology. &lt;/p&gt;\n\n&lt;p&gt;~~~&lt;/p&gt;\n\n&lt;p&gt;and look where we are now.&lt;/p&gt;\n\n&lt;p&gt;if someone&amp;#39;s in atlanta instead of charlotte, for 32 days instead of 28, they get a &amp;#39;fuck you&amp;#39; notice if they try to watch a film. &lt;/p&gt;\n\n&lt;p&gt;being able to preserve the media i love, a buffer against these sick fucking bean counting fucks, is pure joy and love. thank you all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10y9wcy", "is_robot_indexable": true, "report_reasons": null, "author": "spacewalk__", "discussion_type": null, "num_comments": 43, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y9wcy/philosophically_the_netflix_situation_is_making/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y9wcy/philosophically_the_netflix_situation_is_making/", "subreddit_subscribers": 669281, "created_utc": 1675985507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As per title. Am getting a new ssd soon and am wondering what I can do to detect errors asap.", "author_fullname": "t2_ahrudp61", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a guide to checking ssd health on purchase?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xu5ri", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675947600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per title. Am getting a new ssd soon and am wondering what I can do to detect errors asap.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10xu5ri", "is_robot_indexable": true, "report_reasons": null, "author": "Mundane_Grab_8727", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10xu5ri/is_there_a_guide_to_checking_ssd_health_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10xu5ri/is_there_a_guide_to_checking_ssd_health_on/", "subreddit_subscribers": 669281, "created_utc": 1675947600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI used to work for a media company that is now closed and that abandoned its digital properties. Luckily, its Facebook page is still online, including all the contents the we created while working there. \n\nI'm not an admin of the page anymore. Is there any way to bulk download all the public photos and videos from that page without being its admin? \n\nThanks for any help!", "author_fullname": "t2_4uip6l1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I download all the public content from a Facebook page?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xsfwp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675941903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I used to work for a media company that is now closed and that abandoned its digital properties. Luckily, its Facebook page is still online, including all the contents the we created while working there. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not an admin of the page anymore. Is there any way to bulk download all the public photos and videos from that page without being its admin? &lt;/p&gt;\n\n&lt;p&gt;Thanks for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10xsfwp", "is_robot_indexable": true, "report_reasons": null, "author": "jawheeler", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10xsfwp/how_can_i_download_all_the_public_content_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10xsfwp/how_can_i_download_all_the_public_content_from_a/", "subreddit_subscribers": 669281, "created_utc": 1675941903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**Picture of the docking station:** [Maiwo K3082H USB 3.1 GEN1 5Gbps dockingstation](https://gyazo.com/6731a1b46ccb490a4f547646caae2c23)  \n\n\n1) Does having the docking station **powered/turned on** but not plugged in into a pc healthy for the Hard drives? (And does it prevent them from data rotting)  \n\n\n2) Is it okay to have an **external harddrive** plugged to the docking station instead of the pc?  \n\n\n3) Do they safely disconnect when I **Shut Down** my Pc?  \n\n\nI tried googling quest 1 and 2 but couldnt find precise answers, hopefully I can get some quick answers here.", "author_fullname": "t2_176zrt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I bought a 2 Slot 2.5/3.5 Docking Station and I have a few questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xvkgk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675951548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Picture of the docking station:&lt;/strong&gt; &lt;a href=\"https://gyazo.com/6731a1b46ccb490a4f547646caae2c23\"&gt;Maiwo K3082H USB 3.1 GEN1 5Gbps dockingstation&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;1) Does having the docking station &lt;strong&gt;powered/turned on&lt;/strong&gt; but not plugged in into a pc healthy for the Hard drives? (And does it prevent them from data rotting)  &lt;/p&gt;\n\n&lt;p&gt;2) Is it okay to have an &lt;strong&gt;external harddrive&lt;/strong&gt; plugged to the docking station instead of the pc?  &lt;/p&gt;\n\n&lt;p&gt;3) Do they safely disconnect when I &lt;strong&gt;Shut Down&lt;/strong&gt; my Pc?  &lt;/p&gt;\n\n&lt;p&gt;I tried googling quest 1 and 2 but couldnt find precise answers, hopefully I can get some quick answers here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nMN4mS4tjZ8eAKTJ87LkemWxAvuSf5pSszBz5y2Wcp4.jpg?auto=webp&amp;v=enabled&amp;s=4bbc1f90074ca85bf6f90deab7962d783567e1da", "width": 531, "height": 534}, "resolutions": [{"url": "https://external-preview.redd.it/nMN4mS4tjZ8eAKTJ87LkemWxAvuSf5pSszBz5y2Wcp4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=574945032d890160f4761b1e1582f4980d0fba51", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/nMN4mS4tjZ8eAKTJ87LkemWxAvuSf5pSszBz5y2Wcp4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a028dcbd102c1ad38a27326b31eb13d16016c04", "width": 216, "height": 217}, {"url": "https://external-preview.redd.it/nMN4mS4tjZ8eAKTJ87LkemWxAvuSf5pSszBz5y2Wcp4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0bfaa7d06ce572cfc21971449d5a528d017ca6bd", "width": 320, "height": 321}], "variants": {}, "id": "Hkoyv9ZiRGjUDVO6Bly39U4k4gKadnClI7QHd6ZTCR0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10xvkgk", "is_robot_indexable": true, "report_reasons": null, "author": "Mikkeru", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10xvkgk/i_bought_a_2_slot_2535_docking_station_and_i_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10xvkgk/i_bought_a_2_slot_2535_docking_station_and_i_have/", "subreddit_subscribers": 669281, "created_utc": 1675951548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a Synology NAS right now with 5x4TB drives using SHR, I'm looking to increase the storage capacity and I thought I heard off hand that larger sizes (16-20 etc.) are not ideal?  For context, my usage is mostly as network storage for Plex media (server is on a VM elsewhere) and various backups.", "author_fullname": "t2_ff0wt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there such a thing as too big? (NAS Hard drives)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y39fx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675969944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Synology NAS right now with 5x4TB drives using SHR, I&amp;#39;m looking to increase the storage capacity and I thought I heard off hand that larger sizes (16-20 etc.) are not ideal?  For context, my usage is mostly as network storage for Plex media (server is on a VM elsewhere) and various backups.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y39fx", "is_robot_indexable": true, "report_reasons": null, "author": "landsverka", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y39fx/is_there_such_a_thing_as_too_big_nas_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y39fx/is_there_such_a_thing_as_too_big_nas_hard_drives/", "subreddit_subscribers": 669281, "created_utc": 1675969944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The deets:  \nLong time lurker, beginner hoarder here. I currently have a PC case that can handle a max of 11x 3.5 drives, but right now I need a PC more than I need a server. Even so, I still want to start hoarding data immediately. My current plan is just to stick the HDD's as regular drives to a PC, and probably get an LSI to expand the SATA slots on the MOBO. Is there any serious downside to storing the data in a regular PC vs a 24/7 server? Will it be difficult to migrate to a dedicated server when I have more money? My thanks to the kind strangers who will reply to this post.\n\nTLDR:   \nI'm poor and can't afford a dedicated server right now. What's the downside to hoarding on a PC?", "author_fullname": "t2_12q0qi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I use my PC to hoard data instead of using a server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ybnkr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675990164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The deets:&lt;br/&gt;\nLong time lurker, beginner hoarder here. I currently have a PC case that can handle a max of 11x 3.5 drives, but right now I need a PC more than I need a server. Even so, I still want to start hoarding data immediately. My current plan is just to stick the HDD&amp;#39;s as regular drives to a PC, and probably get an LSI to expand the SATA slots on the MOBO. Is there any serious downside to storing the data in a regular PC vs a 24/7 server? Will it be difficult to migrate to a dedicated server when I have more money? My thanks to the kind strangers who will reply to this post.&lt;/p&gt;\n\n&lt;p&gt;TLDR:&lt;br/&gt;\nI&amp;#39;m poor and can&amp;#39;t afford a dedicated server right now. What&amp;#39;s the downside to hoarding on a PC?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ybnkr", "is_robot_indexable": true, "report_reasons": null, "author": "faplesspotato", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ybnkr/can_i_use_my_pc_to_hoard_data_instead_of_using_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ybnkr/can_i_use_my_pc_to_hoard_data_instead_of_using_a/", "subreddit_subscribers": 669281, "created_utc": 1675990164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_rrt44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a reason I'm missing that these are as cheap as they are? Am new to this.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y9qos", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1675985127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "amazon.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.amazon.com/AV-GP-Intellipower-Cache-Drive-WD20EURX/dp/B00DXOJJQQ/ref=sr_1_4?crid=QBMUPKAVIEG4&amp;keywords=2tb+server+hard+drive&amp;qid=1675984717&amp;sprefix=2tb+server+hard+drive%2Caps%2C98&amp;sr=8-4#customerReviews", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y9qos", "is_robot_indexable": true, "report_reasons": null, "author": "alvogel122", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y9qos/is_there_a_reason_im_missing_that_these_are_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.amazon.com/AV-GP-Intellipower-Cache-Drive-WD20EURX/dp/B00DXOJJQQ/ref=sr_1_4?crid=QBMUPKAVIEG4&amp;keywords=2tb+server+hard+drive&amp;qid=1675984717&amp;sprefix=2tb+server+hard+drive%2Caps%2C98&amp;sr=8-4#customerReviews", "subreddit_subscribers": 669281, "created_utc": 1675985127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey I am looking for way to mass retrieve the url of Tiktok I saved in specifics collections. I don't know to download them I just need their url. I tried to get the famous JSON file from my profile but the file is mainly useless.", "author_fullname": "t2_opo9usc7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get the url of all my collections videos from tiktok", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y6obg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675977904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey I am looking for way to mass retrieve the url of Tiktok I saved in specifics collections. I don&amp;#39;t know to download them I just need their url. I tried to get the famous JSON file from my profile but the file is mainly useless.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y6obg", "is_robot_indexable": true, "report_reasons": null, "author": "Jaydayaim", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y6obg/get_the_url_of_all_my_collections_videos_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y6obg/get_the_url_of_all_my_collections_videos_from/", "subreddit_subscribers": 669281, "created_utc": 1675977904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for a way to download the text from a post.  \nthe `--write-metadata` option downloads everything.  \n\n\nIn the configuration file, I tried:  \n\n\n`{`\n\n`\"extractor\": {`\n\n`\"twitter\": {`\n\n`\"text-tweets\":true,`\n\n`\"quoted\":true,`\n\n`\"retweets\":true,`\n\n`\"postprocessor\": [`\n\n`{`\n\n`\"command\": \"echo {content} &gt; {filename}.txt\"`\n\n`}`\n\n`],`\n\n`\"directory\": {`\n\n`\"\": [\"{author[name]}\"]`\n\n`},`\n\n`\"filename\": {`\n\n`\"\": \"{author[name]}/{tweet_id}_{num}\"`\n\n`}`\n\n`}`\n\n`}`\n\n`}`\n\n  \nI would like text only tweet to be downloaded as .txt and the text of images posts be downloaded too.   \nSo if an image is downloaded another file with same name+.txt would contains the tweet content", "author_fullname": "t2_3zmmj0so", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gallery-dl download texts only", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ya1l0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675985873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a way to download the text from a post.&lt;br/&gt;\nthe &lt;code&gt;--write-metadata&lt;/code&gt; option downloads everything.  &lt;/p&gt;\n\n&lt;p&gt;In the configuration file, I tried:  &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;extractor&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;twitter&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;text-tweets&amp;quot;:true,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;quoted&amp;quot;:true,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;retweets&amp;quot;:true,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;postprocessor&amp;quot;: [&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;command&amp;quot;: &amp;quot;echo {content} &amp;gt; {filename}.txt&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;],&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;directory&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;&amp;quot;: [&amp;quot;{author[name]}&amp;quot;]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;},&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;filename&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;&amp;quot;: &amp;quot;{author[name]}/{tweet_id}_{num}&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I would like text only tweet to be downloaded as .txt and the text of images posts be downloaded too.&lt;br/&gt;\nSo if an image is downloaded another file with same name+.txt would contains the tweet content&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ya1l0", "is_robot_indexable": true, "report_reasons": null, "author": "Tyranoc4", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ya1l0/gallerydl_download_texts_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ya1l0/gallerydl_download_texts_only/", "subreddit_subscribers": 669281, "created_utc": 1675985873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, \nI frequent several art-focused subreddits and would love to archive the images that are posted there. They are typically posted as image posts with the artist and name of the art in the post title and the image is hosted on reddit, artstation, imgur, etc. So ideally a downloader would pull both the image and the post title so the name of the art is preserved. \n\nIs there a good downloader that would allow me to download posts from these subreddits? I found a tool called RedditDownloader on GitHub but I'm not sure how good it is or how easy it is to use. Would I be able to automate it so it runs every month and downloads all the posts since the last time it ran?  What sort of computer would I need to run this? (right now I have a business-grade laptop that's a few years old, but I'm planning to build a new pc that would be primarily focused on gaming, but would also have the storage space to do these kinds of things.) \n\nSorry if these questions are common or easy to answer, I'm very knew to this kind of thing.", "author_fullname": "t2_5mr4b04t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to download posts from art-focused subreddits?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1o8w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675966368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, \nI frequent several art-focused subreddits and would love to archive the images that are posted there. They are typically posted as image posts with the artist and name of the art in the post title and the image is hosted on reddit, artstation, imgur, etc. So ideally a downloader would pull both the image and the post title so the name of the art is preserved. &lt;/p&gt;\n\n&lt;p&gt;Is there a good downloader that would allow me to download posts from these subreddits? I found a tool called RedditDownloader on GitHub but I&amp;#39;m not sure how good it is or how easy it is to use. Would I be able to automate it so it runs every month and downloads all the posts since the last time it ran?  What sort of computer would I need to run this? (right now I have a business-grade laptop that&amp;#39;s a few years old, but I&amp;#39;m planning to build a new pc that would be primarily focused on gaming, but would also have the storage space to do these kinds of things.) &lt;/p&gt;\n\n&lt;p&gt;Sorry if these questions are common or easy to answer, I&amp;#39;m very knew to this kind of thing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y1o8w", "is_robot_indexable": true, "report_reasons": null, "author": "Lastdudealive46", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y1o8w/best_way_to_download_posts_from_artfocused/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y1o8w/best_way_to_download_posts_from_artfocused/", "subreddit_subscribers": 669281, "created_utc": 1675966368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Wanting to record streaming content via ROKU to DVD or HDD. What's the most cost effective way to do this? Thanks", "author_fullname": "t2_b0p0a1f2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most cost effective way to record streaming content to DVD or HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1ic9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675965987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wanting to record streaming content via ROKU to DVD or HDD. What&amp;#39;s the most cost effective way to do this? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10y1ic9", "is_robot_indexable": true, "report_reasons": null, "author": "galaxy18r", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y1ic9/most_cost_effective_way_to_record_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y1ic9/most_cost_effective_way_to_record_streaming/", "subreddit_subscribers": 669281, "created_utc": 1675965987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "what i am trying to do? host a static website of a dynamic website. \n\nthere are some old websites written in php and mysql backend. about 1500 pages. the site is pretty heavy because of sql queries and stuff.\n\n\nhow do i take a snapshot of the website that i can host? with all links intact. i tried archivebox but it is not self hostable for the public without archivebox interface coming in between.", "author_fullname": "t2_v1kgt9f8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to take a host-able snapshot?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1bcp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675965572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;what i am trying to do? host a static website of a dynamic website. &lt;/p&gt;\n\n&lt;p&gt;there are some old websites written in php and mysql backend. about 1500 pages. the site is pretty heavy because of sql queries and stuff.&lt;/p&gt;\n\n&lt;p&gt;how do i take a snapshot of the website that i can host? with all links intact. i tried archivebox but it is not self hostable for the public without archivebox interface coming in between.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y1bcp", "is_robot_indexable": true, "report_reasons": null, "author": "noodleswind", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y1bcp/how_to_take_a_hostable_snapshot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y1bcp/how_to_take_a_hostable_snapshot/", "subreddit_subscribers": 669281, "created_utc": 1675965572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all,\n\nBeen searching the internet the last couple of days to see if I can get my HDDs to keep them spinned down for most of the time. \n\nFor some reason every two minutes system does a volume check on the HDDs keeping them awake. Also tried switching to Ubuntu 22.04 but I couldn't find a good replacement for StableBit Drivepools SSD optimizer. So I'm back to trying to fix my Windows.\n\nMy main reason to keep the drives spun down is the cost for the energy. Keeping the drives spinning costs around \u20ac120 a year.\n\n[Here is a photo from the process monitor](https://preview.redd.it/qy3b4oz7y5ha1.png?width=1844&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7b0ed3205d484c035351529d9a713ff50c60cec8)\n\nI tried some regedits to storepart(iirc), installed another virus scanner that I disabled to disable Windows Defender. -arr, Plex, BackBlaze, Stablebit Scanner service (and plugin in drivepool), disabled indexing, SysMain service, changed from drive letters to paths and some other things I can't remember.\n\n&amp;#x200B;\n\nAny help would be greatly appreciated.", "author_fullname": "t2_olyhz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows System process keeps drives awake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 67, "top_awarded_type": null, "hide_score": false, "media_metadata": {"qy3b4oz7y5ha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/qy3b4oz7y5ha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=059e462cd69916d57afc371a6f41b7428915d61b"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/qy3b4oz7y5ha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b5e806e486059fdbbf5971513d803d24d5d7900"}, {"y": 154, "x": 320, "u": "https://preview.redd.it/qy3b4oz7y5ha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3731f125111b254497af369dd5e5a787209d6a5b"}, {"y": 308, "x": 640, "u": "https://preview.redd.it/qy3b4oz7y5ha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9607e3b63f2c34e282ce663b94d83e65255aec0d"}, {"y": 462, "x": 960, "u": "https://preview.redd.it/qy3b4oz7y5ha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a28d0b1a4e4009422e35e980727b0bf027d99146"}, {"y": 520, "x": 1080, "u": "https://preview.redd.it/qy3b4oz7y5ha1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96ef05e6b5442c7a19aefad503ad33f86f25398b"}], "s": {"y": 889, "x": 1844, "u": "https://preview.redd.it/qy3b4oz7y5ha1.png?width=1844&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7b0ed3205d484c035351529d9a713ff50c60cec8"}, "id": "qy3b4oz7y5ha1"}}, "name": "t3_10xup4k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/e3AD4Fy9VOCb7EVCJ2jPCUPS-JI_rDTsfSVngT64EyY.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675949144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;Been searching the internet the last couple of days to see if I can get my HDDs to keep them spinned down for most of the time. &lt;/p&gt;\n\n&lt;p&gt;For some reason every two minutes system does a volume check on the HDDs keeping them awake. Also tried switching to Ubuntu 22.04 but I couldn&amp;#39;t find a good replacement for StableBit Drivepools SSD optimizer. So I&amp;#39;m back to trying to fix my Windows.&lt;/p&gt;\n\n&lt;p&gt;My main reason to keep the drives spun down is the cost for the energy. Keeping the drives spinning costs around \u20ac120 a year.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qy3b4oz7y5ha1.png?width=1844&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7b0ed3205d484c035351529d9a713ff50c60cec8\"&gt;Here is a photo from the process monitor&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I tried some regedits to storepart(iirc), installed another virus scanner that I disabled to disable Windows Defender. -arr, Plex, BackBlaze, Stablebit Scanner service (and plugin in drivepool), disabled indexing, SysMain service, changed from drive letters to paths and some other things I can&amp;#39;t remember.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "60TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10xup4k", "is_robot_indexable": true, "report_reasons": null, "author": "vanthome", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/10xup4k/windows_system_process_keeps_drives_awake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10xup4k/windows_system_process_keeps_drives_awake/", "subreddit_subscribers": 669281, "created_utc": 1675949144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI\u2019m wanting to back up a huge library of blu ray movies. I have a lot of rather rare Asian movies and also horror. As an example of the rarity, I can\u2019t find digital versions of even torrents of a lot of them anywhere. This is one of the main reasons I want them backing up.\n\nI\u2019m a massive proponent of physical media but I\u2019m sick or spending money on discs that die without any digital back up.\n\nI know this is going to take up an awful lot of space so my plan is to invest in a Nas drive and then use something such as plex to library and organise everything.\n\nIt\u2019s all a learning curve to me as it stands in regard to details but this is my basic plan.\n\nI have a few questions/concerns.\n\n1. Is there an updated guide/bible of sorts anywhere for the best way to go about this?\n\n2. Are blu ray writers multi region or will they only burn the blu rays locked to the region you purchased the writer in? I have literally various region blu rays which has never been a problem for me in regards to playing them given I have a multi region player.\n\n3. Once everything is backed up to the nas, are there any steps I can take, even a paid service perhaps, to back up my collection again. On a cloud service or something similar? I\u2019d love to know if the drive ever corrupts or something I\u2019d be able to redownload everything. Given we\u2019d be talking TB of data though, I\u2019m not sure this is feasible.\n\nThanks in advance for any assistance.", "author_fullname": "t2_v06kxt9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wanting to back up a large library of blu rays and have some questions. Region locked writers/secondary back up options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xr0st", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675936769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m wanting to back up a huge library of blu ray movies. I have a lot of rather rare Asian movies and also horror. As an example of the rarity, I can\u2019t find digital versions of even torrents of a lot of them anywhere. This is one of the main reasons I want them backing up.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m a massive proponent of physical media but I\u2019m sick or spending money on discs that die without any digital back up.&lt;/p&gt;\n\n&lt;p&gt;I know this is going to take up an awful lot of space so my plan is to invest in a Nas drive and then use something such as plex to library and organise everything.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s all a learning curve to me as it stands in regard to details but this is my basic plan.&lt;/p&gt;\n\n&lt;p&gt;I have a few questions/concerns.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Is there an updated guide/bible of sorts anywhere for the best way to go about this?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are blu ray writers multi region or will they only burn the blu rays locked to the region you purchased the writer in? I have literally various region blu rays which has never been a problem for me in regards to playing them given I have a multi region player.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Once everything is backed up to the nas, are there any steps I can take, even a paid service perhaps, to back up my collection again. On a cloud service or something similar? I\u2019d love to know if the drive ever corrupts or something I\u2019d be able to redownload everything. Given we\u2019d be talking TB of data though, I\u2019m not sure this is feasible.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for any assistance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10xr0st", "is_robot_indexable": true, "report_reasons": null, "author": "Duckyfooty", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10xr0st/wanting_to_back_up_a_large_library_of_blu_rays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10xr0st/wanting_to_back_up_a_large_library_of_blu_rays/", "subreddit_subscribers": 669281, "created_utc": 1675936769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Pardon my ignorance but I was wondering, would hdds be affected by the x-rays scan at the airport safety checks?", "author_fullname": "t2_lp235k2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDDs and x-rays", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xpn80", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675931468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pardon my ignorance but I was wondering, would hdds be affected by the x-rays scan at the airport safety checks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10xpn80", "is_robot_indexable": true, "report_reasons": null, "author": "flexobaff", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10xpn80/hdds_and_xrays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10xpn80/hdds_and_xrays/", "subreddit_subscribers": 669281, "created_utc": 1675931468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Have your own local copy of all the text on the internet - if you have 500TB to spare!\n\nhttps://commoncrawl.org/connect/blog/\n\nThere's also the 750GB [C4 dataset](https://huggingface.co/datasets/c4), which is a heavily filtered subset containing less text - but more interesting text.", "author_fullname": "t2_cd9nt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anybody downloaded the Common Crawl Dataset?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yegzm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675998005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have your own local copy of all the text on the internet - if you have 500TB to spare!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://commoncrawl.org/connect/blog/\"&gt;https://commoncrawl.org/connect/blog/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also the 750GB &lt;a href=\"https://huggingface.co/datasets/c4\"&gt;C4 dataset&lt;/a&gt;, which is a heavily filtered subset containing less text - but more interesting text.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10yegzm", "is_robot_indexable": true, "report_reasons": null, "author": "currentscurrents", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yegzm/has_anybody_downloaded_the_common_crawl_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yegzm/has_anybody_downloaded_the_common_crawl_dataset/", "subreddit_subscribers": 669281, "created_utc": 1675998005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So my mother (love her to death) has captured every moment since her children's birth with digital cameras, and eventually, the iPhone, and keeps a 2TB iCloud storage option to store all said family memories. We keep another terabyte or so on physical images, and I am working on digitizing and storing on ours family RAID 5 server with total 6 TB capacity.\n\nI was sitting there today loading on 2TB of high school football game recordings when I thought about the future of our families data. When my siblings are out of the house, and we have families and there are grandchildren, family gatherings, more data, wanting to look through old data, etc, etc, I had a data existential crisis.\n\nI am the techie in the family, so it is going to be my responsibility to store all of our data long term, and I ideally don't want to keep expanding my server, because then I'll make it an excuse to finally buy a Storinator, but I digress.\n\n**TL;DR: how the hell should I responsibly, securely, and reliably store (and archive) multiple TBs of family memories so they have a long life, and we can look at the memories in the future?** LTO? AWS plan with Snowball? Keep it on spinning? Help!", "author_fullname": "t2_4e5cpd3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 (and growing) terabytes of data!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yeeo4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675998080.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675997810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my mother (love her to death) has captured every moment since her children&amp;#39;s birth with digital cameras, and eventually, the iPhone, and keeps a 2TB iCloud storage option to store all said family memories. We keep another terabyte or so on physical images, and I am working on digitizing and storing on ours family RAID 5 server with total 6 TB capacity.&lt;/p&gt;\n\n&lt;p&gt;I was sitting there today loading on 2TB of high school football game recordings when I thought about the future of our families data. When my siblings are out of the house, and we have families and there are grandchildren, family gatherings, more data, wanting to look through old data, etc, etc, I had a data existential crisis.&lt;/p&gt;\n\n&lt;p&gt;I am the techie in the family, so it is going to be my responsibility to store all of our data long term, and I ideally don&amp;#39;t want to keep expanding my server, because then I&amp;#39;ll make it an excuse to finally buy a Storinator, but I digress.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR: how the hell should I responsibly, securely, and reliably store (and archive) multiple TBs of family memories so they have a long life, and we can look at the memories in the future?&lt;/strong&gt; LTO? AWS plan with Snowball? Keep it on spinning? Help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10yeeo4", "is_robot_indexable": true, "report_reasons": null, "author": "fuuny_doe", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yeeo4/3_and_growing_terabytes_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yeeo4/3_and_growing_terabytes_of_data/", "subreddit_subscribers": 669281, "created_utc": 1675997810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm sure they use something sensible and modern. Comparing nopic and with pics versions suggests about half the file size of the full version is images.\n\nAnyone know where I can read about the image compression/file type used by. Zim files?", "author_fullname": "t2_jr9a5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What image file formats do the .zim Wikipedia Dumps use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ycvw9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675993526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sure they use something sensible and modern. Comparing nopic and with pics versions suggests about half the file size of the full version is images.&lt;/p&gt;\n\n&lt;p&gt;Anyone know where I can read about the image compression/file type used by. Zim files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "10ycvw9", "is_robot_indexable": true, "report_reasons": null, "author": "verrucagnome", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ycvw9/what_image_file_formats_do_the_zim_wikipedia/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ycvw9/what_image_file_formats_do_the_zim_wikipedia/", "subreddit_subscribers": 669281, "created_utc": 1675993526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone, here is my backup situation right now:\n\n1. 2tb main drive\n2. 2tb backup drive\n3. backed up important stuff to iCloud\n\nI just once a month wipe my backup drive and copy the entire main drive to it which is responsible for 90% of my file management times and I\u2019m hoping to cut that down.\n\nOften times it\u2019s just like &lt;1gb of data change and it seems inefficient doing that. \n\nIs there a way to set up my backup drive so it just scans for changes in the main drive and just implements those? On Mac if that impacts the answer\n\nThank you", "author_fullname": "t2_tqnq009f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about updating backup drive more efficiently", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ycgdd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675992333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, here is my backup situation right now:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;2tb main drive&lt;/li&gt;\n&lt;li&gt;2tb backup drive&lt;/li&gt;\n&lt;li&gt;backed up important stuff to iCloud&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I just once a month wipe my backup drive and copy the entire main drive to it which is responsible for 90% of my file management times and I\u2019m hoping to cut that down.&lt;/p&gt;\n\n&lt;p&gt;Often times it\u2019s just like &amp;lt;1gb of data change and it seems inefficient doing that. &lt;/p&gt;\n\n&lt;p&gt;Is there a way to set up my backup drive so it just scans for changes in the main drive and just implements those? On Mac if that impacts the answer&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10ycgdd", "is_robot_indexable": true, "report_reasons": null, "author": "ihadtomakeajoke", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10ycgdd/question_about_updating_backup_drive_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10ycgdd/question_about_updating_backup_drive_more/", "subreddit_subscribers": 669281, "created_utc": 1675992333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI need an external HDD that will meet those requirements:\n\n1. Not less than 10-12tb\n2. Not SMR\n3. Has decent write/read speeds\n4. Will be connected through my Asus AX86U router\\*\\*\n\n&amp;#x200B;\n\nWhat I discovered recently is that if I plug in my existing slow-ass 2tb hdd to my router through usb, I can use my Mac to download movies through QBittorrent straight to that drive that's connected to router and then on my LG C7 tv watch it directly from disk connected to router (it sees it as a audio/video source). I need an external HDD that I will connect to router, keep it connected at all times, download movies straight to it (movies I download are usually 20-50GB in single file - 4k HDR Dolby Atmos rips) and then watch 'em on my TV.\n\nI bought this: [https://www.amazon.com/dp/B08KTQWV7Z?psc=1&amp;ref=ppx\\_yo2ov\\_dt\\_b\\_product\\_details](https://www.amazon.com/dp/B08KTQWV7Z?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details)\n\nand it seemed fine at first, but my downloads to it were fine for the first 100-200megs (download speed around 8MBps) then it would slow down drastically, to around 1-200kBps. I read some reviews and people said that hdd is SMR so I figured that could be the issue.\n\nAlso, would be a plus if I could also set a limited data folder for Time Machine backups of my Mac (that router let's me set up such folder on usb drive).\n\nI guess I could buy something like Synology with two 12tb Ironwolves but my TV won't be able to read movies from that :(\n\nAny suggestions, ideas, tips or advice where else I should do my own research?\n\n&amp;#x200B;\n\nThanks and sorry if this is not the right place to ask/is a dumb question.", "author_fullname": "t2_tmkvdi7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advise choosing external HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yby6m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675990938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I need an external HDD that will meet those requirements:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Not less than 10-12tb&lt;/li&gt;\n&lt;li&gt;Not SMR&lt;/li&gt;\n&lt;li&gt;Has decent write/read speeds&lt;/li&gt;\n&lt;li&gt;Will be connected through my Asus AX86U router**&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What I discovered recently is that if I plug in my existing slow-ass 2tb hdd to my router through usb, I can use my Mac to download movies through QBittorrent straight to that drive that&amp;#39;s connected to router and then on my LG C7 tv watch it directly from disk connected to router (it sees it as a audio/video source). I need an external HDD that I will connect to router, keep it connected at all times, download movies straight to it (movies I download are usually 20-50GB in single file - 4k HDR Dolby Atmos rips) and then watch &amp;#39;em on my TV.&lt;/p&gt;\n\n&lt;p&gt;I bought this: &lt;a href=\"https://www.amazon.com/dp/B08KTQWV7Z?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details\"&gt;https://www.amazon.com/dp/B08KTQWV7Z?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;and it seemed fine at first, but my downloads to it were fine for the first 100-200megs (download speed around 8MBps) then it would slow down drastically, to around 1-200kBps. I read some reviews and people said that hdd is SMR so I figured that could be the issue.&lt;/p&gt;\n\n&lt;p&gt;Also, would be a plus if I could also set a limited data folder for Time Machine backups of my Mac (that router let&amp;#39;s me set up such folder on usb drive).&lt;/p&gt;\n\n&lt;p&gt;I guess I could buy something like Synology with two 12tb Ironwolves but my TV won&amp;#39;t be able to read movies from that :(&lt;/p&gt;\n\n&lt;p&gt;Any suggestions, ideas, tips or advice where else I should do my own research?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks and sorry if this is not the right place to ask/is a dumb question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10yby6m", "is_robot_indexable": true, "report_reasons": null, "author": "CR7KRUL", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10yby6m/need_advise_choosing_external_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10yby6m/need_advise_choosing_external_hdd/", "subreddit_subscribers": 669281, "created_utc": 1675990938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The other day someone on reddit posted a great program that will take large folders of videos and do batch conversions on them to retain quality but save space.  \n\n\nIt wasnt handbrake or ffmpeg.", "author_fullname": "t2_4u6au", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help remembering a Video Converting Program that is designed to convert entire librarys to more efficent codecs (Not Handbrake or FFMPEG)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y8ha8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675982137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The other day someone on reddit posted a great program that will take large folders of videos and do batch conversions on them to retain quality but save space.  &lt;/p&gt;\n\n&lt;p&gt;It wasnt handbrake or ffmpeg.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "10y8ha8", "is_robot_indexable": true, "report_reasons": null, "author": "dec1mus", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/10y8ha8/need_help_remembering_a_video_converting_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/10y8ha8/need_help_remembering_a_video_converting_program/", "subreddit_subscribers": 669281, "created_utc": 1675982137.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}